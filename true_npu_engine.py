"""
çœŸã®NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã§NPUå‡¦ç†ã‚’è¡Œã†åŠ¹ç‡çš„ãªå®Ÿè£…

ä¸»è¦æ©Ÿèƒ½:
- å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã®ONNXå¤‰æ›ã¨NPUå®Ÿè¡Œ
- åŠ¹ç‡çš„ãªNPUè² è·ç‡å‘ä¸Š
- é«˜é€Ÿãªãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
- ç¢ºå®ŸãªDirectMLçµ±åˆ
"""

import os
import time
import numpy as np
import torch
import onnx
import onnxruntime as ort
from typing import Dict, List, Optional, Tuple, Any
import traceback

class TrueNPUEngine:
    """çœŸã®NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model, tokenizer, device_id: int = 0):
        self.model = model
        self.tokenizer = tokenizer
        self.device_id = device_id
        
        # NPUé–¢é€£
        self.npu_session = None
        self.onnx_model_path = None
        self.is_npu_ready = False
        
        # çµ±è¨ˆæƒ…å ±
        self.npu_inference_count = 0
        self.total_npu_time = 0.0
        
        print(f"ğŸš€ çœŸã®NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–")
        print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹ID: {device_id}")
    
    def setup_npu(self) -> bool:
        """NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸ”§ çœŸã®NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
            
            # ONNXå¤‰æ›
            if not self._convert_model_to_onnx():
                print("âŒ ONNXå¤‰æ›å¤±æ•—")
                return False
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self._create_npu_session():
                print("âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—")
                return False
            
            # NPUå‹•ä½œãƒ†ã‚¹ãƒˆ
            if not self._test_npu_inference():
                print("âŒ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå¤±æ•—")
                return False
            
            self.is_npu_ready = True
            print("âœ… çœŸã®NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _convert_model_to_onnx(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›"""
        try:
            print("ğŸ”„ LLMãƒ¢ãƒ‡ãƒ«ONNXå¤‰æ›é–‹å§‹...")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs("./onnx_models", exist_ok=True)
            
            # å‡ºåŠ›ãƒ‘ã‚¹è¨­å®š
            model_name = self.model.config.name_or_path.replace('/', '_').replace('-', '_')
            self.onnx_model_path = f"./onnx_models/{model_name}_true_npu.onnx"
            
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
            if os.path.exists(self.onnx_model_path):
                print(f"ğŸ“ æ—¢å­˜ONNXãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {self.onnx_model_path}")
                return True
            
            # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ä½œæˆ
            sample_text = "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚"
            sample_inputs = self.tokenizer(
                sample_text,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=512
            )
            
            print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›å½¢çŠ¶: {sample_inputs['input_ids'].shape}")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            # å‹•çš„è»¸è¨­å®š
            dynamic_axes = {
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                'logits': {0: 'batch_size', 1: 'sequence'}
            }
            
            print("ğŸ”§ ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            convert_start = time.time()
            
            # ONNXå¤‰æ›å®Ÿè¡Œ
            with torch.no_grad():
                torch.onnx.export(
                    self.model,
                    (sample_inputs['input_ids'], sample_inputs['attention_mask']),
                    self.onnx_model_path,
                    export_params=True,
                    opset_version=11,  # DirectMLäº’æ›
                    do_constant_folding=True,
                    input_names=['input_ids', 'attention_mask'],
                    output_names=['logits'],
                    dynamic_axes=dynamic_axes,
                    verbose=False
                )
            
            convert_time = time.time() - convert_start
            print(f"âœ… ONNXå¤‰æ›å®Œäº† ({convert_time:.1f}ç§’)")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(self.onnx_model_path) / (1024**3)
            print(f"ğŸ“Š ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f}GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _create_npu_session(self) -> bool:
        """NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("ğŸš€ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            if not os.path.exists(self.onnx_model_path):
                print(f"âŒ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.onnx_model_path}")
                return False
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆæœ€é©åŒ–ï¼‰
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': self.device_id,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,
                    'memory_limit_mb': 8192,  # ãƒ¡ãƒ¢ãƒªåˆ¶é™å¢—åŠ 
                    'enable_graph_serialization': True,  # ã‚°ãƒ©ãƒ•ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³æœ‰åŠ¹
                })
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆæœ€é©åŒ–ï¼‰
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = True  # ãƒ¡ãƒ¢ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³æœ‰åŠ¹
            session_options.enable_cpu_mem_arena = False
            session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL  # ä¸¦åˆ—å®Ÿè¡Œ
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.inter_op_num_threads = 4  # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°å¢—åŠ 
            session_options.intra_op_num_threads = 4
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.npu_session = ort.InferenceSession(
                self.onnx_model_path,
                providers=providers,
                sess_options=session_options
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = self.npu_session.get_providers()
            print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                print("âš ï¸ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹")
                return False
            
            # å…¥å‡ºåŠ›æƒ…å ±ç¢ºèª
            input_info = self.npu_session.get_inputs()
            output_info = self.npu_session.get_outputs()
            
            print("âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ“¥ å…¥åŠ›æ•°: {len(input_info)}")
            for i, inp in enumerate(input_info):
                print(f"  {i}: {inp.name} {inp.shape} {inp.type}")
            print(f"ğŸ“¤ å‡ºåŠ›æ•°: {len(output_info)}")
            for i, out in enumerate(output_info):
                print(f"  {i}: {out.name} {out.shape} {out.type}")
            
            return True
            
        except Exception as e:
            print(f"âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _test_npu_inference(self) -> bool:
        """NPUæ¨è«–ãƒ†ã‚¹ãƒˆ"""
        try:
            print("ğŸ§ª NPUæ¨è«–ãƒ†ã‚¹ãƒˆé–‹å§‹...")
            
            # ãƒ†ã‚¹ãƒˆå…¥åŠ›ä½œæˆ
            test_text = "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
            test_inputs = self.tokenizer(
                test_text,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=512
            )
            
            input_ids = test_inputs['input_ids'].numpy().astype(np.int64)
            attention_mask = test_inputs['attention_mask'].numpy().astype(np.int64)
            
            print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆå…¥åŠ›å½¢çŠ¶: input_ids{input_ids.shape}, attention_mask{attention_mask.shape}")
            
            # NPUæ¨è«–å®Ÿè¡Œ
            test_start = time.time()
            outputs = self.npu_session.run(
                ['logits'],
                {
                    'input_ids': input_ids,
                    'attention_mask': attention_mask
                }
            )
            test_time = time.time() - test_start
            
            logits = outputs[0]
            print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ: logitså½¢çŠ¶{logits.shape}, å®Ÿè¡Œæ™‚é–“{test_time:.3f}ç§’")
            
            # è¤‡æ•°å›å®Ÿè¡Œã§NPUè² è·ç¢ºèª
            print("ğŸ”¥ NPUè² è·ç¢ºèªãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            for i in range(20):  # 20å›å®Ÿè¡Œ
                start = time.time()
                self.npu_session.run(
                    ['logits'],
                    {
                        'input_ids': input_ids,
                        'attention_mask': attention_mask
                    }
                )
                elapsed = time.time() - start
                
                if i % 5 == 0:
                    print(f"  ğŸ”„ NPUè² è·ãƒ†ã‚¹ãƒˆ {i+1}/20 ({elapsed:.3f}ç§’)")
            
            print("âœ… NPUè² è·ç¢ºèªãƒ†ã‚¹ãƒˆå®Œäº†")
            print("ğŸ¯ ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§NPUä½¿ç”¨ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            
            return True
            
        except Exception as e:
            print(f"âŒ NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def generate_with_npu(self, prompt: str, max_new_tokens: int = 50, 
                         temperature: float = 0.7, top_k: int = 50, 
                         top_p: float = 0.95) -> Dict[str, Any]:
        """NPUã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.is_npu_ready:
            return {"error": "NPUãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        try:
            print(f"ğŸš€ NPUç”Ÿæˆé–‹å§‹: \"{prompt}\"")
            generation_start = time.time()
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            input_ids = inputs['input_ids'].numpy().astype(np.int64)
            attention_mask = inputs['attention_mask'].numpy().astype(np.int64)
            
            print(f"ğŸ“Š å…¥åŠ›å½¢çŠ¶: input_ids{input_ids.shape}")
            
            # ç”Ÿæˆãƒ«ãƒ¼ãƒ—
            generated_tokens = []
            current_input_ids = input_ids.copy()
            current_attention_mask = attention_mask.copy()
            
            for step in range(max_new_tokens):
                # NPUæ¨è«–å®Ÿè¡Œ
                npu_start = time.time()
                outputs = self.npu_session.run(
                    ['logits'],
                    {
                        'input_ids': current_input_ids,
                        'attention_mask': current_attention_mask
                    }
                )
                npu_time = time.time() - npu_start
                
                self.npu_inference_count += 1
                self.total_npu_time += npu_time
                
                logits = outputs[0]
                next_token_logits = logits[0, -1, :]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®logits
                
                # æ¸©åº¦é©ç”¨
                if temperature > 0:
                    next_token_logits = next_token_logits / temperature
                
                # Top-k ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if top_k > 0:
                    indices_to_remove = next_token_logits < np.partition(next_token_logits, -top_k)[-top_k]
                    next_token_logits[indices_to_remove] = -float('inf')
                
                # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨
                exp_logits = np.exp(next_token_logits - np.max(next_token_logits))
                probabilities = exp_logits / np.sum(exp_logits)
                
                # Top-p ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if top_p < 1.0:
                    sorted_indices = np.argsort(probabilities)[::-1]
                    cumulative_probs = np.cumsum(probabilities[sorted_indices])
                    cutoff_index = np.searchsorted(cumulative_probs, top_p)
                    
                    # ä¸Šä½p%ä»¥å¤–ã‚’0ã«
                    filtered_probs = np.zeros_like(probabilities)
                    filtered_probs[sorted_indices[:cutoff_index+1]] = probabilities[sorted_indices[:cutoff_index+1]]
                    probabilities = filtered_probs / np.sum(filtered_probs)
                
                # ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
                next_token_id = np.random.choice(len(probabilities), p=probabilities)
                generated_tokens.append(next_token_id)
                
                # å…¥åŠ›æ›´æ–°ï¼ˆæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ç”¨ï¼‰
                new_input_ids = np.concatenate([current_input_ids, [[next_token_id]]], axis=1)
                new_attention_mask = np.concatenate([current_attention_mask, [[1]]], axis=1)
                
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ¶é™
                if new_input_ids.shape[1] > 512:
                    new_input_ids = new_input_ids[:, -512:]
                    new_attention_mask = new_attention_mask[:, -512:]
                
                current_input_ids = new_input_ids
                current_attention_mask = new_attention_mask
                
                # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
                if next_token_id == self.tokenizer.eos_token_id:
                    print(f"ğŸ”š çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³æ¤œå‡º (ã‚¹ãƒ†ãƒƒãƒ— {step+1})")
                    break
                
                # é€²æ—è¡¨ç¤º
                if (step + 1) % 10 == 0:
                    print(f"  ğŸ”„ ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ— {step+1}/{max_new_tokens} (NPU: {npu_time:.3f}ç§’)")
            
            # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            full_text = prompt + generated_text
            
            generation_time = time.time() - generation_start
            avg_npu_time = self.total_npu_time / self.npu_inference_count if self.npu_inference_count > 0 else 0
            
            print(f"âœ… NPUç”Ÿæˆå®Œäº†: {len(generated_tokens)}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.2f}ç§’")
            print(f"ğŸ“Š å¹³å‡NPUæ¨è«–æ™‚é–“: {avg_npu_time:.3f}ç§’")
            
            return {
                "generated_text": full_text,
                "generation_time": generation_time,
                "input_tokens": len(input_ids[0]),
                "output_tokens": len(generated_tokens),
                "tokens_per_sec": len(generated_tokens) / generation_time,
                "npu_inference_count": len(generated_tokens),
                "total_npu_time": self.total_npu_time,
                "avg_npu_time": avg_npu_time,
                "inference_method": "True NPU"
            }
            
        except Exception as e:
            print(f"âŒ NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "is_npu_ready": self.is_npu_ready,
            "npu_inference_count": self.npu_inference_count,
            "total_npu_time": self.total_npu_time,
            "avg_npu_time": self.total_npu_time / self.npu_inference_count if self.npu_inference_count > 0 else 0,
            "onnx_model_path": self.onnx_model_path,
            "device_id": self.device_id
        }
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.npu_session:
            del self.npu_session
            self.npu_session = None
        
        print("ğŸ§¹ çœŸã®NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

