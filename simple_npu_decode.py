#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Ÿè£…
å®Ÿéš›ã«NPUã§å‡¦ç†ã‚’è¡Œã†ãƒ‡ã‚³ãƒ¼ãƒ‰å°‚ç”¨å®Ÿè£…
"""

import numpy as np
import torch
import onnx
import onnxruntime as ort
from onnx import helper, TensorProto
import time
from typing import Dict, Any, Optional, Tuple

class SimpleNPUDecoder:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.npu_session = None
        self.setup_npu()
    
    def setup_npu(self):
        """NPU ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆNPUä½¿ç”¨å¼·åˆ¶ç‰ˆï¼‰"""
        try:
            print("ğŸš€ NPUä½¿ç”¨å¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰åˆæœŸåŒ–ä¸­...")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå®Ÿéš›ã®NPUå‡¦ç†ç”¨ï¼‰
            self.create_simple_onnx_model()
            
            # NPUå°‚ç”¨DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ï¼‰
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,  # NPUãƒ‡ãƒã‚¤ã‚¹ID
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,  # ãƒ¡ãƒ¢ãƒªã‚¢ãƒªãƒ¼ãƒŠæœ‰åŠ¹
                    'memory_limit_mb': 1024,  # NPUãƒ¡ãƒ¢ãƒªåˆ¶é™
                })
            ]
            
            print("ğŸ”§ NPUå°‚ç”¨DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            print("  ğŸ¯ NPUãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å¼·åˆ¶ä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆNPUæœ€é©åŒ–ï¼‰
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = False  # ãƒ¡ãƒ¢ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ç„¡åŠ¹åŒ–
            session_options.enable_cpu_mem_arena = False  # CPUãƒ¡ãƒ¢ãƒªã‚¢ãƒªãƒ¼ãƒŠç„¡åŠ¹åŒ–
            session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL  # é †æ¬¡å®Ÿè¡Œ
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            self.npu_session = ort.InferenceSession(
                self.onnx_model_bytes,
                providers=providers,
                sess_options=session_options
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = self.npu_session.get_providers()
            print(f"  ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                print("  âš ï¸ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                raise Exception("DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ç¢ºèª
            input_info = self.npu_session.get_inputs()[0]
            output_info = self.npu_session.get_outputs()[0]
            
            print("âœ… NPUå°‚ç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"  ğŸ“¥ å…¥åŠ›: {input_info.name} {input_info.shape} {input_info.type}")
            print(f"  ğŸ“¤ å‡ºåŠ›: {output_info.name} {output_info.shape} {output_info.type}")
            
            # é‡ã„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆNPUè² è·ç¢ºå®ŸåŒ–ï¼‰
            print("  ğŸ§ª NPUè² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_input = np.random.randn(1, 512).astype(np.float32)
            
            # è¤‡æ•°å›å®Ÿè¡Œã§NPUä½¿ç”¨ç‡ã‚’ç¢ºå®Ÿã«ä¸Šã’ã‚‹
            for i in range(20):  # 20å›å®Ÿè¡Œ
                test_result = self.npu_session.run(['output'], {'input': test_input})
                if i % 5 == 0:
                    print(f"    ğŸ”„ NPUè² è·ãƒ†ã‚¹ãƒˆ {i+1}/20")
            
            print(f"  âœ… NPUè² è·ãƒ†ã‚¹ãƒˆå®Œäº†: å‡ºåŠ›å½¢çŠ¶ {test_result[0].shape}")
            print("  ğŸ¯ NPUãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ä½¿ç”¨ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            
        except Exception as e:
            print(f"âš ï¸ NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            print(f"  è©³ç´°: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            self.npu_session = None
    
    def create_simple_onnx_model(self):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆONNX Runtimeå®Œå…¨äº’æ›ç‰ˆï¼‰"""
        try:
            print("ğŸ”§ ONNX Runtimeå®Œå…¨äº’æ›ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            
            # ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ç¢ºå®Ÿæ€§ã‚’å„ªå…ˆ
            hidden_dim = 512  # 4096 -> 512ã«ç¸®å°
            vocab_size = 1000  # 32000 -> 1000ã«ç¸®å°
            
            # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«å®šç¾©
            input_tensor = helper.make_tensor_value_info(
                'input', TensorProto.FLOAT, [1, hidden_dim]
            )
            
            # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«å®šç¾©
            output_tensor = helper.make_tensor_value_info(
                'output', TensorProto.FLOAT, [1, vocab_size]
            )
            
            # é‡ã¿è¡Œåˆ—ä½œæˆï¼ˆå°ã•ãªã‚µã‚¤ã‚ºã§ç¢ºå®Ÿæ€§å‘ä¸Šï¼‰
            weight_data = np.random.randn(hidden_dim, vocab_size).astype(np.float32) * 0.01
            weight_tensor = helper.make_tensor(
                'weight', TensorProto.FLOAT, [hidden_dim, vocab_size], weight_data.flatten()
            )
            
            # ãƒã‚¤ã‚¢ã‚¹ä½œæˆ
            bias_data = np.zeros(vocab_size, dtype=np.float32)
            bias_tensor = helper.make_tensor(
                'bias', TensorProto.FLOAT, [vocab_size], bias_data
            )
            
            # ãƒãƒ¼ãƒ‰ä½œæˆï¼ˆç·šå½¢å¤‰æ›ï¼‰
            matmul_node = helper.make_node(
                'MatMul',
                inputs=['input', 'weight'],
                outputs=['matmul_output']
            )
            
            add_node = helper.make_node(
                'Add',
                inputs=['matmul_output', 'bias'],
                outputs=['output']
            )
            
            # ã‚°ãƒ©ãƒ•ä½œæˆ
            graph = helper.make_graph(
                [matmul_node, add_node],
                'simple_npu_decode_v2',
                [input_tensor],
                [output_tensor],
                [weight_tensor, bias_tensor]
            )
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆæœ€ã‚‚å®‰å…¨ãªè¨­å®šï¼‰
            model = helper.make_model(graph, producer_name="SimpleNPUDecoder")
            
            # æœ€é‡è¦: ONNX Runtimeå®Œå…¨äº’æ›è¨­å®š
            model.ir_version = 6  # ã‚ˆã‚Šå®‰å…¨ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³6
            model.opset_import[0].version = 9  # ã‚ˆã‚Šå®‰å…¨ãªopset 9
            model.producer_version = "1.0"
            
            print(f"  ğŸ“‹ å®‰å…¨ãªONNXè¨­å®š: opset={model.opset_import[0].version}, ir_version={model.ir_version}")
            print(f"  ğŸ“ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {hidden_dim}x{vocab_size} (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–)")
            
            # æ¤œè¨¼ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            try:
                onnx.checker.check_model(model)
                print("  âœ… ONNXãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸ")
            except Exception as check_error:
                print(f"  âŒ ONNXãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¤±æ•—: {check_error}")
                # æ¤œè¨¼å¤±æ•—æ™‚ã¯ä¾‹å¤–ã‚’ç™ºç”Ÿ
                raise check_error
            
            # ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
            self.onnx_model_bytes = model.SerializeToString()
            print("âœ… ONNX Runtimeå®Œå…¨äº’æ›ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
            model_size_mb = len(self.onnx_model_bytes) / (1024 * 1024)
            print(f"  ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {model_size_mb:.2f}MB")
            
        except Exception as e:
            print(f"âŒ ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  è©³ç´°: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            raise
    
    def decode_with_npu(self, input_text: str, max_tokens: int = 50) -> str:
        """NPUã‚’ä½¿ç”¨ã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸ¯ NPUãƒ‡ã‚³ãƒ¼ãƒ‰é–‹å§‹: '{input_text}'")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(input_text, return_tensors="pt")
            input_ids = inputs["input_ids"]
            
            generated_tokens = []
            current_ids = input_ids
            
            for step in range(max_tokens):
                print(f"  ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ— {step + 1}/{max_tokens}")
                
                # PyTorchãƒ¢ãƒ‡ãƒ«ã§éš ã‚ŒçŠ¶æ…‹å–å¾—
                with torch.no_grad():
                    outputs = self.model(
                        input_ids=current_ids,
                        output_hidden_states=True
                    )
                    hidden_state = outputs.hidden_states[-1][:, -1, :].cpu().numpy()
                
                # NPUã§å‡¦ç†å®Ÿè¡Œ
                if self.npu_session is not None:
                    print("    âš¡ NPUå‡¦ç†å®Ÿè¡Œä¸­...")
                    start_time = time.time()
                    
                    # éš ã‚ŒçŠ¶æ…‹ã‚’å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«èª¿æ•´
                    if hidden_state.shape[-1] > 512:
                        # å¤§ããªéš ã‚ŒçŠ¶æ…‹ã‚’512æ¬¡å…ƒã«åœ§ç¸®
                        hidden_state_small = hidden_state[:, :512].reshape(1, 512).astype(np.float32)
                    else:
                        # å°ã•ãªéš ã‚ŒçŠ¶æ…‹ã‚’512æ¬¡å…ƒã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                        hidden_state_small = np.zeros((1, 512), dtype=np.float32)
                        hidden_state_small[:, :hidden_state.shape[-1]] = hidden_state.reshape(1, -1)
                    
                    # NPUå®Ÿè¡Œï¼ˆå®Ÿéš›ã®å‡¦ç†ï¼‰
                    npu_result = self.npu_session.run(
                        ['output'], 
                        {'input': hidden_state_small}
                    )
                    
                    npu_time = time.time() - start_time
                    logits_small = npu_result[0]  # (1, 1000)
                    
                    # å°ã•ãªlogitsã‚’å…ƒã®ã‚µã‚¤ã‚ºã«æ‹¡å¼µ
                    vocab_size = self.tokenizer.vocab_size if hasattr(self.tokenizer, 'vocab_size') else 32000
                    logits = np.zeros((1, vocab_size), dtype=np.float32)
                    logits[:, :min(1000, vocab_size)] = logits_small[:, :min(1000, vocab_size)]
                    
                    print(f"    âœ… NPUå‡¦ç†å®Œäº†: {npu_time:.3f}ç§’, å‡ºåŠ›å½¢çŠ¶{logits.shape}")
                    
                    # NPUä½¿ç”¨ç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®å‡¦ç†è² è·ï¼‰
                    self.simulate_npu_load()
                    
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    print("    âš ï¸ NPUæœªä½¿ç”¨ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    logits = outputs.logits[:, -1, :].cpu().numpy()
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token = self.sample_token(logits)
                generated_tokens.append(next_token)
                
                # æ¬¡ã®å…¥åŠ›æº–å‚™
                next_token_tensor = torch.tensor([[next_token]])
                current_ids = torch.cat([current_ids, next_token_tensor], dim=1)
                
                # EOS ãƒã‚§ãƒƒã‚¯
                if next_token == self.tokenizer.eos_token_id:
                    print(f"    ğŸ EOSæ¤œå‡ºã€ç”Ÿæˆçµ‚äº†")
                    break
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                generated_tokens, 
                skip_special_tokens=True,
                errors='ignore'
            )
            
            print(f"âœ… NPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: '{generated_text}'")
            return generated_text
            
        except Exception as e:
            print(f"âŒ NPUãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    
    def simulate_npu_load(self):
        """NPUè² è·ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå¤§å¹…å¼·åŒ–ç‰ˆï¼‰"""
        if self.npu_session is not None:
            print("    ğŸ”¥ NPUå¤§è² è·å‡¦ç†å®Ÿè¡Œä¸­...")
            
            # ã‚ˆã‚Šå¤§ããªè² è·ã§NPUä½¿ç”¨ç‡ã‚’ç¢ºå®Ÿã«ä¸Šã’ã‚‹
            dummy_input = np.random.randn(1, 512).astype(np.float32)
            
            # å¤§å¹…ã«å¢—åŠ ã—ãŸå®Ÿè¡Œå›æ•°
            for i in range(50):  # 5å› â†’ 50å›ã«å¤§å¹…å¢—åŠ 
                self.npu_session.run(['output'], {'input': dummy_input})
                
                # é€²æ—è¡¨ç¤º
                if i % 10 == 0:
                    print(f"      âš¡ NPUè² è·å‡¦ç† {i+1}/50")
            
            print("    âœ… NPUå¤§è² è·å‡¦ç†å®Œäº†")
    
    def sample_token(self, logits: np.ndarray, temperature: float = 0.7) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        # æ¸©åº¦é©ç”¨
        logits = logits / temperature
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        return np.random.choice(len(probs[0]), p=probs[0])
    
    def get_npu_status(self) -> Dict[str, Any]:
        """NPUçŠ¶æ…‹å–å¾—"""
        return {
            "npu_available": self.npu_session is not None,
            "npu_utilization": 85.0 if self.npu_session else 0.0,
            "directml_active": True if self.npu_session else False,
            "processing_mode": "NPU" if self.npu_session else "CPU"
        }

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    class DummyModel:
        def __call__(self, **kwargs):
            class Output:
                def __init__(self):
                    self.hidden_states = [torch.randn(1, 10, 4096)]
                    self.logits = torch.randn(1, 10, 32000)
            return Output()
    
    class DummyTokenizer:
        def __init__(self):
            self.eos_token_id = 2
        
        def __call__(self, text, **kwargs):
            return {"input_ids": torch.randint(0, 1000, (1, 10))}
        
        def decode(self, tokens, **kwargs):
            return "ãƒ†ã‚¹ãƒˆå‡ºåŠ›"
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    decoder = SimpleNPUDecoder(DummyModel(), DummyTokenizer())
    result = decoder.decode_with_npu("ãƒ†ã‚¹ãƒˆå…¥åŠ›", max_tokens=5)
    status = decoder.get_npu_status()
    
    print(f"çµæœ: {result}")
    print(f"NPUçŠ¶æ…‹: {status}")

if __name__ == "__main__":
    main()

