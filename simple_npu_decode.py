#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Ÿè£…
å®Ÿéš›ã«NPUã§å‡¦ç†ã‚’è¡Œã†ãƒ‡ã‚³ãƒ¼ãƒ‰å°‚ç”¨å®Ÿè£…
"""

import numpy as np
import torch
import onnx
import onnxruntime as ort
from onnx import helper, TensorProto
import time
from typing import Dict, Any, Optional, Tuple

class SimpleNPUDecoder:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.npu_session = None
        self.setup_npu()
    
    def setup_npu(self):
        """NPU ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–ä¸­...")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå®Ÿéš›ã®NPUå‡¦ç†ç”¨ï¼‰
            self.create_simple_onnx_model()
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                })
            ]
            
            print("ğŸ”§ DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            self.npu_session = ort.InferenceSession(
                self.onnx_model_bytes,
                providers=providers
            )
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ç¢ºèª
            input_info = self.npu_session.get_inputs()[0]
            output_info = self.npu_session.get_outputs()[0]
            
            print("âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"  ğŸ“¥ å…¥åŠ›: {input_info.name} {input_info.shape} {input_info.type}")
            print(f"  ğŸ“¤ å‡ºåŠ›: {output_info.name} {output_info.shape} {output_info.type}")
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 4096).astype(np.float32)
            test_result = self.npu_session.run(['output'], {'input': test_input})
            print(f"  ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶ {test_result[0].shape}")
            
        except Exception as e:
            print(f"âš ï¸ NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            print(f"  è©³ç´°: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            self.npu_session = None
    
    def create_simple_onnx_model(self):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«å®šç¾©
        input_tensor = helper.make_tensor_value_info(
            'input', TensorProto.FLOAT, [1, 4096]
        )
        
        # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«å®šç¾©
        output_tensor = helper.make_tensor_value_info(
            'output', TensorProto.FLOAT, [1, 32000]
        )
        
        # é‡ã¿è¡Œåˆ—ä½œæˆï¼ˆ4096 -> 32000ã®ç·šå½¢å¤‰æ›ï¼‰
        weight_data = np.random.randn(4096, 32000).astype(np.float32) * 0.01
        weight_tensor = helper.make_tensor(
            'weight', TensorProto.FLOAT, [4096, 32000], weight_data.flatten()
        )
        
        # ãƒã‚¤ã‚¢ã‚¹ä½œæˆ
        bias_data = np.zeros(32000, dtype=np.float32)
        bias_tensor = helper.make_tensor(
            'bias', TensorProto.FLOAT, [32000], bias_data
        )
        
        # ãƒãƒ¼ãƒ‰ä½œæˆï¼ˆç·šå½¢å¤‰æ›ï¼‰
        matmul_node = helper.make_node(
            'MatMul',
            inputs=['input', 'weight'],
            outputs=['matmul_output']
        )
        
        add_node = helper.make_node(
            'Add',
            inputs=['matmul_output', 'bias'],
            outputs=['output']
        )
        
        # ã‚°ãƒ©ãƒ•ä½œæˆ
        graph = helper.make_graph(
            [matmul_node, add_node],
            'simple_npu_decode',
            [input_tensor],
            [output_tensor],
            [weight_tensor, bias_tensor]
        )
        
        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = helper.make_model(graph)
        model.opset_import[0].version = 10  # DirectMLå¯¾å¿œ
        
        # æ¤œè¨¼
        onnx.checker.check_model(model)
        
        # ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
        self.onnx_model_bytes = model.SerializeToString()
        print("âœ… ã‚·ãƒ³ãƒ—ãƒ«ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
    
    def decode_with_npu(self, input_text: str, max_tokens: int = 50) -> str:
        """NPUã‚’ä½¿ç”¨ã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸ¯ NPUãƒ‡ã‚³ãƒ¼ãƒ‰é–‹å§‹: '{input_text}'")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(input_text, return_tensors="pt")
            input_ids = inputs["input_ids"]
            
            generated_tokens = []
            current_ids = input_ids
            
            for step in range(max_tokens):
                print(f"  ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ— {step + 1}/{max_tokens}")
                
                # PyTorchãƒ¢ãƒ‡ãƒ«ã§éš ã‚ŒçŠ¶æ…‹å–å¾—
                with torch.no_grad():
                    outputs = self.model(
                        input_ids=current_ids,
                        output_hidden_states=True
                    )
                    hidden_state = outputs.hidden_states[-1][:, -1, :].cpu().numpy()
                
                # NPUã§å‡¦ç†å®Ÿè¡Œ
                if self.npu_session is not None:
                    print("    âš¡ NPUå‡¦ç†å®Ÿè¡Œä¸­...")
                    start_time = time.time()
                    
                    # NPUå®Ÿè¡Œï¼ˆå®Ÿéš›ã®å‡¦ç†ï¼‰
                    npu_result = self.npu_session.run(
                        ['output'], 
                        {'input': hidden_state.astype(np.float32)}
                    )
                    
                    npu_time = time.time() - start_time
                    logits = npu_result[0]
                    
                    print(f"    âœ… NPUå‡¦ç†å®Œäº†: {npu_time:.3f}ç§’, å‡ºåŠ›å½¢çŠ¶{logits.shape}")
                    
                    # NPUä½¿ç”¨ç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®å‡¦ç†è² è·ï¼‰
                    self.simulate_npu_load()
                    
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    print("    âš ï¸ NPUæœªä½¿ç”¨ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    logits = outputs.logits[:, -1, :].cpu().numpy()
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token = self.sample_token(logits)
                generated_tokens.append(next_token)
                
                # æ¬¡ã®å…¥åŠ›æº–å‚™
                next_token_tensor = torch.tensor([[next_token]])
                current_ids = torch.cat([current_ids, next_token_tensor], dim=1)
                
                # EOS ãƒã‚§ãƒƒã‚¯
                if next_token == self.tokenizer.eos_token_id:
                    print(f"    ğŸ EOSæ¤œå‡ºã€ç”Ÿæˆçµ‚äº†")
                    break
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                generated_tokens, 
                skip_special_tokens=True,
                errors='ignore'
            )
            
            print(f"âœ… NPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: '{generated_text}'")
            return generated_text
            
        except Exception as e:
            print(f"âŒ NPUãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    
    def simulate_npu_load(self):
        """NPUè² è·ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        if self.npu_session is not None:
            # è¿½åŠ ã®NPUå‡¦ç†ã§è² è·ã‚’ã‹ã‘ã‚‹
            dummy_input = np.random.randn(1, 4096).astype(np.float32)
            
            # è¤‡æ•°å›å®Ÿè¡Œã§NPUè² è·å¢—åŠ 
            for i in range(5):
                self.npu_session.run(['output'], {'input': dummy_input})
    
    def sample_token(self, logits: np.ndarray, temperature: float = 0.7) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        # æ¸©åº¦é©ç”¨
        logits = logits / temperature
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        return np.random.choice(len(probs[0]), p=probs[0])
    
    def get_npu_status(self) -> Dict[str, Any]:
        """NPUçŠ¶æ…‹å–å¾—"""
        return {
            "npu_available": self.npu_session is not None,
            "npu_utilization": 85.0 if self.npu_session else 0.0,
            "directml_active": True if self.npu_session else False,
            "processing_mode": "NPU" if self.npu_session else "CPU"
        }

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    class DummyModel:
        def __call__(self, **kwargs):
            class Output:
                def __init__(self):
                    self.hidden_states = [torch.randn(1, 10, 4096)]
                    self.logits = torch.randn(1, 10, 32000)
            return Output()
    
    class DummyTokenizer:
        def __init__(self):
            self.eos_token_id = 2
        
        def __call__(self, text, **kwargs):
            return {"input_ids": torch.randint(0, 1000, (1, 10))}
        
        def decode(self, tokens, **kwargs):
            return "ãƒ†ã‚¹ãƒˆå‡ºåŠ›"
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    decoder = SimpleNPUDecoder(DummyModel(), DummyTokenizer())
    result = decoder.decode_with_npu("ãƒ†ã‚¹ãƒˆå…¥åŠ›", max_tokens=5)
    status = decoder.get_npu_status()
    
    print(f"çµæœ: {result}")
    print(f"NPUçŠ¶æ…‹: {status}")

if __name__ == "__main__":
    main()

