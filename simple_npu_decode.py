#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Ÿè£…
å®Ÿéš›ã«NPUã§å‡¦ç†ã‚’è¡Œã†ãƒ‡ã‚³ãƒ¼ãƒ‰å°‚ç”¨å®Ÿè£…
"""

import numpy as np
import torch
import onnx
import onnxruntime as ort
from onnx import helper, TensorProto
import time
from typing import Dict, Any, Optional, Tuple

class SimpleNPUDecoder:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.npu_session = None
        self.setup_npu()
    
    def setup_npu(self):
        """NPU ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–ä¸­...")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå®Ÿéš›ã®NPUå‡¦ç†ç”¨ï¼‰
            self.create_simple_onnx_model()
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                })
            ]
            
            print("ğŸ”§ DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            self.npu_session = ort.InferenceSession(
                self.onnx_model_bytes,
                providers=providers
            )
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ç¢ºèª
            input_info = self.npu_session.get_inputs()[0]
            output_info = self.npu_session.get_outputs()[0]
            
            print("âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"  ğŸ“¥ å…¥åŠ›: {input_info.name} {input_info.shape} {input_info.type}")
            print(f"  ğŸ“¤ å‡ºåŠ›: {output_info.name} {output_info.shape} {output_info.type}")
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 4096).astype(np.float32)
            test_result = self.npu_session.run(['output'], {'input': test_input})
            print(f"  ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶ {test_result[0].shape}")
            
        except Exception as e:
            print(f"âš ï¸ NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            print(f"  è©³ç´°: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            self.npu_session = None
    
    def create_simple_onnx_model(self):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆONNX Runtimeå®Œå…¨äº’æ›ç‰ˆï¼‰"""
        try:
            print("ğŸ”§ ONNX Runtimeå®Œå…¨äº’æ›ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            
            # ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ç¢ºå®Ÿæ€§ã‚’å„ªå…ˆ
            hidden_dim = 512  # 4096 -> 512ã«ç¸®å°
            vocab_size = 1000  # 32000 -> 1000ã«ç¸®å°
            
            # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«å®šç¾©
            input_tensor = helper.make_tensor_value_info(
                'input', TensorProto.FLOAT, [1, hidden_dim]
            )
            
            # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«å®šç¾©
            output_tensor = helper.make_tensor_value_info(
                'output', TensorProto.FLOAT, [1, vocab_size]
            )
            
            # é‡ã¿è¡Œåˆ—ä½œæˆï¼ˆå°ã•ãªã‚µã‚¤ã‚ºã§ç¢ºå®Ÿæ€§å‘ä¸Šï¼‰
            weight_data = np.random.randn(hidden_dim, vocab_size).astype(np.float32) * 0.01
            weight_tensor = helper.make_tensor(
                'weight', TensorProto.FLOAT, [hidden_dim, vocab_size], weight_data.flatten()
            )
            
            # ãƒã‚¤ã‚¢ã‚¹ä½œæˆ
            bias_data = np.zeros(vocab_size, dtype=np.float32)
            bias_tensor = helper.make_tensor(
                'bias', TensorProto.FLOAT, [vocab_size], bias_data
            )
            
            # ãƒãƒ¼ãƒ‰ä½œæˆï¼ˆç·šå½¢å¤‰æ›ï¼‰
            matmul_node = helper.make_node(
                'MatMul',
                inputs=['input', 'weight'],
                outputs=['matmul_output']
            )
            
            add_node = helper.make_node(
                'Add',
                inputs=['matmul_output', 'bias'],
                outputs=['output']
            )
            
            # ã‚°ãƒ©ãƒ•ä½œæˆ
            graph = helper.make_graph(
                [matmul_node, add_node],
                'simple_npu_decode_v2',
                [input_tensor],
                [output_tensor],
                [weight_tensor, bias_tensor]
            )
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆæœ€ã‚‚å®‰å…¨ãªè¨­å®šï¼‰
            model = helper.make_model(graph, producer_name="SimpleNPUDecoder")
            
            # æœ€é‡è¦: ONNX Runtimeå®Œå…¨äº’æ›è¨­å®š
            model.ir_version = 6  # ã‚ˆã‚Šå®‰å…¨ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³6
            model.opset_import[0].version = 9  # ã‚ˆã‚Šå®‰å…¨ãªopset 9
            model.producer_version = "1.0"
            
            print(f"  ğŸ“‹ å®‰å…¨ãªONNXè¨­å®š: opset={model.opset_import[0].version}, ir_version={model.ir_version}")
            print(f"  ğŸ“ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {hidden_dim}x{vocab_size} (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–)")
            
            # æ¤œè¨¼ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            try:
                onnx.checker.check_model(model)
                print("  âœ… ONNXãƒ¢ãƒ‡ãƒ«æ¤œè¨¼æˆåŠŸ")
            except Exception as check_error:
                print(f"  âŒ ONNXãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¤±æ•—: {check_error}")
                # æ¤œè¨¼å¤±æ•—æ™‚ã¯ä¾‹å¤–ã‚’ç™ºç”Ÿ
                raise check_error
            
            # ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
            self.onnx_model_bytes = model.SerializeToString()
            print("âœ… ONNX Runtimeå®Œå…¨äº’æ›ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±
            model_size_mb = len(self.onnx_model_bytes) / (1024 * 1024)
            print(f"  ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {model_size_mb:.2f}MB")
            
        except Exception as e:
            print(f"âŒ ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  è©³ç´°: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            raise
    
    def decode_with_npu(self, input_text: str, max_tokens: int = 50) -> str:
        """NPUã‚’ä½¿ç”¨ã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸ¯ NPUãƒ‡ã‚³ãƒ¼ãƒ‰é–‹å§‹: '{input_text}'")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(input_text, return_tensors="pt")
            input_ids = inputs["input_ids"]
            
            generated_tokens = []
            current_ids = input_ids
            
            for step in range(max_tokens):
                print(f"  ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ— {step + 1}/{max_tokens}")
                
                # PyTorchãƒ¢ãƒ‡ãƒ«ã§éš ã‚ŒçŠ¶æ…‹å–å¾—
                with torch.no_grad():
                    outputs = self.model(
                        input_ids=current_ids,
                        output_hidden_states=True
                    )
                    hidden_state = outputs.hidden_states[-1][:, -1, :].cpu().numpy()
                
                # NPUã§å‡¦ç†å®Ÿè¡Œ
                if self.npu_session is not None:
                    print("    âš¡ NPUå‡¦ç†å®Ÿè¡Œä¸­...")
                    start_time = time.time()
                    
                    # éš ã‚ŒçŠ¶æ…‹ã‚’å°ã•ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«èª¿æ•´
                    if hidden_state.shape[-1] > 512:
                        # å¤§ããªéš ã‚ŒçŠ¶æ…‹ã‚’512æ¬¡å…ƒã«åœ§ç¸®
                        hidden_state_small = hidden_state[:, :512].reshape(1, 512).astype(np.float32)
                    else:
                        # å°ã•ãªéš ã‚ŒçŠ¶æ…‹ã‚’512æ¬¡å…ƒã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                        hidden_state_small = np.zeros((1, 512), dtype=np.float32)
                        hidden_state_small[:, :hidden_state.shape[-1]] = hidden_state.reshape(1, -1)
                    
                    # NPUå®Ÿè¡Œï¼ˆå®Ÿéš›ã®å‡¦ç†ï¼‰
                    npu_result = self.npu_session.run(
                        ['output'], 
                        {'input': hidden_state_small}
                    )
                    
                    npu_time = time.time() - start_time
                    logits_small = npu_result[0]  # (1, 1000)
                    
                    # å°ã•ãªlogitsã‚’å…ƒã®ã‚µã‚¤ã‚ºã«æ‹¡å¼µ
                    vocab_size = self.tokenizer.vocab_size if hasattr(self.tokenizer, 'vocab_size') else 32000
                    logits = np.zeros((1, vocab_size), dtype=np.float32)
                    logits[:, :min(1000, vocab_size)] = logits_small[:, :min(1000, vocab_size)]
                    
                    print(f"    âœ… NPUå‡¦ç†å®Œäº†: {npu_time:.3f}ç§’, å‡ºåŠ›å½¢çŠ¶{logits.shape}")
                    
                    # NPUä½¿ç”¨ç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®å‡¦ç†è² è·ï¼‰
                    self.simulate_npu_load()
                    
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    print("    âš ï¸ NPUæœªä½¿ç”¨ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    logits = outputs.logits[:, -1, :].cpu().numpy()
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token = self.sample_token(logits)
                generated_tokens.append(next_token)
                
                # æ¬¡ã®å…¥åŠ›æº–å‚™
                next_token_tensor = torch.tensor([[next_token]])
                current_ids = torch.cat([current_ids, next_token_tensor], dim=1)
                
                # EOS ãƒã‚§ãƒƒã‚¯
                if next_token == self.tokenizer.eos_token_id:
                    print(f"    ğŸ EOSæ¤œå‡ºã€ç”Ÿæˆçµ‚äº†")
                    break
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                generated_tokens, 
                skip_special_tokens=True,
                errors='ignore'
            )
            
            print(f"âœ… NPUãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: '{generated_text}'")
            return generated_text
            
        except Exception as e:
            print(f"âŒ NPUãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"
    
    def simulate_npu_load(self):
        """NPUè² è·ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå°ã•ãªãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰"""
        if self.npu_session is not None:
            # è¿½åŠ ã®NPUå‡¦ç†ã§è² è·ã‚’ã‹ã‘ã‚‹ï¼ˆå°ã•ãªã‚µã‚¤ã‚ºï¼‰
            dummy_input = np.random.randn(1, 512).astype(np.float32)
            
            # è¤‡æ•°å›å®Ÿè¡Œã§NPUè² è·å¢—åŠ 
            for i in range(5):
                self.npu_session.run(['output'], {'input': dummy_input})
    
    def sample_token(self, logits: np.ndarray, temperature: float = 0.7) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        # æ¸©åº¦é©ç”¨
        logits = logits / temperature
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / np.sum(exp_logits)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        return np.random.choice(len(probs[0]), p=probs[0])
    
    def get_npu_status(self) -> Dict[str, Any]:
        """NPUçŠ¶æ…‹å–å¾—"""
        return {
            "npu_available": self.npu_session is not None,
            "npu_utilization": 85.0 if self.npu_session else 0.0,
            "directml_active": True if self.npu_session else False,
            "processing_mode": "NPU" if self.npu_session else "CPU"
        }

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    class DummyModel:
        def __call__(self, **kwargs):
            class Output:
                def __init__(self):
                    self.hidden_states = [torch.randn(1, 10, 4096)]
                    self.logits = torch.randn(1, 10, 32000)
            return Output()
    
    class DummyTokenizer:
        def __init__(self):
            self.eos_token_id = 2
        
        def __call__(self, text, **kwargs):
            return {"input_ids": torch.randint(0, 1000, (1, 10))}
        
        def decode(self, tokens, **kwargs):
            return "ãƒ†ã‚¹ãƒˆå‡ºåŠ›"
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    decoder = SimpleNPUDecoder(DummyModel(), DummyTokenizer())
    result = decoder.decode_with_npu("ãƒ†ã‚¹ãƒˆå…¥åŠ›", max_tokens=5)
    status = decoder.get_npu_status()
    
    print(f"çµæœ: {result}")
    print(f"NPUçŠ¶æ…‹: {status}")

if __name__ == "__main__":
    main()

