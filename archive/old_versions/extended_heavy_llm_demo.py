#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ æ‹¡å¼µç‰ˆé‡é‡ç´šLLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢

gpt-oss-20bã€DeepSeekã€æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚€æœ€å¤§è¦æ¨¡LLMãƒ¢ãƒ‡ãƒ«ã§ã®
Infer-OSæœ€é©åŒ–åŠ¹æœã‚’å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã§ä½“é¨“

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
ã€è¶…é‡é‡ç´š (20B+)ã€‘
- openai/gpt-oss-20b (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - GPTç³»æœ€é‡é‡ç´š
- deepseek-ai/deepseek-llm-67b-chat (67Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - è¶…é‡é‡ç´š
- deepseek-ai/deepseek-coder-33b-instruct (33Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–

ã€é‡é‡ç´š (7B-20B)ã€‘
- matsuo-lab/weblab-10b (10Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - æ—¥æœ¬èªæœ€é‡é‡ç´š
- EleutherAI/gpt-neox-20b (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - è‹±èªé‡é‡ç´š
- bigscience/bloom-7b1 (7.1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - å¤šè¨€èªé‡é‡ç´š

ç‰¹å¾´:
- CPU/GPUè‡ªå‹•æ¤œå‡ºãƒ»æœ€é©åŒ–
- é«˜åº¦ãªé‡å­åŒ–å¯¾å¿œï¼ˆMXFP4/INT4/INT8ï¼‰
- å¤šè¨€èªå¯¾å¿œï¼ˆæ—¥æœ¬èªãƒ»è‹±èªãƒ»ä¸­å›½èªç­‰ï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–
- åˆ†æ•£æ¨è«–ãƒ»ãƒ¡ãƒ¢ãƒªã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰

ä½¿ç”¨æ–¹æ³•:
    python extended_heavy_llm_demo.py --model openai/gpt-oss-20b --use-8bit --interactive
    python extended_heavy_llm_demo.py --model deepseek-ai/deepseek-llm-67b-chat --use-4bit --interactive
"""

import sys
import os
import gc
import time
import json
import psutil
import argparse
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import traceback
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import torch.nn as nn
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
    import numpy as np
    
    # æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    try:
        from accelerate import init_empty_weights, load_checkpoint_and_dispatch
        from accelerate.utils import get_balanced_memory
        ACCELERATE_AVAILABLE = True
    except ImportError:
        ACCELERATE_AVAILABLE = False
    
    try:
        from bitsandbytes import BitsAndBytesConfig
        BITSANDBYTES_AVAILABLE = True
    except ImportError:
        BITSANDBYTES_AVAILABLE = False
        
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers accelerate bitsandbytes numpy psutil")
    sys.exit(1)

# æ‹¡å¼µç‰ˆé‡é‡ç´šãƒ¢ãƒ‡ãƒ«å®šç¾©
EXTENDED_HEAVY_MODELS = {
    # è¶…é‡é‡ç´š (20B+)
    "deepseek-ai/deepseek-llm-67b-chat": {
        "parameters": 67_000_000_000,
        "size_gb": {"fp32": 268, "fp16": 134, "int8": 67, "int4": 33.5},
        "min_memory_gb": 150,
        "recommended_memory_gb": 200,
        "description": "è¶…é‡é‡ç´š 67Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ DeepSeek ãƒãƒ£ãƒƒãƒˆç‰¹åŒ–",
        "rank": 1,
        "category": "è¶…é‡é‡ç´š",
        "language": "å¤šè¨€èª",
        "speciality": "å¯¾è©±ãƒ»æ¨è«–ãƒ»æ•°å­¦",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    },
    "deepseek-ai/deepseek-coder-33b-instruct": {
        "parameters": 33_000_000_000,
        "size_gb": {"fp32": 132, "fp16": 66, "int8": 33, "int4": 16.5},
        "min_memory_gb": 80,
        "recommended_memory_gb": 120,
        "description": "è¶…é‡é‡ç´š 33Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ DeepSeek ã‚³ãƒ¼ãƒ‰ç‰¹åŒ–",
        "rank": 2,
        "category": "è¶…é‡é‡ç´š",
        "language": "å¤šè¨€èª",
        "speciality": "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ»ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    },
    "openai/gpt-oss-20b": {
        "parameters": 20_000_000_000,
        "size_gb": {"fp32": 80, "fp16": 40, "int8": 20, "int4": 10},
        "min_memory_gb": 50,
        "recommended_memory_gb": 80,
        "description": "é‡é‡ç´š 20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ GPT-OSS",
        "rank": 3,
        "category": "é‡é‡ç´š",
        "language": "è‹±èªä¸­å¿ƒ",
        "speciality": "æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"],
        "note": "MXFP4é‡å­åŒ–æ¸ˆã¿ã®å ´åˆã¯GPUå¿…é ˆ"
    },
    "EleutherAI/gpt-neox-20b": {
        "parameters": 20_000_000_000,
        "size_gb": {"fp32": 80, "fp16": 40, "int8": 20, "int4": 10},
        "min_memory_gb": 50,
        "recommended_memory_gb": 80,
        "description": "é‡é‡ç´š 20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ GPT-NeoX",
        "rank": 4,
        "category": "é‡é‡ç´š",
        "language": "è‹±èªä¸­å¿ƒ",
        "speciality": "æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    },
    
    # æ—¥æœ¬èªé‡é‡ç´š
    "matsuo-lab/weblab-10b": {
        "parameters": 10_737_418_240,
        "size_gb": {"fp32": 43, "fp16": 21.5, "int8": 10.8, "int4": 5.4},
        "min_memory_gb": 48,
        "recommended_memory_gb": 64,
        "description": "é‡é‡ç´š 10Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªç‰¹åŒ–ï¼ˆæ±å¤§æ¾å°¾ç ”ï¼‰",
        "rank": 5,
        "category": "é‡é‡ç´š",
        "language": "æ—¥æœ¬èª",
        "speciality": "å­¦è¡“ãƒ»æŠ€è¡“æ–‡æ›¸",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    },
    "rinna/youri-7b-chat": {
        "parameters": 7_241_732_096,
        "size_gb": {"fp32": 28, "fp16": 14, "int8": 7, "int4": 3.5},
        "min_memory_gb": 32,
        "recommended_memory_gb": 48,
        "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆç‰¹åŒ–",
        "rank": 6,
        "category": "é‡é‡ç´š",
        "language": "æ—¥æœ¬èª",
        "speciality": "å¯¾è©±ãƒ»ãƒãƒ£ãƒƒãƒˆ",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    },
    
    # å¤šè¨€èªé‡é‡ç´š
    "bigscience/bloom-7b1": {
        "parameters": 7_100_000_000,
        "size_gb": {"fp32": 28, "fp16": 14, "int8": 7, "int4": 3.5},
        "min_memory_gb": 32,
        "recommended_memory_gb": 48,
        "description": "é‡é‡ç´š 7.1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ å¤šè¨€èªBLOOM",
        "rank": 7,
        "category": "é‡é‡ç´š",
        "language": "å¤šè¨€èª",
        "speciality": "å¤šè¨€èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    },
    
    # ä¸­é‡ç´šï¼ˆæ¯”è¼ƒç”¨ï¼‰
    "microsoft/DialoGPT-large": {
        "parameters": 774_000_000,
        "size_gb": {"fp32": 3, "fp16": 1.5, "int8": 0.8, "int4": 0.4},
        "min_memory_gb": 8,
        "recommended_memory_gb": 16,
        "description": "ä¸­é‡ç´š 774Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ å¯¾è©±ç‰¹åŒ–",
        "rank": 8,
        "category": "ä¸­é‡ç´š",
        "language": "è‹±èª",
        "speciality": "å¯¾è©±ãƒ»ãƒãƒ£ãƒƒãƒˆ",
        "gpu_required": False,
        "quantization_support": ["int8", "int4"]
    }
}

# å¤šè¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«
MULTILINGUAL_PROMPTS = {
    "English": {
        "General": [
            "Explain the future of artificial intelligence from a technical perspective.",
            "Write a short story about a character discovering a hidden talent.",
            "Describe the impact of quantum computing on modern technology."
        ],
        "Technical": [
            "Explain machine learning algorithms in simple terms.",
            "Describe the principles of blockchain technology.",
            "What are the advantages of cloud computing?"
        ],
        "Creative": [
            "Write a poem about the beauty of nature.",
            "Create a dialogue between two AI systems.",
            "Describe a futuristic city in the year 2050."
        ]
    },
    "Japanese": {
        "æ–‡ç« ç”Ÿæˆ": [
            "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦ã€æŠ€è¡“çš„ãªè¦³ç‚¹ã‹ã‚‰è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ¡œãŒå’²ãæ˜¥ã®æ—¥ã«ã€ä¸»äººå…¬ãŒæ–°ã—ã„å‡ºä¼šã„ã‚’çµŒé¨“ã™ã‚‹çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®åŸºæœ¬åŸç†ã¨å°†æ¥ã®å¿œç”¨å¯èƒ½æ€§ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
        ],
        "æŠ€è¡“è§£èª¬": [
            "æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹æ·±å±¤å­¦ç¿’ã®ä»•çµ„ã¿ã‚’ã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³æŠ€è¡“ã®ä»•çµ„ã¿ã¨ãƒ“ã‚¸ãƒã‚¹ã¸ã®å¿œç”¨ä¾‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚"
        ],
        "å‰µä½œ": [
            "æœªæ¥éƒ½å¸‚ã‚’èˆå°ã«ã—ãŸSFå°èª¬ã®å†’é ­éƒ¨åˆ†ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
            "2ã¤ã®AIã‚·ã‚¹ãƒ†ãƒ é–“ã®å¯¾è©±ã‚’å‰µä½œã—ã¦ãã ã•ã„ã€‚",
            "2050å¹´ã®æœªæ¥éƒ½å¸‚ã«ã¤ã„ã¦è©©çš„ã«æå†™ã—ã¦ãã ã•ã„ã€‚"
        ]
    },
    "Programming": {
        "Python": [
            "Write a Python function to implement binary search.",
            "Create a class for managing a simple database.",
            "Implement a web scraper using requests and BeautifulSoup."
        ],
        "JavaScript": [
            "Write a JavaScript function to validate email addresses.",
            "Create a React component for a todo list.",
            "Implement a simple REST API using Node.js and Express."
        ],
        "Algorithm": [
            "Explain the quicksort algorithm with code examples.",
            "Implement a graph traversal algorithm (DFS or BFS).",
            "Write a function to find the longest common subsequence."
        ]
    }
}

class ExtendedHeavyLLMDemo:
    """æ‹¡å¼µç‰ˆé‡é‡ç´šLLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str, use_4bit: bool = False, use_8bit: bool = False, force_cpu: bool = False):
        self.model_name = model_name
        self.use_4bit = use_4bit
        self.use_8bit = use_8bit
        self.force_cpu = force_cpu
        
        # ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º
        self.device = self._detect_optimal_device()
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.model = None
        self.tokenizer = None
        
        # æœ€é©åŒ–çŠ¶æ…‹
        self.optimization_applied = False
        self.quantization_info = {}
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.system_info = self._get_system_info()
        
        print(f"ğŸŒ æ‹¡å¼µç‰ˆé‡é‡ç´šLLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
        print(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {model_name}")
        self._print_system_info()
        self._validate_system_requirements()
    
    def _detect_optimal_device(self) -> torch.device:
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’æ¤œå‡º"""
        if self.force_cpu:
            print("ğŸ”§ CPUå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰")
            return torch.device("cpu")
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"ğŸ® GPUæ¤œå‡º: {gpu_memory:.1f}GB VRAM")
            return torch.device("cuda")
        else:
            print("ğŸ’» CPUç’°å¢ƒã§å®Ÿè¡Œ")
            return torch.device("cpu")
    
    def _get_system_info(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        memory = psutil.virtual_memory()
        
        info = {
            "python_version": sys.version,
            "torch_version": torch.__version__,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_used_gb": memory.used / (1024**3),
            "memory_percent": memory.percent,
            "device": str(self.device),
            "cuda_available": torch.cuda.is_available(),
        }
        
        # GPUæƒ…å ±
        if torch.cuda.is_available():
            info.update({
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
            })
        
        # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¯¾å¿œçŠ¶æ³
        info.update({
            "accelerate_available": ACCELERATE_AVAILABLE,
            "bitsandbytes_available": BITSANDBYTES_AVAILABLE,
        })
        
        return info
    
    def _print_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {sys.version.split()[0]}")
        print(f"  PyTorch: {torch.__version__}")
        print(f"  ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print(f"  CPU: {self.system_info['cpu_count']}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {self.system_info['memory_total_gb']:.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {self.system_info['memory_used_gb']:.1f}GB ({self.system_info['memory_percent']:.1f}%)")
        print(f"  åˆ©ç”¨å¯èƒ½: {self.system_info['memory_available_gb']:.1f}GB")
        
        if torch.cuda.is_available():
            print(f"  GPU: {self.system_info['gpu_name']}")
            print(f"  VRAM: {self.system_info['gpu_memory_gb']:.1f}GB")
        
        print(f"\nğŸ”§ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
        print(f"  Accelerate: {'âœ…' if ACCELERATE_AVAILABLE else 'âŒ'}")
        print(f"  BitsAndBytes: {'âœ…' if BITSANDBYTES_AVAILABLE else 'âŒ'}")
    
    def _validate_system_requirements(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æ¤œè¨¼"""
        if self.model_name in EXTENDED_HEAVY_MODELS:
            model_info = EXTENDED_HEAVY_MODELS[self.model_name]
            min_memory = model_info["min_memory_gb"]
            recommended_memory = model_info["recommended_memory_gb"]
            
            print(f"\nğŸŒ æ‹¡å¼µãƒ¢ãƒ‡ãƒ«è¦ä»¶:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {model_info['description']}")
            print(f"  ã‚«ãƒ†ã‚´ãƒª: {model_info['category']}")
            print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['parameters']:,}")
            print(f"  è¨€èª: {model_info['language']}")
            print(f"  å°‚é–€åˆ†é‡: {model_info['speciality']}")
            print(f"  æœ€å°ãƒ¡ãƒ¢ãƒª: {min_memory}GB")
            print(f"  æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {recommended_memory}GB")
            
            if "note" in model_info:
                print(f"  æ³¨æ„: {model_info['note']}")
            
            # é‡å­åŒ–é©ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶
            if self.use_4bit:
                required_memory = model_info["size_gb"]["int4"]
                print(f"  INT4é‡å­åŒ–æ™‚: {required_memory}GB")
            elif self.use_8bit:
                required_memory = model_info["size_gb"]["int8"]
                print(f"  INT8é‡å­åŒ–æ™‚: {required_memory}GB")
            else:
                required_memory = model_info["size_gb"]["fp16"]
                print(f"  FP16æ™‚: {required_memory}GB")
            
            # ãƒ¡ãƒ¢ãƒªå……è¶³æ€§ãƒã‚§ãƒƒã‚¯
            available_memory = self.system_info['memory_available_gb']
            
            if available_memory < required_memory:
                print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                print(f"  å¿…è¦: {required_memory}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
                print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ--use-8bit ã¾ãŸã¯ --use-4bitï¼‰ã®ä½¿ç”¨ã‚’æ¨å¥¨")
            elif available_memory < recommended_memory:
                print(f"âš ï¸  æ¨å¥¨ãƒ¡ãƒ¢ãƒªæœªæº€ã§ã™")
                print(f"  æ¨å¥¨: {recommended_memory}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
                print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šæ€§å‘ä¸Š")
            else:
                print(f"âœ… ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™")
    
    def list_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªæ‹¡å¼µãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
        print(f"\nğŸŒ æ‹¡å¼µç‰ˆé‡é‡ç´šãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        categories = {}
        for model_name, info in EXTENDED_HEAVY_MODELS.items():
            category = info["category"]
            if category not in categories:
                categories[category] = []
            categories[category].append((model_name, info))
        
        # ã‚«ãƒ†ã‚´ãƒªé †ã§è¡¨ç¤º
        category_order = ["è¶…é‡é‡ç´š", "é‡é‡ç´š", "ä¸­é‡ç´š"]
        
        for category in category_order:
            if category in categories:
                print(f"\nã€{category}ã€‘")
                sorted_models = sorted(categories[category], key=lambda x: x[1]["rank"])
                
                for model_name, info in sorted_models:
                    rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ…", "ğŸ“‹", "ğŸ“‹", "ğŸ“‹", "ğŸ“‹"][info["rank"] - 1] if info["rank"] <= 8 else "ğŸ“‹"
                    print(f"  {rank_emoji} {model_name}")
                    print(f"    {info['description']}")
                    print(f"    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {info['parameters']:,}")
                    print(f"    è¨€èª: {info['language']}")
                    print(f"    å°‚é–€åˆ†é‡: {info['speciality']}")
                    print(f"    æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {info['recommended_memory_gb']}GB")
                    if "note" in info:
                        print(f"    æ³¨æ„: {info['note']}")
                    print()
    
    def show_sample_prompts(self):
        """å¤šè¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“ å¤šè¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        
        for language, categories in MULTILINGUAL_PROMPTS.items():
            print(f"\nã€{language}ã€‘")
            for category, prompts in categories.items():
                print(f"\n  â—† {category}")
                for i, prompt in enumerate(prompts, 1):
                    print(f"    {i}. {prompt}")
    
    def create_quantization_config(self) -> Optional[Any]:
        """é‡å­åŒ–è¨­å®šã‚’ä½œæˆï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        if not BITSANDBYTES_AVAILABLE:
            print("âš ï¸ BitsAndBytesæœªå¯¾å¿œã®ãŸã‚ã€é‡å­åŒ–ç„¡ã—ã§å®Ÿè¡Œã—ã¾ã™")
            return None
        
        try:
            if self.use_4bit:
                print("ğŸ”§ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸï¼ˆæ‹¡å¼µæœ€é©åŒ–ï¼‰")
                
                # ãƒ‡ãƒã‚¤ã‚¹åˆ¥æœ€é©åŒ–
                if self.device.type == "cuda":
                    compute_dtype = torch.float16
                    print("  GPUç”¨4bité‡å­åŒ–è¨­å®š")
                else:
                    compute_dtype = torch.float32
                    print("  CPUç”¨4bité‡å­åŒ–è¨­å®š")
                
                config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=compute_dtype,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    llm_int8_enable_fp32_cpu_offload=True if self.device.type == "cpu" else False
                )
                return config
                
            elif self.use_8bit:
                print("ğŸ”§ 8bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸï¼ˆæ‹¡å¼µæœ€é©åŒ–ï¼‰")
                
                config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True if self.device.type == "cpu" else False
                )
                return config
                
        except Exception as e:
            print(f"âš ï¸ é‡å­åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            return None
        
        return None
    
    def load_model_with_optimization(self) -> bool:
        """æœ€é©åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        try:
            print("ğŸ“¥ æ‹¡å¼µç‰ˆé‡é‡ç´šãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print("âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–é–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory:.1f}GB")
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = self.create_quantization_config()
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰è¨­å®šï¼ˆãƒ‡ãƒã‚¤ã‚¹åˆ¥æœ€é©åŒ–ï¼‰
            model_kwargs = {
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
            }
            
            # ãƒ‡ãƒã‚¤ã‚¹åˆ¥è¨­å®š
            if self.device.type == "cuda":
                model_kwargs.update({
                    "torch_dtype": torch.float16,
                    "device_map": "auto",
                })
                print("ğŸ® GPUæœ€é©åŒ–è¨­å®šé©ç”¨")
            else:
                model_kwargs.update({
                    "torch_dtype": torch.float32,
                    "device_map": "cpu",
                })
                print("ğŸ’» CPUæœ€é©åŒ–è¨­å®šé©ç”¨")
            
            # é‡å­åŒ–è¨­å®šã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
            if quantization_config is not None:
                try:
                    model_kwargs["quantization_config"] = quantization_config
                except Exception as e:
                    print(f"âš ï¸ é‡å­åŒ–è¨­å®šé©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
                    print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            
            print(f"ğŸ“¥ æ‹¡å¼µãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆæ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
            except Exception as e:
                print(f"âš ï¸ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                
                # MXFP4é‡å­åŒ–ã‚¨ãƒ©ãƒ¼ã®ç‰¹åˆ¥å‡¦ç†
                if "MXFP4" in str(e) and "GPU" in str(e):
                    print("ğŸ’¡ MXFP4é‡å­åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚CPUç’°å¢ƒã§ã¯é‡å­åŒ–ç„¡ã—ã§å®Ÿè¡Œã—ã¾ã™")
                    
                    # CPUç”¨åŸºæœ¬è¨­å®š
                    basic_kwargs = {
                        "trust_remote_code": True,
                        "torch_dtype": torch.float32,
                        "low_cpu_mem_usage": True,
                        "device_map": "cpu",
                    }
                    
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            **basic_kwargs
                        )
                    except Exception as e2:
                        print(f"âš ï¸ åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e2}")
                        print("ğŸ’¡ æœ€å°è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                        
                        # æœ€å°è¨­å®š
                        minimal_kwargs = {
                            "trust_remote_code": True,
                        }
                        
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            **minimal_kwargs
                        )
                else:
                    # ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                    print("ğŸ’¡ åŸºæœ¬è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                    
                    basic_kwargs = {
                        "trust_remote_code": True,
                        "torch_dtype": torch.float32 if self.device.type == "cpu" else torch.float16,
                        "low_cpu_mem_usage": True,
                    }
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        **basic_kwargs
                    )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ æ‹¡å¼µãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            # æ‹¡å¼µæœ€é©åŒ–é©ç”¨
            self._apply_extended_optimizations()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–çµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            memory_used = final_memory - initial_memory
            
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.1f}GB")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            print("âœ… æ‹¡å¼µãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            return False
    
    def _apply_extended_optimizations(self):
        """æ‹¡å¼µæœ€é©åŒ–ã‚’é©ç”¨"""
        try:
            print("ğŸŒ æ‹¡å¼µæœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
            
            # ãƒ‡ãƒã‚¤ã‚¹åˆ¥æœ€é©åŒ–
            if self.device.type == "cuda":
                print("  ğŸ® GPUæœ€é©åŒ–è¨­å®š")
                # GPUæœ€é©åŒ–
                if hasattr(torch.backends, 'cudnn'):
                    torch.backends.cudnn.benchmark = True
                    print("    âœ… cuDNNæœ€é©åŒ–æœ‰åŠ¹åŒ–")
            else:
                print("  ğŸ’» CPUæœ€é©åŒ–è¨­å®š")
                # CPUæœ€é©åŒ–
                torch.set_num_threads(psutil.cpu_count())
                print(f"    âœ… CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š: {psutil.cpu_count()}")
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                print("  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = True
                print("  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–")
            
            # ãƒ¢ãƒ‡ãƒ«å›ºæœ‰æœ€é©åŒ–
            try:
                # èªå½™ã‚µã‚¤ã‚ºæœ€é©åŒ–
                if hasattr(self.model.config, 'vocab_size'):
                    print(f"  âœ… èªå½™ã‚µã‚¤ã‚º: {self.model.config.vocab_size:,}")
                
                # æ–‡è„ˆé•·æœ€é©åŒ–
                if hasattr(self.model.config, 'max_position_embeddings'):
                    print(f"  âœ… æœ€å¤§æ–‡è„ˆé•·: {self.model.config.max_position_embeddings}")
                elif hasattr(self.model.config, 'max_sequence_length'):
                    print(f"  âœ… æœ€å¤§æ–‡è„ˆé•·: {self.model.config.max_sequence_length}")
                
                # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æƒ…å ±
                if hasattr(self.model.config, 'model_type'):
                    print(f"  âœ… ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {self.model.config.model_type}")
                
            except:
                pass
            
            self.optimization_applied = True
            print("ğŸš€ æ‹¡å¼µæœ€é©åŒ–é©ç”¨å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ æ‹¡å¼µæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_text(self, prompt: str, max_length: int = 300, language: str = "auto") -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰"}
        
        try:
            print(f"\nğŸ¯ æ‹¡å¼µãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt[:100]}{'...' if len(prompt) > 100 else ''}\"")
            print(f"æœ€å¤§é•·: {max_length}")
            print(f"è¨€èª: {language}")
            
            # ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            initial_cpu = psutil.cpu_percent(interval=None)
            
            # GPUä½¿ç”¨é‡æ¸¬å®šï¼ˆGPUç’°å¢ƒã®å ´åˆï¼‰
            initial_gpu_memory = None
            if torch.cuda.is_available():
                initial_gpu_memory = torch.cuda.memory_allocated() / (1024**3)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=1024
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®šï¼ˆæ‹¡å¼µç‰ˆï¼‰
            generation_config = {
                "max_length": max_length,
                "num_return_sequences": 1,
                "temperature": 0.7,
                "do_sample": True,
                "top_p": 0.9,
                "top_k": 50,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
            }
            
            # è¨€èªåˆ¥æœ€é©åŒ–
            if language == "Japanese" or "æ—¥æœ¬èª" in language:
                generation_config.update({
                    "temperature": 0.8,
                    "top_p": 0.95,
                    "repetition_penalty": 1.05,
                })
                print("  ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªæœ€é©åŒ–è¨­å®šé©ç”¨")
            elif "Programming" in language or "Code" in language:
                generation_config.update({
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "repetition_penalty": 1.2,
                })
                print("  ğŸ’» ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°æœ€é©åŒ–è¨­å®šé©ç”¨")
            
            # ç”Ÿæˆå®Ÿè¡Œï¼ˆæ™‚é–“ãƒ»ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®šï¼‰
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
            generated_only = generated_text[len(prompt):].strip()
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            final_cpu = psutil.cpu_percent(interval=None)
            
            final_gpu_memory = None
            if torch.cuda.is_available():
                final_gpu_memory = torch.cuda.memory_allocated() / (1024**3)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            total_tokens = len(outputs[0])
            
            # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            # å“è³ªè©•ä¾¡
            quality_score = self._evaluate_text_quality(generated_only, language)
            
            result = {
                "prompt": prompt,
                "generated_text": generated_only,
                "full_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "language": language,
                "quality_score": quality_score,
                "resource_usage": {
                    "memory_used_gb": final_memory - initial_memory,
                    "memory_total_gb": final_memory,
                    "cpu_usage_percent": final_cpu,
                },
                "optimization_applied": self.optimization_applied,
                "quantization_info": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit
                },
                "device": str(self.device)
            }
            
            # GPUæƒ…å ±è¿½åŠ 
            if initial_gpu_memory is not None and final_gpu_memory is not None:
                result["resource_usage"]["gpu_memory_used_gb"] = final_gpu_memory - initial_gpu_memory
                result["resource_usage"]["gpu_memory_total_gb"] = final_gpu_memory
            
            self._print_generation_results(result)
            return result
            
        except Exception as e:
            error_msg = f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg, "traceback": traceback.format_exc()}
    
    def _evaluate_text_quality(self, text: str, language: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã‚’è©•ä¾¡ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰"""
        try:
            if not text:
                return {"error": "ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™"}
            
            # åŸºæœ¬æŒ‡æ¨™
            char_count = len(text)
            word_count = len(text.split())
            sentence_count = text.count('.') + text.count('!') + text.count('?') + text.count('ã€‚')
            
            # è¨€èªåˆ¥è©•ä¾¡
            if language == "Japanese" or "æ—¥æœ¬èª" in language:
                return self._evaluate_japanese_quality(text)
            elif "Programming" in language or "Code" in language:
                return self._evaluate_code_quality(text)
            else:
                return self._evaluate_english_quality(text)
            
        except Exception as e:
            return {"error": f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _evaluate_japanese_quality(self, text: str) -> Dict:
        """æ—¥æœ¬èªå“è³ªè©•ä¾¡"""
        hiragana_count = sum(1 for c in text if '\u3040' <= c <= '\u309F')
        katakana_count = sum(1 for c in text if '\u30A0' <= c <= '\u30FF')
        kanji_count = sum(1 for c in text if '\u4E00' <= c <= '\u9FAF')
        ascii_count = sum(1 for c in text if ord(c) < 128)
        
        total_chars = len(text)
        japanese_ratio = (hiragana_count + katakana_count + kanji_count) / total_chars if total_chars > 0 else 0
        
        if japanese_ratio > 0.8:
            quality_level = "å„ªç§€"
        elif japanese_ratio > 0.6:
            quality_level = "è‰¯å¥½"
        elif japanese_ratio > 0.4:
            quality_level = "æ™®é€š"
        else:
            quality_level = "è¦æ”¹å–„"
        
        return {
            "language": "Japanese",
            "japanese_ratio": japanese_ratio,
            "quality_level": quality_level,
            "character_breakdown": {
                "hiragana": hiragana_count,
                "katakana": katakana_count,
                "kanji": kanji_count,
                "ascii": ascii_count,
                "total": total_chars
            }
        }
    
    def _evaluate_english_quality(self, text: str) -> Dict:
        """è‹±èªå“è³ªè©•ä¾¡"""
        words = text.split()
        sentences = text.count('.') + text.count('!') + text.count('?')
        
        avg_word_length = sum(len(word.strip('.,!?')) for word in words) / len(words) if words else 0
        avg_sentence_length = len(words) / sentences if sentences > 0 else len(words)
        
        # ç°¡æ˜“å“è³ªè©•ä¾¡
        if avg_word_length > 4 and avg_sentence_length > 10:
            quality_level = "Good"
        elif avg_word_length > 3 and avg_sentence_length > 5:
            quality_level = "Fair"
        else:
            quality_level = "Basic"
        
        return {
            "language": "English",
            "quality_level": quality_level,
            "word_count": len(words),
            "sentence_count": sentences,
            "avg_word_length": avg_word_length,
            "avg_sentence_length": avg_sentence_length
        }
    
    def _evaluate_code_quality(self, text: str) -> Dict:
        """ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡"""
        lines = text.split('\n')
        code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
        comment_lines = [line for line in lines if line.strip().startswith('#')]
        
        # ã‚³ãƒ¼ãƒ‰ç‰¹å¾´æ¤œå‡º
        has_functions = any('def ' in line for line in lines)
        has_classes = any('class ' in line for line in lines)
        has_imports = any('import ' in line or 'from ' in line for line in lines)
        
        quality_score = 0
        if has_functions: quality_score += 1
        if has_classes: quality_score += 1
        if has_imports: quality_score += 1
        if len(comment_lines) > 0: quality_score += 1
        
        if quality_score >= 3:
            quality_level = "Good"
        elif quality_score >= 2:
            quality_level = "Fair"
        else:
            quality_level = "Basic"
        
        return {
            "language": "Programming",
            "quality_level": quality_level,
            "total_lines": len(lines),
            "code_lines": len(code_lines),
            "comment_lines": len(comment_lines),
            "has_functions": has_functions,
            "has_classes": has_classes,
            "has_imports": has_imports
        }
    
    def _print_generation_results(self, result: Dict):
        """ç”Ÿæˆçµæœã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š æ‹¡å¼µç”Ÿæˆçµæœ:")
        print(f"  ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
        print(f"  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['input_tokens']}")
        print(f"  å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['output_tokens']}")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result['tokens_per_second']:.1f} tokens/sec")
        print(f"  ãƒ‡ãƒã‚¤ã‚¹: {result['device']}")
        
        print(f"\nğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡:")
        print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {result['resource_usage']['memory_used_gb']:.1f}GB")
        print(f"  ç·ãƒ¡ãƒ¢ãƒª: {result['resource_usage']['memory_total_gb']:.1f}GB")
        print(f"  CPUä½¿ç”¨ç‡: {result['resource_usage']['cpu_usage_percent']:.1f}%")
        
        if "gpu_memory_used_gb" in result['resource_usage']:
            print(f"  GPUä½¿ç”¨: {result['resource_usage']['gpu_memory_used_gb']:.1f}GB")
            print(f"  ç·GPU: {result['resource_usage']['gpu_memory_total_gb']:.1f}GB")
        
        print(f"\nğŸŒ å“è³ªè©•ä¾¡:")
        if "error" not in result['quality_score']:
            quality = result['quality_score']
            print(f"  è¨€èª: {quality.get('language', 'Unknown')}")
            print(f"  å“è³ªãƒ¬ãƒ™ãƒ«: {quality.get('quality_level', 'Unknown')}")
            
            if quality.get('language') == 'Japanese':
                print(f"  æ—¥æœ¬èªæ¯”ç‡: {quality.get('japanese_ratio', 0):.1%}")
            elif quality.get('language') == 'English':
                print(f"  å¹³å‡å˜èªé•·: {quality.get('avg_word_length', 0):.1f}")
                print(f"  å¹³å‡æ–‡é•·: {quality.get('avg_sentence_length', 0):.1f}")
            elif quality.get('language') == 'Programming':
                print(f"  ã‚³ãƒ¼ãƒ‰è¡Œæ•°: {quality.get('code_lines', 0)}")
                print(f"  é–¢æ•°: {'âœ…' if quality.get('has_functions') else 'âŒ'}")
                print(f"  ã‚¯ãƒ©ã‚¹: {'âœ…' if quality.get('has_classes') else 'âŒ'}")
        else:
            print(f"  è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {result['quality_score']['error']}")
        
        print(f"\nğŸ”§ æœ€é©åŒ–çŠ¶æ…‹:")
        print(f"  æ‹¡å¼µæœ€é©åŒ–: {'âœ…' if result['optimization_applied'] else 'âŒ'}")
        print(f"  4bité‡å­åŒ–: {'âœ…' if result['quantization_info']['use_4bit'] else 'âŒ'}")
        print(f"  8bité‡å­åŒ–: {'âœ…' if result['quantization_info']['use_8bit'] else 'âŒ'}")
        
        print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:")
        print(f"  \"{result['generated_text'][:300]}{'...' if len(result['generated_text']) > 300 else ''}\"")
    
    def interactive_mode(self):
        """æ‹¡å¼µã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print(f"\nğŸŒ æ‹¡å¼µã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ã€'samples'ã§ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºã€'models'ã§ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ï¼‰:")
        
        results = []
        
        while True:
            try:
                prompt = input("\nğŸŒ > ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q', 'çµ‚äº†']:
                    break
                
                if prompt.lower() in ['samples', 'sample', 'ã‚µãƒ³ãƒ—ãƒ«']:
                    self.show_sample_prompts()
                    continue
                
                if prompt.lower() in ['models', 'model', 'ãƒ¢ãƒ‡ãƒ«']:
                    self.list_available_models()
                    continue
                
                if not prompt:
                    continue
                
                # è¨€èªè‡ªå‹•æ¤œå‡º
                language = self._detect_language(prompt)
                
                result = self.generate_text(prompt, language=language)
                if "error" not in result:
                    results.append(result)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ æ‹¡å¼µã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœä¿å­˜
        if results:
            self._save_session_results(results)
    
    def _detect_language(self, text: str) -> str:
        """è¨€èªã‚’è‡ªå‹•æ¤œå‡º"""
        # æ—¥æœ¬èªæ–‡å­—ã®æ¤œå‡º
        japanese_chars = sum(1 for c in text if '\u3040' <= c <= '\u309F' or '\u30A0' <= c <= '\u30FF' or '\u4E00' <= c <= '\u9FAF')
        
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º
        programming_keywords = ['def ', 'class ', 'import ', 'function', 'var ', 'const ', 'let ', '#!/', 'print(', 'console.log']
        has_programming = any(keyword in text.lower() for keyword in programming_keywords)
        
        if japanese_chars > 0:
            return "Japanese"
        elif has_programming:
            return "Programming"
        else:
            return "English"
    
    def _save_session_results(self, results: List[Dict]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_safe_name = self.model_name.replace("/", "_").replace("-", "_")
            filename = f'extended_heavy_llm_session_{model_safe_name}_{timestamp}.json'
            
            session_data = {
                "model_name": self.model_name,
                "timestamp": datetime.now().isoformat(),
                "system_info": self.system_info,
                "optimization_config": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit,
                    "force_cpu": self.force_cpu,
                    "optimization_applied": self.optimization_applied
                },
                "results": results,
                "summary": self._calculate_session_summary(results)
            }
            
            os.makedirs('demo_results', exist_ok=True)
            filepath = os.path.join('demo_results', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ æ‹¡å¼µã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_session_summary(self, results: List[Dict]) -> Dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—"""
        if not results:
            return {}
        
        generation_times = [r['generation_time'] for r in results]
        tokens_per_second = [r['tokens_per_second'] for r in results]
        output_tokens = [r['output_tokens'] for r in results]
        memory_used = [r['resource_usage']['memory_used_gb'] for r in results]
        
        # è¨€èªåˆ†å¸ƒ
        languages = [r.get('language', 'Unknown') for r in results]
        language_distribution = {lang: languages.count(lang) for lang in set(languages)}
        
        return {
            "total_generations": len(results),
            "avg_generation_time": sum(generation_times) / len(generation_times),
            "avg_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second),
            "total_output_tokens": sum(output_tokens),
            "avg_memory_used_gb": sum(memory_used) / len(memory_used),
            "min_generation_time": min(generation_times),
            "max_generation_time": max(generation_times),
            "language_distribution": language_distribution
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="æ‹¡å¼µç‰ˆé‡é‡ç´šLLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
    parser.add_argument("--model", default="openai/gpt-oss-20b", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--prompt", help="ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-length", type=int, default=300, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--use-4bit", action="store_true", help="4bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-8bit", action="store_true", help="8bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--force-cpu", action="store_true", help="CPUå¼·åˆ¶ä½¿ç”¨")
    parser.add_argument("--interactive", action="store_true", help="æ‹¡å¼µã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--list-models", action="store_true", help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º")
    parser.add_argument("--samples", action="store_true", help="å¤šè¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º")
    
    args = parser.parse_args()
    
    if args.list_models:
        demo = ExtendedHeavyLLMDemo("dummy", False, False)
        demo.list_available_models()
        return
    
    if args.samples:
        demo = ExtendedHeavyLLMDemo("dummy", False, False)
        demo.show_sample_prompts()
        return
    
    print(f"""
{'='*80}
ğŸŒ æ‹¡å¼µç‰ˆé‡é‡ç´šLLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢
{'='*80}

å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {args.model}
æœ€é©åŒ–è¨­å®š:
  4bité‡å­åŒ–: {'âœ…' if args.use_4bit else 'âŒ'}
  8bité‡å­åŒ–: {'âœ…' if args.use_8bit else 'âŒ'}
  CPUå¼·åˆ¶: {'âœ…' if args.force_cpu else 'âŒ'}
  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–: {'âœ…' if args.interactive else 'âŒ'}

{'='*80}
""")
    
    try:
        # ãƒ‡ãƒ¢åˆæœŸåŒ–
        demo = ExtendedHeavyLLMDemo(
            model_name=args.model,
            use_4bit=args.use_4bit,
            use_8bit=args.use_8bit,
            force_cpu=args.force_cpu
        )
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if not demo.load_model_with_optimization():
            print("âŒ æ‹¡å¼µãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
        if args.interactive:
            demo.interactive_mode()
        else:
            prompt = args.prompt or "Explain the future of artificial intelligence from a technical perspective."
            result = demo.generate_text(prompt, args.max_length)
            
            if "error" in result:
                print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print("\nğŸ‰ æ‹¡å¼µãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")

if __name__ == "__main__":
    main()

