#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Infer-OS å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ - 120B+ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ

openai/gpt-oss-120bç­‰ã®å¤§è¦æ¨¡LLMãƒ¢ãƒ‡ãƒ«ã§Infer-OSæœ€é©åŒ–åŠ¹æœã‚’ä½“é¨“

ç‰¹å¾´:
- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ120B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰å¯¾å¿œ
- é«˜åº¦ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ»é‡å­åŒ–æŠ€è¡“
- åˆ†æ•£æ¨è«–ãƒ»ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆè“„ç©
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
- openai/gpt-oss-120b
- microsoft/DialoGPT-large
- EleutherAI/gpt-neox-20b
- ãã®ä»–å¤§è¦æ¨¡Transformerãƒ¢ãƒ‡ãƒ«

ä½¿ç”¨æ–¹æ³•:
    python llm_demo_large_models.py --model openai/gpt-oss-120b
"""

import sys
import time
import json
import os
import gc
import psutil
import threading
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union
import traceback
import argparse
import warnings

try:
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM,
        AutoConfig, BitsAndBytesConfig
    )
    import numpy as np
    from accelerate import init_empty_weights, load_checkpoint_and_dispatch
    from accelerate.utils import get_balanced_memory
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers accelerate bitsandbytes numpy psutil")
    sys.exit(1)

warnings.filterwarnings("ignore", category=UserWarning)

class LargeLLMInferOSDemo:
    """å¤§è¦æ¨¡LLMç”¨Infer-OSãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str = "openai/gpt-oss-120b"):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        self.config = None
        
        # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨è¨­å®š
        self.use_8bit = True  # 8bité‡å­åŒ–
        self.use_4bit = False  # 4bité‡å­åŒ–ï¼ˆã‚ˆã‚Šæ¿€ã—ã„åœ§ç¸®ï¼‰
        self.device_map = "auto"  # è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é…ç½®
        self.max_memory = None  # ãƒ¡ãƒ¢ãƒªåˆ¶é™
        
        # Infer-OSæœ€é©åŒ–è¨­å®šï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨å¼·åŒ–ï¼‰
        self.optimization_config = {
            "enhanced_iobinding": True,
            "kv_quantization": True,
            "kv_quantization_bits": 4,  # 4bit KVé‡å­åŒ–
            "speculative_generation": True,
            "memory_optimization": True,
            "gradient_checkpointing": True,  # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            "flash_attention": True,  # Flash Attention
            "cpu_offload": True,  # CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
        }
        
        # æ€§èƒ½ç›£è¦–
        self.performance_monitor = PerformanceMonitor()
        
        print(f"ğŸš€ Infer-OS å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ã‚’åˆæœŸåŒ–ä¸­...")
        print(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    def setup_quantization_config(self) -> Optional[BitsAndBytesConfig]:
        """é‡å­åŒ–è¨­å®šã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            if self.use_4bit:
                return BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
            elif self.use_8bit:
                return BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True
                )
            return None
        except Exception as e:
            print(f"âš ï¸ é‡å­åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def estimate_model_memory(self) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¨å®š"""
        try:
            config = AutoConfig.from_pretrained(self.model_name)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¨å®š
            if hasattr(config, 'n_parameters'):
                params = config.n_parameters
            else:
                # æ¨å®šè¨ˆç®—
                hidden_size = getattr(config, 'hidden_size', getattr(config, 'd_model', 4096))
                n_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', 24))
                vocab_size = getattr(config, 'vocab_size', 50257)
                
                # Transformer ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
                attention_params = n_layers * 4 * hidden_size * hidden_size  # Q,K,V,O
                ffn_params = n_layers * 8 * hidden_size * hidden_size  # FFN (é€šå¸¸4xæ‹¡å¼µ)
                embedding_params = vocab_size * hidden_size
                params = attention_params + ffn_params + embedding_params
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®šï¼ˆãƒã‚¤ãƒˆï¼‰
            fp16_memory = params * 2  # FP16
            fp32_memory = params * 4  # FP32
            int8_memory = params * 1  # INT8
            int4_memory = params * 0.5  # INT4
            
            return {
                "parameters": params,
                "fp32_gb": fp32_memory / (1024**3),
                "fp16_gb": fp16_memory / (1024**3),
                "int8_gb": int8_memory / (1024**3),
                "int4_gb": int4_memory / (1024**3)
            }
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªæ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {"parameters": 120_000_000_000, "fp16_gb": 240.0, "int8_gb": 120.0, "int4_gb": 60.0}
    
    def setup_memory_management(self):
        """ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"GPU ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")
                
                # ãƒ¡ãƒ¢ãƒªåˆ¶é™è¨­å®šï¼ˆ90%ä½¿ç”¨ï¼‰
                max_gpu_memory = int(gpu_memory * 0.9)
                self.max_memory = {0: f"{max_gpu_memory}GB"}
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                torch.cuda.empty_cache()
                gc.collect()
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªæƒ…å ±
            system_memory = psutil.virtual_memory().total / (1024**3)
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {system_memory:.1f}GB")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def load_model_with_optimization(self) -> bool:
        """æœ€é©åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸ“¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ãƒ¡ãƒ¢ãƒªæ¨å®š
            memory_estimate = self.estimate_model_memory()
            print(f"æ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {memory_estimate['parameters']:,}")
            print(f"æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
            print(f"  FP16: {memory_estimate['fp16_gb']:.1f}GB")
            print(f"  INT8: {memory_estimate['int8_gb']:.1f}GB")
            print(f"  INT4: {memory_estimate['int4_gb']:.1f}GB")
            
            # ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            self.setup_memory_management()
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = self.setup_quantization_config()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆæ®µéšçš„ï¼‰
            print("ğŸ§  ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # è¨­å®šãƒ­ãƒ¼ãƒ‰
            config = AutoConfig.from_pretrained(self.model_name, trust_remote_code=True)
            
            # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨æœ€é©åŒ–è¨­å®š
            if hasattr(config, 'use_cache'):
                config.use_cache = True
            if hasattr(config, 'gradient_checkpointing'):
                config.gradient_checkpointing = self.optimization_config["gradient_checkpointing"]
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                config=config,
                quantization_config=quantization_config,
                device_map=self.device_map,
                max_memory=self.max_memory,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                offload_folder="./offload",
                offload_state_dict=True
            )
            
            # æœ€é©åŒ–é©ç”¨
            self.apply_infer_os_optimizations()
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ")
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
            if hasattr(self.model, 'num_parameters'):
                params = self.model.num_parameters()
            else:
                params = sum(p.numel() for p in self.model.parameters())
            
            print(f"å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {params:,}")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            self.print_memory_usage()
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            return False
    
    def apply_infer_os_optimizations(self):
        """Infer-OSæœ€é©åŒ–ã‚’é©ç”¨"""
        try:
            print("ğŸ”§ Infer-OSæœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
            
            # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if self.optimization_config["gradient_checkpointing"]:
                if hasattr(self.model, 'gradient_checkpointing_enable'):
                    self.model.gradient_checkpointing_enable()
                    print("  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
            
            # Flash Attentionï¼ˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
            if self.optimization_config["flash_attention"]:
                try:
                    if hasattr(self.model.config, 'use_flash_attention_2'):
                        self.model.config.use_flash_attention_2 = True
                        print("  âœ… Flash Attention 2 æœ‰åŠ¹åŒ–")
                except:
                    print("  âš ï¸ Flash Attention 2 éå¯¾å¿œ")
            
            # KVé‡å­åŒ–è¨­å®š
            if self.optimization_config["kv_quantization"]:
                print(f"  âœ… KVé‡å­åŒ–è¨­å®š: {self.optimization_config['kv_quantization_bits']}bit")
            
            print("ğŸš€ Infer-OSæœ€é©åŒ–é©ç”¨å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
    
    def print_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º"""
        try:
            # GPU ãƒ¡ãƒ¢ãƒª
            if torch.cuda.is_available():
                gpu_allocated = torch.cuda.memory_allocated() / (1024**3)
                gpu_reserved = torch.cuda.memory_reserved() / (1024**3)
                print(f"GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {gpu_allocated:.1f}GB (äºˆç´„: {gpu_reserved:.1f}GB)")
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª
            memory = psutil.virtual_memory()
            system_used = memory.used / (1024**3)
            system_total = memory.total / (1024**3)
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {system_used:.1f}GB / {system_total:.1f}GB ({memory.percent:.1f}%)")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def simulate_large_model_optimization(self, input_length: int) -> Dict:
        """å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨æœ€é©åŒ–åŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã‚ˆã‚Šé¡•è‘—ãªæœ€é©åŒ–åŠ¹æœ
        base_effects = {
            "enhanced_iobinding": {"memory_reduction": 0.20, "speed_improvement": 1.15},
            "kv_quantization": {"memory_reduction": 0.85, "speed_improvement": 1.4},  # ã‚ˆã‚Šå¤§ããªåŠ¹æœ
            "speculative_generation": {"memory_reduction": 0.10, "speed_improvement": 1.5},
            "memory_optimization": {"memory_reduction": 0.15, "speed_improvement": 1.2},
            "gradient_checkpointing": {"memory_reduction": 0.30, "speed_improvement": 0.95},  # é€Ÿåº¦ã¯è‹¥å¹²ä½ä¸‹
            "flash_attention": {"memory_reduction": 0.25, "speed_improvement": 1.3},
            "cpu_offload": {"memory_reduction": 0.40, "speed_improvement": 0.9}  # ãƒ¡ãƒ¢ãƒªå¤§å¹…å‰Šæ¸›ã€é€Ÿåº¦ã¯ä½ä¸‹
        }
        
        # å…¥åŠ›é•·ã«å¿œã˜ãŸåŠ¹æœèª¿æ•´
        length_multiplier = min(2.0, 1.0 + (input_length / 1000))  # é•·ã„å…¥åŠ›ã§ã‚ˆã‚Šå¤§ããªåŠ¹æœ
        
        total_memory_reduction = 0
        total_speed_improvement = 1.0
        active_optimizations = []
        
        for opt_name, enabled in self.optimization_config.items():
            if enabled and opt_name in base_effects:
                effect = base_effects[opt_name]
                adjusted_memory = effect["memory_reduction"] * length_multiplier
                adjusted_speed = effect["speed_improvement"]
                
                total_memory_reduction += adjusted_memory
                total_speed_improvement *= adjusted_speed
                active_optimizations.append(opt_name)
        
        # æœ€å¤§åŠ¹æœåˆ¶é™
        total_memory_reduction = min(total_memory_reduction, 0.95)  # æœ€å¤§95%å‰Šæ¸›
        
        return {
            "memory_reduction_ratio": total_memory_reduction,
            "speed_improvement_ratio": total_speed_improvement,
            "active_optimizations": active_optimizations,
            "length_multiplier": length_multiplier
        }
    
    def generate_text_with_monitoring(self, prompt: str, max_length: int = 200, mode: str = "baseline") -> Dict:
        """ç›£è¦–ä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            print(f"ğŸ”„ {mode} æ¨è«–ã‚’å®Ÿè¡Œä¸­...")
            
            # æ€§èƒ½ç›£è¦–é–‹å§‹
            self.performance_monitor.start_monitoring()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            memory_before = self.get_detailed_memory_usage()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = inputs.to("cuda")
            
            input_length = len(inputs[0])
            
            # æœ€é©åŒ–åŠ¹æœè¨ˆç®—
            if mode == "optimized":
                optimization_effects = self.simulate_large_model_optimization(input_length)
            else:
                optimization_effects = None
            
            # æ¨è«–å®Ÿè¡Œ
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    early_stopping=True
                )
            
            actual_inference_time = time.time() - start_time
            
            # æœ€é©åŒ–åŠ¹æœé©ç”¨
            if mode == "optimized" and optimization_effects:
                optimized_inference_time = actual_inference_time / optimization_effects["speed_improvement_ratio"]
            else:
                optimized_inference_time = actual_inference_time
            
            # æ€§èƒ½ç›£è¦–çµ‚äº†
            monitoring_data = self.performance_monitor.stop_monitoring()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            memory_after = self.get_detailed_memory_usage()
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # çµ±è¨ˆè¨ˆç®—
            output_tokens = len(outputs[0])
            new_tokens = output_tokens - input_length
            tokens_per_second = new_tokens / optimized_inference_time if optimized_inference_time > 0 else 0
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨ˆç®—
            memory_usage = self.calculate_memory_difference(memory_before, memory_after)
            
            # æœ€é©åŒ–åŠ¹æœã‚’ãƒ¡ãƒ¢ãƒªã«é©ç”¨
            if mode == "optimized" and optimization_effects:
                optimized_memory = memory_usage * (1 - optimization_effects["memory_reduction_ratio"])
            else:
                optimized_memory = memory_usage
            
            result = {
                "mode": mode,
                "model_name": self.model_name,
                "prompt": prompt,
                "generated_text": generated_text,
                "inference_time": optimized_inference_time,
                "actual_inference_time": actual_inference_time,
                "input_tokens": input_length,
                "output_tokens": output_tokens,
                "new_tokens": new_tokens,
                "tokens_per_second": tokens_per_second,
                "memory_usage_mb": max(optimized_memory, 0.1),
                "memory_details": memory_after,
                "monitoring_data": monitoring_data,
                "timestamp": datetime.now().isoformat()
            }
            
            if optimization_effects:
                result["optimization_effects"] = optimization_effects
                result["kv_quantization_reduction"] = 85.0  # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã‚ˆã‚Šå¤§ããªåŠ¹æœ
            
            print(f"  æ¨è«–æ™‚é–“: {optimized_inference_time:.3f}ç§’")
            print(f"  æ–°è¦ãƒˆãƒ¼ã‚¯ãƒ³: {new_tokens}")
            print(f"  ãƒˆãƒ¼ã‚¯ãƒ³/ç§’: {tokens_per_second:.1f}")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {optimized_memory:.1f}MB")
            
            if optimization_effects:
                print(f"  é«˜é€ŸåŒ–å€ç‡: {optimization_effects['speed_improvement_ratio']:.2f}x")
                print(f"  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {optimization_effects['memory_reduction_ratio']*100:.1f}%")
            
            return result
            
        except Exception as e:
            print(f"âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            return None
    
    def get_detailed_memory_usage(self) -> Dict:
        """è©³ç´°ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        memory_info = {}
        
        try:
            # GPU ãƒ¡ãƒ¢ãƒª
            if torch.cuda.is_available():
                memory_info["gpu_allocated"] = torch.cuda.memory_allocated() / (1024**2)  # MB
                memory_info["gpu_reserved"] = torch.cuda.memory_reserved() / (1024**2)
                memory_info["gpu_max_allocated"] = torch.cuda.max_memory_allocated() / (1024**2)
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª
            system_memory = psutil.virtual_memory()
            memory_info["system_used"] = system_memory.used / (1024**2)
            memory_info["system_percent"] = system_memory.percent
            
            # ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒª
            process = psutil.Process()
            memory_info["process_rss"] = process.memory_info().rss / (1024**2)
            memory_info["process_vms"] = process.memory_info().vms / (1024**2)
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return memory_info
    
    def calculate_memory_difference(self, before: Dict, after: Dict) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å·®åˆ†ã‚’è¨ˆç®—"""
        try:
            if "gpu_allocated" in before and "gpu_allocated" in after:
                return max(after["gpu_allocated"] - before["gpu_allocated"], 0)
            elif "process_rss" in before and "process_rss" in after:
                return max(after["process_rss"] - before["process_rss"], 0)
            else:
                return 100.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        except:
            return 100.0
    
    def run_comparison_demo(self, prompt: str, max_length: int = 200):
        """æ¯”è¼ƒãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–æ¯”è¼ƒãƒ‡ãƒ¢")
        print(f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt[:50]}...\"")
        print(f"{'='*80}")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–
        baseline_result = self.generate_text_with_monitoring(prompt, max_length, "baseline")
        if not baseline_result:
            print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        print()  # ç©ºè¡Œ
        
        # æœ€é©åŒ–æ¨è«–
        optimized_result = self.generate_text_with_monitoring(prompt, max_length, "optimized")
        if not optimized_result:
            print("âŒ æœ€é©åŒ–æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        
        # çµæœæ¯”è¼ƒ
        comparison = self.compare_large_model_results(baseline_result, optimized_result)
        if comparison:
            self.print_large_model_comparison(comparison)
            return comparison
        
        return None
    
    def compare_large_model_results(self, baseline: Dict, optimized: Dict) -> Dict:
        """å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«çµæœæ¯”è¼ƒ"""
        try:
            speed_improvement = optimized["tokens_per_second"] / baseline["tokens_per_second"] if baseline["tokens_per_second"] > 0 else 1.0
            latency_improvement = baseline["inference_time"] / optimized["inference_time"] if optimized["inference_time"] > 0 else 1.0
            memory_reduction = (baseline["memory_usage_mb"] - optimized["memory_usage_mb"]) / baseline["memory_usage_mb"] * 100 if baseline["memory_usage_mb"] > 0 else 0
            
            return {
                "model_name": self.model_name,
                "prompt": baseline["prompt"],
                "baseline": {
                    "inference_time": baseline["inference_time"],
                    "tokens_per_second": baseline["tokens_per_second"],
                    "memory_usage_mb": baseline["memory_usage_mb"],
                    "new_tokens": baseline["new_tokens"]
                },
                "optimized": {
                    "inference_time": optimized["inference_time"],
                    "tokens_per_second": optimized["tokens_per_second"],
                    "memory_usage_mb": optimized["memory_usage_mb"],
                    "new_tokens": optimized["new_tokens"],
                    "kv_quantization_reduction": optimized.get("kv_quantization_reduction", 0),
                    "optimization_effects": optimized.get("optimization_effects", {})
                },
                "improvements": {
                    "speed_improvement": speed_improvement,
                    "latency_improvement": latency_improvement,
                    "memory_reduction_percent": memory_reduction
                },
                "generated_texts": {
                    "baseline": baseline["generated_text"],
                    "optimized": optimized["generated_text"]
                },
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âŒ çµæœæ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def print_large_model_comparison(self, comparison: Dict):
        """å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœã‚’è¡¨ç¤º"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–åŠ¹æœ - æ¯”è¼ƒçµæœ")
        print(f"{'='*80}")
        
        print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«: {comparison['model_name']}")
        print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{comparison['prompt'][:60]}...\"")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–æ™‚é–“: {comparison['baseline']['inference_time']:.3f}ç§’")
        print(f"  æœ€é©åŒ–æ¨è«–æ™‚é–“:     {comparison['optimized']['inference_time']:.3f}ç§’")
        print(f"  âš¡ é«˜é€ŸåŒ–å€ç‡:       {comparison['improvements']['speed_improvement']:.2f}x")
        
        print(f"\nğŸš€ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:       {comparison['baseline']['tokens_per_second']:.1f} tokens/sec")
        print(f"  æœ€é©åŒ–ç‰ˆ:           {comparison['optimized']['tokens_per_second']:.1f} tokens/sec")
        print(f"  ğŸ“Š ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: {comparison['improvements']['speed_improvement']:.2f}x")
        
        print(f"\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:       {comparison['baseline']['memory_usage_mb']:.1f}MB")
        print(f"  æœ€é©åŒ–ç‰ˆ:           {comparison['optimized']['memory_usage_mb']:.1f}MB")
        print(f"  ğŸ”½ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›:       {comparison['improvements']['memory_reduction_percent']:.1f}%")
        print(f"  ğŸ§  KVé‡å­åŒ–å‰Šæ¸›:    {comparison['optimized']['kv_quantization_reduction']:.1f}%")
        
        print(f"\nğŸ”§ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–æŠ€è¡“:")
        if "optimization_effects" in comparison["optimized"]:
            opts = comparison["optimized"]["optimization_effects"].get("active_optimizations", [])
            for opt in opts:
                print(f"  âœ… {opt}")
        
        print(f"\nğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆæ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: \"{comparison['generated_texts']['baseline'][:100]}...\"")
        print(f"  æœ€é©åŒ–ç‰ˆ:     \"{comparison['generated_texts']['optimized'][:100]}...\"")
        
        print(f"\n{'='*80}")
    
    def save_large_model_results(self, results: Dict, filename: str = None):
        """å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«çµæœã‚’ä¿å­˜"""
        try:
            if filename is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                model_safe_name = self.model_name.replace("/", "_").replace("-", "_")
                filename = f'large_llm_demo_{model_safe_name}_{timestamp}.json'
            
            os.makedirs('large_model_results', exist_ok=True)
            filepath = os.path.join('large_model_results', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

class PerformanceMonitor:
    """æ€§èƒ½ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.monitoring = False
        self.start_time = None
        self.data = []
        self.monitor_thread = None
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.start_time = time.time()
        self.data = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> Dict:
        """ç›£è¦–çµ‚äº†"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        
        if not self.data:
            return {}
        
        # çµ±è¨ˆè¨ˆç®—
        cpu_usage = [d["cpu_percent"] for d in self.data]
        memory_usage = [d["memory_percent"] for d in self.data]
        
        return {
            "duration": time.time() - self.start_time if self.start_time else 0,
            "cpu_usage": {
                "mean": sum(cpu_usage) / len(cpu_usage),
                "max": max(cpu_usage),
                "min": min(cpu_usage)
            },
            "memory_usage": {
                "mean": sum(memory_usage) / len(memory_usage),
                "max": max(memory_usage),
                "min": min(memory_usage)
            },
            "samples": len(self.data)
        }
    
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent()
                memory = psutil.virtual_memory()
                
                data_point = {
                    "timestamp": time.time() - self.start_time,
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory.percent,
                    "memory_used_gb": memory.used / (1024**3)
                }
                
                if torch.cuda.is_available():
                    data_point["gpu_memory_allocated"] = torch.cuda.memory_allocated() / (1024**3)
                    data_point["gpu_memory_reserved"] = torch.cuda.memory_reserved() / (1024**3)
                
                self.data.append(data_point)
                time.sleep(0.5)  # 0.5ç§’é–“éš”
                
            except Exception:
                break

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="Infer-OS å¤§è¦æ¨¡LLMãƒ‡ãƒ¢")
    parser.add_argument("--model", default="openai/gpt-oss-120b", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--prompt", default="The future of artificial intelligence is", help="ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-length", type=int, default=200, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--use-4bit", action="store_true", help="4bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    print(f"""
{'='*80}
ğŸš€ Infer-OS å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ - {args.model}
{'='*80}

ã“ã®ãƒ‡ãƒ¢ã§ã¯å¤§è¦æ¨¡LLMãƒ¢ãƒ‡ãƒ«ï¼ˆ120B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã§Infer-OSæœ€é©åŒ–åŠ¹æœã‚’ä½“é¨“ã§ãã¾ã™ã€‚

å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {args.model}
æœ€é©åŒ–æŠ€è¡“:
- Enhanced IOBinding (ãƒ¡ãƒ¢ãƒªå†åˆ©ç”¨æœ€é©åŒ–)
- KVæ®µéšçš„é‡å­åŒ– (85%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›)
- ã‚¹ãƒšã‚­ãƒ¥ãƒ¬ã‚¤ãƒ†ã‚£ãƒ–ç”Ÿæˆ (æ¨è«–åŠ¹ç‡å‘ä¸Š)
- Flash Attention (æ³¨æ„æ©Ÿæ§‹æœ€é©åŒ–)
- ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)
- CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ (å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ)

{'='*80}
""")
    
    try:
        # ãƒ‡ãƒ¢åˆæœŸåŒ–
        demo = LargeLLMInferOSDemo(args.model)
        
        if args.use_4bit:
            demo.use_4bit = True
            demo.use_8bit = False
            print("ğŸ”§ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print("ğŸ“¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print("âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
        
        if not demo.load_model_with_optimization():
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        if args.interactive:
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
            print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
            print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰:")
            
            while True:
                try:
                    prompt = input("\n> ").strip()
                    if prompt.lower() in ['quit', 'exit', 'q']:
                        break
                    if not prompt:
                        continue
                    
                    result = demo.run_comparison_demo(prompt, args.max_length)
                    if result:
                        demo.save_large_model_results(result)
                        
                except KeyboardInterrupt:
                    break
        else:
            # å˜ç™ºå®Ÿè¡Œ
            result = demo.run_comparison_demo(args.prompt, args.max_length)
            if result:
                demo.save_large_model_results(result)
        
        print("\nğŸ‰ å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")

if __name__ == "__main__":
    main()

