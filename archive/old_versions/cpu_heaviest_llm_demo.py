#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ‹ï¸ CPUç’°å¢ƒæœ€å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢

CPUç’°å¢ƒã§å‹•ä½œã™ã‚‹æœ€ã‚‚é‡ã„LLMãƒ¢ãƒ‡ãƒ«ï¼ˆEleutherAI/gpt-neox-20bï¼‰ã§ã®
Infer-OSæœ€é©åŒ–åŠ¹æœã‚’å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã§ä½“é¨“

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
- EleutherAI/gpt-neox-20b (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - æœ€é‡é‡ç´š
- EleutherAI/gpt-j-6B (6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - é‡é‡ç´š
- bigscience/bloom-7b1 (7.1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - é‡é‡ç´š
- ãã®ä»–CPUå¯¾å¿œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«

ç‰¹å¾´:
- CPUå°‚ç”¨æœ€é©åŒ–
- å¤§å®¹é‡ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
- æ®µéšçš„é‡å­åŒ–å¯¾å¿œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–

ä½¿ç”¨æ–¹æ³•:
    python cpu_heaviest_llm_demo.py --model EleutherAI/gpt-neox-20b --use-8bit --interactive
"""

import sys
import os
import gc
import time
import json
import psutil
import argparse
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import traceback

try:
    import torch
    import torch.nn as nn
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
    import numpy as np
    
    # æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    try:
        from accelerate import init_empty_weights, load_checkpoint_and_dispatch
        from accelerate.utils import get_balanced_memory
        ACCELERATE_AVAILABLE = True
    except ImportError:
        ACCELERATE_AVAILABLE = False
    
    try:
        from bitsandbytes import BitsAndBytesConfig
        BITSANDBYTES_AVAILABLE = True
    except ImportError:
        BITSANDBYTES_AVAILABLE = False
        
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers accelerate bitsandbytes numpy psutil")
    sys.exit(1)

# CPUå¯¾å¿œæœ€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å®šç¾©
CPU_HEAVY_MODELS = {
    "EleutherAI/gpt-neox-20b": {
        "parameters": 20_554_568_704,
        "size_gb": {"fp32": 80, "fp16": 40, "int8": 20, "int4": 10},
        "min_memory_gb": 64,
        "recommended_memory_gb": 128,
        "description": "æœ€é‡é‡ç´š 20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ GPT-NeoX",
        "rank": 1
    },
    "bigscience/bloom-7b1": {
        "parameters": 7_069_016_064,
        "size_gb": {"fp32": 28, "fp16": 14, "int8": 7, "int4": 3.5},
        "min_memory_gb": 32,
        "recommended_memory_gb": 64,
        "description": "é‡é‡ç´š 7.1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ BLOOM",
        "rank": 2
    },
    "EleutherAI/gpt-j-6B": {
        "parameters": 6_053_381_344,
        "size_gb": {"fp32": 24, "fp16": 12, "int8": 6, "int4": 3},
        "min_memory_gb": 32,
        "recommended_memory_gb": 48,
        "description": "é‡é‡ç´š 6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ GPT-J",
        "rank": 3
    },
    "microsoft/DialoGPT-large": {
        "parameters": 774_030_080,
        "size_gb": {"fp32": 3, "fp16": 1.5, "int8": 0.8, "int4": 0.4},
        "min_memory_gb": 8,
        "recommended_memory_gb": 16,
        "description": "ä¸­é‡ç´š 774Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ DialoGPT",
        "rank": 4
    }
}

class CPUHeaviestLLMDemo:
    """CPUç’°å¢ƒæœ€å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str, use_4bit: bool = False, use_8bit: bool = False):
        self.model_name = model_name
        self.use_4bit = use_4bit
        self.use_8bit = use_8bit
        self.device = torch.device("cpu")  # CPUå°‚ç”¨
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.model = None
        self.tokenizer = None
        
        # æœ€é©åŒ–çŠ¶æ…‹
        self.optimization_applied = False
        self.quantization_info = {}
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.system_info = self._get_system_info()
        
        print(f"ğŸ‹ï¸ CPUç’°å¢ƒæœ€å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
        print(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {model_name}")
        self._print_system_info()
        self._validate_system_requirements()
    
    def _get_system_info(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        memory = psutil.virtual_memory()
        
        info = {
            "python_version": sys.version,
            "torch_version": torch.__version__,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_used_gb": memory.used / (1024**3),
            "memory_percent": memory.percent,
        }
        
        # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¯¾å¿œçŠ¶æ³
        info.update({
            "accelerate_available": ACCELERATE_AVAILABLE,
            "bitsandbytes_available": BITSANDBYTES_AVAILABLE,
        })
        
        return info
    
    def _print_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {sys.version.split()[0]}")
        print(f"  PyTorch: {torch.__version__}")
        print(f"  CPU: {self.system_info['cpu_count']}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {self.system_info['memory_total_gb']:.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {self.system_info['memory_used_gb']:.1f}GB ({self.system_info['memory_percent']:.1f}%)")
        print(f"  åˆ©ç”¨å¯èƒ½: {self.system_info['memory_available_gb']:.1f}GB")
        
        print(f"\nğŸ”§ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
        print(f"  Accelerate: {'âœ…' if ACCELERATE_AVAILABLE else 'âŒ'}")
        print(f"  BitsAndBytes: {'âœ…' if BITSANDBYTES_AVAILABLE else 'âŒ'}")
    
    def _validate_system_requirements(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æ¤œè¨¼"""
        if self.model_name in CPU_HEAVY_MODELS:
            model_info = CPU_HEAVY_MODELS[self.model_name]
            min_memory = model_info["min_memory_gb"]
            recommended_memory = model_info["recommended_memory_gb"]
            
            print(f"\nğŸ‹ï¸ ãƒ¢ãƒ‡ãƒ«è¦ä»¶:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {model_info['description']}")
            print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['parameters']:,}")
            print(f"  æœ€å°ãƒ¡ãƒ¢ãƒª: {min_memory}GB")
            print(f"  æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {recommended_memory}GB")
            
            # é‡å­åŒ–é©ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶
            if self.use_4bit:
                required_memory = model_info["size_gb"]["int4"]
                print(f"  INT4é‡å­åŒ–æ™‚: {required_memory}GB")
            elif self.use_8bit:
                required_memory = model_info["size_gb"]["int8"]
                print(f"  INT8é‡å­åŒ–æ™‚: {required_memory}GB")
            else:
                required_memory = model_info["size_gb"]["fp16"]
                print(f"  FP16æ™‚: {required_memory}GB")
            
            # ãƒ¡ãƒ¢ãƒªå……è¶³æ€§ãƒã‚§ãƒƒã‚¯
            available_memory = self.system_info['memory_available_gb']
            
            if available_memory < required_memory:
                print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                print(f"  å¿…è¦: {required_memory}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
                print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆ--use-8bit ã¾ãŸã¯ --use-4bitï¼‰ã®ä½¿ç”¨ã‚’æ¨å¥¨")
            elif available_memory < recommended_memory:
                print(f"âš ï¸  æ¨å¥¨ãƒ¡ãƒ¢ãƒªæœªæº€ã§ã™")
                print(f"  æ¨å¥¨: {recommended_memory}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
                print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šæ€§å‘ä¸Š")
            else:
                print(f"âœ… ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™")
    
    def list_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
        print(f"\nğŸ‹ï¸ CPUå¯¾å¿œæœ€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        
        sorted_models = sorted(CPU_HEAVY_MODELS.items(), key=lambda x: x[1]["rank"])
        
        for model_name, info in sorted_models:
            rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ…"][info["rank"] - 1] if info["rank"] <= 4 else "ğŸ“‹"
            print(f"  {rank_emoji} {model_name}")
            print(f"    {info['description']}")
            print(f"    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {info['parameters']:,}")
            print(f"    æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {info['recommended_memory_gb']}GB")
            print()
    
    def create_quantization_config(self) -> Optional[Any]:
        """é‡å­åŒ–è¨­å®šã‚’ä½œæˆï¼ˆCPUå¯¾å¿œç‰ˆï¼‰"""
        if not BITSANDBYTES_AVAILABLE:
            print("âš ï¸ BitsAndBytesæœªå¯¾å¿œã®ãŸã‚ã€é‡å­åŒ–ç„¡ã—ã§å®Ÿè¡Œã—ã¾ã™")
            return None
        
        try:
            if self.use_4bit:
                print("ğŸ”§ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸï¼ˆCPUæœ€é©åŒ–ï¼‰")
                config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float32,  # CPUç”¨
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    llm_int8_enable_fp32_cpu_offload=True
                )
                return config
            elif self.use_8bit:
                print("ğŸ”§ 8bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸï¼ˆCPUæœ€é©åŒ–ï¼‰")
                config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True
                )
                return config
        except Exception as e:
            print(f"âš ï¸ é‡å­åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            return None
        
        return None
    
    def load_model_with_optimization(self) -> bool:
        """æœ€é©åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("ğŸ“¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print("âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–é–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory:.1f}GB")
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = self.create_quantization_config()
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰è¨­å®šï¼ˆCPUæœ€é©åŒ–ï¼‰
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float32,  # CPUç”¨
                "low_cpu_mem_usage": True,
                "device_map": "cpu",
            }
            
            # é‡å­åŒ–è¨­å®šã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
            if quantization_config is not None:
                try:
                    model_kwargs["quantization_config"] = quantization_config
                except Exception as e:
                    print(f"âš ï¸ é‡å­åŒ–è¨­å®šé©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
                    print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            
            print(f"ğŸ“¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆæ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
            except Exception as e:
                print(f"âš ï¸ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ’¡ åŸºæœ¬è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: é‡å­åŒ–ç„¡ã—
                basic_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float32,
                    "low_cpu_mem_usage": True,
                }
                
                try:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        **basic_kwargs
                    )
                except Exception as e2:
                    print(f"âš ï¸ åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e2}")
                    print("ğŸ’¡ æœ€å°è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2: æœ€å°è¨­å®š
                    minimal_kwargs = {
                        "trust_remote_code": True,
                    }
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        **minimal_kwargs
                    )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            # CPUæœ€é©åŒ–é©ç”¨
            self._apply_cpu_optimizations()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–çµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            memory_used = final_memory - initial_memory
            
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.1f}GB")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            return False
    
    def _apply_cpu_optimizations(self):
        """CPUå°‚ç”¨æœ€é©åŒ–ã‚’é©ç”¨"""
        try:
            print("ğŸ”§ CPUå°‚ç”¨æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
            
            # CPUæœ€é©åŒ–è¨­å®š
            torch.set_num_threads(psutil.cpu_count())
            print(f"  âœ… CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š: {psutil.cpu_count()}")
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                print("  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = True
                print("  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–")
            
            # CPUå°‚ç”¨æœ€é©åŒ–
            try:
                # JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
                if hasattr(torch.jit, 'optimize_for_inference'):
                    self.model = torch.jit.optimize_for_inference(self.model)
                    print("  âœ… JITæœ€é©åŒ–é©ç”¨")
            except:
                pass
            
            self.optimization_applied = True
            print("ğŸš€ CPUå°‚ç”¨æœ€é©åŒ–é©ç”¨å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ CPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_text(self, prompt: str, max_length: int = 200) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆCPUæœ€é©åŒ–ç‰ˆï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰"}
        
        try:
            print(f"\nğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            print(f"æœ€å¤§é•·: {max_length}")
            
            # ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            initial_cpu = psutil.cpu_percent(interval=None)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ç”Ÿæˆè¨­å®šï¼ˆCPUæœ€é©åŒ–ï¼‰
            generation_config = {
                "max_length": max_length,
                "num_return_sequences": 1,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
            }
            
            # ç”Ÿæˆå®Ÿè¡Œï¼ˆæ™‚é–“ãƒ»ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®šï¼‰
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
            generated_only = generated_text[len(prompt):].strip()
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            final_cpu = psutil.cpu_percent(interval=None)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            total_tokens = len(outputs[0])
            
            # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            result = {
                "prompt": prompt,
                "generated_text": generated_only,
                "full_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "resource_usage": {
                    "memory_used_gb": final_memory - initial_memory,
                    "memory_total_gb": final_memory,
                    "cpu_usage_percent": final_cpu
                },
                "optimization_applied": self.optimization_applied,
                "quantization_info": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit
                }
            }
            
            self._print_generation_results(result)
            return result
            
        except Exception as e:
            error_msg = f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg, "traceback": traceback.format_exc()}
    
    def _print_generation_results(self, result: Dict):
        """ç”Ÿæˆçµæœã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ç”Ÿæˆçµæœ:")
        print(f"  ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
        print(f"  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['input_tokens']}")
        print(f"  å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['output_tokens']}")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result['tokens_per_second']:.1f} tokens/sec")
        
        print(f"\nğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡:")
        print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {result['resource_usage']['memory_used_gb']:.1f}GB")
        print(f"  ç·ãƒ¡ãƒ¢ãƒª: {result['resource_usage']['memory_total_gb']:.1f}GB")
        print(f"  CPUä½¿ç”¨ç‡: {result['resource_usage']['cpu_usage_percent']:.1f}%")
        
        print(f"\nğŸ”§ æœ€é©åŒ–çŠ¶æ…‹:")
        print(f"  CPUæœ€é©åŒ–: {'âœ…' if result['optimization_applied'] else 'âŒ'}")
        print(f"  4bité‡å­åŒ–: {'âœ…' if result['quantization_info']['use_4bit'] else 'âŒ'}")
        print(f"  8bité‡å­åŒ–: {'âœ…' if result['quantization_info']['use_8bit'] else 'âŒ'}")
        
        print(f"\nğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ:")
        print(f"  \"{result['generated_text'][:200]}{'...' if len(result['generated_text']) > 200 else ''}\"")
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print(f"\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰:")
        
        results = []
        
        while True:
            try:
                prompt = input("\n> ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not prompt:
                    continue
                
                result = self.generate_text(prompt)
                if "error" not in result:
                    results.append(result)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœä¿å­˜
        if results:
            self._save_session_results(results)
    
    def _save_session_results(self, results: List[Dict]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_safe_name = self.model_name.replace("/", "_").replace("-", "_")
            filename = f'cpu_heaviest_llm_session_{model_safe_name}_{timestamp}.json'
            
            session_data = {
                "model_name": self.model_name,
                "timestamp": datetime.now().isoformat(),
                "system_info": self.system_info,
                "optimization_config": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit,
                    "optimization_applied": self.optimization_applied
                },
                "results": results,
                "summary": self._calculate_session_summary(results)
            }
            
            os.makedirs('demo_results', exist_ok=True)
            filepath = os.path.join('demo_results', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_session_summary(self, results: List[Dict]) -> Dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—"""
        if not results:
            return {}
        
        generation_times = [r['generation_time'] for r in results]
        tokens_per_second = [r['tokens_per_second'] for r in results]
        output_tokens = [r['output_tokens'] for r in results]
        memory_used = [r['resource_usage']['memory_used_gb'] for r in results]
        
        return {
            "total_generations": len(results),
            "avg_generation_time": sum(generation_times) / len(generation_times),
            "avg_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second),
            "total_output_tokens": sum(output_tokens),
            "avg_memory_used_gb": sum(memory_used) / len(memory_used),
            "min_generation_time": min(generation_times),
            "max_generation_time": max(generation_times)
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="CPUç’°å¢ƒæœ€å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
    parser.add_argument("--model", default="EleutherAI/gpt-neox-20b", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--prompt", help="ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-length", type=int, default=200, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--use-4bit", action="store_true", help="4bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-8bit", action="store_true", help="8bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--list-models", action="store_true", help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º")
    
    args = parser.parse_args()
    
    if args.list_models:
        demo = CPUHeaviestLLMDemo("dummy", False, False)
        demo.list_available_models()
        return
    
    print(f"""
{'='*80}
ğŸ‹ï¸ CPUç’°å¢ƒæœ€å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢
{'='*80}

å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {args.model}
æœ€é©åŒ–è¨­å®š:
  4bité‡å­åŒ–: {'âœ…' if args.use_4bit else 'âŒ'}
  8bité‡å­åŒ–: {'âœ…' if args.use_8bit else 'âŒ'}
  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–: {'âœ…' if args.interactive else 'âŒ'}

{'='*80}
""")
    
    try:
        # ãƒ‡ãƒ¢åˆæœŸåŒ–
        demo = CPUHeaviestLLMDemo(
            model_name=args.model,
            use_4bit=args.use_4bit,
            use_8bit=args.use_8bit
        )
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if not demo.load_model_with_optimization():
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
        if args.interactive:
            demo.interactive_mode()
        else:
            prompt = args.prompt or "The future of artificial intelligence is"
            result = demo.generate_text(prompt, args.max_length)
            
            if "error" in result:
                print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print("\nğŸ‰ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")

if __name__ == "__main__":
    main()

