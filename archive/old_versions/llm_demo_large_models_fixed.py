#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¤– å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰

openai/gpt-oss-120bç­‰ã®å¤§è¦æ¨¡LLMãƒ¢ãƒ‡ãƒ«ï¼ˆ120B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã§ã®
Infer-OSæœ€é©åŒ–åŠ¹æœã‚’å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã§ä½“é¨“

ä¿®æ­£å†…å®¹:
- BitsAndBytesConfigäº’æ›æ€§ã‚¨ãƒ©ãƒ¼ä¿®æ­£
- transformers/bitsandbyteãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½è¿½åŠ 

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
- openai/gpt-oss-120b (120Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
- EleutherAI/gpt-neox-20b (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
- microsoft/DialoGPT-large (774Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
- ãã®ä»–å¤§è¦æ¨¡Transformerãƒ¢ãƒ‡ãƒ«

ä½¿ç”¨æ–¹æ³•:
    python llm_demo_large_models_fixed.py --model openai/gpt-oss-120b --use-4bit --interactive
"""

import sys
import os
import gc
import time
import json
import psutil
import argparse
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import traceback

try:
    import torch
    import torch.nn as nn
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
    import numpy as np
    
    # æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    try:
        from accelerate import init_empty_weights, load_checkpoint_and_dispatch
        from accelerate.utils import get_balanced_memory
        ACCELERATE_AVAILABLE = True
    except ImportError:
        ACCELERATE_AVAILABLE = False
    
    try:
        from bitsandbytes import BitsAndBytesConfig
        BITSANDBYTES_AVAILABLE = True
    except ImportError:
        BITSANDBYTES_AVAILABLE = False
        
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers accelerate bitsandbytes numpy psutil")
    sys.exit(1)

class LargeLLMDemo:
    """å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, model_name: str, use_4bit: bool = False, use_8bit: bool = False):
        self.model_name = model_name
        self.use_4bit = use_4bit
        self.use_8bit = use_8bit
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.model = None
        self.tokenizer = None
        
        # æœ€é©åŒ–çŠ¶æ…‹
        self.optimization_applied = False
        self.quantization_info = {}
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.system_info = self._get_system_info()
        
        print(f"ğŸ¤– å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {model_name}")
        self._print_system_info()
    
    def _get_system_info(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        info = {
            "python_version": sys.version,
            "torch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "memory_available_gb": psutil.virtual_memory().available / (1024**3),
        }
        
        if torch.cuda.is_available():
            info.update({
                "gpu_count": torch.cuda.device_count(),
                "gpu_name": torch.cuda.get_device_name(0),
                "gpu_memory_total_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3),
                "cuda_version": torch.version.cuda,
            })
        
        # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¯¾å¿œçŠ¶æ³
        info.update({
            "accelerate_available": ACCELERATE_AVAILABLE,
            "bitsandbytes_available": BITSANDBYTES_AVAILABLE,
        })
        
        return info
    
    def _print_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {sys.version.split()[0]}")
        print(f"  PyTorch: {torch.__version__}")
        print(f"  CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        print(f"  CPU: {self.system_info['cpu_count']}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {self.system_info['memory_total_gb']:.1f}GB")
        
        if torch.cuda.is_available():
            print(f"  GPU: {self.system_info['gpu_name']}")
            print(f"  GPU ãƒ¡ãƒ¢ãƒª: {self.system_info['gpu_memory_total_gb']:.1f}GB")
        
        print(f"\nğŸ”§ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
        print(f"  Accelerate: {'âœ…' if ACCELERATE_AVAILABLE else 'âŒ'}")
        print(f"  BitsAndBytes: {'âœ…' if BITSANDBYTES_AVAILABLE else 'âŒ'}")
    
    def estimate_model_requirements(self) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«è¦ä»¶ã‚’æ¨å®š"""
        try:
            print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã®è¦ä»¶ã‚’æ¨å®šä¸­...")
            
            config = AutoConfig.from_pretrained(self.model_name, trust_remote_code=True)
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¨å®š
            params = self._estimate_parameters(config)
            
            # ãƒ¡ãƒ¢ãƒªè¦ä»¶æ¨å®š
            memory_requirements = self._estimate_memory_requirements(params)
            
            print(f"æ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {params:,}")
            print(f"æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
            print(f"  FP16: {memory_requirements['fp16_gb']:.1f}GB")
            print(f"  INT8: {memory_requirements['int8_gb']:.1f}GB")
            print(f"  INT4: {memory_requirements['int4_gb']:.1f}GB")
            print(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {self.system_info['memory_total_gb']:.1f}GB")
            
            return {
                "parameters": params,
                "memory_requirements": memory_requirements
            }
            
        except Exception as e:
            print(f"âŒ è¦ä»¶æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _estimate_parameters(self, config) -> int:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ¨å®š"""
        try:
            # è¨­å®šã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦è¡Œ
            if hasattr(config, 'n_parameters'):
                return config.n_parameters
            
            # æ¨å®šè¨ˆç®—
            hidden_size = getattr(config, 'hidden_size', getattr(config, 'd_model', 4096))
            n_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', 24))
            vocab_size = getattr(config, 'vocab_size', 50257)
            
            # Transformer ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
            attention_params = n_layers * 4 * hidden_size * hidden_size
            ffn_intermediate = getattr(config, 'intermediate_size', 4 * hidden_size)
            ffn_params = n_layers * 2 * hidden_size * ffn_intermediate
            embedding_params = vocab_size * hidden_size
            other_params = n_layers * hidden_size * 4
            
            total_params = attention_params + ffn_params + embedding_params + other_params
            return total_params
            
        except Exception as e:
            print(f"âš ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 4_000_000_000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _estimate_memory_requirements(self, params: int) -> Dict:
        """ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’æ¨å®š"""
        # å„ç²¾åº¦ã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆãƒã‚¤ãƒˆï¼‰
        fp16_memory = params * 2
        int8_memory = params * 1
        int4_memory = params * 0.5
        
        # æ¨è«–æ™‚ã®è¿½åŠ ãƒ¡ãƒ¢ãƒªï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ç­‰ï¼‰
        inference_overhead = 0.5
        
        return {
            "fp16_gb": fp16_memory * (1 + inference_overhead) / (1024**3),
            "int8_gb": int8_memory * (1 + inference_overhead) / (1024**3),
            "int4_gb": int4_memory * (1 + inference_overhead) / (1024**3)
        }
    
    def create_quantization_config(self) -> Optional[Any]:
        """é‡å­åŒ–è¨­å®šã‚’ä½œæˆï¼ˆäº’æ›æ€§å¯¾å¿œç‰ˆï¼‰"""
        if not BITSANDBYTES_AVAILABLE:
            return None
        
        try:
            if self.use_4bit:
                print("ğŸ”§ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
                # æ–°ã—ã„BitsAndBytesConfigã®å½¢å¼ã«å¯¾å¿œ
                config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                return config
            elif self.use_8bit:
                print("ğŸ”§ 8bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
                config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True
                )
                return config
        except Exception as e:
            print(f"âš ï¸ é‡å­åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            return None
        
        return None
    
    def load_model_with_optimization(self) -> bool:
        """æœ€é©åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("ğŸ“¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print("âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = self.create_quantization_config()
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰è¨­å®š
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16,
                "low_cpu_mem_usage": True,
            }
            
            # é‡å­åŒ–è¨­å®šã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
            if quantization_config is not None:
                try:
                    model_kwargs["quantization_config"] = quantization_config
                except Exception as e:
                    print(f"âš ï¸ é‡å­åŒ–è¨­å®šé©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
                    print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            
            # ãƒ‡ãƒã‚¤ã‚¹é…ç½®ï¼ˆAccelerateåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
            if ACCELERATE_AVAILABLE:
                model_kwargs["device_map"] = "auto"
            
            print(f"ğŸ“¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
            except Exception as e:
                print(f"âš ï¸ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ’¡ åŸºæœ¬è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬è¨­å®šã§ãƒ­ãƒ¼ãƒ‰
                basic_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                }
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **basic_kwargs
                )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            # æœ€é©åŒ–é©ç”¨
            self._apply_runtime_optimizations()
            
            print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            return False
    
    def _apply_runtime_optimizations(self):
        """å®Ÿè¡Œæ™‚æœ€é©åŒ–ã‚’é©ç”¨"""
        try:
            print("ğŸ”§ å®Ÿè¡Œæ™‚æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
            
            # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                print("  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = True
                print("  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–")
            
            # Flash Attentionï¼ˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
            try:
                if hasattr(self.model.config, 'use_flash_attention_2'):
                    self.model.config.use_flash_attention_2 = True
                    print("  âœ… Flash Attention 2 æœ‰åŠ¹åŒ–")
            except:
                pass
            
            self.optimization_applied = True
            print("ğŸš€ å®Ÿè¡Œæ™‚æœ€é©åŒ–é©ç”¨å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ å®Ÿè¡Œæ™‚æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_text(self, prompt: str, max_length: int = 200) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæœ€é©åŒ–åŠ¹æœæ¸¬å®šä»˜ãï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰"}
        
        try:
            print(f"\nğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            print(f"æœ€å¤§é•·: {max_length}")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            initial_memory = self._get_memory_usage()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            if torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®š
            generation_config = {
                "max_length": max_length,
                "num_return_sequences": 1,
                "temperature": 0.7,
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            # ç”Ÿæˆå®Ÿè¡Œï¼ˆæ™‚é–“æ¸¬å®šï¼‰
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
            generated_only = generated_text[len(prompt):].strip()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            final_memory = self._get_memory_usage()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            total_tokens = len(outputs[0])
            
            # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            result = {
                "prompt": prompt,
                "generated_text": generated_only,
                "full_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "initial_memory": initial_memory,
                "final_memory": final_memory,
                "memory_used": {
                    "system_mb": final_memory["system_mb"] - initial_memory["system_mb"],
                    "gpu_mb": final_memory.get("gpu_mb", 0) - initial_memory.get("gpu_mb", 0)
                },
                "optimization_applied": self.optimization_applied,
                "quantization_info": self.quantization_info
            }
            
            self._print_generation_results(result)
            return result
            
        except Exception as e:
            error_msg = f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg, "traceback": traceback.format_exc()}
    
    def _get_memory_usage(self) -> Dict:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        memory_info = {
            "system_mb": psutil.virtual_memory().used / (1024**2)
        }
        
        if torch.cuda.is_available():
            memory_info["gpu_mb"] = torch.cuda.memory_allocated() / (1024**2)
            memory_info["gpu_reserved_mb"] = torch.cuda.memory_reserved() / (1024**2)
        
        return memory_info
    
    def _print_generation_results(self, result: Dict):
        """ç”Ÿæˆçµæœã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ç”Ÿæˆçµæœ:")
        print(f"  ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
        print(f"  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['input_tokens']}")
        print(f"  å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['output_tokens']}")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result['tokens_per_second']:.1f} tokens/sec")
        
        print(f"\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
        print(f"  ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {result['memory_used']['system_mb']:.1f}MB")
        if torch.cuda.is_available():
            print(f"  GPU ãƒ¡ãƒ¢ãƒª: {result['memory_used']['gpu_mb']:.1f}MB")
        
        print(f"\nğŸ”§ æœ€é©åŒ–çŠ¶æ…‹:")
        print(f"  æœ€é©åŒ–é©ç”¨: {'âœ…' if result['optimization_applied'] else 'âŒ'}")
        print(f"  é‡å­åŒ–: {'âœ…' if self.use_4bit or self.use_8bit else 'âŒ'}")
        
        print(f"\nğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ:")
        print(f"  \"{result['generated_text'][:200]}{'...' if len(result['generated_text']) > 200 else ''}\"")
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print(f"\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰:")
        
        results = []
        
        while True:
            try:
                prompt = input("\n> ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not prompt:
                    continue
                
                result = self.generate_text(prompt)
                if "error" not in result:
                    results.append(result)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœä¿å­˜
        if results:
            self._save_session_results(results)
    
    def _save_session_results(self, results: List[Dict]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_safe_name = self.model_name.replace("/", "_").replace("-", "_")
            filename = f'llm_demo_session_{model_safe_name}_{timestamp}.json'
            
            session_data = {
                "model_name": self.model_name,
                "timestamp": datetime.now().isoformat(),
                "system_info": self.system_info,
                "optimization_config": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit,
                    "optimization_applied": self.optimization_applied
                },
                "results": results,
                "summary": self._calculate_session_summary(results)
            }
            
            os.makedirs('demo_results', exist_ok=True)
            filepath = os.path.join('demo_results', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_session_summary(self, results: List[Dict]) -> Dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—"""
        if not results:
            return {}
        
        generation_times = [r['generation_time'] for r in results]
        tokens_per_second = [r['tokens_per_second'] for r in results]
        output_tokens = [r['output_tokens'] for r in results]
        
        return {
            "total_generations": len(results),
            "avg_generation_time": sum(generation_times) / len(generation_times),
            "avg_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second),
            "total_output_tokens": sum(output_tokens),
            "min_generation_time": min(generation_times),
            "max_generation_time": max(generation_times)
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    parser.add_argument("--model", default="openai/gpt-oss-120b", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--prompt", help="ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-length", type=int, default=200, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--use-4bit", action="store_true", help="4bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-8bit", action="store_true", help="8bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    print(f"""
{'='*80}
ğŸ¤– å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰
{'='*80}

å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {args.model}
æœ€é©åŒ–è¨­å®š:
  4bité‡å­åŒ–: {'âœ…' if args.use_4bit else 'âŒ'}
  8bité‡å­åŒ–: {'âœ…' if args.use_8bit else 'âŒ'}
  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–: {'âœ…' if args.interactive else 'âŒ'}

{'='*80}
""")
    
    try:
        # ãƒ‡ãƒ¢åˆæœŸåŒ–
        demo = LargeLLMDemo(
            model_name=args.model,
            use_4bit=args.use_4bit,
            use_8bit=args.use_8bit
        )
        
        # è¦ä»¶æ¨å®š
        requirements = demo.estimate_model_requirements()
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if not demo.load_model_with_optimization():
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
        if args.interactive:
            demo.interactive_mode()
        else:
            prompt = args.prompt or "The future of artificial intelligence is"
            result = demo.generate_text(prompt, args.max_length)
            
            if "error" in result:
                print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print("\nğŸ‰ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"è©³ç´°: {traceback.format_exc()}")

if __name__ == "__main__":
    main()

