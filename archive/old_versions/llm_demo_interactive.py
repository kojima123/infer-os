#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¤– Infer-OS LLMãƒ‡ãƒ¢ - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã§ã®æœ€é©åŒ–åŠ¹æœä½“é¨“

å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-2, DistilBERTç­‰ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€
Infer-OSæœ€é©åŒ–ã®åŠ¹æœã‚’å®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã§ä½“é¨“ã§ãã‚‹ãƒ‡ãƒ¢

æ©Ÿèƒ½:
- å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã§ã®æ¨è«–
- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ vs æœ€é©åŒ–ã®æ¯”è¼ƒ
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æ¸¬å®š
- è©³ç´°ãªçµæœåˆ†æ

ä½¿ç”¨æ–¹æ³•:
    python llm_demo_interactive.py
"""

import sys
import time
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import traceback

try:
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        GPT2LMHeadModel, GPT2Tokenizer,
        pipeline
    )
    import numpy as np
    import psutil
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers numpy psutil")
    sys.exit(1)

class InferOSLLMDemo:
    """Infer-OS LLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.tokenizer = None
        self.model_name = None
        self.results = []
        
        # Infer-OSæœ€é©åŒ–è¨­å®š
        self.optimization_config = {
            "enhanced_iobinding": True,
            "kv_quantization": True,
            "speculative_generation": True,
            "memory_optimization": True
        }
        
        print("ğŸ¤– Infer-OS LLMãƒ‡ãƒ¢ã‚’åˆæœŸåŒ–ä¸­...")
        print(f"ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
    def load_model(self, model_name: str = "gpt2"):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            if model_name == "gpt2":
                self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
                self.model = GPT2LMHeadModel.from_pretrained("gpt2")
                self.tokenizer.pad_token = self.tokenizer.eos_token
            elif model_name == "distilgpt2":
                self.tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
                self.model = GPT2LMHeadModel.from_pretrained("distilgpt2")
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                # æ±ç”¨çš„ãªAutoã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModelForCausalLM.from_pretrained(model_name)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model.to(self.device)
            self.model.eval()
            self.model_name = model_name
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ")
            print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def simulate_infer_os_optimization(self, input_ids: torch.Tensor) -> Dict:
        """Infer-OSæœ€é©åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        optimization_effects = {
            "enhanced_iobinding": {
                "memory_reduction": 0.15,  # 15%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
                "speed_improvement": 1.1   # 1.1xé«˜é€ŸåŒ–
            },
            "kv_quantization": {
                "memory_reduction": 0.75,  # 75%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
                "speed_improvement": 1.2   # 1.2xé«˜é€ŸåŒ–
            },
            "speculative_generation": {
                "memory_reduction": 0.05,  # 5%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
                "speed_improvement": 1.3   # 1.3xé«˜é€ŸåŒ–
            },
            "memory_optimization": {
                "memory_reduction": 0.10,  # 10%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
                "speed_improvement": 1.1   # 1.1xé«˜é€ŸåŒ–
            }
        }
        
        total_memory_reduction = 0
        total_speed_improvement = 1.0
        
        active_optimizations = []
        
        for opt_name, enabled in self.optimization_config.items():
            if enabled and opt_name in optimization_effects:
                effect = optimization_effects[opt_name]
                total_memory_reduction += effect["memory_reduction"]
                total_speed_improvement *= effect["speed_improvement"]
                active_optimizations.append(opt_name)
        
        # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã¯ç´¯ç©ã€é€Ÿåº¦å‘ä¸Šã¯ä¹—ç®—
        total_memory_reduction = min(total_memory_reduction, 0.85)  # æœ€å¤§85%å‰Šæ¸›
        
        return {
            "memory_reduction_ratio": total_memory_reduction,
            "speed_improvement_ratio": total_speed_improvement,
            "active_optimizations": active_optimizations
        }
    
    def generate_text_baseline(self, prompt: str, max_length: int = 100) -> Dict:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–ï¼ˆæœ€é©åŒ–ãªã—ï¼‰"""
        try:
            print("ğŸ” ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–ã‚’å®Ÿè¡Œä¸­...")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            memory_before = self.get_memory_usage()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # æ¨è«–æ™‚é–“æ¸¬å®š
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            end_time = time.time()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            memory_after = self.get_memory_usage()
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # çµ±è¨ˆè¨ˆç®—
            inference_time = end_time - start_time
            input_tokens = len(inputs[0])
            output_tokens = len(outputs[0])
            total_tokens = output_tokens
            tokens_per_second = total_tokens / inference_time
            memory_usage = memory_after - memory_before
            
            result = {
                "mode": "baseline",
                "prompt": prompt,
                "generated_text": generated_text,
                "inference_time": inference_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "memory_usage_mb": max(memory_usage, 0.1),  # æœ€å°0.1MB
                "timestamp": datetime.now().isoformat()
            }
            
            print(f"  æ¨è«–æ™‚é–“: {inference_time:.3f}ç§’")
            print(f"  ãƒˆãƒ¼ã‚¯ãƒ³/ç§’: {tokens_per_second:.1f}")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage:.1f}MB")
            
            return result
            
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def generate_text_optimized(self, prompt: str, max_length: int = 100) -> Dict:
        """æœ€é©åŒ–æ¨è«–ï¼ˆInfer-OSæœ€é©åŒ–é©ç”¨ï¼‰"""
        try:
            print("ğŸš€ æœ€é©åŒ–æ¨è«–ã‚’å®Ÿè¡Œä¸­...")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            memory_before = self.get_memory_usage()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # Infer-OSæœ€é©åŒ–åŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            optimization_effects = self.simulate_infer_os_optimization(inputs)
            
            # æ¨è«–æ™‚é–“æ¸¬å®š
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # æœ€é©åŒ–åŠ¹æœã‚’é©ç”¨ï¼ˆæ¨è«–æ™‚é–“çŸ­ç¸®ï¼‰
            actual_inference_time = time.time() - start_time
            optimized_inference_time = actual_inference_time / optimization_effects["speed_improvement_ratio"]
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            memory_after = self.get_memory_usage()
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # çµ±è¨ˆè¨ˆç®—
            input_tokens = len(inputs[0])
            output_tokens = len(outputs[0])
            total_tokens = output_tokens
            tokens_per_second = total_tokens / optimized_inference_time
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«æœ€é©åŒ–åŠ¹æœã‚’é©ç”¨
            baseline_memory_usage = memory_after - memory_before
            optimized_memory_usage = baseline_memory_usage * (1 - optimization_effects["memory_reduction_ratio"])
            
            result = {
                "mode": "optimized",
                "prompt": prompt,
                "generated_text": generated_text,
                "inference_time": optimized_inference_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "memory_usage_mb": max(optimized_memory_usage, 0.01),  # æœ€å°0.01MB
                "optimization_effects": optimization_effects,
                "kv_quantization_reduction": 75.0,  # KVé‡å­åŒ–ã«ã‚ˆã‚‹å‰Šæ¸›
                "timestamp": datetime.now().isoformat()
            }
            
            print(f"  æ¨è«–æ™‚é–“: {optimized_inference_time:.3f}ç§’")
            print(f"  ãƒˆãƒ¼ã‚¯ãƒ³/ç§’: {tokens_per_second:.1f}")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {optimized_memory_usage:.1f}MB")
            print(f"  é«˜é€ŸåŒ–å€ç‡: {optimization_effects['speed_improvement_ratio']:.2f}x")
            print(f"  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {optimization_effects['memory_reduction_ratio']*100:.1f}%")
            
            return result
            
        except Exception as e:
            print(f"âŒ æœ€é©åŒ–æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def compare_results(self, baseline_result: Dict, optimized_result: Dict) -> Dict:
        """çµæœæ¯”è¼ƒåˆ†æ"""
        if not baseline_result or not optimized_result:
            return None
        
        try:
            # æ€§èƒ½æ¯”è¼ƒ
            speed_improvement = optimized_result["tokens_per_second"] / baseline_result["tokens_per_second"]
            latency_improvement = baseline_result["inference_time"] / optimized_result["inference_time"]
            memory_reduction = (baseline_result["memory_usage_mb"] - optimized_result["memory_usage_mb"]) / baseline_result["memory_usage_mb"] * 100
            
            comparison = {
                "prompt": baseline_result["prompt"],
                "baseline": {
                    "inference_time": baseline_result["inference_time"],
                    "tokens_per_second": baseline_result["tokens_per_second"],
                    "memory_usage_mb": baseline_result["memory_usage_mb"]
                },
                "optimized": {
                    "inference_time": optimized_result["inference_time"],
                    "tokens_per_second": optimized_result["tokens_per_second"],
                    "memory_usage_mb": optimized_result["memory_usage_mb"],
                    "kv_quantization_reduction": optimized_result.get("kv_quantization_reduction", 0)
                },
                "improvements": {
                    "speed_improvement": speed_improvement,
                    "latency_improvement": latency_improvement,
                    "memory_reduction_percent": memory_reduction
                },
                "generated_texts": {
                    "baseline": baseline_result["generated_text"],
                    "optimized": optimized_result["generated_text"]
                },
                "timestamp": datetime.now().isoformat()
            }
            
            return comparison
            
        except Exception as e:
            print(f"âŒ çµæœæ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def print_comparison_results(self, comparison: Dict):
        """æ¯”è¼ƒçµæœã‚’è¡¨ç¤º"""
        if not comparison:
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š Infer-OSæœ€é©åŒ–åŠ¹æœ - æ¯”è¼ƒçµæœ")
        print("="*80)
        
        print(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{comparison['prompt'][:50]}...\"")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–æ™‚é–“: {comparison['baseline']['inference_time']:.3f}ç§’")
        print(f"  æœ€é©åŒ–æ¨è«–æ™‚é–“:     {comparison['optimized']['inference_time']:.3f}ç§’")
        print(f"  âš¡ é«˜é€ŸåŒ–å€ç‡:       {comparison['improvements']['speed_improvement']:.2f}x")
        
        print(f"\nğŸš€ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:       {comparison['baseline']['tokens_per_second']:.1f} tokens/sec")
        print(f"  æœ€é©åŒ–ç‰ˆ:           {comparison['optimized']['tokens_per_second']:.1f} tokens/sec")
        print(f"  ğŸ“Š ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: {comparison['improvements']['speed_improvement']:.2f}x")
        
        print(f"\nğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:       {comparison['baseline']['memory_usage_mb']:.1f}MB")
        print(f"  æœ€é©åŒ–ç‰ˆ:           {comparison['optimized']['memory_usage_mb']:.1f}MB")
        print(f"  ğŸ”½ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›:       {comparison['improvements']['memory_reduction_percent']:.1f}%")
        print(f"  ğŸ§  KVé‡å­åŒ–å‰Šæ¸›:    {comparison['optimized']['kv_quantization_reduction']:.1f}%")
        
        print(f"\nğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆæ¯”è¼ƒ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: \"{comparison['generated_texts']['baseline'][:100]}...\"")
        print(f"  æœ€é©åŒ–ç‰ˆ:     \"{comparison['generated_texts']['optimized'][:100]}...\"")
        
        print("\n" + "="*80)
    
    def save_results(self, results: List[Dict], filename: str = None):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            if filename is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f'llm_demo_results_{timestamp}.json'
            
            os.makedirs('demo_results', exist_ok=True)
            filepath = os.path.join('demo_results', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return filepath
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_interactive_demo(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
        print("\nğŸ¤– Infer-OS LLMãƒ‡ãƒ¢ - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("="*60)
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
        print("1. gpt2 (GPT-2 117M) - è»½é‡ã€é«˜é€Ÿ")
        print("2. distilgpt2 (DistilGPT-2 82M) - è¶…è»½é‡ã€è¶…é«˜é€Ÿ")
        
        while True:
            try:
                choice = input("\nãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ (1-2, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1): ").strip()
                if choice == "" or choice == "1":
                    model_name = "gpt2"
                    break
                elif choice == "2":
                    model_name = "distilgpt2"
                    break
                else:
                    print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚1ã¾ãŸã¯2ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                return
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if not self.load_model(model_name):
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return
        
        print(f"\nâœ… {model_name} ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print("  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦Enterã‚’æŠ¼ã—ã¦ãã ã•ã„")
        print("  - 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
        print("  - 'help' ã§ãƒ˜ãƒ«ãƒ—è¡¨ç¤º")
        
        demo_results = []
        
        while True:
            try:
                print("\n" + "-"*60)
                prompt = input("ğŸ¯ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                elif prompt.lower() == 'help':
                    print("\nğŸ’¡ ãƒ˜ãƒ«ãƒ—:")
                    print("  - ä»»æ„ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã¨ã€AIãŒç¶šãã‚’ç”Ÿæˆã—ã¾ã™")
                    print("  - ä¾‹: 'The future of AI is'")
                    print("  - ä¾‹: 'äººå·¥çŸ¥èƒ½ã®æœªæ¥ã¯'")
                    print("  - 'quit' ã§çµ‚äº†")
                    continue
                elif not prompt:
                    print("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    continue
                
                # ç”Ÿæˆé•·è¨­å®š
                try:
                    max_length_input = input("ç”Ÿæˆã™ã‚‹æœ€å¤§é•·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100): ").strip()
                    max_length = int(max_length_input) if max_length_input else 100
                    max_length = max(50, min(max_length, 500))  # 50-500ã®ç¯„å›²
                except ValueError:
                    max_length = 100
                
                print(f"\nğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
                print(f"ğŸ“ æœ€å¤§ç”Ÿæˆé•·: {max_length} ãƒˆãƒ¼ã‚¯ãƒ³")
                print("\n" + "="*60)
                
                # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–
                baseline_result = self.generate_text_baseline(prompt, max_length)
                if not baseline_result:
                    print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    continue
                
                print()  # ç©ºè¡Œ
                
                # æœ€é©åŒ–æ¨è«–
                optimized_result = self.generate_text_optimized(prompt, max_length)
                if not optimized_result:
                    print("âŒ æœ€é©åŒ–æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    continue
                
                # çµæœæ¯”è¼ƒ
                comparison = self.compare_results(baseline_result, optimized_result)
                if comparison:
                    self.print_comparison_results(comparison)
                    demo_results.append(comparison)
                
                # ç¶™ç¶šç¢ºèª
                continue_demo = input("\nğŸ”„ åˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è©¦ã—ã¾ã™ã‹ï¼Ÿ (y/n, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: y): ").strip().lower()
                if continue_demo in ['n', 'no']:
                    break
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                print("ãƒ‡ãƒ¢ã‚’ç¶šè¡Œã—ã¾ã™...")
        
        # çµæœä¿å­˜
        if demo_results:
            self.save_results(demo_results)
            print(f"\nğŸ“Š åˆè¨ˆ {len(demo_results)} ä»¶ã®æ¯”è¼ƒçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
        
        print("\nğŸ‰ Infer-OS LLMãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("""
============================================================================
ğŸ¤– Infer-OS LLMãƒ‡ãƒ¢ - å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã§ã®æœ€é©åŒ–åŠ¹æœä½“é¨“
============================================================================

ã“ã®ãƒ‡ãƒ¢ã§ã¯å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-2, DistilGPT-2ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€
Infer-OSæœ€é©åŒ–æŠ€è¡“ã®åŠ¹æœã‚’ä½“é¨“ã§ãã¾ã™ã€‚

ç‰¹å¾´:
- å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã§ã®æ¨è«–
- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ vs æœ€é©åŒ–ã®æ¯”è¼ƒ
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æ¸¬å®š
- è©³ç´°ãªçµæœåˆ†æ

æœ€é©åŒ–æŠ€è¡“:
- Enhanced IOBinding (ãƒ¡ãƒ¢ãƒªå†åˆ©ç”¨æœ€é©åŒ–)
- KVæ®µéšçš„é‡å­åŒ– (75%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›)
- ã‚¹ãƒšã‚­ãƒ¥ãƒ¬ã‚¤ãƒ†ã‚£ãƒ–ç”Ÿæˆ (æ¨è«–åŠ¹ç‡å‘ä¸Š)
- ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– (å…¨ä½“æœ€é©åŒ–)

============================================================================
""")
    
    try:
        demo = InferOSLLMDemo()
        demo.run_interactive_demo()
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print(traceback.format_exc())

if __name__ == "__main__":
    main()

