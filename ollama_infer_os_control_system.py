#!/usr/bin/env python3
"""
Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ 
Ollamaã«ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚’å§”ã­ã€infer-OSã®æœ€é©åŒ–ã®ã¿ã‚’åˆ¶å¾¡

ç‰¹å¾´:
- Ollamaã«ã‚ˆã‚‹å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ç®¡ç†
- infer-OSæœ€é©åŒ–ã®ON/OFFåˆ¶å¾¡
- NPU/GPUä½¿ç”¨ç‡ç›£è¦–
- Windowså®Œå…¨å¯¾å¿œ
- è¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
"""

import os
import sys
import time
import json
import argparse
import threading
import requests
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import psutil
    import onnxruntime as ort
    import numpy as np
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install psutil onnxruntime requests")
    sys.exit(1)

class OllamaInferOSController:
    """Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, ollama_host: str = "http://localhost:11434"):
        self.ollama_host = ollama_host
        self.ollama_api = f"{ollama_host}/api"
        
        # infer-OSæœ€é©åŒ–åˆ¶å¾¡
        self.infer_os_enabled = True
        self.infer_os_config = {
            "npu_optimization": True,
            "memory_optimization": True,
            "cpu_optimization": True,
            "gpu_acceleration": True,
            "quantization": True,
            "parallel_processing": True,
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.available_models = []
        self.current_model = None
        self.npu_monitoring = False
        self.npu_stats = {"usage_changes": 0, "max_usage": 0.0, "avg_usage": 0.0}
        self.onnx_session = None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates = {
            "conversation": """ä»¥ä¸‹ã¯äººé–“ã¨AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä¼šè©±ã§ã™ã€‚AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯è¦ªåˆ‡ã§ã€è©³ç´°ã§ã€ä¸å¯§ã§ã™ã€‚

äººé–“: {prompt}
AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: """,
            
            "instruction": """ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€è©³ã—ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

æŒ‡ç¤º: {prompt}

å›ç­”: """,
            
            "reasoning": """ä»¥ä¸‹ã®å•é¡Œã«ã¤ã„ã¦ã€è«–ç†çš„ã«è€ƒãˆã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {prompt}

è§£ç­”: """,
            
            "creative": """ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€å‰µé€ çš„ã§èˆˆå‘³æ·±ã„å†…å®¹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

ãƒ†ãƒ¼ãƒ: {prompt}

å†…å®¹: """,
            
            "simple": "{prompt}"
        }
        
        self.current_template = "conversation"
        
        print("ğŸš€ Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ”— Ollamaæ¥ç¶šå…ˆ: {ollama_host}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ¯ è¨­è¨ˆæ–¹é‡: Ollamaãƒ¢ãƒ‡ãƒ«ç®¡ç† + infer-OSæœ€é©åŒ–åˆ¶å¾¡")
    
    def check_ollama_connection(self) -> bool:
        """Ollamaæ¥ç¶šç¢ºèª"""
        try:
            print("ğŸ” Ollamaæ¥ç¶šç¢ºèªä¸­...")
            response = requests.get(f"{self.ollama_api}/tags", timeout=5)
            
            if response.status_code == 200:
                print("âœ… Ollamaæ¥ç¶šæˆåŠŸ")
                return True
            else:
                print(f"âŒ Ollamaæ¥ç¶šå¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print("âŒ Ollamaæ¥ç¶šå¤±æ•—: æ¥ç¶šã‚¨ãƒ©ãƒ¼")
            print("ğŸ’¡ OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("   Windows: ollama serve")
            print("   Linux/Mac: ollama serve")
            return False
        except Exception as e:
            print(f"âŒ Ollamaæ¥ç¶šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—"""
        try:
            print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ä¸­...")
            response = requests.get(f"{self.ollama_api}/tags", timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                
                self.available_models = []
                for model in models:
                    model_info = {
                        "name": model.get("name", "unknown"),
                        "size": model.get("size", 0),
                        "modified_at": model.get("modified_at", ""),
                        "digest": model.get("digest", ""),
                        "details": model.get("details", {})
                    }
                    self.available_models.append(model_info)
                
                print(f"âœ… {len(self.available_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½")
                for i, model in enumerate(self.available_models, 1):
                    size_gb = model["size"] / (1024**3) if model["size"] > 0 else 0
                    print(f"  {i}. {model['name']} ({size_gb:.1f}GB)")
                
                return self.available_models
            else:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def select_model(self, model_name: str = None) -> bool:
        """ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        try:
            if not self.available_models:
                self.get_available_models()
            
            if not self.available_models:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # ãƒ¢ãƒ‡ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            if model_name is None:
                selected_model = self.available_models[0]
            else:
                # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã§æ¤œç´¢
                selected_model = None
                for model in self.available_models:
                    if model_name in model["name"]:
                        selected_model = model
                        break
                
                if selected_model is None:
                    print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
            
            self.current_model = selected_model
            size_gb = selected_model["size"] / (1024**3) if selected_model["size"] > 0 else 0
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«é¸æŠå®Œäº†: {selected_model['name']}")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {size_gb:.1f}GB")
            print(f"ğŸ“… æ›´æ–°æ—¥æ™‚: {selected_model['modified_at']}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def apply_infer_os_optimizations(self):
        """infer-OSæœ€é©åŒ–é©ç”¨"""
        if not self.infer_os_enabled:
            print("âš ï¸ infer-OSæœ€é©åŒ–ã¯ç„¡åŠ¹ã§ã™")
            return
        
        print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨ä¸­...")
        
        # NPUæœ€é©åŒ–
        if self.infer_os_config["npu_optimization"]:
            print("ğŸ”§ NPUæœ€é©åŒ–: æœ‰åŠ¹")
            # NPUé–¢é€£ã®æœ€é©åŒ–è¨­å®š
            os.environ['ONNXRUNTIME_PROVIDERS'] = 'VitisAIExecutionProvider,DmlExecutionProvider,CPUExecutionProvider'
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if self.infer_os_config["memory_optimization"]:
            print("ğŸ”§ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: æœ‰åŠ¹")
            os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            os.environ['OLLAMA_NUM_PARALLEL'] = '1'  # ä¸¦åˆ—å‡¦ç†åˆ¶é™
        
        # CPUæœ€é©åŒ–
        if self.infer_os_config["cpu_optimization"]:
            print("ğŸ”§ CPUæœ€é©åŒ–: æœ‰åŠ¹")
            cpu_count = os.cpu_count()
            os.environ['OLLAMA_NUM_THREADS'] = str(min(4, cpu_count))  # CPUä½¿ç”¨åˆ¶é™
        
        # GPUåŠ é€Ÿ
        if self.infer_os_config["gpu_acceleration"]:
            print("ğŸ”§ GPUåŠ é€Ÿ: æœ‰åŠ¹")
            os.environ['OLLAMA_GPU_LAYERS'] = '35'  # GPUå±¤æ•°è¨­å®š
        
        # é‡å­åŒ–
        if self.infer_os_config["quantization"]:
            print("ğŸ”§ é‡å­åŒ–: æœ‰åŠ¹")
            os.environ['OLLAMA_LOAD_TIMEOUT'] = '300'  # é‡å­åŒ–èª­ã¿è¾¼ã¿æ™‚é–“å»¶é•·
        
        # ä¸¦åˆ—å‡¦ç†
        if self.infer_os_config["parallel_processing"]:
            print("ğŸ”§ ä¸¦åˆ—å‡¦ç†: æœ‰åŠ¹")
            os.environ['OLLAMA_CONCURRENT_REQUESTS'] = '2'  # ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™
        
        print("âœ… infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨å®Œäº†")
        
        # è¨­å®šç¢ºèª
        print("ğŸ“Š é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–è¨­å®š:")
        for key, value in self.infer_os_config.items():
            status = "âœ…" if value else "âŒ"
            print(f"  {status} {key}: {value}")
    
    def create_npu_onnx_session(self) -> bool:
        """NPUå¯¾å¿œONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("ğŸ”§ NPUå¯¾å¿œONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # è»½é‡ãªãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆNPUäº’æ›ï¼‰
            os.makedirs("models", exist_ok=True)
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            import torch
            import torch.nn as nn
            
            class SimpleNPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear = nn.Linear(512, 1000)
                    
                def forward(self, x):
                    return self.linear(x)
            
            model = SimpleNPUModel()
            model.eval()
            
            dummy_input = torch.randn(1, 512)
            onnx_path = "models/ollama_npu_model.onnx"
            
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            
            # NPUå¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = []
            
            # VitisAI ExecutionProvider
            if 'VitisAIExecutionProvider' in ort.get_available_providers():
                vitisai_options = {
                    'config_file': '',  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿
                    'target': 'DPUCAHX8H',
                }
                providers.append(('VitisAIExecutionProvider', vitisai_options))
                print("ğŸ¯ VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½")
            
            # DmlExecutionProvider
            if 'DmlExecutionProvider' in ort.get_available_providers():
                providers.append('DmlExecutionProvider')
                print("ğŸ¯ DmlExecutionProvideråˆ©ç”¨å¯èƒ½")
            
            # CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            providers.append('CPUExecutionProvider')
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3
            
            self.onnx_session = ort.InferenceSession(
                onnx_path,
                sess_options=session_options,
                providers=providers
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… NPUå¯¾å¿œONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            return True
            
        except Exception as e:
            print(f"âŒ NPUå¯¾å¿œONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_npu_operation(self) -> bool:
        """NPUå‹•ä½œãƒ†ã‚¹ãƒˆ"""
        if self.onnx_session is None:
            print("âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            print("ğŸ”§ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            
            # ãƒ†ã‚¹ãƒˆå…¥åŠ›ä½œæˆ
            test_input = np.random.randn(1, 512).astype(np.float32)
            
            # æ¨è«–å®Ÿè¡Œ
            outputs = self.onnx_session.run(None, {"input": test_input})
            
            print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶ {outputs[0].shape}")
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_provider = self.onnx_session.get_providers()[0]
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            return True
            
        except Exception as e:
            print(f"âŒ NPUå‹•ä½œãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰- Ollama + infer-OSç‰ˆ")
            
            prev_usage = 0.0
            usage_history = []
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    try:
                        import torch
                        if torch.cuda.is_available():
                            gpu_usage = torch.cuda.utilization()
                        else:
                            gpu_usage = 0.0
                    except:
                        gpu_usage = 0.0
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡º
                    if abs(gpu_usage - prev_usage) > 2.0:  # 2%ä»¥ä¸Šã®å¤‰åŒ–
                        print(f"ğŸ”¥ NPU/GPUä½¿ç”¨ç‡å¤‰åŒ–: {prev_usage:.1f}% â†’ {gpu_usage:.1f}% (Ollama + infer-OS)")
                        self.npu_stats["usage_changes"] += 1
                    
                    # çµ±è¨ˆæ›´æ–°
                    usage_history.append(gpu_usage)
                    if len(usage_history) > 60:  # ç›´è¿‘60ç§’ã®ã¿ä¿æŒ
                        usage_history.pop(0)
                    
                    self.npu_stats["max_usage"] = max(self.npu_stats["max_usage"], gpu_usage)
                    self.npu_stats["avg_usage"] = sum(usage_history) / len(usage_history)
                    
                    prev_usage = gpu_usage
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"âš ï¸ NPUç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(1)
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç›£è¦–é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢ï¼ˆOllama + infer-OSï¼‰")
    
    def generate_text_with_ollama(self, prompt: str, max_tokens: int = 100, template: str = None) -> str:
        """Ollamaã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.current_model is None:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            if template and template in self.templates:
                formatted_prompt = self.templates[template].format(prompt=prompt)
            else:
                formatted_prompt = self.templates[self.current_template].format(prompt=prompt)
            
            print(f"ğŸ’¬ Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{formatted_prompt[:50]}...'")
            print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.current_model['name']}")
            print(f"ğŸ¯ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
            
            # Ollama APIå‘¼ã³å‡ºã—
            payload = {
                "model": self.current_model["name"],
                "prompt": formatted_prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 50,
                    "repeat_penalty": 1.1,
                }
            }
            
            print("ğŸ”§ Ollama APIå‘¼ã³å‡ºã—ä¸­...")
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_api}/generate",
                json=payload,
                timeout=120
            )
            
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("response", "")
                
                print(f"âœ… Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"ğŸ“ ç”Ÿæˆæ–‡å­—æ•°: {len(generated_text)}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if len(generated_text.strip()) < 10:
                    print("âš ï¸ ç”ŸæˆçµæœãŒçŸ­ã™ãã¾ã™")
                    return self.generate_fallback_text(prompt)
                
                return generated_text.strip()
            else:
                print(f"âŒ Ollama APIå‘¼ã³å‡ºã—å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return self.generate_fallback_text(prompt)
                
        except requests.exceptions.Timeout:
            print("âŒ Ollama APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ120ç§’ï¼‰")
            return self.generate_fallback_text(prompt)
        except Exception as e:
            print(f"âŒ Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self.generate_fallback_text(prompt)
    
    def generate_fallback_text(self, prompt: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        fallback_responses = {
            "äººå·¥çŸ¥èƒ½": "äººå·¥çŸ¥èƒ½ã¯ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªæŠ€è¡“åˆ†é‡ã§ã™ã€‚Ollama + infer-OSæœ€é©åŒ–ç’°å¢ƒã§ã‚‚å®‰å®šã—ã¦å‹•ä½œã™ã‚‹æŠ€è¡“ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "é‡å­": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯é©æ–°çš„ãªè¨ˆç®—æŠ€è¡“ã§ã™ã€‚Ollamaç’°å¢ƒã§ã®ç ”ç©¶é–‹ç™ºã‚‚æ´»ç™ºã«è¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚",
            "default": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã®è©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Ollama + infer-OSç’°å¢ƒã§ã®åˆ¶ç´„ã«ã‚ˆã‚Šã€ç°¡æ½”ãªå›ç­”ã®ã¿æä¾›ã„ãŸã—ã¾ã™ã€‚"
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for keyword, response in fallback_responses.items():
            if keyword in prompt and keyword != "default":
                return response
        
        return fallback_responses["default"]
    
    def toggle_infer_os_optimization(self, enabled: bool = None) -> bool:
        """infer-OSæœ€é©åŒ–ã®ON/OFFåˆ‡ã‚Šæ›¿ãˆ"""
        if enabled is None:
            self.infer_os_enabled = not self.infer_os_enabled
        else:
            self.infer_os_enabled = enabled
        
        status = "æœ‰åŠ¹" if self.infer_os_enabled else "ç„¡åŠ¹"
        print(f"ğŸ”„ infer-OSæœ€é©åŒ–ã‚’{status}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
        
        if self.infer_os_enabled:
            self.apply_infer_os_optimizations()
        else:
            print("âš ï¸ infer-OSæœ€é©åŒ–ãŒç„¡åŠ¹ã«ãªã‚Šã¾ã—ãŸ")
            # æœ€é©åŒ–è¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆ
            for key in self.infer_os_config:
                self.infer_os_config[key] = False
        
        return self.infer_os_enabled
    
    def show_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º"""
        print("\nğŸ“Š Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
        print(f"  ğŸ”— Ollamaæ¥ç¶š: {'âœ…' if self.check_ollama_connection() else 'âŒ'}")
        print(f"  ğŸ¯ é¸æŠãƒ¢ãƒ‡ãƒ«: {self.current_model['name'] if self.current_model else 'ãªã—'}")
        print(f"  âš¡ infer-OSæœ€é©åŒ–: {'âœ… æœ‰åŠ¹' if self.infer_os_enabled else 'âŒ ç„¡åŠ¹'}")
        print(f"  ğŸ”§ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³: {'âœ…' if self.onnx_session else 'âŒ'}")
        print(f"  ğŸ“Š NPUç›£è¦–: {'âœ… å®Ÿè¡Œä¸­' if self.npu_monitoring else 'âŒ åœæ­¢ä¸­'}")
        
        if self.infer_os_enabled:
            print("  ğŸ“‹ infer-OSæœ€é©åŒ–è¨­å®š:")
            for key, value in self.infer_os_config.items():
                status = "âœ…" if value else "âŒ"
                print(f"    {status} {key}: {value}")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        print(f"  ğŸ’» CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent:.1f}%")
    
    def show_npu_stats(self):
        """NPUçµ±è¨ˆè¡¨ç¤º"""
        print("\nğŸ“Š NPU/GPUä½¿ç”¨ç‡çµ±è¨ˆï¼ˆOllama + infer-OSç‰ˆï¼‰:")
        print(f"  ğŸ”¥ ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡ºå›æ•°: {self.npu_stats['usage_changes']}")
        print(f"  ğŸ“ˆ æœ€å¤§ä½¿ç”¨ç‡: {self.npu_stats['max_usage']:.1f}%")
        print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {self.npu_stats['avg_usage']:.1f}%")
        
        if self.onnx_session:
            active_provider = self.onnx_session.get_providers()[0]
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        print(f"  ğŸ’» CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent:.1f}%")
        print(f"  âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
    
    def change_template(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´"""
        print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
        for i, (name, template) in enumerate(self.templates.items(), 1):
            print(f"  {i}. {name}")
        
        try:
            choice = input("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
            template_names = list(self.templates.keys())
            
            if choice.isdigit() and 1 <= int(choice) <= len(template_names):
                self.current_template = template_names[int(choice) - 1]
                print(f"âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ '{self.current_template}' ã«å¤‰æ›´ã—ã¾ã—ãŸ")
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        except Exception as e:
            print(f"âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´ã‚¨ãƒ©ãƒ¼: {e}")
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            print("ğŸš€ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            
            # Ollamaæ¥ç¶šç¢ºèª
            if not self.check_ollama_connection():
                return False
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«å–å¾—
            models = self.get_available_models()
            if not models:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                print("ğŸ’¡ Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
                print("   ä¾‹: ollama pull llama2")
                return False
            
            # æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            if not self.select_model():
                return False
            
            # infer-OSæœ€é©åŒ–é©ç”¨
            self.apply_infer_os_optimizations()
            
            # NPUå¯¾å¿œONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            onnx_created = self.create_npu_onnx_session()
            if not onnx_created:
                print("âš ï¸ NPUå¯¾å¿œONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            # NPUå‹•ä½œãƒ†ã‚¹ãƒˆ
            if self.onnx_session:
                npu_test = self.test_npu_operation()
                if not npu_test:
                    print("âš ï¸ NPUå‹•ä½œãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            print("âœ… Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            self.show_system_status()
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        print("\nğŸ¯ Ollama + infer-OSåˆ¶å¾¡ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.current_model['name'] if self.current_model else 'ãªã—'}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        if self.onnx_session:
            active_provider = self.onnx_session.get_providers()[0]
            print(f"ğŸ”§ NPUãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
        
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰:")
        print("  'quit' - çµ‚äº†")
        print("  'stats' - NPUçµ±è¨ˆè¡¨ç¤º")
        print("  'status' - ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º")
        print("  'template' - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´")
        print("  'model' - ãƒ¢ãƒ‡ãƒ«å¤‰æ›´")
        print("  'toggle' - infer-OSæœ€é©åŒ–ON/OFFåˆ‡ã‚Šæ›¿ãˆ")
        print("  'on' - infer-OSæœ€é©åŒ–æœ‰åŠ¹")
        print("  'off' - infer-OSæœ€é©åŒ–ç„¡åŠ¹")
        print("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: conversation, instruction, reasoning, creative, simple")
        print("=" * 70)
        
        # NPUç›£è¦–é–‹å§‹
        self.start_npu_monitoring()
        
        try:
            while True:
                try:
                    infer_os_status = "ON" if self.infer_os_enabled else "OFF"
                    prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [infer-OS:{infer_os_status}] [{self.current_template}]: ").strip()
                    
                    if not prompt:
                        continue
                    
                    if prompt.lower() == 'quit':
                        print("ğŸ‘‹ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                        break
                    
                    if prompt.lower() == 'stats':
                        self.show_npu_stats()
                        continue
                    
                    if prompt.lower() == 'status':
                        self.show_system_status()
                        continue
                    
                    if prompt.lower() == 'template':
                        self.change_template()
                        continue
                    
                    if prompt.lower() == 'model':
                        self.select_model_interactive()
                        continue
                    
                    if prompt.lower() == 'toggle':
                        self.toggle_infer_os_optimization()
                        continue
                    
                    if prompt.lower() == 'on':
                        self.toggle_infer_os_optimization(True)
                        continue
                    
                    if prompt.lower() == 'off':
                        self.toggle_infer_os_optimization(False)
                        continue
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
                    start_time = time.time()
                    result = self.generate_text_with_ollama(prompt, max_tokens=100)
                    end_time = time.time()
                    
                    print(f"\nğŸ¯ Ollama + infer-OSç”Ÿæˆçµæœ:")
                    print(result)
                    print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
                    print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
                    
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                except Exception as e:
                    print(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        finally:
            self.stop_npu_monitoring()
    
    def select_model_interactive(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        try:
            print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
            for i, model in enumerate(self.available_models, 1):
                size_gb = model["size"] / (1024**3) if model["size"] > 0 else 0
                print(f"  {i}. {model['name']} ({size_gb:.1f}GB)")
            
            choice = input("ãƒ¢ãƒ‡ãƒ«ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
            
            if choice.isdigit() and 1 <= int(choice) <= len(self.available_models):
                selected_model = self.available_models[int(choice) - 1]
                self.current_model = selected_model
                print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ '{selected_model['name']}' ã«å¤‰æ›´ã—ã¾ã—ãŸ")
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=100, help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="conversation", help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    parser.add_argument("--model", type=str, help="ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--ollama-host", type=str, default="http://localhost:11434", help="Ollamaæ¥ç¶šå…ˆ")
    parser.add_argument("--infer-os", action="store_true", default=True, help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--no-infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ç„¡åŠ¹")
    
    args = parser.parse_args()
    
    # infer-OSæœ€é©åŒ–è¨­å®š
    infer_os_enabled = args.infer_os and not args.no_infer_os
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = OllamaInferOSController(ollama_host=args.ollama_host)
    system.infer_os_enabled = infer_os_enabled
    
    if not system.initialize_system():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    if args.model:
        if not system.select_model(args.model):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{args.model}' ã®é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
    
    try:
        if args.interactive:
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
            system.run_interactive_mode()
        elif args.prompt:
            # å˜ç™ºç”Ÿæˆ
            system.start_npu_monitoring()
            result = system.generate_text_with_ollama(args.prompt, args.tokens, args.template)
            print(f"\nğŸ¯ ç”Ÿæˆçµæœ:\n{result}")
            system.stop_npu_monitoring()
            system.show_npu_stats()
        else:
            print("ä½¿ç”¨æ–¹æ³•: --interactive ã¾ãŸã¯ --prompt ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("ä¾‹: python ollama_infer_os_control_system.py --interactive")
            print("ä¾‹: python ollama_infer_os_control_system.py --prompt 'äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„' --tokens 200")
            print("ä¾‹: python ollama_infer_os_control_system.py --interactive --no-infer-os")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        if system.npu_monitoring:
            system.stop_npu_monitoring()

if __name__ == "__main__":
    main()

