#!/usr/bin/env python3
"""
ç©¶æ¥µã®NPUä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸è¶³ã¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã‚’å®Œå…¨è§£æ±º
"""

import os
import sys
import json
import traceback
from pathlib import Path


class UltimateNPUFixer:
    """ç©¶æ¥µã®NPUä¿®æ­£ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.model_path = "llama3-8b-amd-npu"
        
    def run_ultimate_fix(self) -> bool:
        """ç©¶æ¥µã®ä¿®æ­£å®Ÿè¡Œ"""
        print("ğŸš€ ç©¶æ¥µã®NPUä¿®æ­£é–‹å§‹")
        print("=" * 60)
        
        success = True
        
        # 1. qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
        print("\nğŸ“¦ 1. qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ")
        if self._create_qlinear_module():
            print("âœ… qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆå®Œäº†")
        else:
            success = False
        
        # 2. ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£
        print("\nğŸ”§ 2. ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£")
        if self._fix_generation_errors():
            print("âœ… ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£å®Œäº†")
        else:
            success = False
        
        # 3. ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
        print("\nğŸ¯ 3. ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä½œæˆ")
        if self._create_ultimate_runner():
            print("âœ… ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä½œæˆå®Œäº†")
        else:
            success = False
        
        return success
    
    def _create_qlinear_module(self) -> bool:
        """qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ"""
        print("ğŸ”§ qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆä¸­...")
        
        try:
            # qlinear.pyä½œæˆï¼ˆAWQé‡å­åŒ–äº’æ›ï¼‰
            qlinear_code = '''"""
qlinear - AWQé‡å­åŒ–äº’æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple


class QuantLinear(nn.Module):
    """é‡å­åŒ–ç·šå½¢å±¤ï¼ˆAWQäº’æ›ï¼‰"""
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True, 
                 w_bit: int = 4, group_size: int = 128):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.w_bit = w_bit
        self.group_size = group_size
        
        # é‡å­åŒ–é‡ã¿ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
        self.qweight = nn.Parameter(torch.randint(0, 2**w_bit, (out_features, in_features // 8)))
        self.qzeros = nn.Parameter(torch.randint(0, 2**w_bit, (out_features, in_features // group_size)))
        self.scales = nn.Parameter(torch.randn(out_features, in_features // group_size))
        
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        else:
            self.register_parameter('bias', None)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸé‡å­åŒ–æ¨è«–ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
        # å®Ÿéš›ã®AWQå®Ÿè£…ã§ã¯è¤‡é›‘ãªé€†é‡å­åŒ–å‡¦ç†ãŒå¿…è¦
        
        # ç–‘ä¼¼çš„ãªé‡ã¿å¾©å…ƒ
        weight = self.scales.unsqueeze(-1) * torch.randn(self.out_features, self.in_features, device=x.device, dtype=x.dtype)
        
        return F.linear(x, weight, self.bias)
    
    def extra_repr(self) -> str:
        return f'in_features={self.in_features}, out_features={self.out_features}, w_bit={self.w_bit}, group_size={self.group_size}'


class WQLinear(QuantLinear):
    """WQLinearï¼ˆAWQäº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
    pass


class AWQLinear(QuantLinear):
    """AWQLinearï¼ˆAWQäº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
    pass


# äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
def make_quant_linear(in_features: int, out_features: int, bias: bool = True, **kwargs) -> QuantLinear:
    """é‡å­åŒ–ç·šå½¢å±¤ä½œæˆ"""
    return QuantLinear(in_features, out_features, bias, **kwargs)


def pack_weights(weight: torch.Tensor, w_bit: int = 4) -> torch.Tensor:
    """é‡ã¿ãƒ‘ãƒƒã‚­ãƒ³ã‚°ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
    return weight.to(torch.int8)


def unpack_weights(packed_weight: torch.Tensor, w_bit: int = 4) -> torch.Tensor:
    """é‡ã¿ã‚¢ãƒ³ãƒ‘ãƒƒã‚­ãƒ³ã‚°ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
    return packed_weight.to(torch.float32)


# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æƒ…å ±
__version__ = "1.0.0"
__author__ = "NPU Optimization Team"
__description__ = "AWQé‡å­åŒ–äº’æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"
'''
            
            with open("qlinear.py", 'w', encoding='utf-8') as f:
                f.write(qlinear_code)
            
            return True
            
        except Exception as e:
            print(f"âŒ qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆå¤±æ•—: {e}")
            return False
    
    def _fix_generation_errors(self) -> bool:
        """ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£"""
        print("ğŸ”§ ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ä¸­...")
        
        try:
            # ä¿®æ­£ç‰ˆç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ä½œæˆ
            generation_utils_code = '''"""
ä¿®æ­£ç‰ˆç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
DialoGPTã¨Llamaã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£
"""

import torch
import torch.nn.functional as F
from typing import Optional, List, Union


class SafeTextGenerator:
    """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_tokenizer()
    
    def _setup_tokenizer(self):
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å®‰å…¨è¨­å®š"""
        if self.tokenizer.pad_token is None:
            if self.tokenizer.eos_token is not None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        if self.tokenizer.bos_token is None:
            if hasattr(self.tokenizer, 'cls_token') and self.tokenizer.cls_token is not None:
                self.tokenizer.bos_token = self.tokenizer.cls_token
            else:
                self.tokenizer.add_special_tokens({'bos_token': '[BOS]'})
    
    def generate_safe(self, prompt: str, max_new_tokens: int = 50, 
                     temperature: float = 0.7, top_p: float = 0.9,
                     do_sample: bool = True) -> str:
        """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            # å…¥åŠ›æº–å‚™
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)
            input_ids = inputs.input_ids
            attention_mask = inputs.attention_mask
            
            # ç”Ÿæˆè¨­å®š
            generation_config = {
                'max_new_tokens': max_new_tokens,
                'do_sample': do_sample,
                'temperature': temperature,
                'top_p': top_p,
                'pad_token_id': self.tokenizer.pad_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'attention_mask': attention_mask,
                'use_cache': True,
                'return_dict_in_generate': True,
                'output_scores': False
            }
            
            # BOS ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ ï¼ˆDialoGPTç”¨ï¼‰
            if hasattr(self.model.config, 'model_type') and 'gpt' in self.model.config.model_type.lower():
                if self.tokenizer.bos_token_id is not None:
                    bos_ids = torch.tensor([[self.tokenizer.bos_token_id]], dtype=input_ids.dtype, device=input_ids.device)
                    input_ids = torch.cat([bos_ids, input_ids], dim=-1)
                    bos_attention = torch.ones((1, 1), dtype=attention_mask.dtype, device=attention_mask.device)
                    attention_mask = torch.cat([bos_attention, attention_mask], dim=-1)
                    generation_config['attention_mask'] = attention_mask
            
            # ç”Ÿæˆå®Ÿè¡Œ
            with torch.no_grad():
                try:
                    # æ¨™æº–ç”Ÿæˆ
                    outputs = self.model.generate(input_ids, **generation_config)
                    if hasattr(outputs, 'sequences'):
                        generated_ids = outputs.sequences
                    else:
                        generated_ids = outputs
                    
                except Exception as e:
                    print(f"âš ï¸ æ¨™æº–ç”Ÿæˆå¤±æ•—: {e}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
                    generated_ids = self._fallback_generate(input_ids, max_new_tokens, temperature)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            if generated_ids.dim() > 1:
                generated_ids = generated_ids[0]
            
            # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»
            if len(generated_ids) > len(input_ids[0]):
                new_tokens = generated_ids[len(input_ids[0]):]
            else:
                new_tokens = generated_ids
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰
            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)
            
            # å¾Œå‡¦ç†
            response = response.strip()
            if not response:
                response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            return response
            
        except Exception as e:
            return f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def _fallback_generate(self, input_ids: torch.Tensor, max_new_tokens: int, temperature: float) -> torch.Tensor:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆï¼ˆæ‰‹å‹•å®Ÿè£…ï¼‰"""
        generated = input_ids.clone()
        
        for _ in range(max_new_tokens):
            try:
                with torch.no_grad():
                    outputs = self.model(generated)
                    if hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    else:
                        logits = outputs[0]
                    
                    # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬
                    next_token_logits = logits[:, -1, :] / temperature
                    
                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                    
                    # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                    if next_token.item() == self.tokenizer.eos_token_id:
                        break
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
                    generated = torch.cat([generated, next_token], dim=-1)
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                break
        
        return generated


def create_safe_generator(model, tokenizer):
    """å®‰å…¨ãªç”Ÿæˆå™¨ä½œæˆ"""
    return SafeTextGenerator(model, tokenizer)
'''
            
            with open("generation_utils.py", 'w', encoding='utf-8') as f:
                f.write(generation_utils_code)
            
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£å¤±æ•—: {e}")
            return False
    
    def _create_ultimate_runner(self) -> bool:
        """ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä½œæˆ"""
        print("ğŸ¯ ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä½œæˆä¸­...")
        
        try:
            ultimate_runner_code = '''#!/usr/bin/env python3
"""
ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 
å…¨ã¦ã®å•é¡Œã‚’è§£æ±ºã—ãŸæœ€çµ‚ç‰ˆ
"""

import os
import sys
import json
import torch
import traceback

# qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import qlinear
    print("âœ… qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")

# ç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from generation_utils import SafeTextGenerator, create_safe_generator
    print("âœ… ç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ç”Ÿæˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")

# modeling_llama_amdã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from modeling_llama_amd import LlamaForCausalLM as NPULlamaForCausalLM, LlamaConfig
    print("âœ… å®Œå…¨ãªmodeling_llama_amdã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ modeling_llama_amdã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")
    try:
        from transformers import LlamaForCausalLM as NPULlamaForCausalLM, LlamaConfig
        print("âš ï¸ æ¨™æº–Llamaã‚’ä½¿ç”¨")
    except ImportError:
        print("âŒ å…¨ã¦ã®Llamaã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—")


class UltimateNPURunner:
    """ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.config = None
        self.generator = None
        self.fallback_models = self._load_fallback_config()
        
    def _load_fallback_config(self) -> list:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šèª­ã¿è¾¼ã¿"""
        try:
            with open("fallback_config.json", 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config.get("fallback_models", ["microsoft/DialoGPT-medium", "gpt2", "distilgpt2"])
        except Exception:
            return ["microsoft/DialoGPT-medium", "gpt2", "distilgpt2"]
    
    def setup_model(self, model_path: str = "llama3-8b-amd-npu") -> bool:
        """ç©¶æ¥µã®ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸš€ ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print("=" * 60)
        
        try:
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
            success = self._setup_tokenizer(model_path)
            if not success:
                return False
            
            print("ğŸ¤– ç©¶æ¥µã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # 1. NPUæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰ï¼ˆqlinearå¯¾å¿œï¼‰
            if self._try_ultimate_npu_load(model_path):
                print("âœ… ç©¶æ¥µã®NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                self._setup_generator()
                return True
            
            # 2. æ¨™æº–ãƒ­ãƒ¼ã‚«ãƒ«ãƒ­ãƒ¼ãƒ‰
            if self._try_standard_load(model_path):
                print("âœ… æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                self._setup_generator()
                return True
            
            # 3. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆå®‰å…¨ç”Ÿæˆå¯¾å¿œï¼‰
            if self._try_safe_fallback_load():
                print("âœ… å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                self._setup_generator()
                return True
            
            print("âŒ å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ãŒå¤±æ•—")
            return False
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _setup_tokenizer(self, model_path: str) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            from transformers import AutoTokenizer
            
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼è©¦è¡Œ
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                print("âœ… ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                return True
            except Exception as e:
                print(f"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¤±æ•—: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
            for fallback_model in self.fallback_models:
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(fallback_model)
                    print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {fallback_model}")
                    return True
                except Exception as e:
                    print(f"âš ï¸ {fallback_model} ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å¤±æ•—: {e}")
                    continue
            
            return False
            
        except Exception as e:
            print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—: {e}")
            return False
    
    def _try_ultimate_npu_load(self, model_path: str) -> bool:
        """ç©¶æ¥µã®NPUæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰ï¼ˆqlinearå¯¾å¿œï¼‰"""
        try:
            npu_weight_file = os.path.join(model_path, "pytorch_llama3_8b_w_bit_4_awq_amd.pt")
            if not os.path.exists(npu_weight_file):
                print("âš ï¸ NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            print(f"âš¡ NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {npu_weight_file}")
            
            # è¨­å®šãƒ­ãƒ¼ãƒ‰
            config_path = os.path.join(model_path, "config.json")
            if os.path.exists(config_path):
                self.config = LlamaConfig.from_pretrained(model_path)
            else:
                self.config = LlamaConfig()
            
            # qlinearå¯¾å¿œãƒ­ãƒ¼ãƒ‰
            try:
                # å®‰å…¨ãªãƒ­ãƒ¼ãƒ‰ï¼ˆweights_only=Falseï¼‰
                model_data = torch.load(npu_weight_file, weights_only=False, map_location='cpu')
                print("âœ… qlinearå¯¾å¿œãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                
                # ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ
                if hasattr(model_data, 'eval'):
                    self.model = model_data
                    print("âœ… NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ç›´æ¥ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                elif hasattr(model_data, 'state_dict'):
                    self.model = NPULlamaForCausalLM(self.config)
                    self.model.load_state_dict(model_data.state_dict(), strict=False)
                    print("âœ… NPUæœ€é©åŒ–state_dictãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                elif isinstance(model_data, dict):
                    self.model = NPULlamaForCausalLM(self.config)
                    self.model.load_state_dict(model_data, strict=False)
                    print("âœ… NPUæœ€é©åŒ–è¾æ›¸ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                else:
                    print(f"âš ï¸ ä¸æ˜ãªNPUæœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿å½¢å¼: {type(model_data)}")
                    return False
                
                self.model.eval()
                return True
                
            except Exception as e:
                print(f"âŒ qlinearå¯¾å¿œãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return False
            
        except Exception as e:
            print(f"âŒ ç©¶æ¥µã®NPUæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False
    
    def _try_standard_load(self, model_path: str) -> bool:
        """æ¨™æº–ãƒ­ãƒ¼ãƒ‰"""
        try:
            from transformers import AutoModelForCausalLM
            
            required_files = ["config.json"]
            for file_name in required_files:
                file_path = os.path.join(model_path, file_name)
                if not os.path.exists(file_path):
                    print(f"âš ï¸ å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                    return False
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                local_files_only=True
            )
            print("âœ… æ¨™æº–ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨™æº–ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False
    
    def _try_safe_fallback_load(self) -> bool:
        """å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ãƒ¼ãƒ‰"""
        try:
            from transformers import AutoModelForCausalLM
            
            for fallback_model in self.fallback_models:
                try:
                    print(f"ğŸ”„ å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«è©¦è¡Œ: {fallback_model}")
                    self.model = AutoModelForCausalLM.from_pretrained(
                        fallback_model,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        device_map="auto" if torch.cuda.is_available() else None
                    )
                    print(f"âœ… å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {fallback_model}")
                    return True
                except Exception as e:
                    print(f"âš ï¸ {fallback_model} ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                    continue
            
            return False
            
        except Exception as e:
            print(f"âŒ å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return False
    
    def _setup_generator(self):
        """å®‰å…¨ç”Ÿæˆå™¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            if 'SafeTextGenerator' in globals():
                self.generator = SafeTextGenerator(self.model, self.tokenizer)
                print("âœ… å®‰å…¨ç”Ÿæˆå™¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            else:
                print("âš ï¸ æ¨™æº–ç”Ÿæˆå™¨ã‚’ä½¿ç”¨")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå™¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")
    
    def generate_text(self, prompt: str, max_tokens: int = 100) -> str:
        """ç©¶æ¥µã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.model or not self.tokenizer:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # å®‰å…¨ç”Ÿæˆå™¨ä½¿ç”¨
            if self.generator:
                return self.generator.generate_safe(prompt, max_tokens)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
            return self._fallback_generate(prompt, max_tokens)
            
        except Exception as e:
            return f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def _fallback_generate(self, prompt: str, max_tokens: int) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ"""
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    attention_mask=inputs.attention_mask
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response if response else "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“"
            
        except Exception as e:
            return f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def run_interactive(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\\nğŸ‡¯ğŸ‡µ ç©¶æ¥µã®NPUæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("ğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†ã€'stats'ã§çµ±è¨ˆè¡¨ç¤º")
        print("=" * 60)
        
        generation_count = 0
        
        while True:
            try:
                prompt = input("\\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ç©¶æ¥µã®NPUã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if prompt.lower() == 'stats':
                    print(f"\\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
                    print(f"  ç”Ÿæˆå›æ•°: {generation_count}")
                    print(f"  ãƒ¢ãƒ‡ãƒ«: {type(self.model).__name__}")
                    print(f"  ç”Ÿæˆå™¨: {'å®‰å…¨ç”Ÿæˆå™¨' if self.generator else 'æ¨™æº–ç”Ÿæˆå™¨'}")
                    continue
                
                if not prompt:
                    continue
                
                print("\\nğŸ”„ ç”Ÿæˆä¸­...")
                response = self.generate_text(prompt)
                print(f"\\nğŸ“ å¿œç­”: {response}")
                generation_count += 1
                
            except KeyboardInterrupt:
                print("\\nğŸ‘‹ ç©¶æ¥µã®NPUã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"\\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ç©¶æ¥µã®NPUæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--model", default="llama3-8b-amd-npu", help="ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--prompt", help="å˜ç™ºå®Ÿè¡Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-tokens", type=int, default=100, help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    runner = UltimateNPURunner()
    
    try:
        if not runner.setup_model(args.model):
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            print("ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿè¡Œã‚’è©¦è¡Œã—ã¦ãã ã•ã„")
            return
        
        if args.prompt:
            print(f"\\nğŸ”„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
            response = runner.generate_text(args.prompt, args.max_tokens)
            print(f"ğŸ“ å¿œç­”: {response}")
        elif args.interactive:
            runner.run_interactive()
        else:
            runner.run_interactive()
        
    except KeyboardInterrupt:
        print("\\nğŸ‘‹ ç©¶æ¥µã®NPUã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\\nâŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()
'''
            
            with open("ultimate_npu_runner.py", 'w', encoding='utf-8') as f:
                f.write(ultimate_runner_code)
            
            return True
            
        except Exception as e:
            print(f"âŒ ç©¶æ¥µã®NPUå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ä½œæˆå¤±æ•—: {e}")
            return False


def main():
    fixer = UltimateNPUFixer()
    
    try:
        success = fixer.run_ultimate_fix()
        
        if success:
            print("\nğŸ‰ ç©¶æ¥µã®NPUä¿®æ­£å®Œäº†ï¼")
            print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:")
            print("   python ultimate_npu_runner.py --interactive")
            print("   python ultimate_npu_runner.py --prompt \"äººå‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„\"")
            print("\nğŸ”§ è§£æ±ºã•ã‚ŒãŸå•é¡Œ:")
            print("   âœ… qlinearãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸è¶³å•é¡Œ")
            print("   âœ… ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆindex out of rangeï¼‰")
            print("   âœ… DialoGPTç”Ÿæˆå•é¡Œ")
            print("   âœ… NPUæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰å•é¡Œ")
        else:
            print("\nâš ï¸ ä¸€éƒ¨ã®ä¿®æ­£ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ä¿®æ­£å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ä¿®æ­£å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()

