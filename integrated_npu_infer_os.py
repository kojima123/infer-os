#!/usr/bin/env python3
"""
çµ±åˆNPU + Infer-OSæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
çœŸã®åŒ…æ‹¬çš„æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Ÿç¾

çµ±åˆæ©Ÿèƒ½:
- ğŸš€ NPUæœ€é©åŒ– (VitisAI ExecutionProvider)
- âš¡ Infer-OSæœ€é©åŒ– (ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã€é«˜åº¦é‡å­åŒ–)
- ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ« (8B-70Bå¯¾å¿œ)
- ğŸ“Š åŒ…æ‹¬çš„æ€§èƒ½ç›£è¦–
- ğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯¾è©±
"""

import os
import sys
import gc
import time
import traceback
import argparse
import platform
import psutil
from typing import Dict, List, Optional, Any, Union
from pathlib import Path

# PyTorché–¢é€£
try:
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        BitsAndBytesConfig,
        set_seed
    )
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âŒ PyTorch/TransformersãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

# ONNXé–¢é€£
try:
    import onnx
    import onnxruntime as ort
    from onnxruntime.quantization import quantize_dynamic, QuantType
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ ONNXé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# NPUé–¢é€£
try:
    import qlinear
    QLINEAR_AVAILABLE = True
except ImportError:
    QLINEAR_AVAILABLE = False
    print("âš ï¸ qlinearãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# Infer-OSæœ€é©åŒ–æ©Ÿèƒ½ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from aggressive_memory_optimizer import AggressiveMemoryOptimizer
    AGGRESSIVE_MEMORY_AVAILABLE = True
except ImportError:
    AGGRESSIVE_MEMORY_AVAILABLE = False
    print("âš ï¸ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

try:
    from advanced_quantization_optimizer import AdvancedQuantizationOptimizer, QuantizationProfile
    ADVANCED_QUANT_AVAILABLE = True
except ImportError:
    ADVANCED_QUANT_AVAILABLE = False
    print("âš ï¸ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

try:
    from windows_npu_optimizer import WindowsNPUOptimizer
    WINDOWS_NPU_AVAILABLE = True
except ImportError:
    WINDOWS_NPU_AVAILABLE = False
    print("âš ï¸ Windows NPUæœ€é©åŒ–æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

try:
    from infer_os_comparison_benchmark import ComparisonBenchmark, InferOSMode
    COMPARISON_BENCHMARK_AVAILABLE = True
except ImportError:
    COMPARISON_BENCHMARK_AVAILABLE = False
    print("âš ï¸ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")


class IntegratedNPUInferOS:
    """çµ±åˆNPU + Infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 model_name: str = "llama3-8b-amd-npu",
                 enable_npu: bool = True,
                 enable_infer_os: bool = True,
                 use_aggressive_memory: bool = True,
                 use_advanced_quant: bool = True,
                 quantization_profile: str = "balanced",
                 enable_windows_npu: bool = True):
        
        self.model_name = model_name
        self.enable_npu = enable_npu
        self.enable_infer_os = enable_infer_os
        self.use_aggressive_memory = use_aggressive_memory
        self.use_advanced_quant = use_advanced_quant
        self.quantization_profile = quantization_profile
        self.enable_windows_npu = enable_windows_npu
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.model = None
        self.tokenizer = None
        self.npu_session = None
        self.aggressive_memory_optimizer = None
        self.advanced_quantizer = None
        self.windows_npu_optimizer = None
        self.comparison_benchmark = None
        
        # çµ±è¨ˆæƒ…å ±
        self.optimization_stats = {
            "npu_enabled": False,
            "infer_os_enabled": False,
            "memory_optimized": False,
            "quantization_applied": False,
            "windows_npu_active": False,
            "total_optimizations": 0
        }
        
        # NPUç’°å¢ƒè¨­å®š
        self._setup_npu_environment()
        
        # Infer-OSæœ€é©åŒ–åˆæœŸåŒ–
        self._initialize_infer_os_optimizations()
    
    def _setup_npu_environment(self):
        """NPUç’°å¢ƒå¤‰æ•°è¨­å®š"""
        if not self.enable_npu:
            return
        
        print("ğŸ”§ NPUç’°å¢ƒè¨­å®šä¸­...")
        
        # Ryzen AIãƒ‘ã‚¹è¨­å®š
        ryzen_ai_paths = [
            "C:\\Program Files\\RyzenAI\\1.5",
            "C:\\Program Files\\RyzenAI\\1.5.1",
            "C:\\Program Files\\RyzenAI\\1.2"
        ]
        
        for path in ryzen_ai_paths:
            if os.path.exists(path):
                os.environ["RYZEN_AI_INSTALLATION_PATH"] = path
                print(f"âœ… Ryzen AIãƒ‘ã‚¹è¨­å®š: {path}")
                break
        
        # NPUã‚ªãƒ¼ãƒãƒ¬ã‚¤è¨­å®š
        if "RYZEN_AI_INSTALLATION_PATH" in os.environ:
            base_path = os.environ["RYZEN_AI_INSTALLATION_PATH"]
            xclbin_path = os.path.join(base_path, "voe-4.0-win_amd64", "xclbins", "strix", "AMD_AIE2P_Nx4_Overlay.xclbin")
            
            if os.path.exists(xclbin_path):
                os.environ["XLNX_VART_FIRMWARE"] = xclbin_path
                os.environ["XLNX_TARGET_NAME"] = "AMD_AIE2P_Nx4_Overlay"
                os.environ["NUM_OF_DPU_RUNNERS"] = "1"
                print("âœ… NPUã‚ªãƒ¼ãƒãƒ¬ã‚¤è¨­å®šå®Œäº†")
                self.optimization_stats["npu_enabled"] = True
            else:
                print("âŒ NPUã‚ªãƒ¼ãƒãƒ¬ã‚¤ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    def _initialize_infer_os_optimizations(self):
        """Infer-OSæœ€é©åŒ–æ©Ÿèƒ½åˆæœŸåŒ–"""
        if not self.enable_infer_os:
            return
        
        print("ğŸš€ Infer-OSæœ€é©åŒ–æ©Ÿèƒ½åˆæœŸåŒ–ä¸­...")
        
        # ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if self.use_aggressive_memory and AGGRESSIVE_MEMORY_AVAILABLE:
            try:
                self.aggressive_memory_optimizer = AggressiveMemoryOptimizer(self.model_name)
                print("âœ… ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ–")
                self.optimization_stats["memory_optimized"] = True
            except Exception as e:
                print(f"âš ï¸ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–
        if self.use_advanced_quant and ADVANCED_QUANT_AVAILABLE:
            try:
                profile_map = {
                    "safe": QuantizationProfile.SAFE,
                    "balanced": QuantizationProfile.BALANCED,
                    "aggressive": QuantizationProfile.AGGRESSIVE
                }
                profile = profile_map.get(self.quantization_profile, QuantizationProfile.BALANCED)
                self.advanced_quantizer = AdvancedQuantizationOptimizer(profile=profile)
                print(f"âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ– ({self.quantization_profile})")
                self.optimization_stats["quantization_applied"] = True
            except Exception as e:
                print(f"âš ï¸ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Windows NPUæœ€é©åŒ–
        if self.enable_windows_npu and WINDOWS_NPU_AVAILABLE:
            try:
                self.windows_npu_optimizer = WindowsNPUOptimizer()
                if self.windows_npu_optimizer.is_npu_available():
                    print("âœ… Windows NPUæœ€é©åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ–")
                    self.optimization_stats["windows_npu_active"] = True
                else:
                    print("âš ï¸ Windows NPUæœ€é©åŒ–: NPUæœªæ¤œå‡º")
            except Exception as e:
                print(f"âš ï¸ Windows NPUæœ€é©åŒ–åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        if COMPARISON_BENCHMARK_AVAILABLE:
            try:
                self.comparison_benchmark = ComparisonBenchmark()
                print("âœ… æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½ã‚’åˆæœŸåŒ–")
            except Exception as e:
                print(f"âš ï¸ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµ±è¨ˆæ›´æ–°
        self.optimization_stats["infer_os_enabled"] = True
        self.optimization_stats["total_optimizations"] = sum([
            self.optimization_stats["npu_enabled"],
            self.optimization_stats["memory_optimized"],
            self.optimization_stats["quantization_applied"],
            self.optimization_stats["windows_npu_active"]
        ])
    
    def setup_model(self) -> bool:
        """çµ±åˆæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print(f"ğŸš€ çµ±åˆæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"âš¡ NPUæœ€é©åŒ–: {'âœ…' if self.enable_npu else 'âŒ'}")
        print(f"ğŸ”§ Infer-OSæœ€é©åŒ–: {'âœ…' if self.enable_infer_os else 'âŒ'}")
        print("=" * 60)
        
        try:
            # Phase 1: Infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            if self.enable_infer_os and self.aggressive_memory_optimizer:
                print("ğŸ”„ Phase 1: Infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰")
                success = self._load_with_infer_os_optimization()
                if not success:
                    print("âš ï¸ Infer-OSæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã€æ¨™æº–ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ")
                    success = self._load_standard_model()
            else:
                print("ğŸ”„ Phase 1: æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰")
                success = self._load_standard_model()
            
            if not success:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # Phase 2: NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if self.enable_npu:
                print("ğŸ”„ Phase 2: NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
                self._setup_npu_optimization()
            
            # Phase 3: çµ±åˆæœ€é©åŒ–é©ç”¨
            print("ğŸ”„ Phase 3: çµ±åˆæœ€é©åŒ–é©ç”¨")
            self._apply_integrated_optimizations()
            
            print("âœ… çµ±åˆæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            self._display_optimization_summary()
            
            return True
            
        except Exception as e:
            print(f"âŒ çµ±åˆæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _load_with_infer_os_optimization(self) -> bool:
        """Infer-OSæœ€é©åŒ–ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("ğŸš€ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
            success = self.aggressive_memory_optimizer.load_model_with_chunked_loading()
            
            if success:
                self.model = self.aggressive_memory_optimizer.model
                self.tokenizer = self.aggressive_memory_optimizer.tokenizer
                print("âœ… Infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True
            else:
                print("âŒ Infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ Infer-OSæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _load_standard_model(self) -> bool:
        """æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
            if "amd-npu" in self.model_name:
                model_files = [
                    "pytorch_llama3_8b_w_bit_4_awq_amd.pt",
                    "alma_w_bit_4_awq_fa_amd.pt"
                ]
                
                for model_file in model_files:
                    model_path = os.path.join(self.model_name, model_file)
                    if os.path.exists(model_path):
                        self.model = torch.load(model_path)
                        self.model.eval()
                        self.model = self.model.to(torch.bfloat16)
                        print(f"âœ… NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {model_file}")
                        return True
                
                print("âŒ NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            else:
                # é€šå¸¸ã®Hugging Faceãƒ¢ãƒ‡ãƒ«
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    torch_dtype=torch.bfloat16,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                print("âœ… æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True
                
        except Exception as e:
            print(f"âŒ æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _setup_npu_optimization(self):
        """NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("âš¡ NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
            
            # VitisAI ExecutionProviderç¢ºèª
            if ONNX_AVAILABLE:
                providers = ort.get_available_providers()
                if 'VitisAIExecutionProvider' in providers:
                    print("âœ… VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½")
                    
                    # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
                    self._create_npu_session()
                else:
                    print("âš ï¸ VitisAI ExecutionProvideråˆ©ç”¨ä¸å¯")
            
            # qlinearé‡å­åŒ–è¨­å®š
            if QLINEAR_AVAILABLE and hasattr(self.model, 'named_modules'):
                print("ğŸ”§ qlinearé‡å­åŒ–è¨­å®šä¸­...")
                for n, m in self.model.named_modules():
                    if hasattr(m, '__class__') and 'QLinearPerGrp' in str(m.__class__):
                        print(f"ğŸ“Š é‡å­åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼è¨­å®š: {n}")
                        if hasattr(m, 'device'):
                            m.device = "aie"
                        if hasattr(m, 'quantize_weights'):
                            m.quantize_weights()
                print("âœ… qlinearé‡å­åŒ–è¨­å®šå®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_npu_session(self):
        """NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            # ç°¡å˜ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
            import numpy as np
            
            # ãƒ€ãƒŸãƒ¼ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            input_shape = (1, 512)
            output_shape = (1, 32000)
            
            # VitisAI ExecutionProviderè¨­å®š
            provider_options = {
                'VitisAIExecutionProvider': {
                    'config_file': self._get_vaip_config_path(),
                    'target': 'AMD_AIE2P_Nx4_Overlay'
                }
            }
            
            providers = [
                ('VitisAIExecutionProvider', provider_options['VitisAIExecutionProvider']),
                'CPUExecutionProvider'
            ]
            
            # å®Ÿéš›ã®NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯å¾Œã§å®Ÿè£…
            print("âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šå®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_vaip_config_path(self) -> str:
        """VitisAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—"""
        if "RYZEN_AI_INSTALLATION_PATH" in os.environ:
            base_path = os.environ["RYZEN_AI_INSTALLATION_PATH"]
            config_path = os.path.join(base_path, "voe-4.0-win_amd64", "vaip_config.json")
            if os.path.exists(config_path):
                return config_path
        
        return ""
    
    def _apply_integrated_optimizations(self):
        """çµ±åˆæœ€é©åŒ–é©ç”¨"""
        try:
            print("ğŸ”§ çµ±åˆæœ€é©åŒ–é©ç”¨ä¸­...")
            
            # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–
            if self.advanced_quantizer and self.model:
                print("ğŸ“Š é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–é©ç”¨ä¸­...")
                try:
                    self.model = self.advanced_quantizer.optimize_model(self.model)
                    print("âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–å®Œäº†")
                except Exception as e:
                    print(f"âš ï¸ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
            # Windows NPUæœ€é©åŒ–
            if self.windows_npu_optimizer and self.windows_npu_optimizer.is_npu_available():
                print("ğŸªŸ Windows NPUæœ€é©åŒ–é©ç”¨ä¸­...")
                try:
                    self.model = self.windows_npu_optimizer.optimize_for_npu(self.model)
                    print("âœ… Windows NPUæœ€é©åŒ–å®Œäº†")
                except Exception as e:
                    print(f"âš ï¸ Windows NPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
            print("âœ… çµ±åˆæœ€é©åŒ–é©ç”¨å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ çµ±åˆæœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _display_optimization_summary(self):
        """æœ€é©åŒ–ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ“Š çµ±åˆæœ€é©åŒ–ã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        print(f"âš¡ NPUæœ€é©åŒ–: {'âœ…' if self.optimization_stats['npu_enabled'] else 'âŒ'}")
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: {'âœ…' if self.optimization_stats['memory_optimized'] else 'âŒ'}")
        print(f"ğŸ“Š é‡å­åŒ–æœ€é©åŒ–: {'âœ…' if self.optimization_stats['quantization_applied'] else 'âŒ'}")
        print(f"ğŸªŸ Windows NPU: {'âœ…' if self.optimization_stats['windows_npu_active'] else 'âŒ'}")
        print(f"ğŸ”§ Infer-OSçµ±åˆ: {'âœ…' if self.optimization_stats['infer_os_enabled'] else 'âŒ'}")
        print(f"ğŸ“ˆ ç·æœ€é©åŒ–æ•°: {self.optimization_stats['total_optimizations']}/4")
        print("=" * 50)
    
    def generate_text(self, prompt: str, max_new_tokens: int = 200) -> str:
        """çµ±åˆæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.model or not self.tokenizer:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            print(f"ğŸ”„ çµ±åˆæœ€é©åŒ–ç”Ÿæˆé–‹å§‹...")
            start_time = time.time()
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹
            if self.comparison_benchmark:
                self.comparison_benchmark.start_benchmark()
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼æº–å‚™
            if "amd-npu" in self.model_name:
                # NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ç”¨
                messages = [
                    {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ]
                
                input_ids = self.tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    return_tensors="pt",
                    return_dict=True
                )
                
                # NPUç”Ÿæˆ
                outputs = self.model.generate(
                    input_ids['input_ids'],
                    max_new_tokens=max_new_tokens,
                    attention_mask=input_ids['attention_mask'],
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                response = outputs[0][input_ids['input_ids'].shape[-1]:]
                response_text = self.tokenizer.decode(response, skip_special_tokens=True)
                
            else:
                # é€šå¸¸ã®Hugging Faceãƒ¢ãƒ‡ãƒ«ç”¨
                messages = [{"role": "user", "content": prompt}]
                
                input_ids = self.tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(self.model.device)
                
                output_ids = self.model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                response = output_ids[0][input_ids.shape[-1]:]
                response_text = self.tokenizer.decode(response, skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            tokens_generated = len(response)
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµ‚äº†
            if self.comparison_benchmark:
                self.comparison_benchmark.end_benchmark()
            
            print(f"âœ… çµ±åˆæœ€é©åŒ–ç”Ÿæˆå®Œäº†: {tokens_generated}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.2f}ç§’")
            print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_generated/generation_time:.2f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            
            return response_text
            
        except Exception as e:
            print(f"âŒ çµ±åˆæœ€é©åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """æ€§èƒ½çµ±è¨ˆå–å¾—"""
        stats = {
            "optimization_summary": self.optimization_stats,
            "model_info": {
                "name": self.model_name,
                "type": "NPUæœ€é©åŒ–" if "amd-npu" in self.model_name else "æ¨™æº–",
                "memory_usage": self._get_memory_usage()
            }
        }
        
        if self.comparison_benchmark:
            stats["benchmark_results"] = self.comparison_benchmark.get_results()
        
        return stats
    
    def _get_memory_usage(self) -> str:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024 ** 3)
            return f"{memory_gb:.1f}GB"
        except:
            return "Unknown"


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="çµ±åˆNPU + Infer-OSæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--model", default="llama3-8b-amd-npu", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--prompt", default="äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", help="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-tokens", type=int, default=200, help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--disable-npu", action="store_true", help="NPUæœ€é©åŒ–ç„¡åŠ¹")
    parser.add_argument("--disable-infer-os", action="store_true", help="Infer-OSæœ€é©åŒ–ç„¡åŠ¹")
    parser.add_argument("--quantization-profile", default="balanced", 
                       choices=["safe", "balanced", "aggressive"], help="é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    
    args = parser.parse_args()
    
    print("ğŸš€ çµ±åˆNPU + Infer-OSæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«")
    print("ğŸ¯ çœŸã®åŒ…æ‹¬çš„æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = IntegratedNPUInferOS(
        model_name=args.model,
        enable_npu=not args.disable_npu,
        enable_infer_os=not args.disable_infer_os,
        quantization_profile=args.quantization_profile
    )
    
    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    if not system.setup_model():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    if args.interactive:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("\nğŸ‡¯ğŸ‡µ çµ±åˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("ğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†ã€'stats'ã§çµ±è¨ˆè¡¨ç¤º")
        print("=" * 70)
        
        while True:
            try:
                prompt = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if prompt.lower() == 'stats':
                    stats = system.get_performance_stats()
                    print("\nğŸ“Š æ€§èƒ½çµ±è¨ˆ:")
                    for key, value in stats.items():
                        print(f"  {key}: {value}")
                    continue
                
                if not prompt.strip():
                    continue
                
                print("\nğŸ”„ çµ±åˆæœ€é©åŒ–ç”Ÿæˆä¸­...")
                response = system.generate_text(prompt, args.max_tokens)
                
                print(f"\nâœ… ç”Ÿæˆå®Œäº†:")
                print(f"ğŸ“ å¿œç­”: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        # å˜ç™ºå®Ÿè¡Œ
        print(f"ğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
        print("\nğŸ”„ çµ±åˆæœ€é©åŒ–ç”Ÿæˆä¸­...")
        
        response = system.generate_text(args.prompt, args.max_tokens)
        
        print(f"\nâœ… ç”Ÿæˆå®Œäº†:")
        print(f"ğŸ“ å¿œç­”: {response}")
        
        # çµ±è¨ˆè¡¨ç¤º
        stats = system.get_performance_stats()
        print(f"\nğŸ“Š æ€§èƒ½çµ±è¨ˆ: {stats}")
    
    print("\nğŸ çµ±åˆNPU + Infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")


if __name__ == "__main__":
    main()

