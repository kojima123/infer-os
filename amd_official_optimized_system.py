# -*- coding: utf-8 -*-
"""
AMDå…¬å¼ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹  infer-OSãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ 
OnnxRuntime GenAI (OGA) + Lemonade SDKä½¿ç”¨
"""

import os
import sys
import time
import argparse
import json
import psutil
import threading
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class PerformanceMonitor:
    """æ€§èƒ½ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.monitoring = False
        self.cpu_samples = []
        self.memory_samples = []
        self.monitor_thread = None
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.cpu_samples = []
        self.memory_samples = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
    
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                self.cpu_samples.append(cpu_percent)
                self.memory_samples.append(memory_percent)
                time.sleep(0.5)
            except:
                break
    
    def get_report(self) -> Dict[str, float]:
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        if not self.cpu_samples or not self.memory_samples:
            return {"avg_cpu": 0.0, "max_cpu": 0.0, "avg_memory": 0.0, "max_memory": 0.0}
        
        return {
            "avg_cpu": sum(self.cpu_samples) / len(self.cpu_samples),
            "max_cpu": max(self.cpu_samples),
            "avg_memory": sum(self.memory_samples) / len(self.memory_samples),
            "max_memory": max(self.memory_samples),
            "samples": len(self.cpu_samples)
        }

class AMDOfficialOptimizedSystem:
    """AMDå…¬å¼ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹ ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, enable_infer_os: bool = False, timeout_seconds: int = 30):
        self.enable_infer_os = enable_infer_os
        self.timeout_seconds = timeout_seconds
        self.session = None
        self.active_provider = None
        self.performance_monitor = PerformanceMonitor()
        
        print("ğŸš€ AMDå…¬å¼ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if enable_infer_os else 'ç„¡åŠ¹'}")
        print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout_seconds}ç§’")
    
    def _setup_infer_os_environment(self):
        """infer-OSç’°å¢ƒè¨­å®šï¼ˆå…¬å¼æ¨å¥¨æ–¹å¼ï¼‰"""
        if self.enable_infer_os:
            print("ğŸ”§ infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šä¸­...")
            
            # å…¬å¼æ¨å¥¨ã®infer-OSç’°å¢ƒå¤‰æ•°
            infer_os_env = {
                'INFER_OS_ENABLE': '1',
                'INFER_OS_OPTIMIZATION_LEVEL': 'high',
                'INFER_OS_NPU_ACCELERATION': '1',
                'INFER_OS_MEMORY_OPTIMIZATION': '1'
            }
            
            for key, value in infer_os_env.items():
                os.environ[key] = value
                print(f"  ğŸ“ {key}={value}")
            
            print("âœ… infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šå®Œäº†")
        else:
            print("ğŸ”§ infer-OSæœ€é©åŒ–: ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼‰")
            # infer-OSç„¡åŠ¹åŒ–
            os.environ.pop('INFER_OS_ENABLE', None)
            os.environ.pop('INFER_OS_OPTIMIZATION_LEVEL', None)
            os.environ.pop('INFER_OS_NPU_ACCELERATION', None)
            os.environ.pop('INFER_OS_MEMORY_OPTIMIZATION', None)
    
    def _create_simple_benchmark_model(self, model_path: str) -> bool:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            print("ğŸ“„ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ï¼ˆå…¬å¼æ¨å¥¨æ§‹é€ ï¼‰
            import torch
            import torch.nn as nn
            
            class BenchmarkModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # å…¬å¼ã‚µãƒ³ãƒ—ãƒ«ã«è¿‘ã„ã‚·ãƒ³ãƒ—ãƒ«æ§‹é€ 
                    self.linear1 = nn.Linear(224, 512)
                    self.relu1 = nn.ReLU()
                    self.linear2 = nn.Linear(512, 256)
                    self.relu2 = nn.ReLU()
                    self.linear3 = nn.Linear(256, 10)
                
                def forward(self, x):
                    x = self.relu1(self.linear1(x))
                    x = self.relu2(self.linear2(x))
                    x = self.linear3(x)
                    return x
            
            model = BenchmarkModel()
            model.eval()
            
            # å…¬å¼æ¨å¥¨ã®å…¥åŠ›å½¢çŠ¶
            dummy_input = torch.randn(1, 224)
            
            # ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå…¬å¼æ¨å¥¨è¨­å®šï¼‰
            torch.onnx.export(
                model,
                dummy_input,
                model_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³èª¿æ•´ï¼ˆRyzen AI 1.5äº’æ›ï¼‰
            import onnx
            onnx_model = onnx.load(model_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, model_path)
            
            print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {model_path}")
            print(f"ğŸ“‹ IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: {onnx_model.ir_version}")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(model_path) / 1024:.1f} KB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _setup_session_with_official_settings(self, model_path: str) -> bool:
        """å…¬å¼æ¨å¥¨è¨­å®šã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("âš¡ å…¬å¼æ¨å¥¨è¨­å®šã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            available_providers = ort.get_available_providers()
            print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {available_providers}")
            
            # å…¬å¼æ¨å¥¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3  # ã‚¨ãƒ©ãƒ¼ã®ã¿
            
            # infer-OSæœ‰åŠ¹æ™‚ã®è¿½åŠ è¨­å®š
            if self.enable_infer_os:
                session_options.enable_cpu_mem_arena = True
                session_options.enable_mem_pattern = True
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                print("ğŸ”§ infer-OSæœ€é©åŒ–: ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€é©åŒ–æœ‰åŠ¹")
            else:
                session_options.enable_cpu_mem_arena = False
                session_options.enable_mem_pattern = False
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
                print("ğŸ”§ infer-OSæœ€é©åŒ–: ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€é©åŒ–ç„¡åŠ¹")
            
            # VitisAI ExecutionProviderï¼ˆå…¬å¼æ¨å¥¨è¨­å®šï¼‰
            if 'VitisAIExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ VitisAIExecutionProviderè©¦è¡Œï¼ˆå…¬å¼æ¨å¥¨è¨­å®šï¼‰...")
                    
                    # å…¬å¼æ¨å¥¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                    vitisai_options = {
                        "cache_dir": "C:/temp/vaip_cache",
                        "cache_key": "benchmark_model",
                        "log_level": "info"
                    }
                    
                    providers = [
                        ('VitisAIExecutionProvider', vitisai_options),
                        'CPUExecutionProvider'
                    ]
                    
                    self.session = ort.InferenceSession(
                        model_path,
                        sess_options=session_options,
                        providers=providers
                    )
                    
                    self.active_provider = 'VitisAIExecutionProvider'
                    print("âœ… VitisAIExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆå…¬å¼æ¨å¥¨è¨­å®šï¼‰")
                    
                except Exception as e:
                    print(f"âš ï¸ VitisAIExecutionProviderå¤±æ•—: {e}")
                    self.session = None
            
            # DmlExecutionProvider ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.session is None and 'DmlExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ DmlExecutionProviderè©¦è¡Œ...")
                    self.session = ort.InferenceSession(
                        model_path,
                        sess_options=session_options,
                        providers=['DmlExecutionProvider', 'CPUExecutionProvider']
                    )
                    self.active_provider = 'DmlExecutionProvider'
                    print("âœ… DmlExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ DmlExecutionProviderå¤±æ•—: {e}")
                    self.session = None
            
            # CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.session is None:
                try:
                    print("ğŸ”„ CPUExecutionProviderè©¦è¡Œ...")
                    self.session = ort.InferenceSession(
                        model_path,
                        sess_options=session_options,
                        providers=['CPUExecutionProvider']
                    )
                    self.active_provider = 'CPUExecutionProvider'
                    print("âœ… CPUExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âŒ CPUExecutionProviderå¤±æ•—: {e}")
                    return False
            
            if self.session is None:
                return False
            
            print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ”§ ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.session.get_providers()}")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            
            # å‹•ä½œãƒ†ã‚¹ãƒˆ
            try:
                test_input = np.random.randn(1, 224).astype(np.float32)
                test_output = self.session.run(None, {'input': test_input})
                print(f"âœ… å‹•ä½œãƒ†ã‚¹ãƒˆå®Œäº†: å‡ºåŠ›å½¢çŠ¶ {test_output[0].shape}")
            except Exception as e:
                print(f"âš ï¸ å‹•ä½œãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
                return False
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
            
            # infer-OSç’°å¢ƒè¨­å®š
            self._setup_infer_os_environment()
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            model_path = "benchmark_model.onnx"
            if not self._create_simple_benchmark_model(model_path):
                return False
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self._setup_session_with_official_settings(model_path):
                return False
            
            print("âœ… AMDå…¬å¼ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    def run_benchmark(self, num_inferences: int = 50) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        if self.session is None:
            print("âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return {}
        
        try:
            print(f"ğŸ¯ infer-OSãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹ï¼ˆæ¨è«–å›æ•°: {num_inferences}ï¼‰")
            print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            
            # æ€§èƒ½ç›£è¦–é–‹å§‹
            self.performance_monitor.start_monitoring()
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            start_time = time.time()
            successful_inferences = 0
            failed_inferences = 0
            
            for i in range(num_inferences):
                try:
                    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                    input_data = np.random.randn(1, 224).astype(np.float32)
                    
                    # æ¨è«–å®Ÿè¡Œ
                    inference_start = time.time()
                    output = self.session.run(None, {'input': input_data})
                    inference_time = time.time() - inference_start
                    
                    successful_inferences += 1
                    
                    # é€²æ—è¡¨ç¤º
                    if (i + 1) % 10 == 0:
                        print(f"  ğŸ“Š é€²æ—: {i + 1}/{num_inferences} ({inference_time*1000:.1f}ms)")
                
                except Exception as e:
                    failed_inferences += 1
                    print(f"  âŒ æ¨è«–{i+1}å¤±æ•—: {e}")
            
            total_time = time.time() - start_time
            
            # æ€§èƒ½ç›£è¦–åœæ­¢
            self.performance_monitor.stop_monitoring()
            performance_report = self.performance_monitor.get_report()
            
            # çµæœè¨ˆç®—
            throughput = successful_inferences / total_time if total_time > 0 else 0
            avg_inference_time = total_time / successful_inferences if successful_inferences > 0 else 0
            
            results = {
                "infer_os_enabled": self.enable_infer_os,
                "active_provider": self.active_provider,
                "total_inferences": num_inferences,
                "successful_inferences": successful_inferences,
                "failed_inferences": failed_inferences,
                "total_time": total_time,
                "throughput": throughput,
                "avg_inference_time": avg_inference_time,
                "performance": performance_report
            }
            
            # çµæœè¡¨ç¤º
            print(f"\nğŸ¯ infer-OSãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
            print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {successful_inferences}")
            print(f"  âŒ å¤±æ•—æ¨è«–å›æ•°: {failed_inferences}")
            print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
            print(f"  ğŸ“Š ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} æ¨è«–/ç§’")
            print(f"  â±ï¸ å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time*1000:.1f}ms")
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"\nğŸ“Š æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ:")
            print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {performance_report['avg_cpu']:.1f}%")
            print(f"  ğŸ’» æœ€å¤§CPUä½¿ç”¨ç‡: {performance_report['max_cpu']:.1f}%")
            print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {performance_report['avg_memory']:.1f}%")
            print(f"  ğŸ’¾ æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {performance_report['max_memory']:.1f}%")
            print(f"  ğŸ”¢ ã‚µãƒ³ãƒ—ãƒ«æ•°: {performance_report['samples']}")
            
            return results
            
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {}

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="AMDå…¬å¼ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹  infer-OSãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--timeout", type=int, default=30, help="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--inferences", type=int, default=50, help="æ¨è«–å›æ•°")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒå®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    if args.compare:
        print("ğŸ”„ infer-OS ON/OFFæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        
        # infer-OS OFF
        print("\n" + "="*60)
        print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰")
        print("="*60)
        system_off = AMDOfficialOptimizedSystem(enable_infer_os=False, timeout_seconds=args.timeout)
        if system_off.initialize():
            results_off = system_off.run_benchmark(args.inferences)
        else:
            print("âŒ infer-OS OFF ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
            return
        
        # infer-OS ON
        print("\n" + "="*60)
        print("ğŸ“Š æœ€é©åŒ–æ¸¬å®šï¼ˆinfer-OS ONï¼‰")
        print("="*60)
        system_on = AMDOfficialOptimizedSystem(enable_infer_os=True, timeout_seconds=args.timeout)
        if system_on.initialize():
            results_on = system_on.run_benchmark(args.inferences)
        else:
            print("âŒ infer-OS ON ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
            return
        
        # æ¯”è¼ƒçµæœè¡¨ç¤º
        if results_off and results_on:
            print("\n" + "="*60)
            print("ğŸ“Š infer-OS ON/OFF æ¯”è¼ƒçµæœ")
            print("="*60)
            
            throughput_improvement = (results_on['throughput'] / results_off['throughput'] - 1) * 100 if results_off['throughput'] > 0 else 0
            time_improvement = (1 - results_on['avg_inference_time'] / results_off['avg_inference_time']) * 100 if results_off['avg_inference_time'] > 0 else 0
            
            print(f"âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ:")
            print(f"  OFF: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ON:  {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  æ”¹å–„: {throughput_improvement:+.1f}%")
            
            print(f"â±ï¸ å¹³å‡æ¨è«–æ™‚é–“:")
            print(f"  OFF: {results_off['avg_inference_time']*1000:.1f}ms")
            print(f"  ON:  {results_on['avg_inference_time']*1000:.1f}ms")
            print(f"  æ”¹å–„: {time_improvement:+.1f}%")
            
            print(f"ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡:")
            print(f"  OFF: {results_off['performance']['avg_cpu']:.1f}%")
            print(f"  ON:  {results_on['performance']['avg_cpu']:.1f}%")
            
            print(f"ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡:")
            print(f"  OFF: {results_off['performance']['avg_memory']:.1f}%")
            print(f"  ON:  {results_on['performance']['avg_memory']:.1f}%")
    
    else:
        # å˜ä¸€å®Ÿè¡Œ
        system = AMDOfficialOptimizedSystem(enable_infer_os=args.infer_os, timeout_seconds=args.timeout)
        if system.initialize():
            system.run_benchmark(args.inferences)
        else:
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")

if __name__ == "__main__":
    main()

