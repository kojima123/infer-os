"""
NPUãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã¨NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è©³ç´°ç¢ºèªç”¨

ä½¿ç”¨æ–¹æ³•:
    python debug_npu_info.py
"""

import sys
import traceback

def check_onnxruntime_providers():
    """ONNXRuntimeãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª"""
    try:
        import onnxruntime as ort
        
        print("ğŸ” ONNXRuntime ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±:")
        print("=" * 50)
        
        # åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        available_providers = ort.get_available_providers()
        print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {len(available_providers)}å€‹")
        for i, provider in enumerate(available_providers, 1):
            print(f"  {i}. {provider}")
        
        # é‡è¦ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèª
        important_providers = [
            'VitisAIExecutionProvider',
            'DmlExecutionProvider', 
            'CUDAExecutionProvider',
            'CPUExecutionProvider'
        ]
        
        print(f"\nğŸ¯ é‡è¦ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®çŠ¶æ³:")
        for provider in important_providers:
            status = "âœ… åˆ©ç”¨å¯èƒ½" if provider in available_providers else "âŒ åˆ©ç”¨ä¸å¯"
            print(f"  {provider}: {status}")
        
        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
        print(f"\nğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±:")
        try:
            device_count = ort.get_device()
            print(f"  ãƒ‡ãƒã‚¤ã‚¹: {device_count}")
        except:
            print("  ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—ä¸å¯")
        
        return True
        
    except ImportError as e:
        print(f"âŒ ONNXRuntimeæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {e}")
        return False
    except Exception as e:
        print(f"âŒ ONNXRuntimeç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False

def check_torch_info():
    """PyTorchæƒ…å ±ç¢ºèª"""
    try:
        import torch
        
        print("\nğŸ”¥ PyTorchæƒ…å ±:")
        print("=" * 50)
        
        print(f"ğŸ“¦ PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
        print(f"ğŸ–¥ï¸ CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"ğŸ¯ CUDAãƒ‡ãƒã‚¤ã‚¹æ•°: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                device_name = torch.cuda.get_device_name(i)
                print(f"  ãƒ‡ãƒã‚¤ã‚¹{i}: {device_name}")
        
        # DirectMLç¢ºèª
        try:
            device = torch.device("cuda:0")
            test_tensor = torch.randn(1, 1).to(device)
            print(f"âœ… DirectMLãƒ†ã‚¹ãƒˆæˆåŠŸ: {test_tensor.device}")
        except Exception as e:
            print(f"âš ï¸ DirectMLãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {e}")
        return False
    except Exception as e:
        print(f"âŒ PyTorchç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False

def test_npu_session():
    """NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    try:
        import onnxruntime as ort
        import numpy as np
        
        print("\nğŸ§ª NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ:")
        print("=" * 50)
        
        # VitisAIå„ªå…ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
        providers = [
            'VitisAIExecutionProvider',
            ('DmlExecutionProvider', {
                'device_id': 0,
                'enable_dynamic_graph_fusion': True,
                'enable_graph_optimization': True,
            }),
            'CPUExecutionProvider'
        ]
        
        print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š:")
        for i, provider in enumerate(providers, 1):
            print(f"  {i}. {provider}")
        
        # ç°¡å˜ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
        import torch
        
        class TestModel(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(10, 5)
            
            def forward(self, x):
                return self.linear(x)
        
        model = TestModel()
        model.eval()
        
        dummy_input = torch.randn(1, 10)
        onnx_path = "./test_npu_model.onnx"
        
        # ONNXå¤‰æ›
        torch.onnx.export(
            model, dummy_input, onnx_path,
            export_params=True, opset_version=11,
            input_names=['input'], output_names=['output']
        )
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ: {onnx_path}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆãƒ†ã‚¹ãƒˆ
        session = ort.InferenceSession(onnx_path, providers=providers)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
        active_providers = session.get_providers()
        print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
        
        # æ¨è«–ãƒ†ã‚¹ãƒˆ
        test_input = np.random.randn(1, 10).astype(np.float32)
        output = session.run(['output'], {'input': test_input})
        
        print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶{output[0].shape}")
        
        # ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¤å®š
        if 'VitisAIExecutionProvider' in active_providers:
            print("ğŸ¯ VitisAI (çœŸã®NPU) ã§å®Ÿè¡Œä¸­")
        elif 'DmlExecutionProvider' in active_providers:
            print("ğŸ¯ DirectML (GPU/NPU) ã§å®Ÿè¡Œä¸­")
        else:
            print("âš ï¸ CPU ã§å®Ÿè¡Œä¸­")
        
        return True
        
    except Exception as e:
        print(f"âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ NPUãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º")
    print("ğŸ¯ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã¨NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è©³ç´°ç¢ºèª")
    print("=" * 60)
    
    # ONNXRuntimeç¢ºèª
    ort_ok = check_onnxruntime_providers()
    
    # PyTorchç¢ºèª
    torch_ok = check_torch_info()
    
    # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
    if ort_ok and torch_ok:
        test_npu_session()
    
    print("\nğŸ“Š ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤ºå®Œäº†")
    print("ğŸ’¡ ã“ã®æƒ…å ±ã‚’ä½¿ã£ã¦ä»£æ›¿NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å‹•ä½œã‚’ç¢ºèªã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()

