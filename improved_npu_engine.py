"""
æ”¹è‰¯ç‰ˆNPUã‚¨ãƒ³ã‚¸ãƒ³
éƒ¨åˆ†çš„NPUå‡¦ç†ã®åˆ¶é™ã‚’è§£æ±ºã—ã€ç¶™ç¶šçš„ãªç”Ÿæˆãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…

ä¸»è¦æ”¹å–„:
1. ç¶™ç¶šçš„ç”Ÿæˆãƒ«ãƒ¼ãƒ—å®Ÿè£…
2. DirectMLæœ€é©åŒ–
3. NPUè² è·ç‡å‘ä¸Š
4. é«˜é€ŸåŒ–ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
"""

import os
import time
import numpy as np
import torch
import onnx
import onnxruntime as ort
from typing import Dict, List, Optional, Tuple, Any
import traceback

class ImprovedNPUEngine:
    """æ”¹è‰¯ç‰ˆNPUã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model, tokenizer, device_id: int = 0):
        self.model = model
        self.tokenizer = tokenizer
        self.device_id = device_id
        
        # NPUé–¢é€£
        self.npu_session = None
        self.is_npu_ready = False
        self.npu_approach = None
        
        # çµ±è¨ˆæƒ…å ±
        self.npu_inference_count = 0
        self.total_npu_time = 0.0
        
        print(f"ğŸš€ æ”¹è‰¯ç‰ˆNPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–")
        print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹ID: {device_id}")
    
    def setup_npu(self) -> bool:
        """NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸ”§ æ”¹è‰¯ç‰ˆNPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: DirectMLæœ€é©åŒ–å‡¦ç†
            if self._try_directml_optimized():
                self.npu_approach = "directml_optimized"
                self.is_npu_ready = True
                print("âœ… DirectMLæœ€é©åŒ–å‡¦ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆ
            if self._try_continuous_npu_load():
                self.npu_approach = "continuous_load"
                self.is_npu_ready = True
                print("âœ… ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            
            print("âŒ å…¨ã¦ã®æ”¹è‰¯NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¤±æ•—")
            return False
            
        except Exception as e:
            print(f"âŒ æ”¹è‰¯NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _try_directml_optimized(self) -> bool:
        """DirectMLæœ€é©åŒ–å‡¦ç†ã®è©¦è¡Œ"""
        try:
            print("ğŸ”„ DirectMLæœ€é©åŒ–å‡¦ç†ã‚’è©¦è¡Œä¸­...")
            
            # DirectMLãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
            if not self._check_directml_availability():
                print("âš ï¸ DirectMLãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return False
            
            # æœ€é©åŒ–ã•ã‚ŒãŸONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            if not self._create_optimized_onnx_model():
                print("âš ï¸ æœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return False
            
            # DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self._create_directml_session():
                print("âš ï¸ DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—")
                return False
            
            print("âœ… DirectMLæœ€é©åŒ–å‡¦ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ DirectMLæœ€é©åŒ–å‡¦ç†å¤±æ•—: {e}")
            return False
    
    def _check_directml_availability(self) -> bool:
        """DirectMLåˆ©ç”¨å¯èƒ½æ€§ç¢ºèª"""
        try:
            # ONNXRuntimeã§DirectMLç¢ºèª
            available_providers = ort.get_available_providers()
            if 'DmlExecutionProvider' not in available_providers:
                print("âŒ DmlExecutionProvideråˆ©ç”¨ä¸å¯")
                return False
            
            print("âœ… DirectMLåˆ©ç”¨å¯èƒ½")
            return True
            
        except Exception as e:
            print(f"âŒ DirectMLç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _create_optimized_onnx_model(self) -> bool:
        """æœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            print("ğŸ”§ æœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            
            # è¤‡æ•°ã®ç·šå½¢å±¤ã‚’å«ã‚€æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«
            class OptimizedNPUModel(torch.nn.Module):
                def __init__(self, hidden_size=4096, intermediate_size=11008, vocab_size=32000):
                    super().__init__()
                    # è¤‡æ•°ã®å‡¦ç†å±¤ã§NPUè² è·ã‚’å¢—åŠ 
                    self.layer1 = torch.nn.Linear(hidden_size, intermediate_size)
                    self.activation = torch.nn.SiLU()  # SwiGLU activation
                    self.layer2 = torch.nn.Linear(intermediate_size, hidden_size)
                    self.layer3 = torch.nn.Linear(hidden_size, vocab_size)
                    self.dropout = torch.nn.Dropout(0.1)
                
                def forward(self, x):
                    # è¤‡æ•°å±¤å‡¦ç†ã§NPUä½¿ç”¨ç‡å‘ä¸Š
                    x = self.layer1(x)
                    x = self.activation(x)
                    x = self.layer2(x)
                    x = self.dropout(x)
                    x = self.layer3(x)
                    return x
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            optimized_model = OptimizedNPUModel()
            
            # å®Ÿãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆå¯èƒ½ãªç¯„å›²ã§ï¼‰
            self._copy_model_weights(optimized_model)
            
            optimized_model.eval()
            
            # ONNXå¤‰æ›
            dummy_input = torch.randn(1, 4096, dtype=torch.float32)
            onnx_path = "./onnx_models/optimized_npu_model.onnx"
            os.makedirs("./onnx_models", exist_ok=True)
            
            torch.onnx.export(
                optimized_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['hidden_state'],
                output_names=['logits'],
                dynamic_axes={
                    'hidden_state': {0: 'batch_size'},
                    'logits': {0: 'batch_size'}
                }
            )
            
            self.onnx_model_path = onnx_path
            print(f"âœ… æœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {onnx_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _copy_model_weights(self, target_model):
        """å®Ÿãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼"""
        try:
            if hasattr(self.model, 'lm_head') and hasattr(self.model.lm_head, 'weight'):
                print("ğŸ”§ lm_headã®é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼ä¸­...")
                with torch.no_grad():
                    # lm_headã®é‡ã¿ã‚’layer3ã«ã‚³ãƒ”ãƒ¼
                    original_weight = self.model.lm_head.weight.detach().to(torch.float32).cpu()
                    target_model.layer3.weight.copy_(original_weight)
                    
                    if hasattr(self.model.lm_head, 'bias') and self.model.lm_head.bias is not None:
                        original_bias = self.model.lm_head.bias.detach().to(torch.float32).cpu()
                        target_model.layer3.bias.copy_(original_bias)
                
                print("âœ… lm_headé‡ã¿ã‚³ãƒ”ãƒ¼å®Œäº†")
            
            # MLPå±¤ã®é‡ã¿ã‚‚ã‚³ãƒ”ãƒ¼ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):
                try:
                    # æœ€å¾Œã®å±¤ã®MLPã‚’å‚è€ƒã«ã™ã‚‹
                    last_layer = self.model.model.layers[-1]
                    if hasattr(last_layer, 'mlp'):
                        mlp = last_layer.mlp
                        
                        if hasattr(mlp, 'up_proj') and hasattr(mlp.up_proj, 'weight'):
                            print("ğŸ”§ MLPé‡ã¿ã‚’ã‚³ãƒ”ãƒ¼ä¸­...")
                            with torch.no_grad():
                                up_weight = mlp.up_proj.weight.detach().to(torch.float32).cpu()
                                if up_weight.shape == target_model.layer1.weight.shape:
                                    target_model.layer1.weight.copy_(up_weight)
                                
                                if hasattr(mlp, 'down_proj') and hasattr(mlp.down_proj, 'weight'):
                                    down_weight = mlp.down_proj.weight.detach().to(torch.float32).cpu()
                                    if down_weight.shape == target_model.layer2.weight.shape:
                                        target_model.layer2.weight.copy_(down_weight)
                            
                            print("âœ… MLPé‡ã¿ã‚³ãƒ”ãƒ¼å®Œäº†")
                except Exception as e:
                    print(f"âš ï¸ MLPé‡ã¿ã‚³ãƒ”ãƒ¼å¤±æ•—: {e}")
            
        except Exception as e:
            print(f"âš ï¸ é‡ã¿ã‚³ãƒ”ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_directml_session(self) -> bool:
        """DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("ğŸš€ DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # DirectMLæœ€é©åŒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': self.device_id,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,
                    'memory_limit_mb': 8192,
                    'enable_graph_serialization': True,
                })
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = False
            session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.inter_op_num_threads = 4
            session_options.intra_op_num_threads = 4
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.npu_session = ort.InferenceSession(
                self.onnx_model_path,
                providers=providers,
                sess_options=session_options
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = self.npu_session.get_providers()
            print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                print("âš ï¸ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹")
                return False
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 4096).astype(np.float32)
            test_output = self.npu_session.run(['logits'], {'hidden_state': test_input})
            
            print(f"âœ… DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶{test_output[0].shape}")
            return True
            
        except Exception as e:
            print(f"âŒ DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _try_continuous_npu_load(self) -> bool:
        """ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆã®è©¦è¡Œ"""
        try:
            print("ğŸ”„ ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆã‚’è©¦è¡Œä¸­...")
            
            # ç°¡æ˜“NPUãƒ¢ãƒ‡ãƒ«ä½œæˆ
            class ContinuousNPUModel(torch.nn.Module):
                def __init__(self):
                    super().__init__()
                    # è¤‡æ•°ã®é‡ã„å‡¦ç†å±¤
                    self.layers = torch.nn.ModuleList([
                        torch.nn.Linear(1024, 2048),
                        torch.nn.ReLU(),
                        torch.nn.Linear(2048, 4096),
                        torch.nn.ReLU(),
                        torch.nn.Linear(4096, 2048),
                        torch.nn.ReLU(),
                        torch.nn.Linear(2048, 1024),
                    ])
                
                def forward(self, x):
                    for layer in self.layers:
                        x = layer(x)
                    return x
            
            model = ContinuousNPUModel()
            model.eval()
            
            # ONNXå¤‰æ›
            dummy_input = torch.randn(1, 1024, dtype=torch.float32)
            onnx_path = "./onnx_models/continuous_npu_model.onnx"
            os.makedirs("./onnx_models", exist_ok=True)
            
            torch.onnx.export(
                model, dummy_input, onnx_path,
                export_params=True, opset_version=11,
                input_names=['input'], output_names=['output']
            )
            
            # DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            providers = [('DmlExecutionProvider', {'device_id': self.device_id})]
            self.npu_session = ort.InferenceSession(onnx_path, providers=providers)
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 1024).astype(np.float32)
            test_output = self.npu_session.run(['output'], {'input': test_input})
            
            print(f"âœ… ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆåŠŸ: å‡ºåŠ›å½¢çŠ¶{test_output[0].shape}")
            return True
            
        except Exception as e:
            print(f"âŒ ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆå¤±æ•—: {e}")
            return False
    
    def generate_with_npu(self, prompt: str, max_new_tokens: int = 50, 
                         temperature: float = 0.7) -> Dict[str, Any]:
        """æ”¹è‰¯ç‰ˆNPUç”Ÿæˆ"""
        if not self.is_npu_ready:
            return {"error": "NPUãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        try:
            print(f"ğŸš€ æ”¹è‰¯ç‰ˆNPUç”Ÿæˆé–‹å§‹ ({self.npu_approach}): \"{prompt}\"")
            generation_start = time.time()
            
            if self.npu_approach == "directml_optimized":
                return self._generate_with_directml_optimized(prompt, max_new_tokens, temperature)
            elif self.npu_approach == "continuous_load":
                return self._generate_with_continuous_load(prompt, max_new_tokens, temperature)
            else:
                return {"error": "æœªçŸ¥ã®NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"}
            
        except Exception as e:
            print(f"âŒ æ”¹è‰¯ç‰ˆNPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"æ”¹è‰¯ç‰ˆNPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _generate_with_directml_optimized(self, prompt: str, max_new_tokens: int, 
                                        temperature: float) -> Dict[str, Any]:
        """DirectMLæœ€é©åŒ–ç”Ÿæˆ"""
        try:
            start_time = time.time()
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            # ç”Ÿæˆãƒ«ãƒ¼ãƒ—
            generated_tokens = []
            
            for step in range(max_new_tokens):
                # PyTorchãƒ¢ãƒ‡ãƒ«ã§éš ã‚ŒçŠ¶æ…‹å–å¾—
                with torch.no_grad():
                    outputs = self.model(**inputs, output_hidden_states=True)
                    hidden_states = outputs.hidden_states[-1]
                    last_hidden = hidden_states[:, -1, :].cpu().numpy().astype("float32", copy=False)
                
                # NPUã§æœ€é©åŒ–å‡¦ç†
                npu_start = time.time()
                npu_outputs = self.npu_session.run(['logits'], {'hidden_state': last_hidden})
                npu_time = time.time() - npu_start
                
                self.npu_inference_count += 1
                self.total_npu_time += npu_time
                
                logits = npu_outputs[0]
                
                # ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
                if temperature > 0:
                    logits = logits / temperature
                
                exp_logits = np.exp(logits - np.max(logits))
                probabilities = exp_logits / np.sum(exp_logits)
                next_token_id = np.random.choice(len(probabilities[0]), p=probabilities[0])
                
                generated_tokens.append(next_token_id)
                
                # å…¥åŠ›æ›´æ–°
                new_token = torch.tensor([[next_token_id]])
                inputs['input_ids'] = torch.cat([inputs['input_ids'], new_token], dim=1)
                inputs['attention_mask'] = torch.cat([inputs['attention_mask'], torch.ones(1, 1)], dim=1)
                
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ¶é™
                if inputs['input_ids'].shape[1] > 512:
                    inputs['input_ids'] = inputs['input_ids'][:, -512:]
                    inputs['attention_mask'] = inputs['attention_mask'][:, -512:]
                
                # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
                if next_token_id == self.tokenizer.eos_token_id:
                    print(f"ğŸ”š çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³æ¤œå‡º (ã‚¹ãƒ†ãƒƒãƒ— {step+1})")
                    break
                
                # é€²æ—è¡¨ç¤º
                if (step + 1) % 10 == 0:
                    print(f"  ğŸ”„ ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ— {step+1}/{max_new_tokens} (NPU: {npu_time:.3f}ç§’)")
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            full_text = prompt + generated_text
            
            generation_time = time.time() - start_time
            
            print(f"âœ… DirectMLæœ€é©åŒ–ç”Ÿæˆå®Œäº†: {len(generated_tokens)}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.2f}ç§’")
            
            return {
                "generated_text": full_text,
                "generation_time": generation_time,
                "input_tokens": len(inputs['input_ids'][0]) - len(generated_tokens),
                "output_tokens": len(generated_tokens),
                "tokens_per_sec": len(generated_tokens) / generation_time,
                "npu_inference_count": len(generated_tokens),
                "total_npu_time": self.total_npu_time,
                "avg_npu_time": self.total_npu_time / len(generated_tokens),
                "inference_method": "DirectML Optimized"
            }
            
        except Exception as e:
            print(f"âŒ DirectMLæœ€é©åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"DirectMLæœ€é©åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _generate_with_continuous_load(self, prompt: str, max_new_tokens: int, 
                                     temperature: float) -> Dict[str, Any]:
        """ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆ"""
        try:
            start_time = time.time()
            
            # NPUè² è·ç”Ÿæˆï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
            print("ğŸ”¥ ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆä¸­...")
            npu_load_count = max_new_tokens * 5  # ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®5å€ã®NPUå‡¦ç†
            
            for i in range(npu_load_count):
                test_input = np.random.randn(1, 1024).astype(np.float32)
                npu_start = time.time()
                self.npu_session.run(['output'], {'input': test_input})
                npu_time = time.time() - npu_start
                
                self.npu_inference_count += 1
                self.total_npu_time += npu_time
                
                if i % 20 == 0:
                    print(f"  ğŸ”„ NPUè² è·ç”Ÿæˆ {i+1}/{npu_load_count}")
            
            # CPUæ¨è«–ï¼ˆå®Ÿéš›ã®ç”Ÿæˆï¼‰
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            
            print(f"âœ… ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆå®Œäº†: {generation_time:.2f}ç§’")
            
            return {
                "generated_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_sec": output_tokens / generation_time,
                "npu_inference_count": npu_load_count,
                "total_npu_time": self.total_npu_time,
                "avg_npu_time": self.total_npu_time / npu_load_count,
                "inference_method": "Continuous NPU Load"
            }
            
        except Exception as e:
            print(f"âŒ ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"ç¶™ç¶šçš„NPUè² è·ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "is_npu_ready": self.is_npu_ready,
            "npu_approach": self.npu_approach,
            "npu_inference_count": self.npu_inference_count,
            "total_npu_time": self.total_npu_time,
            "avg_npu_time": self.total_npu_time / self.npu_inference_count if self.npu_inference_count > 0 else 0,
            "device_id": self.device_id
        }
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.npu_session:
            del self.npu_session
            self.npu_session = None
        
        print("ğŸ§¹ æ”¹è‰¯ç‰ˆNPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

