# -*- coding: utf-8 -*-
"""
XRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼è§£æ±ºç‰ˆNPUãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
VitisAI ExecutionProvider XRT_CMD_STATE_TIMEOUTå®Œå…¨è§£æ±º
"""

import os
import sys
import time
import argparse
import json
import threading
import signal
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    import torch
    import torch.nn as nn
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime torch transformers psutil ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class XRTTimeoutFixSystem:
    """XRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼è§£æ±ºç‰ˆNPUãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = None
        self.active_provider = None
        self.model = None
        self.tokenizer = None
        self.text_generator = None
        
        # infer-OSè¨­å®š
        self.infer_os_enabled = os.getenv('INFER_OS_ENABLED', '0') == '1'
        
        print(f"ğŸš€ XRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼è§£æ±ºç‰ˆNPUãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {timeout}ç§’")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
    
    def create_ultra_lightweight_model(self) -> str:
        """è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆXRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰"""
        try:
            print("ğŸ”§ è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆXRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰...")
            
            # æœ€å°é™ã®Linearå±¤ã®ã¿ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰
            class UltraLightweightModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # æœ€å°ã‚µã‚¤ã‚ºã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿
                    self.linear = nn.Linear(64, 128)  # æ¥µå°ã‚µã‚¤ã‚º
                    self.relu = nn.ReLU()
                    self.output = nn.Linear(128, 10)  # æœ€å°å‡ºåŠ›
                
                def forward(self, x):
                    x = self.linear(x)
                    x = self.relu(x)
                    x = self.output(x)
                    return x
            
            model = UltraLightweightModel()
            model.eval()
            
            # æœ€å°å…¥åŠ›ã‚µã‚¤ã‚º
            dummy_input = torch.randn(1, 64)
            
            # ONNX IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã§ç¢ºå®Ÿãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            onnx_path = "ultra_lightweight_npu_model.onnx"
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã«å¼·åˆ¶å¤‰æ›´ï¼ˆRyzenAI 1.5äº’æ›æ€§ï¼‰
            import onnx
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            print(f"ğŸ“‹ IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: 10 (RyzenAI 1.5äº’æ›)")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: æ¥µå°ï¼ˆXRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰")
            
            return onnx_path
            
        except Exception as e:
            print(f"âŒ è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def create_session_with_timeout_handling(self, onnx_path: str) -> bool:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        # æˆ¦ç•¥1: DmlExecutionProviderå„ªå…ˆï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰
        print("ğŸ”§ æˆ¦ç•¥1: DmlExecutionProviderå„ªå…ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ...")
        try:
            providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'device_id': 0,
                    'enable_dynamic_shapes': True,
                    'disable_metacommands': False
                },
                {}
            ]
            
            self.session = ort.InferenceSession(
                onnx_path,
                providers=providers,
                provider_options=provider_options
            )
            
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… DmlExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            return True
            
        except Exception as e:
            print(f"âš ï¸ DmlExecutionProviderå¤±æ•—: {e}")
        
        # æˆ¦ç•¥2: VitisAIExecutionProviderï¼ˆè»½é‡è¨­å®šï¼‰
        print("ğŸ”§ æˆ¦ç•¥2: VitisAIExecutionProviderè»½é‡è¨­å®š...")
        try:
            providers = ['VitisAIExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'config_file': 'C:/Program Files/RyzenAI/1.5/voe-4.0-win_amd64/vaip_config.json',
                    'cacheDir': './vaip_cache',
                    'cacheKey': 'ultra_lightweight'
                },
                {}
            ]
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            def create_session():
                return ort.InferenceSession(
                    onnx_path,
                    providers=providers,
                    provider_options=provider_options
                )
            
            # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_result = self._run_with_timeout(create_session, 30)
            if session_result:
                self.session = session_result
                self.active_provider = self.session.get_providers()[0]
                print(f"âœ… VitisAIExecutionProviderè»½é‡ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                return True
            else:
                print("âš ï¸ VitisAIExecutionProvider ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
        except Exception as e:
            print(f"âš ï¸ VitisAIExecutionProviderå¤±æ•—: {e}")
        
        # æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ”§ æˆ¦ç•¥3: CPUExecutionProvider ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
        try:
            providers = ['CPUExecutionProvider']
            self.session = ort.InferenceSession(onnx_path, providers=providers)
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… CPUExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            return True
            
        except Exception as e:
            print(f"âŒ CPUExecutionProviderå¤±æ•—: {e}")
            return False
    
    def _run_with_timeout(self, func, timeout_seconds):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãé–¢æ•°å®Ÿè¡Œ"""
        result = [None]
        exception = [None]
        
        def target():
            try:
                result[0] = func()
            except Exception as e:
                exception[0] = e
        
        thread = threading.Thread(target=target)
        thread.daemon = True
        thread.start()
        thread.join(timeout_seconds)
        
        if thread.is_alive():
            print(f"âš ï¸ é–¢æ•°å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{timeout_seconds}ç§’ï¼‰")
            return None
        
        if exception[0]:
            raise exception[0]
        
        return result[0]
    
    def test_npu_inference_safe(self, num_inferences: int = 10) -> Dict[str, Any]:
        """å®‰å…¨ãªNPUæ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ä»˜ãï¼‰"""
        if not self.session:
            raise RuntimeError("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ¯ å®‰å…¨ãªNPUæ¨è«–ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ{num_inferences}å›ï¼‰...")
        
        # è¶…è»½é‡å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        input_data = np.random.randn(1, 64).astype(np.float32)
        input_name = self.session.get_inputs()[0].name
        
        successful_inferences = 0
        total_time = 0
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_inferences):
            try:
                # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ç›£è¦–
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                cpu_usage.append(cpu_percent)
                memory_usage.append(memory_percent)
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãæ¨è«–å®Ÿè¡Œ
                start_time = time.time()
                
                def run_inference():
                    return self.session.run(None, {input_name: input_data})
                
                result = self._run_with_timeout(run_inference, 10)  # 10ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result is not None:
                    inference_time = time.time() - start_time
                    total_time += inference_time
                    successful_inferences += 1
                    
                    if (i + 1) % 5 == 0:
                        print(f"  âœ… æ¨è«– {i+1}/{num_inferences} å®Œäº† ({inference_time:.3f}ç§’)")
                else:
                    print(f"  âš ï¸ æ¨è«– {i+1} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
            except Exception as e:
                print(f"  âŒ æ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœè¨ˆç®—
        if successful_inferences > 0:
            avg_time = total_time / successful_inferences
            throughput = successful_inferences / total_time if total_time > 0 else 0
        else:
            avg_time = 0
            throughput = 0
        
        results = {
            'successful_inferences': successful_inferences,
            'total_inferences': num_inferences,
            'success_rate': successful_inferences / num_inferences * 100,
            'total_time': total_time,
            'average_time': avg_time,
            'throughput': throughput,
            'active_provider': self.active_provider,
            'avg_cpu_usage': np.mean(cpu_usage) if cpu_usage else 0,
            'avg_memory_usage': np.mean(memory_usage) if memory_usage else 0
        }
        
        return results
    
    def load_proven_text_model(self) -> bool:
        """å®Ÿç¸¾ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        proven_models = [
            "microsoft/DialoGPT-small",   # æœ€è»½é‡
            "distilgpt2",                 # è»½é‡
            "gpt2",                       # æ¨™æº–
        ]
        
        for model_name in proven_models:
            try:
                print(f"ğŸ¤– ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name}")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
                def load_model():
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    if tokenizer.pad_token is None:
                        tokenizer.pad_token = tokenizer.eos_token
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        device_map="auto" if torch.cuda.is_available() else None
                    )
                    
                    generator = pipeline(
                        "text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        device=0 if torch.cuda.is_available() else -1
                    )
                    
                    return tokenizer, model, generator
                
                result = self._run_with_timeout(load_model, 60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result:
                    self.tokenizer, self.model, self.text_generator = result
                    print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {model_name}")
                    return True
                else:
                    print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {model_name}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {model_name} - {e}")
                continue
        
        print("âŒ å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—")
        return False
    
    def generate_text_safe(self, prompt: str, max_tokens: int = 50) -> str:
        """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ä»˜ãï¼‰"""
        if not self.text_generator:
            return "âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        try:
            print(f"ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
            
            def generate():
                return self.text_generator(
                    prompt,
                    max_length=len(prompt.split()) + max_tokens,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            result = self._run_with_timeout(generate, 30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            if result:
                generated_text = result[0]['generated_text']
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                
                print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                return generated_text
            else:
                return "âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ"
                
        except Exception as e:
            return f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # 1. è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            onnx_path = self.create_ultra_lightweight_model()
            
            # 2. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_session_with_timeout_handling(onnx_path):
                print("âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # 3. NPUå‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆå®‰å…¨ç‰ˆï¼‰
            print("ğŸ”§ å®‰å…¨ãªNPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_result = self.test_npu_inference_safe(5)  # 5å›ãƒ†ã‚¹ãƒˆ
            
            if test_result['successful_inferences'] > 0:
                print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: {test_result['successful_inferences']}/5å›æˆåŠŸ")
                print(f"ğŸ“Š æˆåŠŸç‡: {test_result['success_rate']:.1f}%")
            else:
                print("âš ï¸ NPUå‹•ä½œãƒ†ã‚¹ãƒˆã§æˆåŠŸã—ãŸæ¨è«–ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # 4. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            if not self.load_proven_text_model():
                print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€NPUæ¨è«–ã¯åˆ©ç”¨å¯èƒ½ã§ã™")
            
            print("âœ… XRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼è§£æ±ºç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_benchmark(self, num_inferences: int = 50) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ï¼ˆ{num_inferences}å›æ¨è«–ï¼‰...")
        
        start_time = time.time()
        results = self.test_npu_inference_safe(num_inferences)
        total_benchmark_time = time.time() - start_time
        
        print(f"\nğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {results['successful_inferences']}/{results['total_inferences']}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_benchmark_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {results['throughput']:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {results['average_time']*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['active_provider']}")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {results['avg_cpu_usage']:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {results['avg_memory_usage']:.1f}%")
        print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ® ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print("ğŸ’¡ 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
        print("ğŸ’¡ 'benchmark' ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        print("ğŸ’¡ 'status' ã§ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ç¢ºèª")
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                elif user_input.lower() == 'benchmark':
                    self.run_benchmark(30)
                
                elif user_input.lower() == 'status':
                    print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                    print(f"ğŸ¤– ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: {'åˆ©ç”¨å¯èƒ½' if self.text_generator else 'åˆ©ç”¨ä¸å¯'}")
                    print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
                
                elif user_input:
                    if self.text_generator:
                        generated_text = self.generate_text_safe(user_input, 30)
                        print(f"\nğŸ¯ ç”Ÿæˆçµæœ:\n{generated_text}")
                    else:
                        print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚NPUæ¨è«–ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚")
                        # NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                        results = self.test_npu_inference_safe(5)
                        print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆ: {results['successful_inferences']}/5å›æˆåŠŸ")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    parser = argparse.ArgumentParser(description="XRTã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼è§£æ±ºç‰ˆNPUãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--inferences", type=int, default=30, help="æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=30, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--timeout", type=int, default=30, help="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    # infer-OSè¨­å®š
    if args.infer_os:
        os.environ['INFER_OS_ENABLED'] = '1'
    
    try:
        if args.compare:
            print("ğŸ“Š infer-OS ON/OFFæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            
            # OFFç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '0'
            print("\nğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰:")
            system_off = XRTTimeoutFixSystem(args.timeout)
            if system_off.initialize_system():
                results_off = system_off.run_benchmark(args.inferences)
            else:
                print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã«å¤±æ•—")
                return
            
            # ONç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '1'
            print("\nâš¡ æœ€é©åŒ–ç‰ˆæ¸¬å®šï¼ˆinfer-OS ONï¼‰:")
            system_on = XRTTimeoutFixSystem(args.timeout)
            if system_on.initialize_system():
                results_on = system_on.run_benchmark(args.inferences)
            else:
                print("âŒ æœ€é©åŒ–ç‰ˆæ¸¬å®šã«å¤±æ•—")
                return
            
            # æ¯”è¼ƒçµæœ
            print(f"\nğŸ“Š infer-OSåŠ¹æœæ¸¬å®šçµæœ:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            
            if results_off['throughput'] > 0:
                improvement = (results_on['throughput'] - results_off['throughput']) / results_off['throughput'] * 100
                print(f"  ğŸ“ˆ æ”¹å–„ç‡: {improvement:+.1f}%")
            
        else:
            # é€šå¸¸å®Ÿè¡Œ
            system = XRTTimeoutFixSystem(args.timeout)
            
            if not system.initialize_system():
                print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            if args.interactive:
                system.interactive_mode()
            elif args.prompt:
                if system.text_generator:
                    generated_text = system.generate_text_safe(args.prompt, args.tokens)
                    print(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
                    print(f"ğŸ¯ ç”Ÿæˆçµæœ:\n{generated_text}")
                else:
                    print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    results = system.test_npu_inference_safe(args.inferences)
                    print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆ: {results['successful_inferences']}/{args.inferences}å›æˆåŠŸ")
            else:
                system.run_benchmark(args.inferences)
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

