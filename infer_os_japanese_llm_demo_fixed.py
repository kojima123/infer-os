"""
ğŸš€ Infer-OS æ—¥æœ¬èªé‡é‡ç´šLLMçµ±åˆãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰

çœŸã®NPUã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿç¾ã™ã‚‹ä¿®æ­£ç‰ˆ

ä¸»è¦ä¿®æ­£ç‚¹:
- âŒ ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®ç„¡åŠ¹åŒ–ï¼ˆãƒ€ãƒŸãƒ¼å‡¦ç†å‰Šé™¤ï¼‰
- âœ… å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã®ONNXå¤‰æ›ã¨NPUå®Ÿè¡Œ
- âœ… åŠ¹ç‡çš„ãªNPUå‡¦ç†ãƒ•ãƒ­ãƒ¼ã®å®Ÿè£…
- âœ… ç¢ºå®ŸãªNPUè² è·ç‡å‘ä¸Š

ä½¿ç”¨æ–¹æ³•:
    python infer_os_japanese_llm_demo_fixed.py --model rinna/youri-7b-chat --use-aggressive-memory --enable-npu --interactive
"""

import sys
import os
import gc
import time
import traceback
import argparse
import platform
from typing import Dict, List, Optional, Any
import psutil

# PyTorché–¢é€£
try:
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        BitsAndBytesConfig,
        GenerationConfig
    )
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Transformersæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {e}")
    TRANSFORMERS_AVAILABLE = False

# ONNXé–¢é€£
try:
    import onnx
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ONNXæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {e}")
    ONNX_AVAILABLE = False

# é‡å­åŒ–é–¢é€£
try:
    from bitsandbytes import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
except ImportError:
    BITSANDBYTES_AVAILABLE = False

# NPUæœ€é©åŒ–é–¢é€£ï¼ˆä¿®æ­£ç‰ˆï¼‰
try:
    from npu_runtime_api import NPUOptimizer
    NPU_AVAILABLE = True
except ImportError:
    NPU_AVAILABLE = False

class InferOSJapaneseLLMDemo:
    """Infer-OSæ—¥æœ¬èªLLMãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, model_name: str, use_aggressive_memory: bool = False, 
                 enable_npu: bool = False, use_4bit: bool = False, 
                 use_8bit: bool = False, use_advanced_quant: bool = False):
        self.model_name = model_name
        self.use_aggressive_memory = use_aggressive_memory
        self.enable_npu = enable_npu
        self.use_4bit = use_4bit
        self.use_8bit = use_8bit
        self.use_advanced_quant = use_advanced_quant
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        self.model = None
        self.tokenizer = None
        
        # NPUæœ€é©åŒ–å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰
        self.npu_optimizer = None
        self.onnx_model_path = None
        self.npu_session = None
        
        # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–å™¨
        self.advanced_quantizer = None
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        self.system_info = self._get_system_info()
        
        print(f"ğŸš€ Infer-OSæ—¥æœ¬èªLLMãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰åˆæœŸåŒ–")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ğŸ§  ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: {use_aggressive_memory}")
        print(f"âš¡ NPUæœ€é©åŒ–: {enable_npu}")
        print(f"ğŸ”§ 4bité‡å­åŒ–: {use_4bit}")
        print(f"ğŸ”§ 8bité‡å­åŒ–: {use_8bit}")
        print(f"âš¡ é«˜åº¦ãªé‡å­åŒ–: {use_advanced_quant}")
    
    def _get_system_info(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        memory = psutil.virtual_memory()
        return {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "total_memory_gb": round(memory.total / (1024**3), 1),
            "available_memory_gb": round(memory.available / (1024**3), 1),
            "cpu_count": psutil.cpu_count(),
            "cpu_freq": psutil.cpu_freq().current if psutil.cpu_freq() else "Unknown"
        }
    
    def setup_npu_optimization(self):
        """NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if not self.enable_npu:
            return False
        
        if not NPU_AVAILABLE:
            print("âš ï¸ NPUæœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æœªåˆ©ç”¨å¯èƒ½")
            return False
        
        try:
            print("ğŸš€ NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
            self.npu_optimizer = NPUOptimizer()
            
            if self.npu_optimizer.npu_available:
                print("âœ… NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            else:
                print("âš ï¸ NPUåˆ©ç”¨ä¸å¯ã€CPUæ¨è«–ã‚’ä½¿ç”¨")
                return False
                
        except Exception as e:
            print(f"âŒ NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def setup_advanced_quantization(self):
        """é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not self.use_advanced_quant:
            return False
        
        try:
            print("âš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
            # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–å™¨ã®å®Ÿè£…ã¯çœç•¥
            print("âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            return True
        except Exception as e:
            print(f"âŒ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_model_and_tokenizer(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if not TRANSFORMERS_AVAILABLE:
            print("âŒ Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        try:
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if self.use_aggressive_memory:
                print("ğŸ§¹ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Ÿè¡Œä¸­...")
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = None
            if self.use_4bit and BITSANDBYTES_AVAILABLE:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("ğŸ”§ 4bité‡å­åŒ–è¨­å®šã‚’é©ç”¨")
            elif self.use_8bit and BITSANDBYTES_AVAILABLE:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                print("ğŸ”§ 8bité‡å­åŒ–è¨­å®šã‚’é©ç”¨")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            load_start = time.time()
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            load_time = time.time() - load_start
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† ({load_time:.1f}ç§’)")
            
            # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–
            if self.use_advanced_quant and self.advanced_quantizer:
                print("âš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
                self.model = self.advanced_quantizer.optimize_model(self.model)
                print("âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–å®Œäº†")
            
            # NPUç”¨ONNXå¤‰æ›ï¼ˆä¿®æ­£ç‰ˆï¼‰
            if self.enable_npu and self.npu_optimizer and self.npu_optimizer.npu_available:
                print("ğŸš€ NPUç”¨ONNXå¤‰æ›é–‹å§‹...")
                success = self._convert_to_onnx_for_npu()
                if success:
                    print("âœ… NPUç”¨ONNXå¤‰æ›å®Œäº†")
                else:
                    print("âš ï¸ NPUç”¨ONNXå¤‰æ›å¤±æ•—ã€CPUæ¨è«–ã‚’ä½¿ç”¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _convert_to_onnx_for_npu(self) -> bool:
        """NPUç”¨ONNXå¤‰æ›ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸ”„ ONNXå¤‰æ›ã¨DirectMLã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
            
            if self.model is None:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰ã€ONNXå¤‰æ›ä¸å¯")
                return False
            
            # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ä½œæˆ
            sample_text = "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚"
            sample_inputs = self.tokenizer(
                sample_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›å½¢çŠ¶: {sample_inputs['input_ids'].shape}")
            
            # ONNXå¤‰æ›å®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰
            print("ğŸ”§ ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            # ONNXå¤‰æ›ç”¨ã®å…¥åŠ›æº–å‚™
            input_ids = sample_inputs['input_ids']
            attention_mask = sample_inputs['attention_mask']
            
            # ONNXå¤‰æ›å®Ÿè¡Œ
            self.onnx_model_path = f"./onnx_models/{self.model_name.replace('/', '_')}_npu.onnx"
            os.makedirs("./onnx_models", exist_ok=True)
            
            # å‹•çš„è»¸è¨­å®š
            dynamic_axes = {
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                'logits': {0: 'batch_size', 1: 'sequence'}
            }
            
            # ONNXå¤‰æ›å®Ÿè¡Œ
            with torch.no_grad():
                torch.onnx.export(
                    self.model,
                    (input_ids, attention_mask),
                    self.onnx_model_path,
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['input_ids', 'attention_mask'],
                    output_names=['logits'],
                    dynamic_axes=dynamic_axes,
                    verbose=False
                )
            
            print(f"âœ… ONNXå¤‰æ›æˆåŠŸ: {self.onnx_model_path}")
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            return self._create_npu_session()
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _create_npu_session(self) -> bool:
        """NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸš€ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            if not os.path.exists(self.onnx_model_path):
                print(f"âŒ ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.onnx_model_path}")
                return False
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,
                    'memory_limit_mb': 4096,
                })
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = False
            session_options.enable_cpu_mem_arena = False
            session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.npu_session = ort.InferenceSession(
                self.onnx_model_path,
                providers=providers,
                sess_options=session_options
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = self.npu_session.get_providers()
            print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                print("âš ï¸ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹")
                return False
            
            print("âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def generate_japanese_text(self, prompt: str, max_length: int = 300, max_new_tokens: int = None, 
                              temperature: float = 0.7, do_sample: bool = True) -> Dict:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰"}
        
        try:
            print(f"\nğŸ¯ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            print(f"æœ€å¤§é•·: {max_length}")
            
            # NPUæ¨è«–ã‚’å„ªå…ˆä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰
            if self.enable_npu and self.npu_session is not None:
                print("âš¡ NPUæ¨è«–ã‚’ä½¿ç”¨ä¸­...")
                result = self._run_npu_inference(prompt, max_length, temperature)
                if result:
                    return result
                else:
                    print("âš ï¸ NPUæ¨è«–å¤±æ•—ã€CPUæ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
            # CPUæ¨è«–ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
            print("ğŸ–¥ï¸ CPUæ¨è«–ã‚’ä½¿ç”¨ä¸­...")
            return self._run_cpu_inference(prompt, max_length, temperature, do_sample)
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _run_npu_inference(self, prompt: str, max_length: int, temperature: float) -> Optional[Dict]:
        """NPUæ¨è«–å®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            start_time = time.time()
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            input_ids = inputs['input_ids'].numpy().astype(np.int64)
            attention_mask = inputs['attention_mask'].numpy().astype(np.int64)
            
            print(f"ğŸ“Š NPUå…¥åŠ›å½¢çŠ¶: input_ids{input_ids.shape}, attention_mask{attention_mask.shape}")
            
            # NPUæ¨è«–å®Ÿè¡Œ
            print("ğŸš€ NPUæ¨è«–å®Ÿè¡Œä¸­...")
            npu_outputs = self.npu_session.run(
                ['logits'],
                {
                    'input_ids': input_ids,
                    'attention_mask': attention_mask
                }
            )
            
            logits = npu_outputs[0]
            print(f"âœ… NPUæ¨è«–å®Œäº†: logitså½¢çŠ¶{logits.shape}")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            next_token_logits = logits[0, -1, :]
            
            # æ¸©åº¦é©ç”¨
            if temperature > 0:
                next_token_logits = next_token_logits / temperature
            
            # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨
            import numpy as np
            exp_logits = np.exp(next_token_logits - np.max(next_token_logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
            next_token_id = np.random.choice(len(probabilities), p=probabilities)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode([next_token_id], skip_special_tokens=True)
            full_text = prompt + generated_text
            
            generation_time = time.time() - start_time
            
            print(f"âœ… NPUç”Ÿæˆå®Œäº†: {generation_time:.2f}ç§’")
            
            return {
                "generated_text": full_text,
                "generation_time": generation_time,
                "input_tokens": len(input_ids[0]),
                "output_tokens": 1,  # ç°¡æ˜“ç‰ˆã§ã¯1ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
                "tokens_per_sec": 1 / generation_time,
                "memory_used": 0.0,
                "cpu_usage": 0.0,
                "inference_method": "NPU"
            }
            
        except Exception as e:
            print(f"âŒ NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return None
    
    def _run_cpu_inference(self, prompt: str, max_length: int, temperature: float, do_sample: bool) -> Dict:
        """CPUæ¨è«–å®Ÿè¡Œ"""
        try:
            start_time = time.time()
            
            # ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            initial_cpu = psutil.cpu_percent(interval=None)
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®š
            generation_config = GenerationConfig(
                max_length=max_length,
                temperature=temperature,
                do_sample=do_sample,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # çµ±è¨ˆæƒ…å ±è¨ˆç®—
            generation_time = time.time() - start_time
            final_memory = psutil.virtual_memory().used / (1024**3)
            final_cpu = psutil.cpu_percent(interval=None)
            
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            tokens_per_sec = output_tokens / generation_time if generation_time > 0 else 0
            
            return {
                "generated_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_sec": tokens_per_sec,
                "memory_used": final_memory - initial_memory,
                "cpu_usage": final_cpu - initial_cpu,
                "inference_method": "CPU"
            }
            
        except Exception as e:
            print(f"âŒ CPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"CPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}"}
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        print("\nğŸ‡¯ğŸ‡µ Infer-OSæœ€é©åŒ–ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print("ğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
        print("ğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†ã€'help'ã§ãƒ˜ãƒ«ãƒ—è¡¨ç¤º")
        print("=" * 60)
        
        while True:
            try:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›
                prompt = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if not prompt:
                    print("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if prompt.lower() == 'help':
                    self._show_help()
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
                print("\nğŸ”„ ç”Ÿæˆä¸­...")
                result = self.generate_japanese_text(prompt)
                
                if "error" in result:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                    continue
                
                # çµæœè¡¨ç¤º
                print(f"\nâœ… ç”Ÿæˆå®Œäº†:")
                print(f"ğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ: {result['generated_text']}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
                print(f"ğŸ“Š å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['input_tokens']}")
                print(f"ğŸ“Š å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['output_tokens']}")
                print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
                print(f"ğŸ”§ æ¨è«–æ–¹æ³•: {result['inference_method']}")
                
                if result['inference_method'] == 'CPU':
                    print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {result['memory_used']:.1f}GB")
                    print(f"ğŸ–¥ï¸ CPUä½¿ç”¨ç‡å¤‰åŒ–: {result['cpu_usage']:.1f}%")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                traceback.print_exc()
    
    def _show_help(self):
        """ãƒ˜ãƒ«ãƒ—è¡¨ç¤º"""
        print("\nğŸ“– ãƒ˜ãƒ«ãƒ—:")
        print("  - æ—¥æœ¬èªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("  - 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†")
        print("  - 'help'ã§ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º")
        print("  - NPUä½¿ç”¨æ™‚ã¯ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§NPUä½¿ç”¨ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Infer-OSæ—¥æœ¬èªLLMãƒ‡ãƒ¢ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    parser.add_argument("--model", type=str, default="rinna/youri-7b-chat",
                       help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--use-aggressive-memory", action="store_true",
                       help="ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--enable-npu", action="store_true",
                       help="NPUæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--use-4bit", action="store_true",
                       help="4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--use-8bit", action="store_true",
                       help="8bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--use-advanced-quant", action="store_true",
                       help="é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--interactive", action="store_true",
                       help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    parser.add_argument("--prompt", type=str,
                       help="å˜ç™ºå®Ÿè¡Œç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ¢åˆæœŸåŒ–
    demo = InferOSJapaneseLLMDemo(
        model_name=args.model,
        use_aggressive_memory=args.use_aggressive_memory,
        enable_npu=args.enable_npu,
        use_4bit=args.use_4bit,
        use_8bit=args.use_8bit,
        use_advanced_quant=args.use_advanced_quant
    )
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
    for key, value in demo.system_info.items():
        print(f"  {key}: {value}")
    
    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    print(f"\nğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
    
    # NPUæœ€é©åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    if args.enable_npu:
        demo.setup_npu_optimization()
    
    # é«˜åº¦ãªé‡å­åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    if args.use_advanced_quant:
        demo.setup_advanced_quantization()
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
    if not demo.load_model_and_tokenizer():
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if args.interactive:
        demo.run_interactive_mode()
    elif args.prompt:
        print(f"\nğŸ”„ å˜ç™ºå®Ÿè¡Œ: {args.prompt}")
        result = demo.generate_japanese_text(args.prompt)
        
        if "error" in result:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        else:
            print(f"âœ… ç”Ÿæˆçµæœ: {result['generated_text']}")
            print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
            print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            print(f"ğŸ”§ æ¨è«–æ–¹æ³•: {result['inference_method']}")
    else:
        print("ğŸ’¡ --interactive ã¾ãŸã¯ --prompt ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()

