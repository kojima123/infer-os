#!/usr/bin/env python3
"""
NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Hugging Faceã‹ã‚‰æœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path


class NPUModelDownloader:
    """NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        self.models = {
            "llama3-8b-amd-npu": {
                "repo": "dahara1/llama3-8b-amd-npu",
                "size": "8B",
                "description": "Llama3 8B NPUæœ€é©åŒ–ç‰ˆï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰",
                "npu_ready": True,
                "japanese_support": True,
                "estimated_size": "4.5GB"
            },
            "ALMA-Ja-V3-amd-npu": {
                "repo": "dahara1/ALMA-Ja-V3-amd-npu", 
                "size": "7B",
                "description": "ALMA 7B æ—¥æœ¬èªç¿»è¨³ç‰¹åŒ–NPUç‰ˆ",
                "npu_ready": True,
                "japanese_support": True,
                "estimated_size": "4.0GB"
            },
            "llama3.1-8b-instruct-amd-npu": {
                "repo": "dahara1/llama3.1-8b-Instruct-amd-npu",
                "size": "8B", 
                "description": "Llama3.1 8B NPUæœ€é©åŒ–ç‰ˆ",
                "npu_ready": True,
                "japanese_support": True,
                "estimated_size": "4.5GB"
            },
            "llama-translate-amd-npu": {
                "repo": "dahara1/llama-translate-amd-npu",
                "size": "7B",
                "description": "Llamaç¿»è¨³ç‰¹åŒ–NPUç‰ˆ",
                "npu_ready": True,
                "japanese_support": True,
                "estimated_size": "4.0GB"
            },
            "Llama-3.1-70B-Japanese": {
                "repo": "cyberagent/Llama-3.1-70B-Japanese-Instruct-2407",
                "size": "70B",
                "description": "Llama3.1 70B æ—¥æœ¬èªç‰¹åŒ–ç‰ˆï¼ˆè¦ONNXå¤‰æ›ï¼‰",
                "npu_ready": False,
                "japanese_support": True,
                "estimated_size": "140GB"
            }
        }
    
    def list_models(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º"""
        print("ğŸ” åˆ©ç”¨å¯èƒ½ãªNPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«")
        print("=" * 80)
        
        for model_key, info in self.models.items():
            npu_status = "âœ… NPUå¯¾å¿œæ¸ˆã¿" if info["npu_ready"] else "ğŸ”„ ONNXå¤‰æ›å¿…è¦"
            jp_status = "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œ" if info["japanese_support"] else "ğŸŒ å¤šè¨€èª"
            
            print(f"ğŸ“± {model_key}")
            print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {info['size']} ({info['estimated_size']})")
            print(f"   ğŸ“ èª¬æ˜: {info['description']}")
            print(f"   âš¡ NPU: {npu_status}")
            print(f"   ğŸ—£ï¸ è¨€èª: {jp_status}")
            print(f"   ğŸ“¦ ãƒªãƒã‚¸ãƒˆãƒª: {info['repo']}")
            print()
    
    def check_requirements(self) -> bool:
        """å¿…è¦ãªä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        print("ğŸ” ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        # huggingface-hubãƒã‚§ãƒƒã‚¯
        try:
            import huggingface_hub
            print("âœ… huggingface-hub: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿")
        except ImportError:
            print("âŒ huggingface-hub: æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            print("ğŸ’¡ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install huggingface-hub")
            return False
        
        # huggingface-cliãƒã‚§ãƒƒã‚¯
        try:
            result = subprocess.run(["huggingface-cli", "--help"], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… huggingface-cli: åˆ©ç”¨å¯èƒ½")
            else:
                print("âŒ huggingface-cli: åˆ©ç”¨ä¸å¯")
                return False
        except FileNotFoundError:
            print("âŒ huggingface-cli: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("ğŸ’¡ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install -U 'huggingface_hub[cli]'")
            return False
        
        return True
    
    def download_model(self, model_key: str, local_dir: str = None) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if model_key not in self.models:
            print(f"âŒ æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«: {model_key}")
            self.list_models()
            return False
        
        model_info = self.models[model_key]
        repo = model_info["repo"]
        
        if local_dir is None:
            local_dir = model_key
        
        print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_key}")
        print(f"ğŸ“¦ ãƒªãƒã‚¸ãƒˆãƒª: {repo}")
        print(f"ğŸ“Š ã‚µã‚¤ã‚º: {model_info['size']} ({model_info['estimated_size']})")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {local_dir}")
        print("=" * 60)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path(local_dir).mkdir(parents=True, exist_ok=True)
        
        # huggingface-cliã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        cmd = [
            "huggingface-cli",
            "download",
            repo,
            "--revision", "main",
            "--local-dir", local_dir
        ]
        
        try:
            print("ğŸ”„ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œä¸­...")
            print(f"ğŸ’» ã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, check=True, text=True)
            
            print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã®ç¢ºèª
            self._verify_download(local_dir, model_info)
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _verify_download(self, local_dir: str, model_info: dict):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèª"""
        print("ğŸ” ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèªä¸­...")
        
        local_path = Path(local_dir)
        
        # åŸºæœ¬ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        required_files = ["config.json", "tokenizer.json"]
        optional_files = ["README.md", "tokenizer_config.json"]
        
        for file_name in required_files:
            file_path = local_path / file_name
            if file_path.exists():
                print(f"âœ… {file_name}: å­˜åœ¨")
            else:
                print(f"âš ï¸ {file_name}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        if model_info["npu_ready"]:
            npu_files = [
                "pytorch_llama3_8b_w_bit_4_awq_amd.pt",
                "alma_w_bit_4_awq_fa_amd.pt"
            ]
            
            npu_file_found = False
            for npu_file in npu_files:
                npu_path = local_path / npu_file
                if npu_path.exists():
                    print(f"âš¡ NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«: {npu_file} âœ…")
                    npu_file_found = True
                    break
            
            if not npu_file_found:
                print("âš ï¸ NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç¢ºèª
        try:
            total_size = sum(f.stat().st_size for f in local_path.rglob('*') if f.is_file())
            size_gb = total_size / (1024 ** 3)
            print(f"ğŸ’¾ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: {size_gb:.1f}GB")
        except Exception as e:
            print(f"âš ï¸ ã‚µã‚¤ã‚ºè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        
        print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèªå®Œäº†")
    
    def download_all_npu_ready(self):
        """NPUå¯¾å¿œæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        print("ğŸ“¥ NPUå¯¾å¿œæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        print("=" * 60)
        
        npu_ready_models = [key for key, info in self.models.items() if info["npu_ready"]]
        
        print(f"ğŸ¯ å¯¾è±¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(npu_ready_models)}")
        for model in npu_ready_models:
            print(f"  ğŸ“± {model} ({self.models[model]['size']})")
        print()
        
        success_count = 0
        for model_key in npu_ready_models:
            print(f"\nğŸ“¥ {model_key} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            if self.download_model(model_key):
                success_count += 1
                print(f"âœ… {model_key} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
            else:
                print(f"âŒ {model_key} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
        
        print(f"\nğŸ ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(npu_ready_models)}")
    
    def get_download_instructions(self, model_key: str):
        """æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †è¡¨ç¤º"""
        if model_key not in self.models:
            print(f"âŒ æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«: {model_key}")
            return
        
        model_info = self.models[model_key]
        repo = model_info["repo"]
        
        print(f"ğŸ“‹ {model_key} æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †")
        print("=" * 60)
        print("1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:")
        print("   pip install -U 'huggingface_hub[cli]'")
        print()
        print("2. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚³ãƒãƒ³ãƒ‰:")
        print(f"   huggingface-cli download {repo} --revision main --local-dir {model_key}")
        print()
        print("3. ç’°å¢ƒè¨­å®šï¼ˆNPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰:")
        if model_info["npu_ready"]:
            print("   set XLNX_VART_FIRMWARE=<ryzen_ai_path>\\voe-4.0-win_amd64\\1x4.xclbin")
            print("   set NUM_OF_DPU_RUNNERS=1")
        else:
            print("   ONNXå¤‰æ›ãŒå¿…è¦ã§ã™")
        print()
        print("4. å®Ÿè¡Œ:")
        print(f"   python npu_optimized_japanese_models.py --model {model_key} --interactive")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼")
    parser.add_argument("--list", action="store_true", help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º")
    parser.add_argument("--download", help="æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    parser.add_argument("--download-all-npu", action="store_true", help="NPUå¯¾å¿œæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    parser.add_argument("--local-dir", help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--instructions", help="æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã®æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †è¡¨ç¤º")
    parser.add_argument("--check", action="store_true", help="ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯")
    
    args = parser.parse_args()
    
    downloader = NPUModelDownloader()
    
    if args.list:
        downloader.list_models()
    elif args.check:
        if downloader.check_requirements():
            print("âœ… å…¨ã¦ã®ä¾å­˜é–¢ä¿‚ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã™")
        else:
            print("âŒ ä¾å­˜é–¢ä¿‚ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
    elif args.instructions:
        downloader.get_download_instructions(args.instructions)
    elif args.download:
        if not downloader.check_requirements():
            print("âŒ ä¾å­˜é–¢ä¿‚ã‚’å…ˆã«è§£æ±ºã—ã¦ãã ã•ã„")
            return
        
        downloader.download_model(args.download, args.local_dir)
    elif args.download_all_npu:
        if not downloader.check_requirements():
            print("âŒ ä¾å­˜é–¢ä¿‚ã‚’å…ˆã«è§£æ±ºã—ã¦ãã ã•ã„")
            return
        
        downloader.download_all_npu_ready()
    else:
        print("ğŸš€ NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼")
        print("=" * 60)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  --list                 : ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º")
        print("  --check                : ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯")
        print("  --download MODEL       : æŒ‡å®šãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        print("  --download-all-npu     : NPUå¯¾å¿œæ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        print("  --instructions MODEL   : æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †è¡¨ç¤º")
        print()
        print("ä¾‹:")
        print("  python download_npu_models.py --list")
        print("  python download_npu_models.py --download llama3-8b-amd-npu")
        print("  python download_npu_models.py --download-all-npu")


if __name__ == "__main__":
    main()

