#!/usr/bin/env python3
"""
NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ‡ãƒ¢
çœŸã®NPUæ´»ç”¨ã‚’å®Ÿç¾ã™ã‚‹åŒ…æ‹¬çš„ãªãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import time
import argparse
import subprocess
from pathlib import Path


class NPUJapaneseDemoRunner:
    """NPUæ—¥æœ¬èªãƒ‡ãƒ¢å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.available_models = {
            "llama3-8b-amd-npu": {
                "size": "8B",
                "type": "NPUæœ€é©åŒ–æ¸ˆã¿",
                "japanese": "å¤šè¨€èªå¯¾å¿œ",
                "npu_ready": True,
                "recommended": True,
                "description": "æœ€ã‚‚å®‰å®šã—ãŸNPUå¯¾å¿œæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«"
            },
            "ALMA-Ja-V3-amd-npu": {
                "size": "7B", 
                "type": "ç¿»è¨³ç‰¹åŒ–NPU",
                "japanese": "æ—¥æœ¬èªç¿»è¨³ç‰¹åŒ–",
                "npu_ready": True,
                "recommended": True,
                "description": "æ—¥æœ¬èªç¿»è¨³ã«æœ€é©åŒ–ã•ã‚ŒãŸNPUãƒ¢ãƒ‡ãƒ«"
            },
            "cyberagent/Llama-3.1-70B-Japanese-Instruct-2407": {
                "size": "70B",
                "type": "å¤§è¦æ¨¡æ—¥æœ¬èª",
                "japanese": "æ—¥æœ¬èªç‰¹åŒ–",
                "npu_ready": False,
                "recommended": False,
                "description": "æœ€é‡é‡ç´šæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ï¼ˆONNXå¤‰æ›ãƒãƒ£ãƒ¬ãƒ³ã‚¸ï¼‰"
            }
        }
        
        self.test_prompts = {
            "general": [
                "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
                "æ—¥æœ¬ã®å››å­£ã®ç¾ã—ã•ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®ä»•çµ„ã¿ã‚’ç°¡å˜ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
            ],
            "translation": [
                "æ¬¡ã®è‹±èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„: 'The future of artificial intelligence is bright and full of possibilities.'",
                "æ¬¡ã®æ—¥æœ¬èªã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„: 'æ¡œã®èŠ±ãŒå’²ãæ˜¥ã¯æ—¥æœ¬ã§æœ€ã‚‚ç¾ã—ã„å­£ç¯€ã§ã™ã€‚'",
                "æ¬¡ã®æ–‡ç« ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„: 'Machine learning algorithms are revolutionizing various industries.'"
            ],
            "technical": [
                "NPUï¼ˆNeural Processing Unitï¼‰ã¨GPUã®é•ã„ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                "æ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹é‡å­åŒ–ã®é‡è¦æ€§ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
                "ã‚¨ãƒƒã‚¸AIã®åˆ©ç‚¹ã¨èª²é¡Œã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚"
            ]
        }
    
    def show_welcome(self):
        """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º"""
        print("ğŸš€ NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ‡ãƒ¢")
        print("ğŸ¯ çœŸã®NPUæ´»ç”¨å®Ÿç¾ç‰ˆ")
        print("=" * 70)
        print("ğŸ’¡ ã“ã®ãƒ‡ãƒ¢ã§ã¯ä»¥ä¸‹ã‚’å®Ÿç¾ã—ã¾ã™:")
        print("  âœ… NPUæœ€é©åŒ–æ¸ˆã¿æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œç¢ºèª")
        print("  âœ… çœŸã®NPUå‡¦ç†ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è² è·ç‡å‘ä¸Š")
        print("  âœ… é«˜å“è³ªãªæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
        print("  âœ… è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ")
        print("=" * 70)
    
    def show_model_selection(self):
        """ãƒ¢ãƒ‡ãƒ«é¸æŠç”»é¢è¡¨ç¤º"""
        print("\nğŸ“± åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
        print("-" * 50)
        
        for i, (model_key, info) in enumerate(self.available_models.items(), 1):
            status = "âœ… æ¨å¥¨" if info["recommended"] else "ğŸ”„ å®Ÿé¨“çš„"
            npu_status = "âš¡ NPUå¯¾å¿œ" if info["npu_ready"] else "ğŸ”§ ONNXå¤‰æ›å¿…è¦"
            
            print(f"{i}. {model_key}")
            print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {info['size']}")
            print(f"   ğŸ”§ ã‚¿ã‚¤ãƒ—: {info['type']}")
            print(f"   ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª: {info['japanese']}")
            print(f"   {npu_status} | {status}")
            print(f"   ğŸ“ èª¬æ˜: {info['description']}")
            print()
    
    def check_model_availability(self, model_name: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        print(f"ğŸ” {model_name} ã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
        if os.path.exists(model_name):
            print(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹: {model_name}")
            
            # NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            npu_files = [
                "pytorch_llama3_8b_w_bit_4_awq_amd.pt",
                "alma_w_bit_4_awq_fa_amd.pt"
            ]
            
            for npu_file in npu_files:
                npu_path = Path(model_name) / npu_file
                if npu_path.exists():
                    print(f"âš¡ NPUæœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {npu_file}")
                    return True
            
            # é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
            config_path = Path(model_name) / "config.json"
            if config_path.exists():
                print(f"ğŸ“‹ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: config.json")
                return True
        
        print(f"âŒ {model_name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    def download_model_if_needed(self, model_name: str) -> bool:
        """å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if self.check_model_availability(model_name):
            return True
        
        print(f"ğŸ“¥ {model_name} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã‹ï¼Ÿ")
        response = input("y/n: ").lower().strip()
        
        if response in ['y', 'yes', 'ã¯ã„']:
            print(f"ğŸ“¥ {model_name} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
            
            try:
                cmd = ["python", "download_npu_models.py", "--download", model_name]
                result = subprocess.run(cmd, check=True, text=True)
                
                print(f"âœ… {model_name} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                return True
                
            except subprocess.CalledProcessError as e:
                print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                return False
            except Exception as e:
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        else:
            print("âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
            return False
    
    def run_model_demo(self, model_name: str, prompt_type: str = "general"):
        """ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢å®Ÿè¡Œ"""
        print(f"\nğŸš€ {model_name} ãƒ‡ãƒ¢å®Ÿè¡Œé–‹å§‹")
        print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—: {prompt_type}")
        print("-" * 50)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ
        prompts = self.test_prompts.get(prompt_type, self.test_prompts["general"])
        
        for i, prompt in enumerate(prompts, 1):
            print(f"\nğŸ¤– ãƒ†ã‚¹ãƒˆ {i}/{len(prompts)}")
            print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            print("ğŸ”„ ç”Ÿæˆä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
            start_time = time.time()
            
            try:
                cmd = [
                    "python", "npu_optimized_japanese_models.py",
                    "--model", model_name,
                    "--prompt", prompt,
                    "--max-tokens", "150"
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                
                execution_time = time.time() - start_time
                
                if result.returncode == 0:
                    print(f"âœ… ç”Ÿæˆå®Œäº† ({execution_time:.1f}ç§’)")
                    
                    # å‡ºåŠ›ã‹ã‚‰å¿œç­”éƒ¨åˆ†ã‚’æŠ½å‡º
                    output_lines = result.stdout.split('\n')
                    response_found = False
                    
                    for line in output_lines:
                        if "ğŸ“ å¿œç­”:" in line:
                            response = line.replace("ğŸ“ å¿œç­”:", "").strip()
                            print(f"ğŸ’¬ å¿œç­”: {response}")
                            response_found = True
                            break
                    
                    if not response_found:
                        print("âš ï¸ å¿œç­”ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                        print("ğŸ“„ ç”Ÿå‡ºåŠ›:")
                        print(result.stdout[-500:])  # æœ€å¾Œã®500æ–‡å­—
                else:
                    print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})")
                    print("ğŸ“„ ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:")
                    print(result.stderr[-500:])
                    
            except subprocess.TimeoutExpired:
                print("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ5åˆ†ï¼‰")
            except Exception as e:
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            
            print("-" * 30)
    
    def run_performance_comparison(self):
        """æ€§èƒ½æ¯”è¼ƒå®Ÿè¡Œ"""
        print("\nğŸ NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ")
        print("=" * 60)
        
        # NPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿æ¯”è¼ƒ
        npu_models = [key for key, info in self.available_models.items() if info["npu_ready"]]
        
        test_prompt = "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        
        results = {}
        
        for model_name in npu_models:
            print(f"\nğŸ“Š {model_name} æ€§èƒ½æ¸¬å®šä¸­...")
            
            if not self.check_model_availability(model_name):
                print(f"âŒ {model_name} ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                continue
            
            start_time = time.time()
            
            try:
                cmd = [
                    "python", "npu_optimized_japanese_models.py",
                    "--model", model_name,
                    "--prompt", test_prompt,
                    "--max-tokens", "100"
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
                
                execution_time = time.time() - start_time
                
                if result.returncode == 0:
                    results[model_name] = {
                        "time": execution_time,
                        "success": True,
                        "size": self.available_models[model_name]["size"]
                    }
                    print(f"âœ… å®Œäº†: {execution_time:.1f}ç§’")
                else:
                    results[model_name] = {
                        "time": execution_time,
                        "success": False,
                        "size": self.available_models[model_name]["size"]
                    }
                    print(f"âŒ å¤±æ•—: {execution_time:.1f}ç§’")
                    
            except subprocess.TimeoutExpired:
                results[model_name] = {
                    "time": 180,
                    "success": False,
                    "size": self.available_models[model_name]["size"]
                }
                print("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ")
        print("=" * 60)
        
        for model_name, result in results.items():
            status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±æ•—"
            print(f"ğŸ“± {model_name}")
            print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {result['size']}")
            print(f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {result['time']:.1f}ç§’")
            print(f"   ğŸ¯ çµæœ: {status}")
            print()
    
    def run_interactive_mode(self, model_name: str):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        print(f"\nğŸ® {model_name} ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("ğŸ’¡ 'exit'ã§çµ‚äº†")
        print("-" * 50)
        
        try:
            cmd = [
                "python", "npu_optimized_japanese_models.py",
                "--model", model_name,
                "--interactive"
            ]
            
            subprocess.run(cmd, check=True)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ‡ãƒ¢")
    parser.add_argument("--model", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--prompt-type", default="general", 
                       choices=["general", "translation", "technical"],
                       help="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--compare", action="store_true", help="æ€§èƒ½æ¯”è¼ƒå®Ÿè¡Œ")
    parser.add_argument("--download-all", action="store_true", help="NPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    demo = NPUJapaneseDemoRunner()
    demo.show_welcome()
    
    if args.download_all:
        print("\nğŸ“¥ NPUå¯¾å¿œãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        try:
            cmd = ["python", "download_npu_models.py", "--download-all-npu"]
            subprocess.run(cmd, check=True)
            print("âœ… ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            print(f"âŒ ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    if args.compare:
        demo.run_performance_comparison()
        return
    
    if args.model:
        # æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã§ãƒ‡ãƒ¢å®Ÿè¡Œ
        if args.model not in demo.available_models:
            print(f"âŒ æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«: {args.model}")
            demo.show_model_selection()
            return
        
        if not demo.download_model_if_needed(args.model):
            print("âŒ ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        if args.interactive:
            demo.run_interactive_mode(args.model)
        else:
            demo.run_model_demo(args.model, args.prompt_type)
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ¢ãƒ‡ãƒ«é¸æŠ
        demo.show_model_selection()
        
        try:
            choice = input("\nğŸ“± ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ (1-3): ").strip()
            
            model_list = list(demo.available_models.keys())
            
            if choice.isdigit() and 1 <= int(choice) <= len(model_list):
                selected_model = model_list[int(choice) - 1]
                
                if not demo.download_model_if_needed(selected_model):
                    print("âŒ ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return
                
                # ãƒ‡ãƒ¢ã‚¿ã‚¤ãƒ—é¸æŠ
                print("\nğŸ¯ å®Ÿè¡Œã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„:")
                print("1. åŸºæœ¬ãƒ‡ãƒ¢ï¼ˆä¸€èˆ¬çš„ãªè³ªå•ï¼‰")
                print("2. ç¿»è¨³ãƒ‡ãƒ¢ï¼ˆç¿»è¨³ã‚¿ã‚¹ã‚¯ï¼‰")
                print("3. æŠ€è¡“ãƒ‡ãƒ¢ï¼ˆæŠ€è¡“çš„ãªè³ªå•ï¼‰")
                print("4. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
                print("5. æ€§èƒ½æ¯”è¼ƒ")
                
                demo_choice = input("é¸æŠ (1-5): ").strip()
                
                if demo_choice == "1":
                    demo.run_model_demo(selected_model, "general")
                elif demo_choice == "2":
                    demo.run_model_demo(selected_model, "translation")
                elif demo_choice == "3":
                    demo.run_model_demo(selected_model, "technical")
                elif demo_choice == "4":
                    demo.run_interactive_mode(selected_model)
                elif demo_choice == "5":
                    demo.run_performance_comparison()
                else:
                    print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()

