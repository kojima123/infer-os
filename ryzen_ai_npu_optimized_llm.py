# -*- coding: utf-8 -*-
"""
Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ 
kyo-takano/open-calm-7b-8bit ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
CyberAgent OpenCALM 8bité‡å­åŒ–ç‰ˆã«ã‚ˆã‚‹é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆ
"""

import os
import sys
import time
import argparse
import json
import threading
import psutil
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import onnxruntime as ort
    import numpy as np
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from huggingface_hub import snapshot_download
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("pip install torch transformers onnxruntime huggingface_hub bitsandbytes")
    sys.exit(1)

class RyzenAIJapaneseLLMSystem:
    """Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, enable_infer_os: bool = False):
        self.enable_infer_os = enable_infer_os
        self.model_id = "kyo-takano/open-calm-7b-8bit"
        self.model_dir = Path("./models/open-calm-7b-8bit")
        self.onnx_path = self.model_dir / "open_calm_7b_8bit_npu.onnx"
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.pytorch_model = None
        self.tokenizer = None
        self.onnx_session = None
        self.active_provider = None
        
        # NPUç›£è¦–
        self.npu_monitoring = False
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        self.total_inferences = 0
        
        print(f"ğŸš€ Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«è©³ç´°: CyberAgent OpenCALM-7B 8bité‡å­åŒ–ç‰ˆ")
        print(f"ğŸŒ è¨€èª: æ—¥æœ¬èªç‰¹åŒ–")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if enable_infer_os else 'ç„¡åŠ¹'}")
    
    def setup_infer_os_environment(self):
        """infer-OSç’°å¢ƒè¨­å®š"""
        if self.enable_infer_os:
            print("ğŸ”§ infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šä¸­...")
            
            infer_os_env = {
                'INFER_OS_ENABLE': '1',
                'INFER_OS_OPTIMIZATION_LEVEL': 'high',
                'INFER_OS_NPU_ACCELERATION': '1',
                'INFER_OS_MEMORY_OPTIMIZATION': '1',
                'INFER_OS_JAPANESE_OPTIMIZATION': '1'
            }
            
            for key, value in infer_os_env.items():
                os.environ[key] = value
                print(f"  ğŸ“ {key}={value}")
            
            print("âœ… infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šå®Œäº†")
        else:
            print("ğŸ”§ infer-OSæœ€é©åŒ–: ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼‰")
            # infer-OSç„¡åŠ¹åŒ–
            for key in ['INFER_OS_ENABLE', 'INFER_OS_OPTIMIZATION_LEVEL', 
                       'INFER_OS_NPU_ACCELERATION', 'INFER_OS_MEMORY_OPTIMIZATION',
                       'INFER_OS_JAPANESE_OPTIMIZATION']:
                os.environ.pop(key, None)
    
    def download_model(self) -> bool:
        """æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            if self.model_dir.exists() and (self.model_dir / "config.json").exists():
                print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {self.model_dir}")
                return True
            
            print(f"ğŸ“¥ {self.model_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
            print(f"ğŸ“ CyberAgent OpenCALM-7B 8bité‡å­åŒ–ç‰ˆ")
            print(f"ğŸŒ æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«")
            print(f"âš ï¸ æ³¨æ„: å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            start_time = time.time()
            
            # HuggingFace Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            model_path = snapshot_download(
                repo_id=self.model_id,
                cache_dir="./models",
                resume_download=True,
                local_files_only=False
            )
            
            # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆ
            if not self.model_dir.exists():
                self.model_dir.parent.mkdir(exist_ok=True)
                os.symlink(model_path, self.model_dir)
            
            download_time = time.time() - start_time
            
            print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
            print(f"ğŸ“ ä¿å­˜å…ˆ: {self.model_dir}")
            print(f"â±ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {download_time:.1f}ç§’")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_pytorch_model(self) -> bool:
        """PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ“¥ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
            print(f"ğŸ”§ 8bité‡å­åŒ–: bitsandbytesä½¿ç”¨")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_dir))
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: èªå½™ã‚µã‚¤ã‚º {len(self.tokenizer)}")
            
            # 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            print("ğŸ”§ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            
            self.pytorch_model = AutoModelForCausalLM.from_pretrained(
                str(self.model_dir),
                load_in_8bit=True,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            print(f"âœ… PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"ğŸ”§ é‡å­åŒ–: 8bit")
            print(f"ğŸ“Š ãƒ‡ãƒã‚¤ã‚¹: {self.pytorch_model.device}")
            print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.memory_allocated() / 1024**3:.2f} GB" if torch.cuda.is_available() else "CPUä½¿ç”¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def export_to_onnx(self) -> bool:
        """ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆNPUæœ€é©åŒ–ï¼‰"""
        try:
            if self.onnx_path.exists():
                print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«å­˜åœ¨: {self.onnx_path}")
                return True
            
            if self.pytorch_model is None:
                print("âŒ PyTorchãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
            
            print("ğŸ”„ ONNXå½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹ï¼ˆNPUæœ€é©åŒ–ï¼‰...")
            print("âš ï¸ æ³¨æ„: åˆå›ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            self.onnx_path.parent.mkdir(exist_ok=True)
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆï¼ˆæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”¨ï¼‰
            dummy_text = "AIã«ã‚ˆã£ã¦ç§é”ã®æš®ã‚‰ã—ã¯ã€"
            dummy_inputs = self.tokenizer(
                dummy_text,
                return_tensors="pt",
                max_length=32,
                padding="max_length",
                truncation=True
            )
            
            dummy_input_ids = dummy_inputs["input_ids"].to(self.pytorch_model.device)
            
            print(f"ğŸ“ ãƒ€ãƒŸãƒ¼å…¥åŠ›: '{dummy_text}'")
            print(f"ğŸ”¢ å…¥åŠ›å½¢çŠ¶: {dummy_input_ids.shape}")
            
            # ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆRyzen AI NPUæœ€é©åŒ–ï¼‰
            start_time = time.time()
            
            torch.onnx.export(
                self.pytorch_model,
                dummy_input_ids,
                str(self.onnx_path),
                export_params=True,
                opset_version=13,  # Ryzen AI 1.5å¯¾å¿œ
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                    'logits': {0: 'batch_size', 1: 'sequence_length'}
                },
                verbose=False
            )
            
            export_time = time.time() - start_time
            
            print(f"âœ… ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†!")
            print(f"ğŸ“ ONNXãƒ•ã‚¡ã‚¤ãƒ«: {self.onnx_path}")
            print(f"â±ï¸ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ™‚é–“: {export_time:.1f}ç§’")
            print(f"ğŸ“¦ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {self.onnx_path.stat().st_size / 1024**2:.1f} MB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def setup_onnx_session(self) -> bool:
        """ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆNPUæœ€é©åŒ–ï¼‰"""
        try:
            if not self.onnx_path.exists():
                print(f"âŒ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.onnx_path}")
                return False
            
            print("âš¡ NPUæœ€é©åŒ–ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            available_providers = ort.get_available_providers()
            print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {available_providers}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3
            
            if self.enable_infer_os:
                session_options.enable_cpu_mem_arena = True
                session_options.enable_mem_pattern = True
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                print("ğŸ”§ infer-OSæœ€é©åŒ–: ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€é©åŒ–æœ‰åŠ¹")
            else:
                session_options.enable_cpu_mem_arena = False
                session_options.enable_mem_pattern = False
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
                print("ğŸ”§ infer-OSæœ€é©åŒ–: ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€é©åŒ–ç„¡åŠ¹")
            
            # VitisAI ExecutionProviderï¼ˆNPUï¼‰å„ªå…ˆ
            if 'VitisAIExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ VitisAIExecutionProviderè©¦è¡Œï¼ˆNPUæœ€é©åŒ–ï¼‰...")
                    
                    vitisai_options = {
                        "cache_dir": "C:/temp/vaip_cache",
                        "cache_key": "open_calm_7b_8bit_japanese",
                        "log_level": "warning"
                    }
                    
                    providers = [
                        ('VitisAIExecutionProvider', vitisai_options),
                        'CPUExecutionProvider'
                    ]
                    
                    self.onnx_session = ort.InferenceSession(
                        str(self.onnx_path),
                        sess_options=session_options,
                        providers=providers
                    )
                    
                    self.active_provider = 'VitisAIExecutionProvider'
                    print("âœ… VitisAIExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆNPUæœ€é©åŒ–ï¼‰")
                    
                except Exception as e:
                    print(f"âš ï¸ VitisAIExecutionProviderå¤±æ•—: {e}")
                    self.onnx_session = None
            
            # DmlExecutionProvider ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.onnx_session is None and 'DmlExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ DmlExecutionProviderè©¦è¡Œ...")
                    self.onnx_session = ort.InferenceSession(
                        str(self.onnx_path),
                        sess_options=session_options,
                        providers=['DmlExecutionProvider', 'CPUExecutionProvider']
                    )
                    self.active_provider = 'DmlExecutionProvider'
                    print("âœ… DmlExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ DmlExecutionProviderå¤±æ•—: {e}")
                    self.onnx_session = None
            
            # CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.onnx_session is None:
                try:
                    print("ğŸ”„ CPUExecutionProviderè©¦è¡Œ...")
                    self.onnx_session = ort.InferenceSession(
                        str(self.onnx_path),
                        sess_options=session_options,
                        providers=['CPUExecutionProvider']
                    )
                    self.active_provider = 'CPUExecutionProvider'
                    print("âœ… CPUExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âŒ CPUExecutionProviderå¤±æ•—: {e}")
                    return False
            
            if self.onnx_session is None:
                return False
            
            print(f"âœ… NPUæœ€é©åŒ–ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ”§ ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.onnx_session.get_providers()}")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            
            # NPUå‹•ä½œãƒ†ã‚¹ãƒˆ
            try:
                test_text = "ã“ã‚“ã«ã¡ã¯"
                test_inputs = self.tokenizer(
                    test_text,
                    return_tensors="np",
                    max_length=16,
                    padding="max_length",
                    truncation=True
                )
                
                test_output = self.onnx_session.run(None, {'input_ids': test_inputs['input_ids']})
                print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Œäº†: å‡ºåŠ›å½¢çŠ¶ {test_output[0].shape}")
                
                if self.active_provider == 'VitisAIExecutionProvider':
                    print("ğŸ”¥ VitisAI NPUå‡¦ç†ç¢ºèª: æ—¥æœ¬èªå¯¾å¿œOK")
                
            except Exception as e:
                print(f"âš ï¸ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
                return False
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        
        def monitor_npu():
            while self.npu_monitoring:
                try:
                    # Windows Performance Countersä½¿ç”¨ï¼ˆNPUä½¿ç”¨ç‡ï¼‰
                    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªNPUç›£è¦–APIã‚’ä½¿ç”¨
                    current_usage = 0.0
                    
                    # CPUä½¿ç”¨ç‡ã‚’NPUä½¿ç”¨ç‡ã®ä»£æ›¿ã¨ã—ã¦ä½¿ç”¨ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
                    cpu_usage = psutil.cpu_percent(interval=0.1)
                    if self.active_provider == 'VitisAIExecutionProvider':
                        # VitisAIä½¿ç”¨æ™‚ã¯CPUä½¿ç”¨ç‡ã®ä¸€éƒ¨ã‚’NPUä½¿ç”¨ç‡ã¨ã—ã¦æ¨å®š
                        current_usage = min(cpu_usage * 0.3, 100.0)
                    
                    self.npu_usage_history.append(current_usage)
                    self.max_npu_usage = max(self.max_npu_usage, current_usage)
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡ºï¼ˆ1%ä»¥ä¸Šã®å¤‰åŒ–æ™‚ã®ã¿ãƒ­ã‚°ï¼‰
                    if len(self.npu_usage_history) > 1:
                        prev_usage = self.npu_usage_history[-2]
                        if abs(current_usage - prev_usage) >= 1.0:
                            if current_usage > 5.0:  # 5%ä»¥ä¸Šã®ä½¿ç”¨ç‡æ™‚ã®ã¿
                                print(f"ğŸ”¥ NPUä½¿ç”¨ç‡å¤‰åŒ–: {prev_usage:.1f}% â†’ {current_usage:.1f}%")
                                if self.active_provider == 'VitisAIExecutionProvider':
                                    self.npu_active_count += 1
                    
                    time.sleep(1.0)  # 1ç§’é–“éš”ç›£è¦–
                    
                except Exception:
                    pass
        
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
        print("ğŸ“Š NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰")
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        print("ğŸ“Š NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢")
    
    def get_npu_statistics(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.npu_usage_history:
            return {}
        
        avg_usage = sum(self.npu_usage_history) / len(self.npu_usage_history)
        npu_activity_rate = (self.npu_active_count / max(self.total_inferences, 1)) * 100
        
        return {
            "max_npu_usage": self.max_npu_usage,
            "avg_npu_usage": avg_usage,
            "npu_active_count": self.npu_active_count,
            "total_inferences": self.total_inferences,
            "npu_activity_rate": npu_activity_rate,
            "active_provider": self.active_provider
        }
    
    def generate_text_pytorch(self, prompt: str, max_new_tokens: int = 50) -> str:
        """PyTorchæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if self.pytorch_model is None or self.tokenizer is None:
                return "âŒ PyTorchãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
            
            print(f"ğŸ’¬ PyTorchæ—¥æœ¬èªç”Ÿæˆ: '{prompt}'")
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.pytorch_model.device)
            
            with torch.no_grad():
                outputs = self.pytorch_model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    min_new_tokens=10,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            return generated_text
            
        except Exception as e:
            return f"âŒ PyTorchç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def generate_text_onnx(self, prompt: str, max_new_tokens: int = 50) -> str:
        """ONNX NPUæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if self.onnx_session is None or self.tokenizer is None:
                return "âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            print(f"ğŸ’¬ ONNX NPUæ—¥æœ¬èªç”Ÿæˆ: '{prompt}'")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="np",
                max_length=128,
                padding="max_length",
                truncation=True
            )
            
            input_ids = inputs['input_ids']
            generated_tokens = input_ids[0].tolist()
            
            # è‡ªå·±å›å¸°ç”Ÿæˆ
            for _ in range(max_new_tokens):
                # ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§æ¨è«–
                current_input = np.array([generated_tokens[-128:]], dtype=np.int64)  # æœ€æ–°128ãƒˆãƒ¼ã‚¯ãƒ³
                
                if self.active_provider == 'VitisAIExecutionProvider':
                    print("âš¡ VitisAI NPUæ¨è«–å®Ÿè¡Œä¸­...")
                
                outputs = self.onnx_session.run(None, {'input_ids': current_input})
                logits = outputs[0]
                
                # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
                next_token_logits = logits[0, -1, :]
                
                # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                next_token_logits = next_token_logits / 0.7
                
                # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
                exp_logits = np.exp(next_token_logits - np.max(next_token_logits))
                probs = exp_logits / np.sum(exp_logits)
                
                # Top-pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                sorted_indices = np.argsort(probs)[::-1]
                cumsum_probs = np.cumsum(probs[sorted_indices])
                cutoff_index = np.searchsorted(cumsum_probs, 0.9) + 1
                top_indices = sorted_indices[:cutoff_index]
                top_probs = probs[top_indices]
                top_probs = top_probs / np.sum(top_probs)
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token = np.random.choice(top_indices, p=top_probs)
                
                # EOSãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
                if next_token == self.tokenizer.eos_token_id:
                    break
                
                generated_tokens.append(int(next_token))
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            self.total_inferences += 1
            
            if self.active_provider == 'VitisAIExecutionProvider':
                print("âœ… VitisAI NPUæ¨è«–å®Œäº†")
            
            return generated_text
            
        except Exception as e:
            return f"âŒ ONNX NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def run_benchmark(self, num_inferences: int = 30) -> Dict[str, Any]:
        """NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        try:
            print(f"ğŸ“Š NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {num_inferences}å›æ¨è«–")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
            print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"ğŸŒ è¨€èª: æ—¥æœ¬èª")
            
            self.start_npu_monitoring()
            
            start_time = time.time()
            successful_inferences = 0
            
            test_prompts = [
                "AIã«ã‚ˆã£ã¦ç§é”ã®æš®ã‚‰ã—ã¯ã€",
                "æ—¥æœ¬ã®æœªæ¥ã«ã¤ã„ã¦è€ƒãˆã‚‹ã¨ã€",
                "æŠ€è¡“é©æ–°ãŒç¤¾ä¼šã«ä¸ãˆã‚‹å½±éŸ¿ã¯ã€",
                "äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã‚ˆã‚Šã€",
                "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªã®ã¯ã€"
            ]
            
            for i in range(num_inferences):
                try:
                    prompt = test_prompts[i % len(test_prompts)]
                    
                    if self.onnx_session:
                        result = self.generate_text_onnx(prompt, max_new_tokens=20)
                    else:
                        result = self.generate_text_pytorch(prompt, max_new_tokens=20)
                    
                    if not result.startswith("âŒ"):
                        successful_inferences += 1
                        print(f"âœ… æ¨è«– {i+1}/{num_inferences}: æˆåŠŸ")
                    else:
                        print(f"âŒ æ¨è«– {i+1}/{num_inferences}: å¤±æ•—")
                    
                except Exception as e:
                    print(f"âŒ æ¨è«– {i+1}/{num_inferences}: ã‚¨ãƒ©ãƒ¼ - {e}")
            
            total_time = time.time() - start_time
            self.stop_npu_monitoring()
            
            # çµ±è¨ˆè¨ˆç®—
            success_rate = (successful_inferences / num_inferences) * 100
            throughput = successful_inferences / total_time
            avg_inference_time = total_time / num_inferences * 1000  # ms
            
            npu_stats = self.get_npu_statistics()
            
            results = {
                "successful_inferences": successful_inferences,
                "total_inferences": num_inferences,
                "success_rate": success_rate,
                "total_time": total_time,
                "throughput": throughput,
                "avg_inference_time": avg_inference_time,
                "active_provider": self.active_provider,
                "model_id": self.model_id,
                **npu_stats
            }
            
            print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
            print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {successful_inferences}/{num_inferences}")
            print(f"  ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
            print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time:.1f}ms")
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            
            if npu_stats:
                print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_npu_usage']:.1f}%")
                print(f"  ğŸ“Š å¹³å‡NPUä½¿ç”¨ç‡: {npu_stats['avg_npu_usage']:.1f}%")
                print(f"  ğŸ¯ NPUå‹•ä½œç‡: {npu_stats['npu_activity_rate']:.1f}%")
            
            return results
            
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰"""
        print(f"\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
        print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
        print(f"ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†, 'npu'ã§NPUçŠ¶æ³ç¢ºèª")
        print(f"=" * 60)
        
        self.start_npu_monitoring()
        
        try:
            while True:
                prompt = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if prompt.lower() == 'npu':
                    npu_stats = self.get_npu_statistics()
                    if npu_stats:
                        print(f"ğŸ”¥ NPUçµ±è¨ˆ:")
                        print(f"  æœ€å¤§ä½¿ç”¨ç‡: {npu_stats['max_npu_usage']:.1f}%")
                        print(f"  å¹³å‡ä½¿ç”¨ç‡: {npu_stats['avg_npu_usage']:.1f}%")
                        print(f"  å‹•ä½œå›æ•°: {npu_stats['npu_active_count']}")
                        print(f"  ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {npu_stats['active_provider']}")
                    continue
                
                if not prompt:
                    continue
                
                print(f"ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
                
                start_time = time.time()
                
                if self.onnx_session:
                    result = self.generate_text_onnx(prompt, max_new_tokens=64)
                else:
                    result = self.generate_text_pytorch(prompt, max_new_tokens=64)
                
                generation_time = time.time() - start_time
                
                print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                print(f"{result}")
                print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
        finally:
            self.stop_npu_monitoring()
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“åˆæœŸåŒ–"""
        try:
            print("ğŸš€ Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            
            # infer-OSç’°å¢ƒè¨­å®š
            self.setup_infer_os_environment()
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if not self.download_model():
                return False
            
            # PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if not self.load_pytorch_model():
                return False
            
            # ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if not self.export_to_onnx():
                return False
            
            # ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.setup_onnx_session():
                print("âš ï¸ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—ã€PyTorchãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š")
            
            print("âœ… Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument("--inferences", type=int, default=30, help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    parser.add_argument("--tokens", type=int, default=50, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    try:
        if args.compare:
            print("ğŸ“Š infer-OS ON/OFFæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆinfer-OS OFFï¼‰
            print("\nğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰")
            system_off = RyzenAIJapaneseLLMSystem(enable_infer_os=False)
            if system_off.initialize_system():
                results_off = system_off.run_benchmark(args.inferences)
            
            # æœ€é©åŒ–ç‰ˆï¼ˆinfer-OS ONï¼‰
            print("\nâš¡ æœ€é©åŒ–ç‰ˆæ¸¬å®šï¼ˆinfer-OS ONï¼‰")
            system_on = RyzenAIJapaneseLLMSystem(enable_infer_os=True)
            if system_on.initialize_system():
                results_on = system_on.run_benchmark(args.inferences)
            
            # æ¯”è¼ƒçµæœ
            if results_off and results_on:
                improvement = ((results_on['throughput'] - results_off['throughput']) / results_off['throughput']) * 100
                print(f"\nğŸ“Š infer-OSåŠ¹æœæ¸¬å®šçµæœ:")
                print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
                print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
                print(f"  ğŸ“ˆ æ”¹å–„ç‡: {improvement:+.1f}%")
        
        else:
            system = RyzenAIJapaneseLLMSystem(enable_infer_os=args.infer_os)
            
            if not system.initialize_system():
                print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            if args.interactive:
                system.interactive_mode()
            elif args.benchmark:
                system.run_benchmark(args.inferences)
            elif args.prompt:
                print(f"ğŸ’¬ å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: '{args.prompt}'")
                
                if system.onnx_session:
                    result = system.generate_text_onnx(args.prompt, args.tokens)
                else:
                    result = system.generate_text_pytorch(args.prompt, args.tokens)
                
                print(f"ğŸ¯ ç”Ÿæˆçµæœ:")
                print(f"{result}")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
                system.run_benchmark(5)
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

