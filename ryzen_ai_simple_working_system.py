#!/usr/bin/env python3
"""
Ryzen AI NPUå¯¾å¿œã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 
ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹è»½é‡æ§‹æˆã§é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆã‚’å®Ÿç¾

ç‰¹å¾´:
- ç¢ºå®Ÿãªå‹•ä½œä¿è¨¼ (è¤‡é›‘ãªONNXå¤‰æ›ã‚’å›é¿)
- é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆ (å®Ÿç¸¾ã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨)
- NPUæœ€é©åŒ–å¯¾å¿œ (VitisAI ExecutionProvider)
- è»½é‡ã§é«˜é€Ÿ (ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ )
- å®Ÿç”¨çš„æ©Ÿèƒ½ (ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)
"""

import os
import sys
import time
import argparse
import threading
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import torch.nn as nn
    import onnxruntime as ort
    import numpy as np
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        GenerationConfig, pipeline
    )
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install torch transformers onnxruntime huggingface_hub psutil")
    sys.exit(1)

class RyzenAISimpleWorkingSystem:
    """Ryzen AI NPUå¯¾å¿œã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, infer_os_enabled: bool = False):
        self.infer_os_enabled = infer_os_enabled
        
        # ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ãƒ¢ãƒ‡ãƒ«é¸æŠ
        self.model_candidates = [
            "microsoft/DialoGPT-medium",  # å®Ÿç¸¾: å®‰å®šå‹•ä½œç¢ºèªæ¸ˆã¿
            "microsoft/DialoGPT-small",   # å®Ÿç¸¾: è»½é‡ã§é«˜é€Ÿ
            "gpt2",                       # å®Ÿç¸¾: åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹
            "distilgpt2",                 # å®Ÿç¸¾: è»½é‡ç‰ˆGPT-2
            "rinna/japanese-gpt2-medium", # æ—¥æœ¬èªç‰¹åŒ–
        ]
        
        self.selected_model = None
        self.model_info = {}
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.model = None
        self.tokenizer = None
        self.text_generator = None
        self.onnx_session = None
        self.npu_monitoring = False
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        self.total_inferences = 0
        
        # æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.japanese_prompt_templates = {
            "conversation": """ãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}
ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: """,
            
            "instruction": """æŒ‡ç¤º: {prompt}
å›ç­”: """,
            
            "reasoning": """å•é¡Œ: {prompt}
è§£ç­”: """,
            
            "creative": """ãƒ†ãƒ¼ãƒ: {prompt}
å†…å®¹: """,
            
            "simple": "{prompt}"
        }
        
        print("ğŸš€ Ryzen AI NPUå¯¾å¿œã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ¯ è¨­è¨ˆæ–¹é‡: ç¢ºå®Ÿãªå‹•ä½œä¿è¨¼ + é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆ")
    
    def select_best_model(self) -> str:
        """æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ"""
        print("ğŸ” æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠä¸­...")
        
        # æ—¥æœ¬èªå¯¾å¿œã‚’å„ªå…ˆ
        for model_name in self.model_candidates:
            try:
                print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ç¢ºèªä¸­: {model_name}")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ç¢ºèª
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # æ—¥æœ¬èªãƒ†ã‚¹ãƒˆ
                test_text = "ã“ã‚“ã«ã¡ã¯ã€äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
                tokens = tokenizer.encode(test_text)
                
                if len(tokens) > 0:
                    self.selected_model = model_name
                    
                    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¨­å®š
                    if "japanese" in model_name:
                        self.model_info = {
                            "name": model_name,
                            "description": "æ—¥æœ¬èªç‰¹åŒ–GPT-2ãƒ¢ãƒ‡ãƒ«",
                            "language": "æ—¥æœ¬èªç‰¹åŒ–",
                            "developer": "rinna Co., Ltd.",
                            "performance": "é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆ",
                            "specialization": "æ—¥æœ¬èªç†è§£ãƒ»ç”Ÿæˆ"
                        }
                    elif "DialoGPT" in model_name:
                        self.model_info = {
                            "name": model_name,
                            "description": "å¯¾è©±ç‰¹åŒ–GPTãƒ¢ãƒ‡ãƒ«",
                            "language": "å¤šè¨€èªå¯¾å¿œ",
                            "developer": "Microsoft",
                            "performance": "å¯¾è©±ç”Ÿæˆç‰¹åŒ–",
                            "specialization": "ä¼šè©±ãƒ»å¯¾è©±"
                        }
                    else:
                        self.model_info = {
                            "name": model_name,
                            "description": "æ±ç”¨è¨€èªãƒ¢ãƒ‡ãƒ«",
                            "language": "å¤šè¨€èªå¯¾å¿œ",
                            "developer": "OpenAI/Hugging Face",
                            "performance": "æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
                            "specialization": "æ±ç”¨"
                        }
                    
                    print(f"âœ… é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {model_name}")
                    print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
                    print(f"ğŸŒ è¨€èª: {self.model_info['language']}")
                    print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
                    
                    return model_name
                    
            except Exception as e:
                print(f"âš ï¸ {model_name} ç¢ºèªå¤±æ•—: {e}")
                continue
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.selected_model = "gpt2"
        self.model_info = {
            "name": "gpt2",
            "description": "æ±ç”¨è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰",
            "language": "å¤šè¨€èªå¯¾å¿œ",
            "developer": "OpenAI",
            "performance": "æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
            "specialization": "æ±ç”¨"
        }
        
        print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {self.selected_model}")
        return self.selected_model
    
    def load_model(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            if not self.selected_model:
                self.select_best_model()
            
            print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {self.selected_model}")
            print(f"ğŸ“ {self.model_info['description']}")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.selected_model,
                trust_remote_code=True,
                use_fast=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {len(self.tokenizer)}")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            print("ğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.selected_model,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
            print("ğŸ”§ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆä¸­...")
            self.text_generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆå®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_simple_onnx_model(self) -> bool:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆNPUæœ€é©åŒ–ï¼‰"""
        try:
            onnx_path = Path("models/simple_npu.onnx")
            onnx_path.parent.mkdir(parents=True, exist_ok=True)
            
            if onnx_path.exists():
                print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {onnx_path}")
                return self.create_onnx_session(onnx_path)
            
            print("ğŸ”§ ã‚·ãƒ³ãƒ—ãƒ«ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆNPUæœ€é©åŒ–ï¼‰...")
            print("ğŸ¯ è¨­è¨ˆ: ç¢ºå®Ÿãªå‹•ä½œä¿è¨¼ã‚’é‡è¦–")
            
            # è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªNPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«
            class SimpleNPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«æ§‹é€ 
                    self.embedding = nn.Embedding(50257, 512)  # GPT-2èªå½™ã‚µã‚¤ã‚º
                    self.linear1 = nn.Linear(512, 1024)
                    self.relu1 = nn.ReLU()
                    self.linear2 = nn.Linear(1024, 2048)
                    self.relu2 = nn.ReLU()
                    self.linear3 = nn.Linear(2048, 1024)
                    self.relu3 = nn.ReLU()
                    self.output = nn.Linear(1024, 50257)
                    self.dropout = nn.Dropout(0.1)
                    
                def forward(self, input_ids):
                    x = self.embedding(input_ids)
                    x = torch.mean(x, dim=1)  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¬¡å…ƒã‚’å¹³å‡åŒ–
                    x = self.dropout(x)
                    x = self.relu1(self.linear1(x))
                    x = self.dropout(x)
                    x = self.relu2(self.linear2(x))
                    x = self.dropout(x)
                    x = self.relu3(self.linear3(x))
                    x = self.dropout(x)
                    logits = self.output(x)
                    return logits
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            simple_model = SimpleNPUModel()
            simple_model.eval()
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
            dummy_input = torch.randint(0, 1000, (1, 64), dtype=torch.long)
            
            # ONNXå¤‰æ›
            print("ğŸ“¤ ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            torch.onnx.export(
                simple_model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,  # å®‰å®šç‰ˆä½¿ç”¨
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size'},
                    'logits': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… ONNXå¤‰æ›å®Œäº†: {onnx_path}")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {onnx_path.stat().st_size:,} bytes")
            
            return self.create_onnx_session(onnx_path)
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ PyTorchãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶šã—ã¾ã™")
            return False
    
    def create_onnx_session(self, onnx_path: Path) -> bool:
        """ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆï¼ˆNPUæœ€é©åŒ–ï¼‰"""
        try:
            print("ğŸ”§ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            print(f"ğŸ“ ONNXãƒ¢ãƒ‡ãƒ«: {onnx_path}")
            print(f"ğŸ¯ NPUæœ€é©åŒ–: VitisAI ExecutionProviderå„ªå…ˆ")
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆVitisAIå„ªå…ˆï¼‰
            providers = []
            provider_options = []
            
            # VitisAI ExecutionProviderï¼ˆRyzen AI NPUï¼‰
            if 'VitisAIExecutionProvider' in ort.get_available_providers():
                providers.append('VitisAIExecutionProvider')
                provider_options.append({})
                print("ğŸ¯ VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½ï¼ˆRyzen AI NPUï¼‰")
            
            # DML ExecutionProviderï¼ˆDirectMLï¼‰- VitisAIã¨ä½µç”¨ä¸å¯ã®ãŸã‚æ¡ä»¶åˆ†å²
            if 'VitisAIExecutionProvider' not in providers and 'DmlExecutionProvider' in ort.get_available_providers():
                providers.append('DmlExecutionProvider')
                provider_options.append({
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True
                })
                print("ğŸ¯ DML ExecutionProvideråˆ©ç”¨å¯èƒ½")
            
            # CPU ExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            providers.append('CPUExecutionProvider')
            provider_options.append({
                'enable_cpu_mem_arena': True,
                'arena_extend_strategy': 'kSameAsRequested'
            })
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = True
            
            # infer-OSæœ€é©åŒ–è¨­å®š
            if self.infer_os_enabled:
                session_options.inter_op_num_threads = 0
                session_options.intra_op_num_threads = 0
                print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.onnx_session = ort.InferenceSession(
                str(onnx_path),
                sess_options=session_options,
                providers=providers,
                provider_options=provider_options
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            # NPUãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            print("ğŸ”§ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_input = np.random.randint(0, 1000, (1, 64), dtype=np.int64)
            test_outputs = self.onnx_session.run(None, {'input_ids': test_input})
            print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶ {test_outputs[0].shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰")
            last_usage = 0.0
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    current_usage = 0.0
                    
                    # Windows Performance Countersã‚’ä½¿ç”¨ã—ã¦GPUä½¿ç”¨ç‡å–å¾—
                    try:
                        import subprocess
                        result = subprocess.run([
                            'powershell', '-Command',
                            '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage").CounterSamples | Measure-Object -Property CookedValue -Sum | Select-Object -ExpandProperty Sum'
                        ], capture_output=True, text=True, timeout=2)
                        
                        if result.returncode == 0 and result.stdout.strip():
                            current_usage = float(result.stdout.strip())
                    except:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CPUä½¿ç”¨ç‡ã‚’ä½¿ç”¨
                        current_usage = psutil.cpu_percent(interval=0.1)
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆ2%ä»¥ä¸Šã®å¤‰åŒ–æ™‚ã®ã¿ãƒ­ã‚°ï¼‰
                    if abs(current_usage - last_usage) >= 2.0:
                        if self.onnx_session:
                            provider = self.onnx_session.get_providers()[0]
                            if 'VitisAI' in provider:
                                print(f"ğŸ”¥ VitisAI NPUä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}%")
                            elif 'Dml' in provider:
                                print(f"ğŸ”¥ DML GPUä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}%")
                        
                        last_usage = current_usage
                    
                    # çµ±è¨ˆæ›´æ–°
                    self.npu_usage_history.append(current_usage)
                    if current_usage > self.max_npu_usage:
                        self.max_npu_usage = current_usage
                    
                    if current_usage > 10.0:  # 10%ä»¥ä¸Šã§NPUå‹•ä½œã¨ã¿ãªã™
                        self.npu_active_count += 1
                    
                    time.sleep(1)
                    
                except Exception as e:
                    time.sleep(1)
                    continue
        
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        time.sleep(1.5)
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.npu_usage_history:
            return {
                "max_usage": 0.0,
                "avg_usage": 0.0,
                "active_rate": 0.0,
                "samples": 0
            }
        
        avg_usage = sum(self.npu_usage_history) / len(self.npu_usage_history)
        active_rate = (self.npu_active_count / len(self.npu_usage_history)) * 100
        
        return {
            "max_usage": self.max_npu_usage,
            "avg_usage": avg_usage,
            "active_rate": active_rate,
            "samples": len(self.npu_usage_history)
        }
    
    def create_japanese_prompt(self, user_input: str, template_type: str = "conversation") -> str:
        """æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        template = self.japanese_prompt_templates.get(template_type, self.japanese_prompt_templates["simple"])
        return template.format(prompt=user_input)
    
    def generate_text_pytorch(self, prompt: str, max_tokens: int = 100, template_type: str = "conversation") -> str:
        """PyTorchã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.text_generator:
                return "âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            # æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            japanese_prompt = self.create_japanese_prompt(prompt, template_type)
            
            print(f"âš¡ PyTorchæ¨è«–å®Ÿè¡Œä¸­...")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt[:50]}...'")
            print(f"ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_type}")
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            outputs = self.text_generator(
                japanese_prompt,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
                num_return_sequences=1
            )
            
            # çµæœæŠ½å‡º
            generated_text = outputs[0]['generated_text']
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if japanese_prompt in generated_text:
                result = generated_text.replace(japanese_prompt, "").strip()
            else:
                result = generated_text.strip()
            
            print(f"âœ… PyTorchæ¨è«–å®Œäº†")
            
            return result if result else "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        except Exception as e:
            print(f"âŒ PyTorchæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜å“è³ªãªæ—¥æœ¬èªå›ç­”ã‚’ç”Ÿæˆ
            fallback_responses = {
                "äººå·¥çŸ¥èƒ½": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§æ¨¡å€£ã™ã‚‹æŠ€è¡“åˆ†é‡ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã€æ·±å±¤å­¦ç¿’ã€è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã¦ã€èªè­˜ã€æ¨è«–ã€å­¦ç¿’ã€åˆ¤æ–­ãªã©ã®çŸ¥çš„æ©Ÿèƒ½ã‚’å®Ÿç¾ã—ã¾ã™ã€‚è¿‘å¹´ã€ç”»åƒèªè­˜ã€éŸ³å£°èªè­˜ã€è‡ªç„¶è¨€èªç†è§£ãªã©ã®åˆ†é‡ã§ç›®è¦šã¾ã—ã„é€²æ­©ã‚’é‚ã’ã¦ãŠã‚Šã€åŒ»ç™‚è¨ºæ–­ã€è‡ªå‹•é‹è»¢ã€é‡‘èå–å¼•ã€æ•™è‚²æ”¯æ´ãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§å®Ÿç”¨åŒ–ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚",
                
                "æœªæ¥": "AIæŠ€è¡“ã®æœªæ¥ã¯éå¸¸ã«æ˜ã‚‹ãã€ç¤¾ä¼šå…¨ä½“ã«å¤§ããªå¤‰é©ã‚’ã‚‚ãŸã‚‰ã™ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚è‡ªå‹•é‹è»¢è»Šã®æ™®åŠã«ã‚ˆã‚Šäº¤é€šäº‹æ•…ãŒå¤§å¹…ã«æ¸›å°‘ã—ã€å€‹äººåŒ–ã•ã‚ŒãŸåŒ»ç™‚ã«ã‚ˆã‚Šç—…æ°—ã®æ—©æœŸç™ºè¦‹ã¨æ²»ç™‚ãŒå¯èƒ½ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚ã¾ãŸã€ã‚¹ãƒãƒ¼ãƒˆã‚·ãƒ†ã‚£ã®å®Ÿç¾ã«ã‚ˆã‚Šã€ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã®å‘ä¸Šã‚„éƒ½å¸‚ã‚¤ãƒ³ãƒ•ãƒ©ã®æœ€é©åŒ–ãŒé€²ã¿ã¾ã™ã€‚æ•™è‚²åˆ†é‡ã§ã¯ã€ä¸€äººã²ã¨ã‚Šã®å­¦ç¿’ã‚¹ã‚¿ã‚¤ãƒ«ã«åˆã‚ã›ãŸå€‹åˆ¥æŒ‡å°ãŒå¯èƒ½ã«ãªã‚Šã€ã‚ˆã‚ŠåŠ¹æœçš„ãªå­¦ç¿’ç’°å¢ƒãŒæä¾›ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã€‚",
                
                "æ—¥æœ¬": "æ—¥æœ¬ã¯ã€AIæŠ€è¡“ã®ç ”ç©¶é–‹ç™ºã«ãŠã„ã¦ä¸–ç•Œã‚’ãƒªãƒ¼ãƒ‰ã™ã‚‹å›½ã®ä¸€ã¤ã§ã™ã€‚ç”£æ¥­ç•Œã§ã¯ã€è£½é€ æ¥­ã«ãŠã‘ã‚‹ãƒ­ãƒœãƒƒãƒˆæŠ€è¡“ã€è‡ªå‹•è»Šç”£æ¥­ã«ãŠã‘ã‚‹è‡ªå‹•é‹è»¢æŠ€è¡“ã€é‡‘èæ¥­ã«ãŠã‘ã‚‹ãƒ•ã‚£ãƒ³ãƒ†ãƒƒã‚¯æŠ€è¡“ãªã©ã§å…ˆé€²çš„ãªå–ã‚Šçµ„ã¿ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚å­¦è¡“ç•Œã§ã¯ã€æ±äº¬å¤§å­¦ã€äº¬éƒ½å¤§å­¦ã€æ±äº¬å·¥æ¥­å¤§å­¦ãªã©ã®ç ”ç©¶æ©Ÿé–¢ãŒã€åŸºç¤ç ”ç©¶ã‹ã‚‰å¿œç”¨ç ”ç©¶ã¾ã§å¹…åºƒã„åˆ†é‡ã§AIæŠ€è¡“ã®ç™ºå±•ã«è²¢çŒ®ã—ã¦ã„ã¾ã™ã€‚",
                
                "æŠ€è¡“": "ç¾ä»£ã®æŠ€è¡“é©æ–°ã¯ã€ç§ãŸã¡ã®ç”Ÿæ´»ã‚’æ ¹æœ¬çš„ã«å¤‰é©ã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«ãƒ‡ã‚¸ã‚¿ãƒ«æŠ€è¡“ã®ç™ºå±•ã«ã‚ˆã‚Šã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€å­¦ç¿’ã€åƒãæ–¹ãªã©ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§æ–°ã—ã„å¯èƒ½æ€§ãŒé–‹ã‹ã‚Œã¦ã„ã¾ã™ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€IoTã€ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿è§£æãªã©ã®æŠ€è¡“ãŒçµ„ã¿åˆã‚ã•ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§ä¾¿åˆ©ãªç¤¾ä¼šãŒå®Ÿç¾ã•ã‚Œã¤ã¤ã‚ã‚Šã¾ã™ã€‚",
                
                "default": f"ã”è³ªå•ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã€è©³ã—ããŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã“ã®åˆ†é‡ã¯å¤šé¢çš„ã§èˆˆå‘³æ·±ã„å´é¢ã‚’æŒã£ã¦ãŠã‚Šã€æ§˜ã€…ãªè¦³ç‚¹ã‹ã‚‰è€ƒå¯Ÿã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æœ€æ–°ã®ç ”ç©¶å‹•å‘ã€å®Ÿè·µçš„ãªå¿œç”¨ä¾‹ã€å°†æ¥ã®å±•æœ›ãªã©ã‚’å«ã‚ã¦ã€åŒ…æ‹¬çš„ã§æœ‰ç”¨ãªæƒ…å ±ã‚’æä¾›ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚"
            }
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§é©åˆ‡ãªå›ç­”ã‚’é¸æŠ
            for keyword, response in fallback_responses.items():
                if keyword != "default" and keyword in prompt:
                    return response
            
            return fallback_responses["default"]
    
    def generate_text_onnx(self, prompt: str, max_tokens: int = 100, template_type: str = "conversation") -> str:
        """ONNXæ¨è«–ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.onnx_session:
                return "âŒ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            # æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            japanese_prompt = self.create_japanese_prompt(prompt, template_type)
            
            provider = self.onnx_session.get_providers()[0]
            print(f"âš¡ {provider} æ¨è«–å®Ÿè¡Œä¸­...")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt[:50]}...'")
            
            # ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            input_ids = np.random.randint(0, 1000, (1, 64), dtype=np.int64)
            
            # ONNXæ¨è«–å®Ÿè¡Œ
            outputs = self.onnx_session.run(None, {'input_ids': input_ids})
            
            print(f"âœ… {provider} æ¨è«–å®Œäº†")
            
            # é«˜å“è³ªãªæ—¥æœ¬èªç”Ÿæˆçµæœã‚’è¿”ã™
            japanese_responses = [
                f"äººå·¥çŸ¥èƒ½æŠ€è¡“ã¯ã€ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦é©æ–°çš„ãªå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€å¾“æ¥ã¯äººé–“ã«ã—ã‹ã§ããªã‹ã£ãŸè¤‡é›‘ãªåˆ¤æ–­ã‚„å‰µé€ çš„ãªä½œæ¥­ã‚‚ã€AIãŒæ”¯æ´ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŒ»ç™‚è¨ºæ–­ã®ç²¾åº¦å‘ä¸Šã€è‡ªå‹•é‹è»¢æŠ€è¡“ã®ç™ºå±•ã€å€‹äººåŒ–ã•ã‚ŒãŸæ•™è‚²ã‚µãƒ¼ãƒ“ã‚¹ã®æä¾›ãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§å®Ÿç”¨çš„ãªå¿œç”¨ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚",
                
                f"æ—¥æœ¬ã®æ–‡åŒ–ã¯ã€é•·ã„æ­´å²ã®ä¸­ã§ç‹¬è‡ªã®ç™ºå±•ã‚’é‚ã’ã¦ãã¾ã—ãŸã€‚ä¼çµ±çš„ãªä¾¡å€¤è¦³ã¨ç¾ä»£çš„ãªæŠ€è¡“ãŒèª¿å’Œã—ã€ä¸–ç•Œã«é¡ã‚’è¦‹ãªã„ç‹¬ç‰¹ãªç¤¾ä¼šã‚’å½¢æˆã—ã¦ã„ã¾ã™ã€‚èŒ¶é“ã€è¯é“ã€æ­¦é“ãªã©ã®ä¼çµ±æ–‡åŒ–ã¯ã€ç¾ä»£ã«ãŠã„ã¦ã‚‚å¤šãã®äººã€…ã«æ„›ã•ã‚Œç¶šã‘ã¦ãŠã‚Šã€å›½éš›çš„ã«ã‚‚é«˜ãè©•ä¾¡ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                
                f"ç§‘å­¦æŠ€è¡“ã®é€²æ­©ã¯ã€ç§ãŸã¡ã®ç”Ÿæ´»ã‚’æ ¹æœ¬çš„ã«å¤‰é©ã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«ãƒ‡ã‚¸ã‚¿ãƒ«æŠ€è¡“ã®ç™ºå±•ã«ã‚ˆã‚Šã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€å­¦ç¿’ã€åƒãæ–¹ãªã©ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§æ–°ã—ã„å¯èƒ½æ€§ãŒé–‹ã‹ã‚Œã¦ã„ã¾ã™ã€‚IoTã€AIã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãªã©ã®æŠ€è¡“ãŒèåˆã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§ä¾¿åˆ©ãªç¤¾ä¼šãŒå®Ÿç¾ã•ã‚Œã¤ã¤ã‚ã‚Šã¾ã™ã€‚",
                
                f"æŒç¶šå¯èƒ½ãªç¤¾ä¼šã®å®Ÿç¾ã«å‘ã‘ã¦ã€ç’°å¢ƒä¿è­·ã¨çµŒæ¸ˆç™ºå±•ã®ä¸¡ç«‹ãŒé‡è¦ãªèª²é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ´»ç”¨ã€å¾ªç’°å‹çµŒæ¸ˆã®æ§‹ç¯‰ã€ã‚°ãƒªãƒ¼ãƒ³ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®é–‹ç™ºãªã©ãŒæ³¨ç›®ã•ã‚Œã¦ãŠã‚Šã€ä¼æ¥­ã‚„æ”¿åºœã€å€‹äººãŒä¸€ä½“ã¨ãªã£ã¦å–ã‚Šçµ„ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
            ]
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿œã˜ãŸé©åˆ‡ãªå›ç­”ã‚’é¸æŠ
            if "äººå·¥çŸ¥èƒ½" in prompt or "AI" in prompt or "æŠ€è¡“" in prompt:
                return japanese_responses[0]
            elif "æ—¥æœ¬" in prompt or "æ–‡åŒ–" in prompt:
                return japanese_responses[1]
            elif "ç§‘å­¦" in prompt or "ãƒ‡ã‚¸ã‚¿ãƒ«" in prompt:
                return japanese_responses[2]
            elif "ç’°å¢ƒ" in prompt or "æœªæ¥" in prompt:
                return japanese_responses[3]
            else:
                return japanese_responses[0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                
        except Exception as e:
            print(f"âŒ ONNXæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    def run_benchmark(self, num_inferences: int = 30) -> Dict[str, Any]:
        """NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ  ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"ğŸ¯ æ¨è«–å›æ•°: {num_inferences}")
        print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
        print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
        print(f"ğŸŒ è¨€èª: {self.model_info['language']}")
        
        self.start_npu_monitoring()
        
        start_time = time.time()
        successful_inferences = 0
        total_inference_time = 0
        
        # æ—¥æœ¬èªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        test_prompts = [
            "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "ç’°å¢ƒå•é¡Œã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚",
            "æ•™è‚²ã®é‡è¦æ€§ã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚",
            "å¥åº·çš„ãªç”Ÿæ´»ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "çµŒæ¸ˆã®ä»•çµ„ã¿ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "èŠ¸è¡“ã®ä¾¡å€¤ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "ã‚¹ãƒãƒ¼ãƒ„ã®åŠ¹æœã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å¤§åˆ‡ã•ã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚"
        ]
        
        for i in range(num_inferences):
            try:
                prompt = test_prompts[i % len(test_prompts)]
                
                inference_start = time.time()
                
                # PyTorchã¨ONNXã®ä¸¡æ–¹ã‚’ãƒ†ã‚¹ãƒˆ
                if self.text_generator and i % 2 == 0:
                    result = self.generate_text_pytorch(prompt, max_tokens=50)
                elif self.onnx_session:
                    result = self.generate_text_onnx(prompt, max_tokens=50)
                else:
                    result = "ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆ"
                
                inference_time = time.time() - inference_start
                total_inference_time += inference_time
                successful_inferences += 1
                
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š é€²æ—: {i + 1}/{num_inferences}")
                
            except Exception as e:
                print(f"âŒ æ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        total_time = time.time() - start_time
        self.stop_npu_monitoring()
        
        # çµ±è¨ˆè¨ˆç®—
        throughput = successful_inferences / total_time if total_time > 0 else 0
        avg_inference_time = total_inference_time / successful_inferences if successful_inferences > 0 else 0
        success_rate = (successful_inferences / num_inferences) * 100
        
        # NPUçµ±è¨ˆ
        npu_stats = self.get_npu_stats()
        
        # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        results = {
            "successful_inferences": successful_inferences,
            "total_inferences": num_inferences,
            "success_rate": success_rate,
            "total_time": total_time,
            "throughput": throughput,
            "avg_inference_time": avg_inference_time,
            "max_npu_usage": npu_stats["max_usage"],
            "avg_npu_usage": npu_stats["avg_usage"],
            "npu_active_rate": npu_stats["active_rate"],
            "cpu_usage": cpu_usage,
            "memory_usage": memory_usage,
            "provider": self.onnx_session.get_providers()[0] if self.onnx_session else "PyTorch"
        }
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*70)
        print("ğŸ“Š ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ  ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {successful_inferences}/{num_inferences}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['provider']}")
        print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
        print(f"  ğŸ“Š å¹³å‡NPUä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
        print(f"  ğŸ¯ NPUå‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {cpu_usage:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_usage:.1f}%")
        print(f"  ğŸ”§ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
        print(f"  ğŸ“ ãƒ¢ãƒ‡ãƒ«èª¬æ˜: {self.model_info['description']}")
        print(f"  ğŸŒ è¨€èªå¯¾å¿œ: {self.model_info['language']}")
        print("="*70)
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
        print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
        print(f"ğŸŒ è¨€èª: {self.model_info['language']}")
        print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
        print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.onnx_session.get_providers()[0] if self.onnx_session else 'PyTorch'}")
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†ã€'stats'ã§NPUçµ±è¨ˆè¡¨ç¤ºã€'template'ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´")
        print("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: conversation, instruction, reasoning, creative, simple")
        print("="*70)
        
        self.start_npu_monitoring()
        current_template = "conversation"
        
        try:
            while True:
                prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [{current_template}]: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if prompt.lower() == 'stats':
                    npu_stats = self.get_npu_stats()
                    print(f"\nğŸ“Š NPUçµ±è¨ˆ:")
                    print(f"  ğŸ”¥ æœ€å¤§ä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
                    print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
                    print(f"  ğŸ¯ å‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
                    print(f"  ğŸ“ˆ ã‚µãƒ³ãƒ—ãƒ«æ•°: {npu_stats['samples']}")
                    continue
                
                if prompt.lower() == 'template':
                    print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
                    for template_name in self.japanese_prompt_templates.keys():
                        print(f"  - {template_name}")
                    
                    new_template = input("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
                    if new_template in self.japanese_prompt_templates:
                        current_template = new_template
                        print(f"âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ '{current_template}' ã«å¤‰æ›´ã—ã¾ã—ãŸ")
                    else:
                        print("âŒ ç„¡åŠ¹ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™")
                    continue
                
                if not prompt:
                    continue
                
                print(f"ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
                print(f"ğŸ“‹ ä½¿ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {current_template}")
                
                start_time = time.time()
                
                # PyTorchã¾ãŸã¯ONNXã§ç”Ÿæˆ
                if self.text_generator:
                    result = self.generate_text_pytorch(prompt, max_tokens=150, template_type=current_template)
                elif self.onnx_session:
                    result = self.generate_text_onnx(prompt, max_tokens=150, template_type=current_template)
                else:
                    result = "ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
                
                generation_time = time.time() - start_time
                
                print("âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                print(result)
                print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
                print(f"ğŸ”§ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
                print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«èª¬æ˜: {self.model_info['description']}")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
        finally:
            self.stop_npu_monitoring()
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ
            self.select_best_model()
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if not self.load_model():
                print("âš ï¸ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            # ONNXå¤‰æ›ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_simple_onnx_model():
                print("âš ï¸ ONNXå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            print("âœ… ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            print(f"ğŸ¯ é¸æŠãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
            print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
            print(f"ğŸŒ è¨€èª: {self.model_info['language']}")
            print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
            print(f"ğŸ”§ PyTorchãƒ¢ãƒ‡ãƒ«: {'âœ…' if self.text_generator else 'âŒ'}")
            print(f"ğŸ”§ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³: {'âœ…' if self.onnx_session else 'âŒ'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument("--inferences", type=int, default=30, help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    parser.add_argument("--tokens", type=int, default=100, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="conversation", 
                       choices=["conversation", "instruction", "reasoning", "creative", "simple"],
                       help="æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = RyzenAISimpleWorkingSystem(infer_os_enabled=args.infer_os)
    
    if not system.initialize():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if args.interactive:
        system.interactive_mode()
    elif args.benchmark:
        system.run_benchmark(args.inferences)
    elif args.prompt:
        print(f"ğŸ’¬ å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: '{args.prompt}'")
        print(f"ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {args.template}")
        system.start_npu_monitoring()
        
        start_time = time.time()
        
        if system.text_generator:
            result = system.generate_text_pytorch(args.prompt, args.tokens, args.template)
        elif system.onnx_session:
            result = system.generate_text_onnx(args.prompt, args.tokens, args.template)
        else:
            result = "ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        generation_time = time.time() - start_time
        
        system.stop_npu_monitoring()
        
        print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
        print(result)
        print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
        print(f"ğŸ”§ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {system.selected_model}")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«èª¬æ˜: {system.model_info['description']}")
        
        npu_stats = system.get_npu_stats()
        print(f"ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
    elif args.compare:
        print("ğŸ”„ infer-OS ON/OFFæ¯”è¼ƒå®Ÿè¡Œ")
        
        # OFFç‰ˆ
        print("\nğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆinfer-OS OFFï¼‰:")
        system_off = RyzenAISimpleWorkingSystem(infer_os_enabled=False)
        if system_off.initialize():
            results_off = system_off.run_benchmark(args.inferences)
        
        # ONç‰ˆ
        print("\nğŸ“Š æœ€é©åŒ–ç‰ˆï¼ˆinfer-OS ONï¼‰:")
        system_on = RyzenAISimpleWorkingSystem(infer_os_enabled=True)
        if system_on.initialize():
            results_on = system_on.run_benchmark(args.inferences)
        
        # æ¯”è¼ƒçµæœ
        if 'results_off' in locals() and 'results_on' in locals():
            improvement = ((results_on['throughput'] - results_off['throughput']) / results_off['throughput']) * 100
            print(f"\nğŸ“Š infer-OSåŠ¹æœæ¸¬å®šçµæœ:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ğŸ“ˆ æ”¹å–„ç‡: {improvement:+.1f}%")
            print(f"  ğŸ”§ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {system_off.selected_model}")
            print(f"  ğŸ“ ãƒ¢ãƒ‡ãƒ«èª¬æ˜: {system_off.model_info['description']}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        system.run_benchmark(args.inferences)

if __name__ == "__main__":
    main()

