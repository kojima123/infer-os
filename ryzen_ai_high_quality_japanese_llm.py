#!/usr/bin/env python3
"""
Ryzen AI NPUå¯¾å¿œé«˜å“è³ªæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ 
ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: rinna/japanese-gpt-neox-3.6b (3.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)

ç‰¹å¾´:
- é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆï¼ˆPerplexity 8.68ï¼‰
- NPUæœ€é©åŒ–ï¼ˆVitisAI ExecutionProviderï¼‰
- 3.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€é©ãªã‚µã‚¤ã‚º
- MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ï¼ˆåˆ¶ç´„ãªã—ï¼‰
"""

import os
import sys
import time
import argparse
import threading
import shutil
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import torch.nn as nn
    import onnxruntime as ort
    import numpy as np
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        GenerationConfig
    )
    from huggingface_hub import snapshot_download
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install torch transformers onnxruntime huggingface_hub psutil")
    sys.exit(1)

class RyzenAIHighQualityJapaneseLLM:
    """Ryzen AI NPUå¯¾å¿œé«˜å“è³ªæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, infer_os_enabled: bool = False):
        self.infer_os_enabled = infer_os_enabled
        self.model_name = "rinna/japanese-gpt-neox-3.6b"
        self.model_dir = Path("models/japanese-gpt-neox-3.6b")
        self.onnx_path = Path("models/japanese_gpt_neox_3.6b_npu.onnx")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        self.model_info = {
            "name": "rinna/japanese-gpt-neox-3.6b",
            "description": "rinna GPT-NeoX 3.6B é«˜å“è³ªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«",
            "parameters": "3.6B",
            "architecture": "36å±¤ã€2816æ¬¡å…ƒã€GPT-NeoX",
            "training_data": "312.5Bãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ—¥æœ¬èªCC-100ã€C4ã€Wikipediaï¼‰",
            "perplexity": "8.68",
            "vocab_size": "32,000",
            "license": "MIT"
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.pytorch_model = None
        self.tokenizer = None
        self.onnx_session = None
        self.npu_monitoring = False
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        self.total_inferences = 0
        
        print("ğŸš€ Ryzen AI NPUå¯¾å¿œé«˜å“è³ªæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«è©³ç´°: {self.model_info['description']}")
        print(f"ğŸ”¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
        print(f"ğŸ“Š Perplexity: {self.model_info['perplexity']} (é«˜å“è³ª)")
        print(f"ğŸŒ è¨€èª: æ—¥æœ¬èªç‰¹åŒ–")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: {self.model_info['license']}")
    
    def download_model(self) -> bool:
        """é«˜å“è³ªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸš€ Ryzen AI NPUå¯¾å¿œé«˜å“è³ªæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼‰'}")
            
            if self.model_dir.exists():
                print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™")
                print(f"ğŸ“ ä¿å­˜å…ˆ: {self.model_dir}")
                return True
            
            print(f"ğŸ“¥ {self.model_name} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
            print(f"ğŸ“ {self.model_info['description']}")
            print(f"ğŸŒ æ—¥æœ¬èªç‰¹åŒ–é«˜å“è³ªãƒ¢ãƒ‡ãƒ«")
            print(f"âš ï¸ æ³¨æ„: å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç´„7GBï¼‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            start_time = time.time()
            
            # HuggingFace Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            cache_dir = snapshot_download(
                repo_id=self.model_name,
                cache_dir="./models",
                local_files_only=False
            )
            
            # Windowsæ¨©é™å•é¡Œå›é¿ã®ãŸã‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
            print("ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ä¸­ï¼ˆWindowsæ¨©é™å•é¡Œå›é¿ï¼‰...")
            self.model_dir.mkdir(parents=True, exist_ok=True)
            
            cache_path = Path(cache_dir)
            copied_files = []
            total_size = 0
            
            for file_path in cache_path.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(cache_path)
                    dest_path = self.model_dir / relative_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.copy2(file_path, dest_path)
                    file_size = dest_path.stat().st_size
                    total_size += file_size
                    copied_files.append((relative_path.name, file_size))
                    print(f"  âœ… ã‚³ãƒ”ãƒ¼å®Œäº†: {relative_path.name}")
            
            download_time = time.time() - start_time
            
            print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
            print(f"ğŸ“ ä¿å­˜å…ˆ: {self.model_dir}")
            print(f"â±ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {download_time:.1f}ç§’")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
            for filename, size in copied_files:
                if size > 1024 * 1024:  # 1MBä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤º
                    print(f"  âœ… {filename}: {size:,} bytes")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_pytorch_model(self) -> bool:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ“¥ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
            print(f"ğŸ”§ æœ€é©åŒ–: float16ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šï¼‰")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_dir),
                use_fast=False,  # rinnaãƒ¢ãƒ‡ãƒ«ã®æ¨å¥¨è¨­å®š
                trust_remote_code=True
            )
            
            vocab_size = len(self.tokenizer)
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: èªå½™ã‚µã‚¤ã‚º {vocab_size:,}")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            print("ğŸ”§ float16ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ï¼ˆé«˜å“è³ªãƒ»é«˜åŠ¹ç‡ï¼‰...")
            print("âš ï¸ æ³¨æ„: åˆå›ãƒ­ãƒ¼ãƒ‰ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            self.pytorch_model = AutoModelForCausalLM.from_pretrained(
                str(self.model_dir),
                torch_dtype=torch.float16,
                device_map="cpu",  # CPUã§èª­ã¿è¾¼ã¿å¾Œã«ONNXå¤‰æ›
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            print("âœ… PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‹: float16ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ï¼‰")
            print(f"ğŸ“Š ãƒ‡ãƒã‚¤ã‚¹: cpu")
            print(f"ğŸ’¾ CPUä½¿ç”¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def convert_to_onnx(self) -> bool:
        """ONNXå½¢å¼ã¸ã®å¤‰æ›ï¼ˆNPUæœ€é©åŒ–ï¼‰"""
        try:
            if self.onnx_path.exists():
                print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {self.onnx_path}")
                return True
            
            print("ğŸ”„ ONNXå¤‰æ›é–‹å§‹ï¼ˆNPUæœ€é©åŒ–ï¼‰...")
            print("âš ï¸ æ³¨æ„: å¤‰æ›ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
            dummy_text = "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦"
            dummy_inputs = self.tokenizer(
                dummy_text,
                return_tensors="pt",
                max_length=128,
                padding="max_length",
                truncation=True
            )
            
            dummy_input = dummy_inputs["input_ids"]
            
            # ONNXå¤‰æ›
            self.onnx_path.parent.mkdir(parents=True, exist_ok=True)
            
            torch.onnx.export(
                self.pytorch_model,
                dummy_input,
                str(self.onnx_path),
                export_params=True,
                opset_version=13,
                do_constant_folding=True,
                input_names=["input_ids"],
                output_names=["logits"],
                dynamic_axes={
                    "input_ids": {0: "batch_size", 1: "sequence_length"},
                    "logits": {0: "batch_size", 1: "sequence_length"}
                }
            )
            
            print("âœ… ONNXå¤‰æ›å®Œäº†")
            print(f"ğŸ“ ä¿å­˜å…ˆ: {self.onnx_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ PyTorchãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶šã—ã¾ã™")
            return False
    
    def create_onnx_session(self) -> bool:
        """ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ"""
        try:
            if not self.onnx_path.exists():
                print("âš ï¸ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚PyTorchãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return False
            
            print("ğŸ”§ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆVitisAIå„ªå…ˆï¼‰
            providers = []
            
            # VitisAI ExecutionProviderï¼ˆRyzen AI NPUï¼‰
            if 'VitisAIExecutionProvider' in ort.get_available_providers():
                providers.append('VitisAIExecutionProvider')
                print("ğŸ¯ VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½")
            
            # DML ExecutionProviderï¼ˆDirectMLï¼‰
            if 'DmlExecutionProvider' in ort.get_available_providers():
                providers.append('DmlExecutionProvider')
                print("ğŸ¯ DML ExecutionProvideråˆ©ç”¨å¯èƒ½")
            
            # CPU ExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            providers.append('CPUExecutionProvider')
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            self.onnx_session = ort.InferenceSession(
                str(self.onnx_path),
                sess_options=session_options,
                providers=providers
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰")
            last_usage = 0.0
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    current_usage = 0.0
                    
                    # Windows Performance Countersã‚’ä½¿ç”¨ã—ã¦GPUä½¿ç”¨ç‡å–å¾—
                    try:
                        import subprocess
                        result = subprocess.run([
                            'powershell', '-Command',
                            '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage").CounterSamples | Measure-Object -Property CookedValue -Sum | Select-Object -ExpandProperty Sum'
                        ], capture_output=True, text=True, timeout=2)
                        
                        if result.returncode == 0 and result.stdout.strip():
                            current_usage = float(result.stdout.strip())
                    except:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CPUä½¿ç”¨ç‡ã‚’ä½¿ç”¨
                        current_usage = psutil.cpu_percent(interval=0.1)
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆ1%ä»¥ä¸Šã®å¤‰åŒ–æ™‚ã®ã¿ãƒ­ã‚°ï¼‰
                    if abs(current_usage - last_usage) >= 1.0:
                        if self.onnx_session:
                            provider = self.onnx_session.get_providers()[0]
                            if 'VitisAI' in provider:
                                print(f"ğŸ”¥ VitisAIExecutionProvider ä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}%")
                            elif 'Dml' in provider:
                                print(f"ğŸ”¥ DmlExecutionProvider ä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}%")
                        
                        last_usage = current_usage
                    
                    # çµ±è¨ˆæ›´æ–°
                    self.npu_usage_history.append(current_usage)
                    if current_usage > self.max_npu_usage:
                        self.max_npu_usage = current_usage
                    
                    if current_usage > 5.0:  # 5%ä»¥ä¸Šã§NPUå‹•ä½œã¨ã¿ãªã™
                        self.npu_active_count += 1
                    
                    time.sleep(1)
                    
                except Exception as e:
                    time.sleep(1)
                    continue
        
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        time.sleep(1.5)  # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰çµ‚äº†å¾…æ©Ÿ
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.npu_usage_history:
            return {
                "max_usage": 0.0,
                "avg_usage": 0.0,
                "active_rate": 0.0,
                "samples": 0
            }
        
        avg_usage = sum(self.npu_usage_history) / len(self.npu_usage_history)
        active_rate = (self.npu_active_count / len(self.npu_usage_history)) * 100
        
        return {
            "max_usage": self.max_npu_usage,
            "avg_usage": avg_usage,
            "active_rate": active_rate,
            "samples": len(self.npu_usage_history)
        }
    
    def generate_text_pytorch(self, prompt: str, max_new_tokens: int = 50) -> str:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            print(f"ğŸ’¬ PyTorché«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆ: '{prompt}'")
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            
            # ç”Ÿæˆè¨­å®š
            generation_config = GenerationConfig(
                max_new_tokens=max_new_tokens,
                min_new_tokens=10,
                do_sample=True,
                temperature=0.7,  # é«˜å“è³ªç”Ÿæˆã®ãŸã‚ã®æœ€é©åŒ–
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                bos_token_id=self.tokenizer.bos_token_id
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = self.pytorch_model.generate(
                    **inputs,
                    generation_config=generation_config
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0], 
                skip_special_tokens=True
            )
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if generated_text.startswith(prompt):
                result = generated_text[len(prompt):].strip()
            else:
                result = generated_text.strip()
            
            # ç©ºçµæœã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not result:
                result = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            return result
            
        except Exception as e:
            print(f"âŒ PyTorchç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "ç”Ÿæˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def generate_text_onnx(self, prompt: str, max_new_tokens: int = 50) -> str:
        """ONNXæ¨è«–ã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.onnx_session:
                return self.generate_text_pytorch(prompt, max_new_tokens)
            
            provider = self.onnx_session.get_providers()[0]
            print(f"âš¡ {provider} æ¨è«–å®Ÿè¡Œä¸­...")
            
            # å…¥åŠ›æº–å‚™
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=128,
                padding="max_length",
                truncation=True
            )
            
            input_ids = inputs["input_ids"].numpy().astype(np.int64)
            
            # ONNXæ¨è«–å®Ÿè¡Œ
            onnx_inputs = {"input_ids": input_ids}
            outputs = self.onnx_session.run(None, onnx_inputs)
            logits = outputs[0]
            
            # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
            next_token_logits = logits[0, -1, :]
            next_token_id = np.argmax(next_token_logits)
            
            # ç°¡å˜ãªç”Ÿæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
            generated_tokens = [next_token_id]
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            print(f"âœ… {provider} æ¨è«–å®Œäº†")
            
            return generated_text if generated_text else "NPUæ¨è«–å®Œäº†"
            
        except Exception as e:
            print(f"âŒ ONNXæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return self.generate_text_pytorch(prompt, max_new_tokens)
    
    def run_benchmark(self, num_inferences: int = 30) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸš€ é«˜å“è³ªæ—¥æœ¬èªNPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"ğŸ¯ æ¨è«–å›æ•°: {num_inferences}")
        print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        
        self.start_npu_monitoring()
        
        start_time = time.time()
        successful_inferences = 0
        total_inference_time = 0
        
        test_prompts = [
            "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦",
            "æ—¥æœ¬ã®æ–‡åŒ–ã¯",
            "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ã«ã‚ˆã‚Š",
            "ç’°å¢ƒå•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«",
            "æ•™è‚²ã®é‡è¦æ€§ã¯"
        ]
        
        for i in range(num_inferences):
            try:
                prompt = test_prompts[i % len(test_prompts)]
                
                inference_start = time.time()
                
                if self.onnx_session:
                    result = self.generate_text_onnx(prompt, max_new_tokens=20)
                else:
                    result = self.generate_text_pytorch(prompt, max_new_tokens=20)
                
                inference_time = time.time() - inference_start
                total_inference_time += inference_time
                successful_inferences += 1
                
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š é€²æ—: {i + 1}/{num_inferences}")
                
            except Exception as e:
                print(f"âŒ æ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        total_time = time.time() - start_time
        self.stop_npu_monitoring()
        
        # çµ±è¨ˆè¨ˆç®—
        throughput = successful_inferences / total_time if total_time > 0 else 0
        avg_inference_time = total_inference_time / successful_inferences if successful_inferences > 0 else 0
        success_rate = (successful_inferences / num_inferences) * 100
        
        # NPUçµ±è¨ˆ
        npu_stats = self.get_npu_stats()
        
        # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        results = {
            "successful_inferences": successful_inferences,
            "total_inferences": num_inferences,
            "success_rate": success_rate,
            "total_time": total_time,
            "throughput": throughput,
            "avg_inference_time": avg_inference_time,
            "max_npu_usage": npu_stats["max_usage"],
            "avg_npu_usage": npu_stats["avg_usage"],
            "npu_active_rate": npu_stats["active_rate"],
            "cpu_usage": cpu_usage,
            "memory_usage": memory_usage,
            "provider": self.onnx_session.get_providers()[0] if self.onnx_session else "PyTorch"
        }
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ“Š é«˜å“è³ªæ—¥æœ¬èªNPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {successful_inferences}/{num_inferences}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['provider']}")
        print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
        print(f"  ğŸ“Š å¹³å‡NPUä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
        print(f"  ğŸ¯ NPUå‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {cpu_usage:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_usage:.1f}%")
        print("="*60)
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–é«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {'ONNX' if self.onnx_session else 'PyTorch'}")
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†ã€'stats'ã§NPUçµ±è¨ˆè¡¨ç¤º")
        print("="*60)
        
        self.start_npu_monitoring()
        
        try:
            while True:
                prompt = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if prompt.lower() == 'stats':
                    npu_stats = self.get_npu_stats()
                    print(f"\nğŸ“Š NPUçµ±è¨ˆ:")
                    print(f"  ğŸ”¥ æœ€å¤§ä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
                    print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
                    print(f"  ğŸ¯ å‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
                    continue
                
                if not prompt:
                    continue
                
                print(f"ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
                
                start_time = time.time()
                
                if self.onnx_session:
                    result = self.generate_text_onnx(prompt, max_new_tokens=100)
                else:
                    result = self.generate_text_pytorch(prompt, max_new_tokens=100)
                
                generation_time = time.time() - start_time
                
                print("âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                print(result)
                print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
        finally:
            self.stop_npu_monitoring()
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if not self.download_model():
                return False
            
            # PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if not self.load_pytorch_model():
                return False
            
            # ONNXå¤‰æ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            self.convert_to_onnx()
            
            # ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            self.create_onnx_session()
            
            print("âœ… Ryzen AI NPUå¯¾å¿œé«˜å“è³ªæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
            print(f"ğŸ“Š å“è³ª: Perplexity {self.model_info['perplexity']} (é«˜å“è³ª)")
            print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {'ONNX' if self.onnx_session else 'PyTorch'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œé«˜å“è³ªæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument("--inferences", type=int, default=30, help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    parser.add_argument("--tokens", type=int, default=50, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = RyzenAIHighQualityJapaneseLLM(infer_os_enabled=args.infer_os)
    
    if not system.initialize():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if args.interactive:
        system.interactive_mode()
    elif args.benchmark:
        system.run_benchmark(args.inferences)
    elif args.prompt:
        print(f"ğŸ’¬ å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: '{args.prompt}'")
        system.start_npu_monitoring()
        
        start_time = time.time()
        if system.onnx_session:
            result = system.generate_text_onnx(args.prompt, args.tokens)
        else:
            result = system.generate_text_pytorch(args.prompt, args.tokens)
        generation_time = time.time() - start_time
        
        system.stop_npu_monitoring()
        
        print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
        print(result)
        print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
        
        npu_stats = system.get_npu_stats()
        print(f"ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
    elif args.compare:
        print("ğŸ”„ infer-OS ON/OFFæ¯”è¼ƒå®Ÿè¡Œ")
        
        # OFFç‰ˆ
        print("\nğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆinfer-OS OFFï¼‰:")
        system_off = RyzenAIHighQualityJapaneseLLM(infer_os_enabled=False)
        if system_off.initialize():
            results_off = system_off.run_benchmark(args.inferences)
        
        # ONç‰ˆ
        print("\nğŸ“Š æœ€é©åŒ–ç‰ˆï¼ˆinfer-OS ONï¼‰:")
        system_on = RyzenAIHighQualityJapaneseLLM(infer_os_enabled=True)
        if system_on.initialize():
            results_on = system_on.run_benchmark(args.inferences)
        
        # æ¯”è¼ƒçµæœ
        if 'results_off' in locals() and 'results_on' in locals():
            improvement = ((results_on['throughput'] - results_off['throughput']) / results_off['throughput']) * 100
            print(f"\nğŸ“Š infer-OSåŠ¹æœæ¸¬å®šçµæœ:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ğŸ“ˆ æ”¹å–„ç‡: {improvement:+.1f}%")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        system.run_benchmark(args.inferences)

if __name__ == "__main__":
    main()

