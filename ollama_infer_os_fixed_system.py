#!/usr/bin/env python3
"""
Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåˆ¶ç´„å•é¡Œä¿®æ­£ç‰ˆï¼‰
åˆ†æçµæœã«åŸºã¥ãå•é¡Œä¿®æ­£:
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç°¡ç´ åŒ–
- Ollama APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç«¶åˆè§£æ±º
- ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å¼·åŒ–
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šæ”¹å–„
"""

import os
import sys
import time
import json
import argparse
import threading
import requests
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import psutil
    import onnxruntime as ort
    import numpy as np
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install psutil onnxruntime requests")
    sys.exit(1)

class OllamaInferOSFixedController:
    """Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåˆ¶ç´„å•é¡Œä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, ollama_host: str = "http://localhost:11434"):
        self.ollama_host = ollama_host
        self.ollama_api = f"{ollama_host}/api"
        
        # infer-OSæœ€é©åŒ–åˆ¶å¾¡
        self.infer_os_enabled = True
        self.infer_os_config = {
            "npu_optimization": True,
            "memory_optimization": True,
            "cpu_optimization": True,
            "gpu_acceleration": True,
            "quantization": True,
            "parallel_processing": True,
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.available_models = []
        self.current_model = None
        self.npu_monitoring = False
        self.npu_stats = {"usage_changes": 0, "max_usage": 0.0, "avg_usage": 0.0}
        self.onnx_session = None
        
        # ä¿®æ­£: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates = {
            "conversation": "{prompt}",  # ä¿®æ­£: è¤‡é›‘ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‰Šé™¤
            "instruction": "æŒ‡ç¤º: {prompt}\n\nå›ç­”:",
            "reasoning": "å•é¡Œ: {prompt}\n\nè§£ç­”:",
            "creative": "ãƒ†ãƒ¼ãƒ: {prompt}\n\nå†…å®¹:",
            "simple": "{prompt}"
        }
        
        self.current_template = "simple"  # ä¿®æ­£: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’simpleã«å¤‰æ›´
        
        print("ğŸš€ Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåˆ¶ç´„å•é¡Œä¿®æ­£ç‰ˆï¼‰åˆæœŸåŒ–")
        print(f"ğŸ”— Ollamaæ¥ç¶šå…ˆ: {ollama_host}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ¯ è¨­è¨ˆæ–¹é‡: åˆ¶ç´„å•é¡Œä¿®æ­£ + ã‚·ãƒ³ãƒ—ãƒ«åŒ– + å®‰å®šæ€§é‡è¦–")
        print(f"ğŸ”§ ä¿®æ­£å†…å®¹: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç°¡ç´ åŒ– + APIæœ€é©åŒ– + ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¿®æ­£")
    
    def check_ollama_connection(self) -> bool:
        """Ollamaæ¥ç¶šç¢ºèªï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸ” Ollamaæ¥ç¶šç¢ºèªä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            response = requests.get(f"{self.ollama_api}/tags", timeout=10)  # ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·
            
            if response.status_code == 200:
                print("âœ… Ollamaæ¥ç¶šæˆåŠŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
                return True
            else:
                print(f"âŒ Ollamaæ¥ç¶šå¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print("âŒ Ollamaæ¥ç¶šå¤±æ•—: æ¥ç¶šã‚¨ãƒ©ãƒ¼")
            print("ğŸ’¡ OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("   Windows: ollama serve")
            return False
        except Exception as e:
            print(f"âŒ Ollamaæ¥ç¶šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            response = requests.get(f"{self.ollama_api}/tags", timeout=15)  # ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·
            
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                
                self.available_models = []
                for model in models:
                    model_info = {
                        "name": model.get("name", "unknown"),
                        "size": model.get("size", 0),
                        "modified_at": model.get("modified_at", ""),
                        "digest": model.get("digest", ""),
                        "details": model.get("details", {})
                    }
                    self.available_models.append(model_info)
                
                print(f"âœ… {len(self.available_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ï¼ˆä¿®æ­£ç‰ˆï¼‰")
                for i, model in enumerate(self.available_models, 1):
                    size_gb = model["size"] / (1024**3) if model["size"] > 0 else 0
                    print(f"  {i}. {model['name']} ({size_gb:.1f}GB)")
                
                return self.available_models
            else:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def select_model(self, model_name: str = None) -> bool:
        """ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            if not self.available_models:
                self.get_available_models()
            
            if not self.available_models:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            # ãƒ¢ãƒ‡ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            if model_name is None:
                selected_model = self.available_models[0]
            else:
                # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åã§æ¤œç´¢
                selected_model = None
                for model in self.available_models:
                    if model_name in model["name"]:
                        selected_model = model
                        break
                
                if selected_model is None:
                    print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
            
            self.current_model = selected_model
            size_gb = selected_model["size"] / (1024**3) if selected_model["size"] > 0 else 0
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«é¸æŠå®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰: {selected_model['name']}")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {size_gb:.1f}GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def apply_infer_os_optimizations(self):
        """infer-OSæœ€é©åŒ–é©ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if not self.infer_os_enabled:
            print("âš ï¸ infer-OSæœ€é©åŒ–ã¯ç„¡åŠ¹ã§ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            return
        
        print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨ä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
        
        # ä¿®æ­£: ã‚ˆã‚Šå®‰å…¨ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if self.infer_os_config["memory_optimization"]:
            print("ğŸ”§ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: æœ‰åŠ¹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'  # 1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿
            os.environ['OLLAMA_NUM_PARALLEL'] = '1'  # ä¸¦åˆ—å‡¦ç†åˆ¶é™
            os.environ['OLLAMA_LOAD_TIMEOUT'] = '60'  # ä¿®æ­£: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆçŸ­ç¸®
        
        # ä¿®æ­£: ã‚ˆã‚Šå®‰å…¨ãªCPUæœ€é©åŒ–
        if self.infer_os_config["cpu_optimization"]:
            print("ğŸ”§ CPUæœ€é©åŒ–: æœ‰åŠ¹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            cpu_count = os.cpu_count()
            os.environ['OLLAMA_NUM_THREADS'] = str(min(2, cpu_count))  # ä¿®æ­£: ã‚ˆã‚Šä¿å®ˆçš„
        
        # ä¿®æ­£: ã‚ˆã‚Šå®‰å…¨ãªGPUè¨­å®š
        if self.infer_os_config["gpu_acceleration"]:
            print("ğŸ”§ GPUåŠ é€Ÿ: æœ‰åŠ¹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            os.environ['OLLAMA_GPU_LAYERS'] = '20'  # ä¿®æ­£: ã‚ˆã‚Šä¿å®ˆçš„ãªå€¤
        
        # ä¿®æ­£: NPUæœ€é©åŒ–ã‚’å˜ç´”åŒ–
        if self.infer_os_config["npu_optimization"]:
            print("ğŸ”§ NPUæœ€é©åŒ–: æœ‰åŠ¹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            # ä¿®æ­£: å˜ä¸€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã¿è¨­å®š
            os.environ['ONNXRUNTIME_PROVIDERS'] = 'DmlExecutionProvider,CPUExecutionProvider'
        
        print("âœ… infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        
        # è¨­å®šç¢ºèª
        print("ğŸ“Š é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–è¨­å®šï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        for key, value in self.infer_os_config.items():
            status = "âœ…" if value else "âŒ"
            print(f"  {status} {key}: {value}")
    
    def create_safe_onnx_session(self) -> bool:
        """å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸ”§ å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            
            # è»½é‡ãªãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            os.makedirs("models", exist_ok=True)
            
            import torch
            import torch.nn as nn
            
            class SimpleFixedModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear = nn.Linear(256, 512)  # ä¿®æ­£: ã‚ˆã‚Šè»½é‡
                    
                def forward(self, x):
                    return self.linear(x)
            
            model = SimpleFixedModel()
            model.eval()
            
            dummy_input = torch.randn(1, 256)  # ä¿®æ­£: ã‚ˆã‚Šè»½é‡
            onnx_path = "models/ollama_fixed_npu_model.onnx"
            
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output']
            )
            
            print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰: {onnx_path}")
            
            # ä¿®æ­£: å˜ä¸€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æˆ¦ç•¥
            providers = []
            available_providers = ort.get_available_providers()
            
            # ä¿®æ­£: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç«¶åˆå›é¿
            if 'DmlExecutionProvider' in available_providers:
                providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
                print("ğŸ¯ DmlExecutionProviderä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            elif 'VitisAIExecutionProvider' in available_providers:
                providers = ['VitisAIExecutionProvider', 'CPUExecutionProvider']
                print("ğŸ¯ VitisAIExecutionProviderä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            else:
                providers = ['CPUExecutionProvider']
                print("ğŸ¯ CPUExecutionProviderä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3
            session_options.enable_cpu_mem_arena = True  # ä¿®æ­£: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            
            self.onnx_session = ort.InferenceSession(
                onnx_path,
                sess_options=session_options,
                providers=providers
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")
            return False
    
    def test_npu_operation(self) -> bool:
        """NPUå‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.onnx_session is None:
            print("âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            print("ğŸ”§ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            
            # ãƒ†ã‚¹ãƒˆå…¥åŠ›ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
            test_input = np.random.randn(1, 256).astype(np.float32)  # ä¿®æ­£: ã‚ˆã‚Šè»½é‡
            
            # æ¨è«–å®Ÿè¡Œ
            outputs = self.onnx_session.run(None, {"input": test_input})
            
            print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸï¼ˆä¿®æ­£ç‰ˆï¼‰: å‡ºåŠ›å½¢çŠ¶ {outputs[0].shape}")
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_provider = self.onnx_session.get_providers()[0]
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            return True
            
        except Exception as e:
            print(f"âŒ NPUå‹•ä½œãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰- ä¿®æ­£ç‰ˆ")
            
            prev_usage = 0.0
            usage_history = []
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    try:
                        import torch
                        if torch.cuda.is_available():
                            gpu_usage = torch.cuda.utilization()
                        else:
                            gpu_usage = 0.0
                    except:
                        gpu_usage = 0.0
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡º
                    if abs(gpu_usage - prev_usage) > 2.0:  # 2%ä»¥ä¸Šã®å¤‰åŒ–
                        print(f"ğŸ”¥ NPU/GPUä½¿ç”¨ç‡å¤‰åŒ–: {prev_usage:.1f}% â†’ {gpu_usage:.1f}% (ä¿®æ­£ç‰ˆ)")
                        self.npu_stats["usage_changes"] += 1
                    
                    # çµ±è¨ˆæ›´æ–°
                    usage_history.append(gpu_usage)
                    if len(usage_history) > 60:  # ç›´è¿‘60ç§’ã®ã¿ä¿æŒ
                        usage_history.pop(0)
                    
                    self.npu_stats["max_usage"] = max(self.npu_stats["max_usage"], gpu_usage)
                    self.npu_stats["avg_usage"] = sum(usage_history) / len(usage_history)
                    
                    prev_usage = gpu_usage
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"âš ï¸ NPUç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(1)
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç›£è¦–é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    
    def generate_text_with_ollama_fixed(self, prompt: str, max_tokens: int = 100, template: str = None) -> str:
        """Ollamaã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.current_model is None:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ä¿®æ­£: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†
            if template and template in self.templates:
                formatted_prompt = self.templates[template].format(prompt=prompt)
            else:
                formatted_prompt = prompt  # ä¿®æ­£: ç›´æ¥ä½¿ç”¨ã‚’å„ªå…ˆ
            
            print(f"ğŸ’¬ Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰: '{formatted_prompt[:30]}...'")
            print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.current_model['name']}")
            print(f"ğŸ¯ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
            
            # ä¿®æ­£: æœ€é©åŒ–ã•ã‚ŒãŸOllama APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            payload = {
                "model": self.current_model["name"],
                "prompt": formatted_prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.8,  # ä¿®æ­£: ã‚ˆã‚Šé«˜ã„å‰µé€ æ€§
                    "top_p": 0.95,  # ä¿®æ­£: ã‚ˆã‚Šå¤šæ§˜ãªå‡ºåŠ›
                    "top_k": 40,  # ä¿®æ­£: ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
                    "repeat_penalty": 1.05,  # ä¿®æ­£: è»½ã„ç¹°ã‚Šè¿”ã—é˜²æ­¢
                    "stop": ["\n\n", "äººé–“:", "Human:", "Assistant:"],  # ä¿®æ­£: åœæ­¢æ¡ä»¶è¿½åŠ 
                }
            }
            
            print("ğŸ”§ Ollama APIå‘¼ã³å‡ºã—ä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            start_time = time.time()
            
            # ä¿®æ­£: ã‚ˆã‚ŠçŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            response = requests.post(
                f"{self.ollama_api}/generate",
                json=payload,
                timeout=30  # ä¿®æ­£: 30ç§’ã«çŸ­ç¸®
            )
            
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("response", "").strip()
                
                print(f"âœ… Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
                print(f"ğŸ“ ç”Ÿæˆæ–‡å­—æ•°: {len(generated_text)}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
                
                # ä¿®æ­£: ã‚ˆã‚Šç·©ã„å“è³ªãƒã‚§ãƒƒã‚¯
                if len(generated_text) < 5:
                    print("âš ï¸ ç”ŸæˆçµæœãŒçŸ­ã™ãã¾ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
                    return self.generate_fallback_text_fixed(prompt)
                
                return generated_text
            else:
                print(f"âŒ Ollama APIå‘¼ã³å‡ºã—å¤±æ•—ï¼ˆä¿®æ­£ç‰ˆï¼‰: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                print(f"ğŸ“ ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text[:200]}...")
                return self.generate_fallback_text_fixed(prompt)
                
        except requests.exceptions.Timeout:
            print("âŒ Ollama APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ30ç§’ï¼‰ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            return self.generate_fallback_text_fixed(prompt)
        except Exception as e:
            print(f"âŒ Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")
            return self.generate_fallback_text_fixed(prompt)
    
    def generate_fallback_text_fixed(self, prompt: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        # ä¿®æ­£: ã‚ˆã‚Šæœ‰ç”¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        fallback_responses = {
            "äººå·¥çŸ¥èƒ½": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãªã©ã®æŠ€è¡“ã‚’ç”¨ã„ã¦ã€äººé–“ã®ã‚ˆã†ãªçŸ¥çš„ãªå‡¦ç†ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚ç¾åœ¨ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ãŒé€²ã‚“ã§ãŠã‚Šã€ä»Šå¾Œã•ã‚‰ãªã‚‹ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "é‡å­": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸé©æ–°çš„ãªè¨ˆç®—æŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§ã¯å›°é›£ãªå•é¡Œã‚’é«˜é€Ÿã§è§£æ±ºã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€æš—å·è§£èª­ã‚„è–¬ç‰©é–‹ç™ºãªã©ã®åˆ†é‡ã§ã®å¿œç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "äººå‚": "äººå‚ï¼ˆã«ã‚“ã˜ã‚“ï¼‰ã¯ã€ã‚»ãƒªç§‘ã®é‡èœã§ã€Î²-ã‚«ãƒ­ãƒ†ãƒ³ã‚’è±Šå¯Œã«å«ã‚€æ „é¤Šä¾¡ã®é«˜ã„é£Ÿæã§ã™ã€‚ç”Ÿé£Ÿã€ç…®ç‰©ã€ç‚’ã‚ç‰©ãªã©æ§˜ã€…ãªèª¿ç†æ³•ã§æ¥½ã—ã‚ã€ç”˜ã¿ãŒã‚ã£ã¦å­ä¾›ã«ã‚‚äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãƒ†ã‚¹ãƒˆ": "ãƒ†ã‚¹ãƒˆã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã‚„çŸ¥è­˜ã®å‹•ä½œç¢ºèªã‚„è©•ä¾¡ã‚’è¡Œã†é‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚é©åˆ‡ãªãƒ†ã‚¹ãƒˆã«ã‚ˆã‚Šã€å“è³ªã®å‘ä¸Šã‚„å•é¡Œã®æ—©æœŸç™ºè¦‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
            "default": f"ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã€åŸºæœ¬çš„ãªæƒ…å ±ã‚’ãŠä¼ãˆã—ã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚"
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for keyword, response in fallback_responses.items():
            if keyword in prompt and keyword != "default":
                return response
        
        return fallback_responses["default"]
    
    def toggle_infer_os_optimization(self, enabled: bool = None) -> bool:
        """infer-OSæœ€é©åŒ–ã®ON/OFFåˆ‡ã‚Šæ›¿ãˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if enabled is None:
            self.infer_os_enabled = not self.infer_os_enabled
        else:
            self.infer_os_enabled = enabled
        
        status = "æœ‰åŠ¹" if self.infer_os_enabled else "ç„¡åŠ¹"
        print(f"ğŸ”„ infer-OSæœ€é©åŒ–ã‚’{status}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
        
        if self.infer_os_enabled:
            self.apply_infer_os_optimizations()
        else:
            print("âš ï¸ infer-OSæœ€é©åŒ–ãŒç„¡åŠ¹ã«ãªã‚Šã¾ã—ãŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
            # æœ€é©åŒ–è¨­å®šã‚’ãƒªã‚»ãƒƒãƒˆ
            for key in self.infer_os_config:
                self.infer_os_config[key] = False
        
        return self.infer_os_enabled
    
    def show_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("\nğŸ“Š Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        print(f"  ğŸ”— Ollamaæ¥ç¶š: {'âœ…' if self.check_ollama_connection() else 'âŒ'}")
        print(f"  ğŸ¯ é¸æŠãƒ¢ãƒ‡ãƒ«: {self.current_model['name'] if self.current_model else 'ãªã—'}")
        print(f"  âš¡ infer-OSæœ€é©åŒ–: {'âœ… æœ‰åŠ¹' if self.infer_os_enabled else 'âŒ ç„¡åŠ¹'}")
        print(f"  ğŸ”§ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³: {'âœ…' if self.onnx_session else 'âŒ'}")
        print(f"  ğŸ“Š NPUç›£è¦–: {'âœ… å®Ÿè¡Œä¸­' if self.npu_monitoring else 'âŒ åœæ­¢ä¸­'}")
        
        if self.infer_os_enabled:
            print("  ğŸ“‹ infer-OSæœ€é©åŒ–è¨­å®šï¼ˆä¿®æ­£ç‰ˆï¼‰:")
            for key, value in self.infer_os_config.items():
                status = "âœ…" if value else "âŒ"
                print(f"    {status} {key}: {value}")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        print(f"  ğŸ’» CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent:.1f}%")
    
    def show_npu_stats(self):
        """NPUçµ±è¨ˆè¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("\nğŸ“Š NPU/GPUä½¿ç”¨ç‡çµ±è¨ˆï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        print(f"  ğŸ”¥ ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡ºå›æ•°: {self.npu_stats['usage_changes']}")
        print(f"  ğŸ“ˆ æœ€å¤§ä½¿ç”¨ç‡: {self.npu_stats['max_usage']:.1f}%")
        print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {self.npu_stats['avg_usage']:.1f}%")
        
        if self.onnx_session:
            active_provider = self.onnx_session.get_providers()[0]
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        print(f"  ğŸ’» CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent:.1f}%")
        print(f"  âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
    
    def change_template(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        for i, (name, template) in enumerate(self.templates.items(), 1):
            print(f"  {i}. {name} - {template[:30]}...")
        
        try:
            choice = input("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
            template_names = list(self.templates.keys())
            
            if choice.isdigit() and 1 <= int(choice) <= len(template_names):
                self.current_template = template_names[int(choice) - 1]
                print(f"âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ '{self.current_template}' ã«å¤‰æ›´ã—ã¾ã—ãŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        except Exception as e:
            print(f"âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´ã‚¨ãƒ©ãƒ¼: {e}")
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸš€ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            
            # Ollamaæ¥ç¶šç¢ºèª
            if not self.check_ollama_connection():
                return False
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«å–å¾—
            models = self.get_available_models()
            if not models:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                print("ğŸ’¡ Ollamaã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
                print("   ä¾‹: ollama pull llama2")
                return False
            
            # æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
            if not self.select_model():
                return False
            
            # infer-OSæœ€é©åŒ–é©ç”¨
            self.apply_infer_os_optimizations()
            
            # å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            onnx_created = self.create_safe_onnx_session()
            if not onnx_created:
                print("âš ï¸ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            
            # NPUå‹•ä½œãƒ†ã‚¹ãƒˆ
            if self.onnx_session:
                npu_test = self.test_npu_operation()
                if not npu_test:
                    print("âš ï¸ NPUå‹•ä½œãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            
            print("âœ… Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            self.show_system_status()
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")
            return False
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("\nğŸ¯ Ollama + infer-OSåˆ¶å¾¡ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.current_model['name'] if self.current_model else 'ãªã—'}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        if self.onnx_session:
            active_provider = self.onnx_session.get_providers()[0]
            print(f"ğŸ”§ NPUãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
        
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰ï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        print("  'quit' - çµ‚äº†")
        print("  'stats' - NPUçµ±è¨ˆè¡¨ç¤º")
        print("  'status' - ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º")
        print("  'template' - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´")
        print("  'model' - ãƒ¢ãƒ‡ãƒ«å¤‰æ›´")
        print("  'toggle' - infer-OSæœ€é©åŒ–ON/OFFåˆ‡ã‚Šæ›¿ãˆ")
        print("  'on' - infer-OSæœ€é©åŒ–æœ‰åŠ¹")
        print("  'off' - infer-OSæœ€é©åŒ–ç„¡åŠ¹")
        print("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: conversation, instruction, reasoning, creative, simple")
        print("=" * 70)
        
        # NPUç›£è¦–é–‹å§‹
        self.start_npu_monitoring()
        
        try:
            while True:
                try:
                    infer_os_status = "ON" if self.infer_os_enabled else "OFF"
                    prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [infer-OS:{infer_os_status}] [{self.current_template}]: ").strip()
                    
                    if not prompt:
                        continue
                    
                    if prompt.lower() == 'quit':
                        print("ğŸ‘‹ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
                        break
                    
                    if prompt.lower() == 'stats':
                        self.show_npu_stats()
                        continue
                    
                    if prompt.lower() == 'status':
                        self.show_system_status()
                        continue
                    
                    if prompt.lower() == 'template':
                        self.change_template()
                        continue
                    
                    if prompt.lower() == 'model':
                        self.select_model_interactive()
                        continue
                    
                    if prompt.lower() == 'toggle':
                        self.toggle_infer_os_optimization()
                        continue
                    
                    if prompt.lower() == 'on':
                        self.toggle_infer_os_optimization(True)
                        continue
                    
                    if prompt.lower() == 'off':
                        self.toggle_infer_os_optimization(False)
                        continue
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰
                    start_time = time.time()
                    result = self.generate_text_with_ollama_fixed(prompt, max_tokens=100)
                    end_time = time.time()
                    
                    print(f"\nğŸ¯ Ollama + infer-OSç”Ÿæˆçµæœï¼ˆä¿®æ­£ç‰ˆï¼‰:")
                    print(result)
                    print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
                    print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
                    
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
                    break
                except Exception as e:
                    print(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")
                    continue
        
        finally:
            self.stop_npu_monitoring()
    
    def select_model_interactive(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰:")
            for i, model in enumerate(self.available_models, 1):
                size_gb = model["size"] / (1024**3) if model["size"] > 0 else 0
                print(f"  {i}. {model['name']} ({size_gb:.1f}GB)")
            
            choice = input("ãƒ¢ãƒ‡ãƒ«ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
            
            if choice.isdigit() and 1 <= int(choice) <= len(self.available_models):
                selected_model = self.available_models[int(choice) - 1]
                self.current_model = selected_model
                print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ '{selected_model['name']}' ã«å¤‰æ›´ã—ã¾ã—ãŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    parser = argparse.ArgumentParser(description="Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆåˆ¶ç´„å•é¡Œä¿®æ­£ç‰ˆï¼‰")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=100, help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="simple", help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")  # ä¿®æ­£: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’simpleã«
    parser.add_argument("--model", type=str, help="ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--ollama-host", type=str, default="http://localhost:11434", help="Ollamaæ¥ç¶šå…ˆ")
    parser.add_argument("--infer-os", action="store_true", default=True, help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--no-infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ç„¡åŠ¹")
    
    args = parser.parse_args()
    
    # infer-OSæœ€é©åŒ–è¨­å®š
    infer_os_enabled = args.infer_os and not args.no_infer_os
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆä¿®æ­£ç‰ˆï¼‰
    system = OllamaInferOSFixedController(ollama_host=args.ollama_host)
    system.infer_os_enabled = infer_os_enabled
    
    if not system.initialize_system():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
        sys.exit(1)
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    if args.model:
        if not system.select_model(args.model):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{args.model}' ã®é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆä¿®æ­£ç‰ˆï¼‰")
            sys.exit(1)
    
    try:
        if args.interactive:
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
            system.run_interactive_mode()
        elif args.prompt:
            # å˜ç™ºç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
            system.start_npu_monitoring()
            result = system.generate_text_with_ollama_fixed(args.prompt, args.tokens, args.template)
            print(f"\nğŸ¯ ç”Ÿæˆçµæœï¼ˆä¿®æ­£ç‰ˆï¼‰:\n{result}")
            system.stop_npu_monitoring()
            system.show_npu_stats()
        else:
            print("ä½¿ç”¨æ–¹æ³•ï¼ˆä¿®æ­£ç‰ˆï¼‰: --interactive ã¾ãŸã¯ --prompt ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("ä¾‹: python ollama_infer_os_fixed_system.py --interactive")
            print("ä¾‹: python ollama_infer_os_fixed_system.py --prompt 'äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„' --tokens 200")
            print("ä¾‹: python ollama_infer_os_fixed_system.py --interactive --no-infer-os")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™ï¼ˆä¿®æ­£ç‰ˆï¼‰")
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£ç‰ˆï¼‰: {e}")
    finally:
        if system.npu_monitoring:
            system.stop_npu_monitoring()

if __name__ == "__main__":
    main()

