# -*- coding: utf-8 -*-
"""
ğŸš€ NPU Decodeçµ±åˆã‚·ã‚¹ãƒ†ãƒ  (v1.0)

ä»•æ§˜æ›¸ã«åŸºã¥ãã€ŒDecodeã®ã¿NPUã€çµ±åˆå®Ÿè£…
- æ—¢å­˜ã®PyTorchãƒ¢ãƒ‡ãƒ«ã¨NPU Runtimeã®çµ±åˆ
- æ®µéšçš„NPUç§»è¡Œï¼ˆPhase 1: RMSNormã®ã¿ï¼‰
- ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
"""

import os
import sys
import time
import numpy as np
import torch
from typing import Dict, List, Optional, Tuple, Any, Union
import traceback

# NPU Runtime API
from npu_runtime_api import (
    NPURuntime, NPUStatus, NPUQuantType, 
    NPUModelDesc, NPUQuantProfile, NPUDecodeArgs
)

class NPUDecodeIntegrator:
    """NPU Decodeçµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, pytorch_model, tokenizer):
        """NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰åˆæœŸåŒ–"""
        self.pytorch_model = pytorch_model
        self.model = pytorch_model  # NPUãƒ‡ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã™ã‚‹ãŸã‚
        self.tokenizer = tokenizer
        self.npu_runtime = None
        self.npu_graph = None
        self.npu_available = False
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_tokens": 0,
            "npu_tokens": 0,
            "cpu_tokens": 0,
            "npu_time": 0.0,
            "cpu_time": 0.0,
            "npu_errors": 0,
            "fallback_count": 0
        }
        
    def initialize_npu(self) -> bool:
        """NPUåˆæœŸåŒ–"""
        try:
            print("ğŸš€ NPU Decodeçµ±åˆåˆæœŸåŒ–é–‹å§‹...")
            
            # NPU RuntimeåˆæœŸåŒ–
            self.npu_runtime = NPURuntime()
            status = self.npu_runtime.init()
            
            if status != NPUStatus.NPU_OK:
                print("âš ï¸ NPU RuntimeåˆæœŸåŒ–å¤±æ•—ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return False
            
            # ãƒ¢ãƒ‡ãƒ«è¨˜è¿°å­ä½œæˆï¼ˆPyTorchãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¨å®šï¼‰
            model_desc = self._create_model_desc()
            
            # é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            quant_profile = NPUQuantProfile(
                weights=NPUQuantType.NPU_QUANT_W8A8,
                kv_level_near=64,
                kv_level_mid=1024,
                kv_block=32
            )
            
            # NPUã‚°ãƒ©ãƒ•æ§‹ç¯‰
            status, graph = self.npu_runtime.build_graph(model_desc, quant_profile)
            if status != NPUStatus.NPU_OK or graph is None:
                print("âš ï¸ NPUã‚°ãƒ©ãƒ•æ§‹ç¯‰å¤±æ•—ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return False
            
            self.npu_graph = graph
            self.npu_available = True
            
            print("âœ… NPU Decodeçµ±åˆåˆæœŸåŒ–å®Œäº†")
            print(f"  ğŸ“Š NPUå¯¾å¿œãƒ¬ã‚¤ãƒ¤ãƒ¼: {list(graph.sessions.keys())}")
            return True
            
        except Exception as e:
            print(f"âŒ NPUåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _create_model_desc(self) -> NPUModelDesc:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã‹ã‚‰NPUãƒ¢ãƒ‡ãƒ«è¨˜è¿°å­ã‚’ä½œæˆ"""
        try:
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            config = getattr(self.pytorch_model, 'config', None)
            
            if config:
                return NPUModelDesc(
                    max_ctx=getattr(config, 'max_position_embeddings', 8192),
                    heads=getattr(config, 'num_attention_heads', 32),
                    head_dim=getattr(config, 'hidden_size', 4096) // getattr(config, 'num_attention_heads', 32),
                    layers=getattr(config, 'num_hidden_layers', 32),
                    gqa_group=getattr(config, 'num_key_value_heads', 32) // getattr(config, 'num_attention_heads', 32) if hasattr(config, 'num_key_value_heads') else 1,
                    vocab_size=getattr(config, 'vocab_size', 32000),
                    hidden_dim=getattr(config, 'hidden_size', 4096)
                )
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆ7Bãƒ¢ãƒ‡ãƒ«æƒ³å®šï¼‰
                return NPUModelDesc(
                    max_ctx=8192,
                    heads=32,
                    head_dim=128,
                    layers=32,
                    gqa_group=1,
                    vocab_size=32000,
                    hidden_dim=4096
                )
                
        except Exception as e:
            print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«è¨˜è¿°å­ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return NPUModelDesc()
    
    def generate_with_npu_decode(
        self, 
        input_text: str, 
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        do_sample: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ"""
        try:
            start_time = time.time()
            
            print("ğŸ¯ NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ç”Ÿæˆé–‹å§‹")
            print(f"  ğŸ“ å…¥åŠ›: {input_text[:50]}...")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(input_text, return_tensors="pt")
            input_ids = inputs["input_ids"]
            attention_mask = inputs.get("attention_mask", None)
            
            print(f"  ğŸ“Š å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_ids.shape[1]}")
            
            # ç”Ÿæˆãƒ«ãƒ¼ãƒ—
            generated_tokens = []
            current_ids = input_ids
            
            for step in range(max_new_tokens):
                # NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
                next_token_logits = self._decode_step_with_npu(
                    current_ids, 
                    attention_mask,
                    step
                )
                
                if next_token_logits is None:
                    print("âŒ ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—")
                    break
                
                # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
                if do_sample and temperature > 0:
                    # æ¸©åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    probs = torch.softmax(next_token_logits / temperature, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    # Greedyé¸æŠ
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                generated_tokens.append(next_token.item())
                
                # å…¥åŠ›æ›´æ–°
                current_ids = torch.cat([current_ids, next_token], dim=1)
                
                # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if next_token.item() == self.tokenizer.eos_token_id:
                    # EOSæ¤œå‡ºæ™‚ã‚‚æœ€å°é™ã®ç”Ÿæˆã‚’ä¿è¨¼
                    if step < 5:  # æœ€åˆã®5ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯EOSã‚’ç„¡è¦–
                        print(f"  âš ï¸ æ—©æœŸEOSæ¤œå‡º (step {step})ã€ç”Ÿæˆç¶™ç¶š")
                        continue
                    else:
                        print(f"  ğŸ EOSæ¤œå‡ºã€ç”Ÿæˆçµ‚äº† (step {step})")
                        break
                
                # é€²æ—è¡¨ç¤º
                if (step + 1) % 10 == 0:
                    print(f"  ğŸ“Š ç”Ÿæˆé€²æ—: {step + 1}/{max_new_tokens} tokens")
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            full_text = input_text + generated_text
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            generation_time = time.time() - start_time
            tokens_generated = len(generated_tokens)
            
            self.stats["total_tokens"] += tokens_generated
            
            # çµæœè¿”å´
            result = {
                "generated_text": full_text,
                "new_text": generated_text,
                "input_tokens": input_ids.shape[1],
                "output_tokens": tokens_generated,
                "generation_time": generation_time,
                "tokens_per_sec": tokens_generated / generation_time if generation_time > 0 else 0,
                "npu_utilization": self._calculate_npu_utilization(),
                "stats": self.stats.copy()
            }
            
            print(f"âœ… NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†")
            print(f"  ğŸ“Š ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {tokens_generated}")
            print(f"  â±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’")
            print(f"  ğŸš€ é€Ÿåº¦: {result['tokens_per_sec']:.1f} tokens/sec")
            print(f"  âš¡ NPUåˆ©ç”¨ç‡: {result['npu_utilization']:.1f}%")
            
            return result
            
        except Exception as e:
            print(f"âŒ NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": str(e)}
    
    def _decode_step_with_npu(
        self, 
        input_ids: torch.Tensor, 
        attention_mask: Optional[torch.Tensor],
        step: int
    ) -> Optional[torch.Tensor]:
        """NPUçµ±åˆãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ†ãƒƒãƒ—"""
        try:
            step_start = time.time()
            
            # Phase 1: éƒ¨åˆ†çš„NPUå®Ÿè¡Œ
            if self.npu_available and self.npu_graph:
                try:
                    # NPUãƒ‡ã‚³ãƒ¼ãƒ‰è©¦è¡Œ
                    decode_args = NPUDecodeArgs(
                        kv_handle=None,
                        t_new=1,
                        ctx_len=input_ids.shape[1]
                    )
                    
                    # PyTorchãƒ¢ãƒ‡ãƒ«ã¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
                    status, npu_logits = self.npu_runtime.decode(
                        self.npu_graph, 
                        decode_args,
                        pytorch_model=self.model,  # å®Ÿéš›ã®PyTorchãƒ¢ãƒ‡ãƒ«
                        input_ids=input_ids,       # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³
                        attention_mask=attention_mask  # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯
                    )
                    
                    if status == NPUStatus.NPU_OK and npu_logits is not None:
                        # NPUæˆåŠŸ
                        npu_time = time.time() - step_start
                        self.stats["npu_tokens"] += 1
                        self.stats["npu_time"] += npu_time
                        
                        # NumPy -> PyTorchå¤‰æ›
                        logits_tensor = torch.from_numpy(npu_logits)
                        
                        if step % 20 == 0:  # å®šæœŸçš„ã«è¡¨ç¤º
                            print(f"    âš¡ NPUå®Ÿè¡ŒæˆåŠŸ (step {step}): {npu_time*1000:.1f}ms")
                        
                        return logits_tensor
                    else:
                        # NPUå¤±æ•—ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        self.stats["fallback_count"] += 1
                        if step % 20 == 0:
                            print(f"    âš ï¸ NPUå®Ÿè¡Œå¤±æ•—ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (step {step})")
                        
                except Exception as e:
                    # NPUã‚¨ãƒ©ãƒ¼ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    self.stats["fallback_count"] += 1
                    if step % 20 == 0:
                        print(f"    âš ï¸ NPUã‚¨ãƒ©ãƒ¼ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (step {step}): {e}")
            
            # CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            cpu_start = time.time()
            
            with torch.no_grad():
                outputs = self.pytorch_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    use_cache=True
                )
                
                logits = outputs.logits[:, -1, :]  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®logits
                
                cpu_time = time.time() - cpu_start
                self.stats["cpu_tokens"] += 1
                self.stats["cpu_time"] += cpu_time
                
                if step % 20 == 0:  # å®šæœŸçš„ã«è¡¨ç¤º
                    print(f"    ğŸ–¥ï¸ CPUå®Ÿè¡Œ (step {step}): {cpu_time*1000:.1f}ms")
                
                return logits
                
        except Exception as e:
            print(f"âŒ ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ†ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _calculate_npu_utilization(self) -> float:
        """NPUåˆ©ç”¨ç‡è¨ˆç®—"""
        total_tokens = self.stats["npu_tokens"] + self.stats["cpu_tokens"]
        if total_tokens == 0:
            return 0.0
        return (self.stats["npu_tokens"] / total_tokens) * 100
    
    def get_performance_report(self) -> Dict[str, Any]:
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        total_tokens = self.stats["npu_tokens"] + self.stats["cpu_tokens"]
        total_time = self.stats["npu_time"] + self.stats["cpu_time"]
        
        report = {
            "total_tokens": total_tokens,
            "npu_tokens": self.stats["npu_tokens"],
            "cpu_tokens": self.stats["cpu_tokens"],
            "npu_utilization_percent": self._calculate_npu_utilization(),
            "total_time_sec": total_time,
            "average_tokens_per_sec": total_tokens / total_time if total_time > 0 else 0,
            "npu_average_time_ms": (self.stats["npu_time"] / self.stats["npu_tokens"] * 1000) if self.stats["npu_tokens"] > 0 else 0,
            "cpu_average_time_ms": (self.stats["cpu_time"] / self.stats["cpu_tokens"] * 1000) if self.stats["cpu_tokens"] > 0 else 0,
            "fallback_count": self.stats["fallback_count"],
            "fallback_rate_percent": (self.stats["fallback_count"] / total_tokens * 100) if total_tokens > 0 else 0
        }
        
        # NPU Runtimeæ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆ
        if self.npu_runtime:
            npu_report = self.npu_runtime.get_performance_report()
            report.update({f"npu_{k}": v for k, v in npu_report.items()})
        
        return report
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        try:
            print("ğŸ”„ NPU Decodeçµ±åˆãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ä¸­...")
            
            if self.npu_runtime:
                self.npu_runtime.teardown()
                self.npu_runtime = None
            
            self.npu_graph = None
            self.npu_available = False
            
            print("âœ… NPU Decodeçµ±åˆãƒªã‚½ãƒ¼ã‚¹è§£æ”¾å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ã‚¨ãƒ©ãƒ¼: {e}")

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
def test_npu_decode_integration():
    """NPU Decodeçµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª NPU Decodeçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå®Ÿéš›ã®ä½¿ç”¨ã§ã¯å®Ÿãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
    class DummyModel:
        def __init__(self):
            self.config = type('Config', (), {
                'max_position_embeddings': 8192,
                'num_attention_heads': 32,
                'hidden_size': 4096,
                'num_hidden_layers': 32,
                'vocab_size': 32000
            })()
        
        def __call__(self, input_ids, attention_mask=None, use_cache=False):
            # ãƒ€ãƒŸãƒ¼logits
            batch_size, seq_len = input_ids.shape
            vocab_size = 32000
            logits = torch.randn(batch_size, seq_len, vocab_size)
            return type('Output', (), {'logits': logits})()
    
    class DummyTokenizer:
        def __init__(self):
            self.eos_token_id = 2
        
        def __call__(self, text, return_tensors=None):
            # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            tokens = [1, 2, 3, 4, 5]  # ãƒ€ãƒŸãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³
            return {"input_ids": torch.tensor([tokens])}
        
        def decode(self, tokens, skip_special_tokens=False):
            return f"Generated text with {len(tokens)} tokens"
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    dummy_model = DummyModel()
    dummy_tokenizer = DummyTokenizer()
    
    integrator = NPUDecodeIntegrator(dummy_model, dummy_tokenizer, "test-model")
    
    # NPUåˆæœŸåŒ–
    npu_success = integrator.initialize_npu()
    print(f"NPUåˆæœŸåŒ–çµæœ: {'æˆåŠŸ' if npu_success else 'å¤±æ•—ï¼ˆCPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰'}")
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    result = integrator.generate_with_npu_decode(
        "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        max_new_tokens=20,
        temperature=0.7
    )
    
    if "error" not in result:
        print("âœ… ç”Ÿæˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
        print(f"  ğŸ“Š çµæœ: {result}")
    else:
        print(f"âŒ ç”Ÿæˆãƒ†ã‚¹ãƒˆå¤±æ•—: {result['error']}")
    
    # æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
    report = integrator.get_performance_report()
    print(f"ğŸ“Š æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ: {report}")
    
    # ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
    integrator.cleanup()
    print("âœ… NPU Decodeçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_npu_decode_integration()

