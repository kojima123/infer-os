#!/usr/bin/env python3
"""
NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«å®Ÿè£…
çœŸã®NPUæ´»ç”¨ã‚’å®Ÿç¾ã™ã‚‹æ—¥æœ¬èªLLMãƒ‡ãƒ¢

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
1. llama3-8b-amd-npu (8B) - NPUå®Œå…¨å¯¾å¿œæ¸ˆã¿
2. Llama-3.1-70B-Japanese-Instruct-2407 (70B) - ONNXå¤‰æ›ãƒãƒ£ãƒ¬ãƒ³ã‚¸
"""

import os
import sys
import time
import torch
import psutil
import logging
import argparse
from pathlib import Path
from typing import Optional, Dict, Any, List

# Transformersé–¢é€£
import transformers
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    set_seed
)

# ONNXé–¢é€£
try:
    import onnx
    import onnxruntime as ort
    from onnxruntime.quantization import quantize_dynamic, QuantType
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ ONNXé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# NPUé–¢é€£
try:
    import qlinear
    QLINEAR_AVAILABLE = True
except ImportError:
    QLINEAR_AVAILABLE = False
    print("âš ï¸ qlinearãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


class NPUOptimizedJapaneseModel:
    """NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str = "llama3-8b-amd-npu"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.npu_session = None
        self.model_type = self._detect_model_type()
        
        # NPUç’°å¢ƒè¨­å®š
        self._setup_npu_environment()
        
    def _detect_model_type(self) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º"""
        if "llama3-8b-amd-npu" in self.model_name:
            return "npu_optimized"
        elif "Llama-3.1-70B-Japanese" in self.model_name:
            return "large_japanese"
        elif "ALMA-Ja-V3-amd-npu" in self.model_name:
            return "translation_optimized"
        else:
            return "unknown"
    
    def _setup_npu_environment(self):
        """NPUç’°å¢ƒå¤‰æ•°è¨­å®š"""
        print("ğŸ”§ NPUç’°å¢ƒè¨­å®šä¸­...")
        
        # Ryzen AIãƒ‘ã‚¹è¨­å®š
        ryzen_ai_paths = [
            "C:\\Program Files\\RyzenAI\\1.5",
            "C:\\Program Files\\RyzenAI\\1.5.1",
            "C:\\Program Files\\RyzenAI\\1.2"
        ]
        
        for path in ryzen_ai_paths:
            if os.path.exists(path):
                os.environ["RYZEN_AI_INSTALLATION_PATH"] = path
                print(f"âœ… Ryzen AIãƒ‘ã‚¹è¨­å®š: {path}")
                break
        
        # NPUã‚ªãƒ¼ãƒãƒ¬ã‚¤è¨­å®š
        if "RYZEN_AI_INSTALLATION_PATH" in os.environ:
            base_path = os.environ["RYZEN_AI_INSTALLATION_PATH"]
            xclbin_path = os.path.join(base_path, "voe-4.0-win_amd64", "xclbins", "strix", "AMD_AIE2P_Nx4_Overlay.xclbin")
            
            if os.path.exists(xclbin_path):
                os.environ["XLNX_VART_FIRMWARE"] = xclbin_path
                os.environ["XLNX_TARGET_NAME"] = "AMD_AIE2P_Nx4_Overlay"
                os.environ["NUM_OF_DPU_RUNNERS"] = "1"
                print("âœ… NPUã‚ªãƒ¼ãƒãƒ¬ã‚¤è¨­å®šå®Œäº†")
            else:
                print("âŒ NPUã‚ªãƒ¼ãƒãƒ¬ã‚¤ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print(f"ğŸš€ {self.model_name} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        if self.model_type == "npu_optimized":
            return self._setup_npu_optimized_model()
        elif self.model_type == "large_japanese":
            return self._setup_large_japanese_model()
        elif self.model_type == "translation_optimized":
            return self._setup_translation_model()
        else:
            print(f"âŒ æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {self.model_type}")
            return False
    
    def _setup_npu_optimized_model(self) -> bool:
        """NPUæœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆ8Bï¼‰ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸ”§ NPUæœ€é©åŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        try:
            # CPUè¨­å®š
            p = psutil.Process()
            p.cpu_affinity([0, 1, 2, 3])
            torch.set_num_threads(4)
            
            # ãƒ­ã‚°è¨­å®š
            transformers.logging.set_verbosity_error()
            logging.disable(logging.CRITICAL)
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆäº‹å‰è¨“ç·´æ¸ˆã¿NPUãƒ¢ãƒ‡ãƒ«ï¼‰
            print("ğŸ¤– NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
            model_path = os.path.join(self.model_name, "pytorch_llama3_8b_w_bit_4_awq_amd.pt")
            
            if os.path.exists(model_path):
                self.model = torch.load(model_path)
                self.model.eval()
                self.model = self.model.to(torch.bfloat16)
                
                # NPUé‡å­åŒ–è¨­å®š
                if QLINEAR_AVAILABLE:
                    print("âš¡ NPUé‡å­åŒ–è¨­å®šä¸­...")
                    for n, m in self.model.named_modules():
                        if isinstance(m, qlinear.QLinearPerGrp):
                            print(f"  ğŸ“Š é‡å­åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼: {n}")
                            m.device = "aie"
                            m.quantize_weights()
                    print("âœ… NPUé‡å­åŒ–å®Œäº†")
                else:
                    print("âš ï¸ qlinearãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                
                print("âœ… NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            else:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                return False
                
        except Exception as e:
            print(f"âŒ NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _setup_large_japanese_model(self) -> bool:
        """å¤§è¦æ¨¡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ï¼ˆ70Bï¼‰ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸ”§ å¤§è¦æ¨¡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ï¼ˆ70Bï¼‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
            print("ğŸ¤– å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            
            print("ğŸ”„ ONNXå¤‰æ›æº–å‚™ä¸­...")
            # ONNXå¤‰æ›ã¯æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè£…
            
            print("âœ… å¤§è¦æ¨¡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ å¤§è¦æ¨¡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _setup_translation_model(self) -> bool:
        """ç¿»è¨³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸ”§ ç¿»è¨³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            model_path = os.path.join(self.model_name, "alma_w_bit_4_awq_fa_amd.pt")
            
            if os.path.exists(model_path):
                self.model = torch.load(model_path)
                self.model.eval()
                self.model = self.model.to(torch.bfloat16)
                
                # NPUé‡å­åŒ–è¨­å®š
                if QLINEAR_AVAILABLE:
                    for n, m in self.model.named_modules():
                        if isinstance(m, qlinear.QLinearPerGrp):
                            print(f"ğŸ“Š é‡å­åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼: {n}")
                            m.device = "aie"
                            m.quantize_weights()
                
                print("âœ… ç¿»è¨³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            else:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                return False
                
        except Exception as e:
            print(f"âŒ ç¿»è¨³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_text(self, prompt: str, max_new_tokens: int = 200) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.model or not self.tokenizer:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            start_time = time.time()
            
            if self.model_type == "npu_optimized":
                return self._generate_npu_optimized(prompt, max_new_tokens)
            elif self.model_type == "large_japanese":
                return self._generate_large_japanese(prompt, max_new_tokens)
            elif self.model_type == "translation_optimized":
                return self._generate_translation(prompt, max_new_tokens)
            else:
                return "âŒ æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"
                
        except Exception as e:
            return f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
    
    def _generate_npu_optimized(self, prompt: str, max_new_tokens: int) -> str:
        """NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ"""
        print("âš¡ NPUæœ€é©åŒ–ç”Ÿæˆé–‹å§‹...")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼
        messages = [
            {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯Œãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼é©ç”¨
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt",
            return_dict=True
        )
        
        # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
        terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        
        # NPUç”Ÿæˆ
        start_time = time.time()
        outputs = self.model.generate(
            input_ids['input_ids'],
            max_new_tokens=max_new_tokens,
            eos_token_id=terminators,
            attention_mask=input_ids['attention_mask'],
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
        response = outputs[0][input_ids['input_ids'].shape[-1]:]
        response_text = self.tokenizer.decode(response, skip_special_tokens=True)
        
        generation_time = time.time() - start_time
        tokens_generated = len(response)
        
        print(f"âš¡ NPUç”Ÿæˆå®Œäº†: {tokens_generated}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.2f}ç§’")
        print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_generated/generation_time:.2f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
        
        return response_text
    
    def _generate_large_japanese(self, prompt: str, max_new_tokens: int) -> str:
        """å¤§è¦æ¨¡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ"""
        print("ğŸ”¥ å¤§è¦æ¨¡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ç”Ÿæˆé–‹å§‹...")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼é©ç”¨
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.model.device)
        
        # ç”Ÿæˆ
        start_time = time.time()
        output_ids = self.model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
        response = output_ids[0][input_ids.shape[-1]:]
        response_text = self.tokenizer.decode(response, skip_special_tokens=True)
        
        generation_time = time.time() - start_time
        tokens_generated = len(response)
        
        print(f"ğŸ”¥ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆå®Œäº†: {tokens_generated}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.2f}ç§’")
        print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_generated/generation_time:.2f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
        
        return response_text
    
    def _generate_translation(self, prompt: str, max_new_tokens: int) -> str:
        """ç¿»è¨³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã®ç”Ÿæˆ"""
        print("ğŸŒ ç¿»è¨³ç‰¹åŒ–ç”Ÿæˆé–‹å§‹...")
        
        # ç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼
        system = """ã‚ãªãŸã¯é«˜åº¦ãªæŠ€èƒ½ã‚’æŒã¤ãƒ—ãƒ­ã®æ—¥æœ¬èª-è‹±èªãŠã‚ˆã³è‹±èª-æ—¥æœ¬èªç¿»è¨³è€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"""
        
        full_prompt = f"""{system}

### æŒ‡ç¤º:
{prompt}

### å¿œç­”:
"""
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼é©ç”¨
        tokenized_input = self.tokenizer(
            full_prompt,
            return_tensors="pt",
            padding=True,
            max_length=1600,
            truncation=True
        )
        
        # ç”Ÿæˆ
        start_time = time.time()
        outputs = self.model.generate(
            tokenized_input['input_ids'],
            max_new_tokens=max_new_tokens,
            eos_token_id=self.tokenizer.eos_token_id,
            attention_mask=tokenized_input['attention_mask'],
            do_sample=True,
            temperature=0.3,
            top_p=0.5
        )
        
        # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
        response = outputs[0][tokenized_input['input_ids'].shape[-1]:]
        response_text = self.tokenizer.decode(response, skip_special_tokens=True)
        
        generation_time = time.time() - start_time
        tokens_generated = len(response)
        
        print(f"ğŸŒ ç¿»è¨³ç”Ÿæˆå®Œäº†: {tokens_generated}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.2f}ç§’")
        print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_generated/generation_time:.2f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
        
        return response_text
    
    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        info = {
            "model_name": self.model_name,
            "model_type": self.model_type,
            "npu_optimized": self.model_type in ["npu_optimized", "translation_optimized"],
            "japanese_support": True,
            "estimated_parameters": self._get_parameter_count(),
            "memory_usage": self._get_memory_usage()
        }
        return info
    
    def _get_parameter_count(self) -> str:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¨å®š"""
        if "8b" in self.model_name.lower():
            return "8B"
        elif "70b" in self.model_name.lower():
            return "70B"
        elif "7b" in self.model_name.lower():
            return "7B"
        else:
            return "Unknown"
    
    def _get_memory_usage(self) -> str:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_gb = memory_info.rss / (1024 ** 3)
            return f"{memory_gb:.1f}GB"
        except:
            return "Unknown"


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢")
    parser.add_argument("--model", default="llama3-8b-amd-npu", 
                       choices=["llama3-8b-amd-npu", "cyberagent/Llama-3.1-70B-Japanese-Instruct-2407", "ALMA-Ja-V3-amd-npu"],
                       help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--prompt", default="äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", help="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-tokens", type=int, default=200, help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢é–‹å§‹")
    print("ğŸ¯ çœŸã®NPUæ´»ç”¨å®Ÿç¾ç‰ˆ")
    print("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = NPUOptimizedJapaneseModel(args.model)
    
    # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    if not model.setup_model():
        print("âŒ ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
    info = model.get_model_info()
    print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {info['model_name']}")
    print(f"ğŸ”§ ã‚¿ã‚¤ãƒ—: {info['model_type']}")
    print(f"âš¡ NPUæœ€é©åŒ–: {'âœ…' if info['npu_optimized'] else 'âŒ'}")
    print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œ: {'âœ…' if info['japanese_support'] else 'âŒ'}")
    print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['estimated_parameters']}")
    print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {info['memory_usage']}")
    print("=" * 60)
    
    if args.interactive:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("ğŸ‡¯ğŸ‡µ NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("ğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†")
        print("=" * 60)
        
        while True:
            try:
                prompt = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if not prompt.strip():
                    continue
                
                print("\nğŸ”„ ç”Ÿæˆä¸­...")
                response = model.generate_text(prompt, args.max_tokens)
                
                print(f"\nâœ… ç”Ÿæˆå®Œäº†:")
                print(f"ğŸ“ å¿œç­”: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        # å˜ç™ºå®Ÿè¡Œ
        print(f"ğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
        print("\nğŸ”„ ç”Ÿæˆä¸­...")
        
        response = model.generate_text(args.prompt, args.max_tokens)
        
        print(f"\nâœ… ç”Ÿæˆå®Œäº†:")
        print(f"ğŸ“ å¿œç­”: {response}")
    
    print("\nğŸ NPUæœ€é©åŒ–æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¢å®Œäº†")


if __name__ == "__main__":
    main()

