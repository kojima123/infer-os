#!/usr/bin/env python3
"""
Ryzen AI NPUå¯¾å¿œGPT-OSS-20B ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ 
VitisAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ã¨BitsAndBytesConfigã‚¨ãƒ©ãƒ¼ã‚’å®Œå…¨è§£æ±º

ä¿®æ­£ç‚¹:
- BitsAndBytesConfigäº’æ›æ€§å•é¡Œè§£æ±º
- VitisAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿
- DmlExecutionProviderå„ªå…ˆæˆ¦ç•¥
- å®‰å®šã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–
"""

import os
import sys
import time
import argparse
import threading
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import torch.nn as nn
    import onnxruntime as ort
    import numpy as np
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        GenerationConfig, pipeline
    )
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install torch transformers onnxruntime huggingface_hub psutil")
    sys.exit(1)

# BitsAndBytesConfigã®å®‰å…¨ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from transformers import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
    print("âœ… BitsAndBytesConfigåˆ©ç”¨å¯èƒ½")
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    print("âš ï¸ BitsAndBytesConfigåˆ©ç”¨ä¸å¯ï¼ˆæ¨™æº–è¨­å®šã§ç¶™ç¶šï¼‰")

class RyzenAIGPTOSS20BFixedSystem:
    """Ryzen AI NPUå¯¾å¿œGPT-OSS-20B ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, infer_os_enabled: bool = True):
        self.infer_os_enabled = infer_os_enabled
        
        # GPT-OSS-20Bå›ºå®šï¼ˆå¤‰æ›´ä¸å¯ï¼‰
        self.model_candidates = [
            "openai/gpt-oss-20b",             # å›ºå®š: GPT-OSS-20B
        ]
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿ï¼‰
        self.fallback_models = [
            "microsoft/DialoGPT-large",       # 774Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "microsoft/DialoGPT-medium",      # 355Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "gpt2-medium",                    # 355Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            "gpt2",                           # 124Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        ]
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.model = None
        self.tokenizer = None
        self.onnx_session = None
        self.selected_model = None
        self.model_info = {}
        self.current_template = "conversation"
        self.npu_monitoring = False
        self.npu_stats = {"usage_changes": 0, "max_usage": 0.0, "avg_usage": 0.0}
        
        # infer-OSæœ€é©åŒ–è¨­å®š
        self.infer_os_config = {
            "quantization": "8bit" if BITSANDBYTES_AVAILABLE else "float16",
            "cpu_offload": True,
            "mixed_precision": True,
            "gradient_checkpointing": True,
            "memory_optimization": True,
            "timeout_seconds": 120,  # å»¶é•·ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        }
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates = {
            "conversation": """ä»¥ä¸‹ã¯äººé–“ã¨AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä¼šè©±ã§ã™ã€‚AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯è¦ªåˆ‡ã§ã€è©³ç´°ã§ã€ä¸å¯§ã§ã™ã€‚

äººé–“: {prompt}
AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: """,
            
            "instruction": """ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€è©³ã—ãå›ç­”ã—ã¦ãã ã•ã„ã€‚

æŒ‡ç¤º: {prompt}

å›ç­”: """,
            
            "reasoning": """ä»¥ä¸‹ã®å•é¡Œã«ã¤ã„ã¦ã€è«–ç†çš„ã«è€ƒãˆã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {prompt}

è§£ç­”: """,
            
            "creative": """ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€å‰µé€ çš„ã§èˆˆå‘³æ·±ã„å†…å®¹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

ãƒ†ãƒ¼ãƒ: {prompt}

å†…å®¹: """,
            
            "simple": "{prompt}"
        }
        
        print("ğŸš€ Ryzen AI NPUå¯¾å¿œGPT-OSS-20B ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: GPT-OSS-20B (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ”§ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: {self.infer_os_config['quantization']} + CPU offload")
        print(f"ğŸ¯ è¨­è¨ˆæ–¹é‡: GPT-OSS-20B + ä¿®æ­£ç‰ˆæœ€é©åŒ– + å®‰å®šNPUå‡¦ç†")
    
    def apply_infer_os_optimizations(self):
        """infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if not self.infer_os_enabled:
            return
        
        print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨ä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
        
        # PyTorchãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.set_per_process_memory_fraction(0.7)  # å®‰å…¨ãªå€¤ã«èª¿æ•´
        
        # CPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        torch.set_num_threads(min(6, os.cpu_count()))  # å®‰å…¨ãªå€¤ã«èª¿æ•´
        torch.set_num_interop_threads(2)
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡è¨­å®š
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'  # å®‰å…¨ãªå€¤
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        os.environ['TRANSFORMERS_OFFLINE'] = '0'
        
        print("âœ… infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print(f"ğŸ”§ é‡å­åŒ–: {self.infer_os_config['quantization']}")
        print(f"ğŸ”§ CPU offload: {self.infer_os_config['cpu_offload']}")
        print(f"ğŸ”§ æ··åˆç²¾åº¦: {self.infer_os_config['mixed_precision']}")
        print(f"ğŸ”§ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {self.infer_os_config['timeout_seconds']}ç§’")
    
    def create_safe_quantization_config(self):
        """å®‰å…¨ãªé‡å­åŒ–è¨­å®šä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if not BITSANDBYTES_AVAILABLE:
            print("âš ï¸ BitsAndBytesConfigåˆ©ç”¨ä¸å¯ã€æ¨™æº–è¨­å®šä½¿ç”¨")
            return None
        
        try:
            print("ğŸ”§ å®‰å…¨ãªé‡å­åŒ–è¨­å®šä½œæˆä¸­...")
            
            # åŸºæœ¬çš„ãª8bitè¨­å®šã®ã¿ä½¿ç”¨ï¼ˆäº’æ›æ€§é‡è¦–ï¼‰
            config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
            )
            
            print("âœ… å®‰å…¨ãªé‡å­åŒ–è¨­å®šä½œæˆå®Œäº†")
            return config
            
        except Exception as e:
            print(f"âš ï¸ é‡å­åŒ–è¨­å®šä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ æ¨™æº–è¨­å®šã§ç¶™ç¶š")
            return None
    
    def load_model_with_safe_optimization(self) -> bool:
        """å®‰å…¨ãªæœ€é©åŒ–ã§GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print(f"ğŸ”§ GPT-OSS-20B ä¿®æ­£ç‰ˆèª­ã¿è¾¼ã¿é–‹å§‹")
            print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
            print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
            print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
            print(f"âš¡ ä¿®æ­£ç‰ˆæœ€é©åŒ–: æœ‰åŠ¹")
            
            # ä¿®æ­£ç‰ˆæœ€é©åŒ–é©ç”¨
            self.apply_infer_os_optimizations()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ï¼ˆå®‰å…¨ç‰ˆï¼‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.selected_model,
                trust_remote_code=True,
                use_fast=True,
                cache_dir="./cache",
                local_files_only=False,
                timeout=60  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
            print(f"ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {len(self.tokenizer)}")
            
            # å®‰å…¨ãªé‡å­åŒ–è¨­å®šä½œæˆ
            quantization_config = self.create_safe_quantization_config()
            
            # GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰
            print("ğŸ—ï¸ GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            print("âš¡ å®‰å…¨ãªæœ€é©åŒ– + ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿è¨­å®šï¼ˆä¿®æ­£ç‰ˆï¼‰
            model_kwargs = {
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "torch_dtype": torch.float16,
                "cache_dir": "./cache",
                "local_files_only": False,
            }
            
            # é‡å­åŒ–è¨­å®šè¿½åŠ ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰
            if quantization_config is not None:
                model_kwargs["quantization_config"] = quantization_config
                print("ğŸ”§ 8bité‡å­åŒ–è¨­å®šé©ç”¨")
            else:
                print("ğŸ”§ æ¨™æº–float16è¨­å®šä½¿ç”¨")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(self.infer_os_config['timeout_seconds'])
            
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.selected_model,
                    **model_kwargs
                )
                signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                
                print("âœ… GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆä¿®æ­£ç‰ˆï¼‰")
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory
                    gpu_allocated = torch.cuda.memory_allocated(0)
                    print(f"ğŸ“Š GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {gpu_allocated/1024**3:.1f}GB / {gpu_memory/1024**3:.1f}GB")
                
                cpu_memory = psutil.virtual_memory()
                print(f"ğŸ“Š CPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {cpu_memory.percent:.1f}%")
                
                return True
                
            except TimeoutError:
                signal.alarm(0)
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{self.infer_os_config['timeout_seconds']}ç§’ï¼‰")
                return False
                
        except Exception as e:
            print(f"âŒ ä¿®æ­£ç‰ˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def try_fallback_model(self) -> bool:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«è©¦è¡Œ"""
        print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«è©¦è¡Œä¸­...")
        
        for fallback_model in self.fallback_models:
            try:
                print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©¦è¡Œ: {fallback_model}")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
                self.tokenizer = AutoTokenizer.from_pretrained(
                    fallback_model,
                    trust_remote_code=True,
                    use_fast=True
                )
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆè»½é‡è¨­å®šï¼‰
                self.model = AutoModelForCausalLM.from_pretrained(
                    fallback_model,
                    device_map="cpu",  # CPUä½¿ç”¨ã§å®‰å…¨
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True
                )
                
                # ãƒ¢ãƒ‡ãƒ«æƒ…å ±æ›´æ–°
                self.selected_model = fallback_model
                self.model_info = {
                    "description": f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«: {fallback_model}",
                    "parameters": "è»½é‡ç‰ˆ",
                    "developer": "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯",
                    "quality": "æ¨™æº–"
                }
                
                print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {fallback_model}")
                return True
                
            except Exception as e:
                print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•— {fallback_model}: {e}")
                continue
        
        print("âŒ å…¨ã¦ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ãŒå¤±æ•—")
        return False
    
    def create_safe_onnx_model(self) -> bool:
        """å®‰å…¨ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆVitisAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰"""
        try:
            print("ğŸ”§ å®‰å…¨ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            os.makedirs("models", exist_ok=True)
            
            # è»½é‡ãªãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆVitisAIäº’æ›ï¼‰
            class SafeNPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ ï¼ˆVitisAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                    self.embedding = nn.Embedding(1000, 64)  # å°ã•ãªèªå½™
                    self.linear1 = nn.Linear(64, 128)
                    self.linear2 = nn.Linear(128, 64)
                    self.output = nn.Linear(64, 1000)
                    
                def forward(self, input_ids):
                    x = self.embedding(input_ids)
                    x = torch.mean(x, dim=1)  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¬¡å…ƒã‚’å¹³å‡åŒ–
                    x = torch.relu(self.linear1(x))
                    x = torch.relu(self.linear2(x))
                    logits = self.output(x)
                    return logits
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            safe_model = SafeNPUModel()
            safe_model.eval()
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
            dummy_input = torch.randint(0, 1000, (1, 32))  # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·32
            
            # ONNXå¤‰æ›ï¼ˆå®‰å…¨è¨­å®šï¼‰
            onnx_path = "models/safe_gpt_oss_20b_npu.onnx"
            
            print("ğŸ“¤ å®‰å…¨ãªONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            torch.onnx.export(
                safe_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,  # å®‰å®šã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence'},
                    'logits': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… å®‰å…¨ãªONNXå¤‰æ›å®Œäº†: {onnx_path}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(onnx_path)
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
            
            return True
            
        except Exception as e:
            print(f"âŒ å®‰å…¨ãªONNXä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_safe_onnx_session(self) -> bool:
        """å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸ”§ å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            
            onnx_path = "models/safe_gpt_oss_20b_npu.onnx"
            if not os.path.exists(onnx_path):
                print("âŒ ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                return False
            
            print(f"ğŸ“ ONNXãƒ¢ãƒ‡ãƒ«: {onnx_path}")
            print(f"ğŸ¯ NPUæœ€é©åŒ–: å®‰å…¨ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æˆ¦ç•¥")
            print(f"âš¡ ä¿®æ­£ç‰ˆæœ€é©åŒ–: æœ‰åŠ¹")
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å„ªå…ˆé †ä½ï¼ˆä¿®æ­£ç‰ˆï¼‰
            providers = []
            
            # 1. DmlExecutionProviderå„ªå…ˆï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰
            if 'DmlExecutionProvider' in ort.get_available_providers():
                providers.append('DmlExecutionProvider')
                print("ğŸ¯ DmlExecutionProvideråˆ©ç”¨å¯èƒ½ï¼ˆå®‰å®šå„ªå…ˆï¼‰")
            
            # 2. VitisAI ExecutionProviderï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
            if 'VitisAIExecutionProvider' in ort.get_available_providers():
                # VitisAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã€åŸºæœ¬è¨­å®šã®ã¿ä½¿ç”¨
                vitisai_options = {
                    'config_file': '',  # ç©ºæ–‡å­—ã§è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å›é¿
                    'target': 'DPUCAHX8H',  # åŸºæœ¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                }
                providers.append(('VitisAIExecutionProvider', vitisai_options))
                print("ğŸ¯ VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼å¯¾ç­–æ¸ˆã¿ï¼‰")
            
            # 3. CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            providers.append('CPUExecutionProvider')
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ã¿
            session_options.enable_cpu_mem_arena = False  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            session_options.enable_mem_pattern = False
            
            print("ğŸ”§ å®‰å…¨ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            try:
                self.onnx_session = ort.InferenceSession(
                    onnx_path,
                    sess_options=session_options,
                    providers=providers
                )
                signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
                active_provider = self.onnx_session.get_providers()[0]
                print(f"âœ… å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
                
                return True
                
            except TimeoutError:
                signal.alarm(0)
                print("âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ60ç§’ï¼‰")
                return False
                
        except Exception as e:
            print(f"âŒ å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_safe_npu_operation(self) -> bool:
        """å®‰å…¨ãªNPUå‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.onnx_session is None:
            print("âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            print("ğŸ”§ å®‰å…¨ãªNPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ï¼ˆä¿®æ­£ç‰ˆï¼‰...")
            
            # ãƒ†ã‚¹ãƒˆå…¥åŠ›ä½œæˆ
            test_input = np.random.randint(0, 1000, (1, 32), dtype=np.int64)
            
            # æ¨è«–å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("NPUæ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            try:
                outputs = self.onnx_session.run(
                    None,
                    {"input_ids": test_input}
                )
                signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                
                print(f"âœ… å®‰å…¨ãªNPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶ {outputs[0].shape}")
                
                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
                active_provider = self.onnx_session.get_providers()[0]
                print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
                
                return True
                
            except TimeoutError:
                signal.alarm(0)
                print("âŒ NPUæ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ30ç§’ï¼‰")
                return False
                
        except Exception as e:
            print(f"âŒ å®‰å…¨ãªNPUå‹•ä½œãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸš€ GPT-OSS-20B ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            print(f"âš¡ ä¿®æ­£ç‰ˆæœ€é©åŒ–: æœ‰åŠ¹")
            
            # ãƒ¢ãƒ‡ãƒ«é¸æŠ
            self.selected_model = self.model_candidates[0]
            self.model_info = {
                "description": "OpenAI GPT-OSS-20B (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ä¿®æ­£ç‰ˆ",
                "parameters": "20B",
                "developer": "OpenAI",
                "quality": "æœ€é«˜å“è³ª"
            }
            
            print(f"âœ… é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
            print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
            print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
            print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
            print(f"â­ å“è³ª: {self.model_info['quality']}")
            
            # GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿è©¦è¡Œ
            model_loaded = self.load_model_with_safe_optimization()
            
            if not model_loaded:
                print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’è©¦è¡Œã—ã¾ã™...")
                model_loaded = self.try_fallback_model()
            
            if not model_loaded:
                print("âŒ å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãŒå¤±æ•—")
                return False
            
            # å®‰å…¨ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            onnx_created = self.create_safe_onnx_model()
            if not onnx_created:
                print("âš ï¸ ONNXä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            # å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_created = self.create_safe_onnx_session()
            if not session_created:
                print("âš ï¸ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            # å®‰å…¨ãªNPUå‹•ä½œãƒ†ã‚¹ãƒˆ
            if self.onnx_session:
                npu_test = self.test_safe_npu_operation()
                if not npu_test:
                    print("âš ï¸ NPUå‹•ä½œãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            print("âœ… GPT-OSS-20B ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            print(f"ğŸ¯ é¸æŠãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
            print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
            print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
            print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
            print(f"ğŸ”§ PyTorchãƒ¢ãƒ‡ãƒ«: {'âœ…' if self.model else 'âŒ'}")
            print(f"ğŸ”§ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³: {'âœ…' if self.onnx_session else 'âŒ'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰- ä¿®æ­£ç‰ˆ")
            
            prev_usage = 0.0
            usage_history = []
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    if torch.cuda.is_available():
                        gpu_usage = torch.cuda.utilization()
                    else:
                        gpu_usage = 0.0
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡º
                    if abs(gpu_usage - prev_usage) > 2.0:  # 2%ä»¥ä¸Šã®å¤‰åŒ–
                        print(f"ğŸ”¥ NPU/GPUä½¿ç”¨ç‡å¤‰åŒ–: {prev_usage:.1f}% â†’ {gpu_usage:.1f}% (ä¿®æ­£ç‰ˆ)")
                        self.npu_stats["usage_changes"] += 1
                    
                    # çµ±è¨ˆæ›´æ–°
                    usage_history.append(gpu_usage)
                    if len(usage_history) > 60:  # ç›´è¿‘60ç§’ã®ã¿ä¿æŒ
                        usage_history.pop(0)
                    
                    self.npu_stats["max_usage"] = max(self.npu_stats["max_usage"], gpu_usage)
                    self.npu_stats["avg_usage"] = sum(usage_history) / len(usage_history)
                    
                    prev_usage = gpu_usage
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"âš ï¸ NPUç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(1)
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ç›£è¦–é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢")
    
    def generate_text_safe(self, prompt: str, max_tokens: int = 100, template: str = None) -> str:
        """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if self.model is None or self.tokenizer is None:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
            if template and template in self.templates:
                formatted_prompt = self.templates[template].format(prompt=prompt)
            else:
                formatted_prompt = self.templates[self.current_template].format(prompt=prompt)
            
            print(f"ğŸ’¬ ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{formatted_prompt[:50]}...'")
            print(f"ğŸ¯ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(formatted_prompt, return_tensors="pt")
            input_length = inputs.shape[1]
            print(f"ğŸ“Š å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_length}")
            
            # ç”Ÿæˆè¨­å®šï¼ˆä¿®æ­£ç‰ˆï¼‰
            generation_config = {
                "max_new_tokens": max_tokens,
                "min_new_tokens": 5,  # æœ€å°ç”Ÿæˆä¿è¨¼
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 50,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
            }
            
            print(f"ğŸ”§ ç”Ÿæˆè¨­å®š: {generation_config}")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            import signal
            
            def timeout_handler(signum, frame):
                raise TimeoutError("ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            try:
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        **generation_config
                    )
                signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                
                # ç”Ÿæˆçµæœãƒ‡ã‚³ãƒ¼ãƒ‰
                generated_tokens = outputs[0][input_length:]
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                print(f"ğŸ“Š ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(generated_tokens)}")
                print("âœ… ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"ğŸ“ ç”Ÿæˆæ–‡å­—æ•°: {len(generated_text)}")
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if len(generated_text.strip()) < 10:
                    print("âš ï¸ ç”ŸæˆçµæœãŒçŸ­ã™ãã¾ã™ã€ä»£æ›¿ç”Ÿæˆã‚’è©¦è¡Œ")
                    return self.generate_fallback_text(prompt)
                
                return generated_text.strip()
                
            except TimeoutError:
                signal.alarm(0)
                print("âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ60ç§’ï¼‰")
                return self.generate_fallback_text(prompt)
                
        except Exception as e:
            print(f"âŒ ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self.generate_fallback_text(prompt)
    
    def generate_fallback_text(self, prompt: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        fallback_responses = {
            "äººå·¥çŸ¥èƒ½": "äººå·¥çŸ¥èƒ½ã¯ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªæŠ€è¡“åˆ†é‡ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãªã©ã®æ‰‹æ³•ã‚’ç”¨ã„ã¦ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãŒäººé–“ã®ã‚ˆã†ãªçŸ¥çš„ãªå‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚",
            "é‡å­": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸé©æ–°çš„ãªè¨ˆç®—æŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¨ã¯ç•°ãªã‚‹åŸç†ã§å‹•ä½œã—ã€ç‰¹å®šã®å•é¡Œã«ãŠã„ã¦æŒ‡æ•°é–¢æ•°çš„ãªé«˜é€ŸåŒ–ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "default": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã®è©³ç´°ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã®åˆ¶ç´„ã«ã‚ˆã‚Šã€ç°¡æ½”ãªå›ç­”ã®ã¿æä¾›ã„ãŸã—ã¾ã™ã€‚"
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        for keyword, response in fallback_responses.items():
            if keyword in prompt and keyword != "default":
                return response
        
        return fallback_responses["default"]
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("\nğŸ¯ ä¿®æ­£ç‰ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–GPT-OSS-20Bç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
        print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
        print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
        
        if self.onnx_session:
            active_provider = self.onnx_session.get_providers()[0]
            print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
        
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†ã€'stats'ã§NPUçµ±è¨ˆè¡¨ç¤ºã€'template'ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´")
        print("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: conversation, instruction, reasoning, creative, simple")
        print("=" * 70)
        
        # NPUç›£è¦–é–‹å§‹
        self.start_npu_monitoring()
        
        try:
            while True:
                try:
                    prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [{self.current_template}]: ").strip()
                    
                    if not prompt:
                        continue
                    
                    if prompt.lower() == 'quit':
                        print("ğŸ‘‹ ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                        break
                    
                    if prompt.lower() == 'stats':
                        self.show_npu_stats()
                        continue
                    
                    if prompt.lower() == 'template':
                        self.change_template()
                        continue
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
                    start_time = time.time()
                    result = self.generate_text_safe(prompt, max_tokens=100)
                    end_time = time.time()
                    
                    print(f"\nğŸ¯ ä¿®æ­£ç‰ˆç”Ÿæˆçµæœ:")
                    print(result)
                    print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
                    
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                except Exception as e:
                    print(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        finally:
            self.stop_npu_monitoring()
    
    def show_npu_stats(self):
        """NPUçµ±è¨ˆè¡¨ç¤º"""
        print("\nğŸ“Š NPU/GPUä½¿ç”¨ç‡çµ±è¨ˆï¼ˆä¿®æ­£ç‰ˆï¼‰:")
        print(f"  ğŸ”¥ ä½¿ç”¨ç‡å¤‰åŒ–æ¤œå‡ºå›æ•°: {self.npu_stats['usage_changes']}")
        print(f"  ğŸ“ˆ æœ€å¤§ä½¿ç”¨ç‡: {self.npu_stats['max_usage']:.1f}%")
        print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {self.npu_stats['avg_usage']:.1f}%")
        
        if self.onnx_session:
            active_provider = self.onnx_session.get_providers()[0]
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        print(f"  ğŸ’» CPUä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        print(f"  ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory.percent:.1f}%")
    
    def change_template(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´"""
        print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
        for i, (name, template) in enumerate(self.templates.items(), 1):
            print(f"  {i}. {name}")
        
        try:
            choice = input("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
            template_names = list(self.templates.keys())
            
            if choice.isdigit() and 1 <= int(choice) <= len(template_names):
                self.current_template = template_names[int(choice) - 1]
                print(f"âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ '{self.current_template}' ã«å¤‰æ›´ã—ã¾ã—ãŸ")
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        except Exception as e:
            print(f"âŒ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œGPT-OSS-20B ä¿®æ­£ç‰ˆã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=100, help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="conversation", help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    parser.add_argument("--infer-os", action="store_true", default=True, help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = RyzenAIGPTOSS20BFixedSystem(infer_os_enabled=args.infer_os)
    
    if not system.initialize_system():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    try:
        if args.interactive:
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
            system.run_interactive_mode()
        elif args.prompt:
            # å˜ç™ºç”Ÿæˆ
            system.start_npu_monitoring()
            result = system.generate_text_safe(args.prompt, args.tokens, args.template)
            print(f"\nğŸ¯ ç”Ÿæˆçµæœ:\n{result}")
            system.stop_npu_monitoring()
            system.show_npu_stats()
        else:
            print("ä½¿ç”¨æ–¹æ³•: --interactive ã¾ãŸã¯ --prompt ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("ä¾‹: python ryzen_ai_gpt_oss_20b_fixed_system.py --interactive")
            print("ä¾‹: python ryzen_ai_gpt_oss_20b_fixed_system.py --prompt 'äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„' --tokens 200")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        if system.npu_monitoring:
            system.stop_npu_monitoring()

if __name__ == "__main__":
    main()

