#!/usr/bin/env python3
"""
Phase 2-3: Router APIå®Ÿè£…
Layer Skipã€FFN Skipã€Token Haltingã«ã‚ˆã‚‹å‹•çš„æœ€é©åŒ–

Phase 2 ç›®æ¨™æ€§èƒ½: 18-19 tok/s (Layer Skip)
Phase 3 ç›®æ¨™æ€§èƒ½: 20-21 tok/s (FFN Skip + Token Halting)
ä¸»è¦å®Ÿè£…: çµ±åˆRouter APIã€å‹•çš„ã‚¹ã‚­ãƒƒãƒ—æ©Ÿæ§‹ã€æ¨è«–æœ€é©åŒ–

ä½œæˆè€…: Manus AI
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.0
"""

import os
import sys
import time
import json
import logging
import threading
import math
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from enum import Enum

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
import psutil
import numpy as np

# Phase 1 å®Ÿè£…ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from phase1_implementation import (
        Phase1SystemConfig,
        NPUMetrics,
        XDNASDKInterface,
        FourTierMemoryManager,
        FlexGenPlusPlusPhase1,
        InferOSControllerPhase1
    )
    from phase0_implementation import PerformanceMetrics
except ImportError as e:
    print(f"âŒ Phase 1 å®Ÿè£…ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    print("phase1_implementation.py ãŒåŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('phase2_3_implementation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SkipType(Enum):
    """ã‚¹ã‚­ãƒƒãƒ—ã‚¿ã‚¤ãƒ—"""
    LAYER_SKIP = "layer_skip"
    FFN_SKIP = "ffn_skip"
    ATTENTION_SKIP = "attention_skip"
    TOKEN_HALTING = "token_halting"

class SkipDecision(Enum):
    """ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š"""
    EXECUTE = "execute"
    SKIP = "skip"
    PARTIAL = "partial"

@dataclass
class Phase2_3SystemConfig(Phase1SystemConfig):
    """Phase 2-3 ã‚·ã‚¹ãƒ†ãƒ è¨­å®š"""
    # Router APIè¨­å®š
    enable_layer_skip: bool = True
    enable_ffn_skip: bool = True
    enable_token_halting: bool = True
    
    # Layer Skipè¨­å®š
    layer_skip_threshold: float = 0.1  # ã‚¹ã‚­ãƒƒãƒ—é–¾å€¤
    layer_skip_ratio: float = 0.3      # æœ€å¤§30%ã®å±¤ã‚’ã‚¹ã‚­ãƒƒãƒ—
    
    # FFN Skipè¨­å®š
    ffn_skip_threshold: float = 0.15
    ffn_skip_ratio: float = 0.4        # æœ€å¤§40%ã®FFNã‚’ã‚¹ã‚­ãƒƒãƒ—
    
    # Token Haltingè¨­å®š
    token_halting_threshold: float = 0.95  # ä¿¡é ¼åº¦95%ã§åœæ­¢
    min_tokens: int = 10               # æœ€ä½ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
    max_tokens_per_step: int = 5       # ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šæœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    
    # Phase 2-3 æ€§èƒ½ç›®æ¨™
    phase2_target_tokens_per_second: float = 18.5  # Phase 2 ç›®æ¨™
    phase3_target_tokens_per_second: float = 20.5  # Phase 3 ç›®æ¨™
    
    # Router APIåˆ¶å¾¡è¨­å®š
    router_update_frequency: int = 100  # 100æ¨è«–ã”ã¨ã«æ›´æ–°
    adaptive_threshold: bool = True     # é©å¿œçš„é–¾å€¤èª¿æ•´
    
    # å“è³ªä¿è¨¼è¨­å®š
    max_quality_degradation: float = 0.05  # æœ€å¤§5%ã®å“è³ªåŠ£åŒ–è¨±å®¹

@dataclass
class SkipMetrics:
    """ã‚¹ã‚­ãƒƒãƒ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    layer_skips: int = 0
    ffn_skips: int = 0
    token_halts: int = 0
    total_layers: int = 0
    total_ffns: int = 0
    total_tokens: int = 0
    
    skip_ratio_layer: float = 0.0
    skip_ratio_ffn: float = 0.0
    halt_ratio_token: float = 0.0
    
    quality_score: float = 1.0
    performance_gain: float = 0.0

@dataclass
class RouterDecision:
    """Routeråˆ¤å®šçµæœ"""
    skip_type: SkipType
    decision: SkipDecision
    confidence: float
    layer_index: Optional[int] = None
    token_index: Optional[int] = None
    reasoning: str = ""

class LayerImportanceAnalyzer:
    """å±¤é‡è¦åº¦åˆ†æå™¨"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.layer_importance_cache = {}
        self.attention_patterns = {}
        self.ffn_activation_patterns = {}
        
        # å±¤ã®é‡è¦åº¦ã‚’äº‹å‰è¨ˆç®—
        self._precompute_layer_importance()
    
    def _precompute_layer_importance(self):
        """å±¤é‡è¦åº¦ã®äº‹å‰è¨ˆç®—"""
        logger.info("Precomputing layer importance...")
        
        # ãƒ¢ãƒ‡ãƒ«ã®å±¤ã‚’åˆ†æ
        layer_count = 0
        for name, module in self.model.named_modules():
            if self._is_transformer_layer(module):
                importance = self._calculate_layer_importance(name, module)
                self.layer_importance_cache[layer_count] = importance
                layer_count += 1
        
        logger.info(f"Analyzed {layer_count} transformer layers")
    
    def _is_transformer_layer(self, module: nn.Module) -> bool:
        """Transformerå±¤ã®åˆ¤å®š"""
        # ä¸€èˆ¬çš„ãªTransformerå±¤ã®ç‰¹å¾´ã‚’æ¤œå‡º
        has_attention = any("attention" in name.lower() for name, _ in module.named_modules())
        has_ffn = any("mlp" in name.lower() or "ffn" in name.lower() for name, _ in module.named_modules())
        return has_attention and has_ffn
    
    def _calculate_layer_importance(self, layer_name: str, layer_module: nn.Module) -> float:
        """å±¤é‡è¦åº¦ã®è¨ˆç®—"""
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«åŸºã¥ãé‡è¦åº¦
        param_count = sum(p.numel() for p in layer_module.parameters())
        
        # å±¤ã®ä½ç½®ã«åŸºã¥ãé‡è¦åº¦ï¼ˆä¸­é–“å±¤ã»ã©é‡è¦ï¼‰
        layer_position_weight = self._get_position_weight(layer_name)
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸé‡è¦åº¦
        importance = (param_count / 1e6) * layer_position_weight
        return min(1.0, importance)
    
    def _get_position_weight(self, layer_name: str) -> float:
        """å±¤ä½ç½®ã«åŸºã¥ãé‡ã¿"""
        # å±¤ç•ªå·ã‚’æŠ½å‡ºï¼ˆç°¡ç•¥åŒ–ï¼‰
        import re
        numbers = re.findall(r'\d+', layer_name)
        if numbers:
            layer_num = int(numbers[0])
            # ä¸­é–“å±¤ï¼ˆå…¨ä½“ã®30-70%ï¼‰ã«é«˜ã„é‡ã¿ã‚’ä»˜ä¸
            total_layers = 32  # ä»®å®š
            position_ratio = layer_num / total_layers
            
            if 0.3 <= position_ratio <= 0.7:
                return 1.0  # ä¸­é–“å±¤ã¯é‡è¦
            elif position_ratio < 0.3:
                return 0.7  # åˆæœŸå±¤ã¯ä¸­ç¨‹åº¦
            else:
                return 0.8  # å¾ŒæœŸå±¤ã¯é‡è¦
        
        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def get_layer_importance(self, layer_index: int) -> float:
        """å±¤é‡è¦åº¦ã®å–å¾—"""
        return self.layer_importance_cache.get(layer_index, 0.5)
    
    def analyze_attention_pattern(self, attention_weights: torch.Tensor, layer_index: int) -> Dict[str, float]:
        """Attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        # Attentioné‡ã¿ã®çµ±è¨ˆåˆ†æ
        attention_entropy = self._calculate_attention_entropy(attention_weights)
        attention_sparsity = self._calculate_attention_sparsity(attention_weights)
        
        pattern = {
            "entropy": attention_entropy,
            "sparsity": attention_sparsity,
            "max_weight": float(torch.max(attention_weights)),
            "mean_weight": float(torch.mean(attention_weights))
        }
        
        self.attention_patterns[layer_index] = pattern
        return pattern
    
    def _calculate_attention_entropy(self, attention_weights: torch.Tensor) -> float:
        """Attentioné‡ã¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        # æ­£è¦åŒ–ã•ã‚ŒãŸAttentioné‡ã¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        probs = F.softmax(attention_weights.flatten(), dim=0)
        entropy = -torch.sum(probs * torch.log(probs + 1e-8))
        return float(entropy)
    
    def _calculate_attention_sparsity(self, attention_weights: torch.Tensor) -> float:
        """Attentioné‡ã¿ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§è¨ˆç®—"""
        threshold = 0.01
        sparse_ratio = float(torch.sum(attention_weights < threshold)) / attention_weights.numel()
        return sparse_ratio

class RouterAPI:
    """çµ±åˆRouter API"""
    
    def __init__(self, config: Phase2_3SystemConfig, model: nn.Module):
        self.config = config
        self.model = model
        self.layer_analyzer = LayerImportanceAnalyzer(model)
        
        # RouterçŠ¶æ…‹
        self.decision_history = []
        self.performance_history = []
        self.quality_history = []
        
        # é©å¿œçš„é–¾å€¤
        self.adaptive_thresholds = {
            SkipType.LAYER_SKIP: config.layer_skip_threshold,
            SkipType.FFN_SKIP: config.ffn_skip_threshold,
            SkipType.TOKEN_HALTING: config.token_halting_threshold
        }
        
        # çµ±è¨ˆæƒ…å ±
        self.router_stats = {
            "total_decisions": 0,
            "skip_decisions": 0,
            "execute_decisions": 0,
            "partial_decisions": 0,
            "accuracy": 0.0
        }
    
    def make_layer_skip_decision(self, layer_index: int, 
                                hidden_states: torch.Tensor,
                                attention_weights: Optional[torch.Tensor] = None) -> RouterDecision:
        """Layer Skipåˆ¤å®š"""
        
        # å±¤é‡è¦åº¦ã®å–å¾—
        layer_importance = self.layer_analyzer.get_layer_importance(layer_index)
        
        # éš ã‚ŒçŠ¶æ…‹ã®å¤‰åŒ–é‡åˆ†æ
        state_change_magnitude = self._analyze_state_change(hidden_states)
        
        # Attentionãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        attention_score = 1.0
        if attention_weights is not None:
            attention_pattern = self.layer_analyzer.analyze_attention_pattern(attention_weights, layer_index)
            attention_score = 1.0 - attention_pattern["sparsity"]  # ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ãŒé«˜ã„ã»ã©ã‚¹ã‚­ãƒƒãƒ—å€™è£œ
        
        # ç·åˆåˆ¤å®šã‚¹ã‚³ã‚¢
        skip_score = (
            (1.0 - layer_importance) * 0.4 +
            (1.0 - state_change_magnitude) * 0.4 +
            (1.0 - attention_score) * 0.2
        )
        
        # é–¾å€¤ã¨ã®æ¯”è¼ƒ
        threshold = self.adaptive_thresholds[SkipType.LAYER_SKIP]
        
        if skip_score > threshold:
            decision = SkipDecision.SKIP
            reasoning = f"Low importance ({layer_importance:.3f}), low change ({state_change_magnitude:.3f})"
        else:
            decision = SkipDecision.EXECUTE
            reasoning = f"High importance ({layer_importance:.3f}) or significant change ({state_change_magnitude:.3f})"
        
        router_decision = RouterDecision(
            skip_type=SkipType.LAYER_SKIP,
            decision=decision,
            confidence=abs(skip_score - threshold),
            layer_index=layer_index,
            reasoning=reasoning
        )
        
        self._record_decision(router_decision)
        return router_decision
    
    def make_ffn_skip_decision(self, layer_index: int,
                              ffn_input: torch.Tensor,
                              attention_output: torch.Tensor) -> RouterDecision:
        """FFN Skipåˆ¤å®š"""
        
        # FFNå…¥åŠ›ã®åˆ†æ
        input_magnitude = torch.norm(ffn_input).item()
        input_sparsity = self._calculate_tensor_sparsity(ffn_input)
        
        # Attentionå‡ºåŠ›ã¨ã®ç›¸é–¢åˆ†æ
        attention_magnitude = torch.norm(attention_output).item()
        correlation = self._calculate_correlation(ffn_input, attention_output)
        
        # FFNã‚¹ã‚­ãƒƒãƒ—ã‚¹ã‚³ã‚¢
        skip_score = (
            (1.0 - min(1.0, input_magnitude / 10.0)) * 0.3 +  # å…¥åŠ›ã®å¤§ãã•
            input_sparsity * 0.3 +                             # å…¥åŠ›ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§
            (1.0 - abs(correlation)) * 0.4                     # Attentionã¨ã®ç›¸é–¢
        )
        
        threshold = self.adaptive_thresholds[SkipType.FFN_SKIP]
        
        if skip_score > threshold:
            decision = SkipDecision.SKIP
            reasoning = f"Low input magnitude ({input_magnitude:.3f}), high sparsity ({input_sparsity:.3f})"
        else:
            decision = SkipDecision.EXECUTE
            reasoning = f"Significant input ({input_magnitude:.3f}) or strong correlation ({correlation:.3f})"
        
        router_decision = RouterDecision(
            skip_type=SkipType.FFN_SKIP,
            decision=decision,
            confidence=abs(skip_score - threshold),
            layer_index=layer_index,
            reasoning=reasoning
        )
        
        self._record_decision(router_decision)
        return router_decision
    
    def make_token_halting_decision(self, token_index: int,
                                   current_logits: torch.Tensor,
                                   generated_tokens: List[int],
                                   target_length: int) -> RouterDecision:
        """Token Haltingåˆ¤å®š"""
        
        # ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®ä¿¡é ¼åº¦
        token_confidence = self._calculate_token_confidence(current_logits)
        
        # ç”Ÿæˆæ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸€è²«æ€§
        sequence_consistency = self._calculate_sequence_consistency(generated_tokens)
        
        # é•·ã•ã«åŸºã¥ãåœæ­¢åˆ¤å®š
        length_factor = min(1.0, len(generated_tokens) / target_length)
        
        # åœæ­¢ã‚¹ã‚³ã‚¢
        halt_score = (
            token_confidence * 0.5 +
            sequence_consistency * 0.3 +
            length_factor * 0.2
        )
        
        threshold = self.adaptive_thresholds[SkipType.TOKEN_HALTING]
        
        # æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ç¢ºèª
        if len(generated_tokens) < self.config.min_tokens:
            decision = SkipDecision.EXECUTE
            reasoning = f"Below minimum tokens ({len(generated_tokens)} < {self.config.min_tokens})"
        elif halt_score > threshold:
            decision = SkipDecision.SKIP  # ç”Ÿæˆåœæ­¢
            reasoning = f"High confidence ({token_confidence:.3f}), consistent sequence ({sequence_consistency:.3f})"
        else:
            decision = SkipDecision.EXECUTE  # ç”Ÿæˆç¶™ç¶š
            reasoning = f"Low confidence ({token_confidence:.3f}) or inconsistent sequence"
        
        router_decision = RouterDecision(
            skip_type=SkipType.TOKEN_HALTING,
            decision=decision,
            confidence=abs(halt_score - threshold),
            token_index=token_index,
            reasoning=reasoning
        )
        
        self._record_decision(router_decision)
        return router_decision
    
    def _analyze_state_change(self, hidden_states: torch.Tensor) -> float:
        """éš ã‚ŒçŠ¶æ…‹ã®å¤‰åŒ–é‡åˆ†æ"""
        # éš ã‚ŒçŠ¶æ…‹ã®ãƒãƒ«ãƒ 
        state_norm = torch.norm(hidden_states).item()
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸå¤‰åŒ–é‡ï¼ˆ0-1ï¼‰
        normalized_change = min(1.0, state_norm / 100.0)
        return normalized_change
    
    def _calculate_tensor_sparsity(self, tensor: torch.Tensor, threshold: float = 0.01) -> float:
        """ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§è¨ˆç®—"""
        sparse_elements = torch.sum(torch.abs(tensor) < threshold).item()
        total_elements = tensor.numel()
        return sparse_elements / total_elements
    
    def _calculate_correlation(self, tensor1: torch.Tensor, tensor2: torch.Tensor) -> float:
        """ãƒ†ãƒ³ã‚½ãƒ«é–“ã®ç›¸é–¢è¨ˆç®—"""
        # å¹³å¦åŒ–ã—ã¦ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
        flat1 = tensor1.flatten()
        flat2 = tensor2.flatten()
        
        # é•·ã•ã‚’åˆã‚ã›ã‚‹
        min_len = min(len(flat1), len(flat2))
        flat1 = flat1[:min_len]
        flat2 = flat2[:min_len]
        
        # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—
        correlation = torch.corrcoef(torch.stack([flat1, flat2]))[0, 1]
        return float(correlation) if not torch.isnan(correlation) else 0.0
    
    def _calculate_token_confidence(self, logits: torch.Tensor) -> float:
        """ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®ä¿¡é ¼åº¦è¨ˆç®—"""
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡ã®æœ€å¤§å€¤
        probs = F.softmax(logits, dim=-1)
        max_prob = torch.max(probs).item()
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãä¿¡é ¼åº¦
        entropy = -torch.sum(probs * torch.log(probs + 1e-8)).item()
        normalized_entropy = entropy / math.log(probs.size(-1))  # æ­£è¦åŒ–
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        confidence = max_prob * (1.0 - normalized_entropy)
        return confidence
    
    def _calculate_sequence_consistency(self, tokens: List[int]) -> float:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä¸€è²«æ€§è¨ˆç®—"""
        if len(tokens) < 2:
            return 1.0
        
        # éš£æ¥ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤‰åŒ–ç‡
        changes = sum(1 for i in range(1, len(tokens)) if tokens[i] != tokens[i-1])
        change_rate = changes / (len(tokens) - 1)
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆå¤‰åŒ–ç‡ãŒä½ã„ã»ã©ä¸€è²«æ€§ãŒé«˜ã„ï¼‰
        consistency = 1.0 - min(1.0, change_rate)
        return consistency
    
    def _record_decision(self, decision: RouterDecision):
        """åˆ¤å®šè¨˜éŒ²"""
        self.decision_history.append({
            "timestamp": time.time(),
            "decision": decision
        })
        
        # çµ±è¨ˆæ›´æ–°
        self.router_stats["total_decisions"] += 1
        
        if decision.decision == SkipDecision.SKIP:
            self.router_stats["skip_decisions"] += 1
        elif decision.decision == SkipDecision.EXECUTE:
            self.router_stats["execute_decisions"] += 1
        else:
            self.router_stats["partial_decisions"] += 1
    
    def update_adaptive_thresholds(self, performance_feedback: Dict[str, float]):
        """é©å¿œçš„é–¾å€¤ã®æ›´æ–°"""
        if not self.config.adaptive_threshold:
            return
        
        # æ€§èƒ½ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ãé–¾å€¤èª¿æ•´
        performance_score = performance_feedback.get("tokens_per_second", 0.0)
        quality_score = performance_feedback.get("quality_score", 1.0)
        
        # æ€§èƒ½ãŒç›®æ¨™ã‚’ä¸‹å›ã‚‹å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã‚’æ¸›ã‚‰ã™ï¼‰
        if performance_score < self.config.phase2_target_tokens_per_second:
            adjustment = -0.01
        else:
            adjustment = 0.01
        
        # å“è³ªãŒåŠ£åŒ–ã—ã¦ã„ã‚‹å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹
        if quality_score < (1.0 - self.config.max_quality_degradation):
            adjustment = -0.02
        
        # é–¾å€¤ã®æ›´æ–°
        for skip_type in self.adaptive_thresholds:
            old_threshold = self.adaptive_thresholds[skip_type]
            new_threshold = max(0.0, min(1.0, old_threshold + adjustment))
            self.adaptive_thresholds[skip_type] = new_threshold
            
            if abs(new_threshold - old_threshold) > 0.001:
                logger.debug(f"Threshold updated for {skip_type}: {old_threshold:.3f} â†’ {new_threshold:.3f}")
    
    def get_router_statistics(self) -> Dict[str, Any]:
        """Routerçµ±è¨ˆã®å–å¾—"""
        stats = self.router_stats.copy()
        
        # è¿½åŠ çµ±è¨ˆã®è¨ˆç®—
        if stats["total_decisions"] > 0:
            stats["skip_ratio"] = stats["skip_decisions"] / stats["total_decisions"]
            stats["execute_ratio"] = stats["execute_decisions"] / stats["total_decisions"]
        
        stats["adaptive_thresholds"] = self.adaptive_thresholds.copy()
        stats["decision_history_length"] = len(self.decision_history)
        
        return stats

class DynamicSkipEngine:
    """å‹•çš„ã‚¹ã‚­ãƒƒãƒ—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Phase2_3SystemConfig, router_api: RouterAPI):
        self.config = config
        self.router_api = router_api
        
        # ã‚¹ã‚­ãƒƒãƒ—å®Ÿè¡Œçµ±è¨ˆ
        self.skip_metrics = SkipMetrics()
        self.execution_history = []
        
        # å“è³ªç›£è¦–
        self.quality_monitor = QualityMonitor(config)
        
        # æ€§èƒ½æœ€é©åŒ–
        self.performance_optimizer = PerformanceOptimizer(config)
    
    def execute_layer_with_skip(self, layer_module: nn.Module, 
                               layer_index: int,
                               hidden_states: torch.Tensor,
                               attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½ä»˜ãå±¤å®Ÿè¡Œ"""
        
        # Routeråˆ¤å®š
        decision = self.router_api.make_layer_skip_decision(layer_index, hidden_states)
        
        if decision.decision == SkipDecision.SKIP:
            # å±¤ã‚’ã‚¹ã‚­ãƒƒãƒ—
            self.skip_metrics.layer_skips += 1
            logger.debug(f"Layer {layer_index} skipped: {decision.reasoning}")
            return hidden_states  # å…¥åŠ›ã‚’ãã®ã¾ã¾è¿”ã™
        
        elif decision.decision == SkipDecision.PARTIAL:
            # éƒ¨åˆ†å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ï¼‰
            output = self._execute_partial_layer(layer_module, hidden_states, attention_mask)
            logger.debug(f"Layer {layer_index} partially executed")
            return output
        
        else:
            # é€šå¸¸å®Ÿè¡Œ
            output = layer_module(hidden_states, attention_mask=attention_mask)
            if isinstance(output, tuple):
                output = output[0]  # hidden_statesã®ã¿å–å¾—
            return output
    
    def execute_ffn_with_skip(self, ffn_module: nn.Module,
                             layer_index: int,
                             ffn_input: torch.Tensor,
                             attention_output: torch.Tensor) -> torch.Tensor:
        """ã‚¹ã‚­ãƒƒãƒ—æ©Ÿèƒ½ä»˜ãFFNå®Ÿè¡Œ"""
        
        # Routeråˆ¤å®š
        decision = self.router_api.make_ffn_skip_decision(layer_index, ffn_input, attention_output)
        
        if decision.decision == SkipDecision.SKIP:
            # FFNã‚’ã‚¹ã‚­ãƒƒãƒ—
            self.skip_metrics.ffn_skips += 1
            logger.debug(f"FFN {layer_index} skipped: {decision.reasoning}")
            return ffn_input  # å…¥åŠ›ã‚’ãã®ã¾ã¾è¿”ã™
        
        elif decision.decision == SkipDecision.PARTIAL:
            # éƒ¨åˆ†å®Ÿè¡Œ
            output = self._execute_partial_ffn(ffn_module, ffn_input)
            logger.debug(f"FFN {layer_index} partially executed")
            return output
        
        else:
            # é€šå¸¸å®Ÿè¡Œ
            return ffn_module(ffn_input)
    
    def execute_generation_with_halting(self, model: nn.Module,
                                      tokenizer,
                                      input_ids: torch.Tensor,
                                      max_tokens: int) -> Tuple[torch.Tensor, List[RouterDecision]]:
        """Token Haltingæ©Ÿèƒ½ä»˜ãç”Ÿæˆ"""
        
        generated_tokens = input_ids[0].tolist()
        halting_decisions = []
        
        for step in range(max_tokens):
            # ç¾åœ¨ã®å…¥åŠ›ã§æ¨è«–
            with torch.no_grad():
                outputs = model(torch.tensor([generated_tokens]).to(input_ids.device))
                logits = outputs.logits[0, -1, :]
            
            # Token Haltingåˆ¤å®š
            decision = self.router_api.make_token_halting_decision(
                step, logits, generated_tokens, max_tokens
            )
            halting_decisions.append(decision)
            
            if decision.decision == SkipDecision.SKIP:
                # ç”Ÿæˆåœæ­¢
                self.skip_metrics.token_halts += 1
                logger.debug(f"Token generation halted at step {step}: {decision.reasoning}")
                break
            
            # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ
            next_token = torch.multinomial(F.softmax(logits, dim=-1), 1).item()
            generated_tokens.append(next_token)
            
            # EOS ãƒˆãƒ¼ã‚¯ãƒ³ã§åœæ­¢
            if next_token == tokenizer.eos_token_id:
                break
        
        return torch.tensor([generated_tokens]), halting_decisions
    
    def _execute_partial_layer(self, layer_module: nn.Module,
                              hidden_states: torch.Tensor,
                              attention_mask: Optional[torch.Tensor]) -> torch.Tensor:
        """å±¤ã®éƒ¨åˆ†å®Ÿè¡Œ"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå±¤å®Ÿè¡Œï¼ˆä¾‹ï¼šAttentionã®ã¿å®Ÿè¡Œã€FFNã‚¹ã‚­ãƒƒãƒ—ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å±¤ã®æ§‹é€ ã«å¿œã˜ã¦é©åˆ‡ãªéƒ¨åˆ†å®Ÿè¡Œã‚’è¡Œã†
        
        # ã“ã“ã§ã¯ç°¡å˜ãªç·šå½¢å¤‰æ›ã®ã¿å®Ÿè¡Œ
        if hasattr(layer_module, 'attention'):
            attention_output = layer_module.attention(hidden_states, attention_mask=attention_mask)
            if isinstance(attention_output, tuple):
                attention_output = attention_output[0]
            return attention_output
        
        return hidden_states
    
    def _execute_partial_ffn(self, ffn_module: nn.Module, ffn_input: torch.Tensor) -> torch.Tensor:
        """FFNã®éƒ¨åˆ†å®Ÿè¡Œ"""
        # FFNã®æœ€åˆã®å±¤ã®ã¿å®Ÿè¡Œï¼ˆä¾‹ï¼‰
        if hasattr(ffn_module, 'dense') or hasattr(ffn_module, 'fc1'):
            first_layer = getattr(ffn_module, 'dense', getattr(ffn_module, 'fc1', None))
            if first_layer:
                return first_layer(ffn_input)
        
        return ffn_input
    
    def update_skip_metrics(self):
        """ã‚¹ã‚­ãƒƒãƒ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°"""
        if self.skip_metrics.total_layers > 0:
            self.skip_metrics.skip_ratio_layer = self.skip_metrics.layer_skips / self.skip_metrics.total_layers
        
        if self.skip_metrics.total_ffns > 0:
            self.skip_metrics.skip_ratio_ffn = self.skip_metrics.ffn_skips / self.skip_metrics.total_ffns
        
        if self.skip_metrics.total_tokens > 0:
            self.skip_metrics.halt_ratio_token = self.skip_metrics.token_halts / self.skip_metrics.total_tokens
    
    def get_skip_statistics(self) -> Dict[str, Any]:
        """ã‚¹ã‚­ãƒƒãƒ—çµ±è¨ˆã®å–å¾—"""
        self.update_skip_metrics()
        
        return {
            "layer_skips": self.skip_metrics.layer_skips,
            "ffn_skips": self.skip_metrics.ffn_skips,
            "token_halts": self.skip_metrics.token_halts,
            "skip_ratio_layer": self.skip_metrics.skip_ratio_layer,
            "skip_ratio_ffn": self.skip_metrics.skip_ratio_ffn,
            "halt_ratio_token": self.skip_metrics.halt_ratio_token,
            "quality_score": self.skip_metrics.quality_score,
            "performance_gain": self.skip_metrics.performance_gain
        }

class QualityMonitor:
    """å“è³ªç›£è¦–æ©Ÿæ§‹"""
    
    def __init__(self, config: Phase2_3SystemConfig):
        self.config = config
        self.quality_history = []
        self.baseline_quality = 1.0
        
    def evaluate_quality(self, original_output: str, optimized_output: str) -> float:
        """å“è³ªè©•ä¾¡"""
        # ç°¡å˜ãªå“è³ªè©•ä¾¡ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚ˆã‚Šé«˜åº¦ãªè©•ä¾¡æ‰‹æ³•ã‚’ä½¿ç”¨ï¼‰
        
        # é•·ã•ã®é¡ä¼¼æ€§
        length_similarity = min(len(optimized_output), len(original_output)) / max(len(optimized_output), len(original_output))
        
        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®é¡ä¼¼æ€§
        char_similarity = self._calculate_char_similarity(original_output, optimized_output)
        
        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        quality_score = (length_similarity * 0.3 + char_similarity * 0.7)
        
        self.quality_history.append({
            "timestamp": time.time(),
            "quality_score": quality_score,
            "length_similarity": length_similarity,
            "char_similarity": char_similarity
        })
        
        return quality_score
    
    def _calculate_char_similarity(self, text1: str, text2: str) -> float:
        """æ–‡å­—ãƒ¬ãƒ™ãƒ«é¡ä¼¼æ€§ã®è¨ˆç®—"""
        # ç°¡å˜ãªãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§
        if not text1 and not text2:
            return 1.0
        if not text1 or not text2:
            return 0.0
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸé¡ä¼¼æ€§è¨ˆç®—
        common_chars = set(text1) & set(text2)
        total_chars = set(text1) | set(text2)
        
        if not total_chars:
            return 1.0
        
        return len(common_chars) / len(total_chars)
    
    def is_quality_acceptable(self) -> bool:
        """å“è³ªè¨±å®¹æ€§ã®åˆ¤å®š"""
        if not self.quality_history:
            return True
        
        recent_quality = self.quality_history[-1]["quality_score"]
        degradation = self.baseline_quality - recent_quality
        
        return degradation <= self.config.max_quality_degradation
    
    def get_quality_statistics(self) -> Dict[str, float]:
        """å“è³ªçµ±è¨ˆã®å–å¾—"""
        if not self.quality_history:
            return {"average_quality": 1.0, "quality_degradation": 0.0}
        
        recent_scores = [entry["quality_score"] for entry in self.quality_history[-10:]]
        average_quality = sum(recent_scores) / len(recent_scores)
        quality_degradation = max(0.0, self.baseline_quality - average_quality)
        
        return {
            "average_quality": average_quality,
            "quality_degradation": quality_degradation,
            "quality_samples": len(self.quality_history)
        }

class PerformanceOptimizer:
    """æ€§èƒ½æœ€é©åŒ–æ©Ÿæ§‹"""
    
    def __init__(self, config: Phase2_3SystemConfig):
        self.config = config
        self.performance_history = []
        self.optimization_strategies = []
        
    def optimize_skip_parameters(self, current_performance: float, target_performance: float):
        """ã‚¹ã‚­ãƒƒãƒ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–"""
        performance_gap = target_performance - current_performance
        
        if performance_gap > 0:
            # æ€§èƒ½å‘ä¸ŠãŒå¿…è¦ï¼šã‚ˆã‚Šç©æ¥µçš„ãªã‚¹ã‚­ãƒƒãƒ—
            optimization = {
                "layer_skip_threshold": max(0.05, self.config.layer_skip_threshold - 0.02),
                "ffn_skip_threshold": max(0.1, self.config.ffn_skip_threshold - 0.02),
                "token_halting_threshold": min(0.98, self.config.token_halting_threshold + 0.01)
            }
        else:
            # æ€§èƒ½ãŒååˆ†ï¼šå“è³ªé‡è¦–
            optimization = {
                "layer_skip_threshold": min(0.2, self.config.layer_skip_threshold + 0.01),
                "ffn_skip_threshold": min(0.25, self.config.ffn_skip_threshold + 0.01),
                "token_halting_threshold": max(0.9, self.config.token_halting_threshold - 0.01)
            }
        
        self.optimization_strategies.append({
            "timestamp": time.time(),
            "performance_gap": performance_gap,
            "optimization": optimization
        })
        
        return optimization

class FlexGenPlusPlusPhase2_3(FlexGenPlusPlusPhase1):
    """FlexGen++ Phase 2-3 å®Ÿè£…"""
    
    def __init__(self, config: Phase2_3SystemConfig):
        # Phase 1 ã®åˆæœŸåŒ–
        super().__init__(config)
        
        # Phase 2-3 å›ºæœ‰ã®è¨­å®š
        self.config = config
        self.router_api = RouterAPI(config, self.model)
        self.skip_engine = DynamicSkipEngine(config, self.router_api)
        
        # Phase 2-3 ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.phase2_3_metrics = {
            "router_decisions": 0,
            "skip_efficiency": 0.0,
            "quality_maintenance": 1.0,
            "adaptive_optimizations": 0
        }
        
        # æœ€é©åŒ–ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.optimization_counter = 0
    
    def initialize_model(self) -> bool:
        """Phase 2-3 ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        logger.info("Initializing FlexGen++ Phase 2-3...")
        
        # Phase 1 ã®åˆæœŸåŒ–
        if not super().initialize_model():
            return False
        
        # Router API ã®åˆæœŸåŒ–
        self._initialize_router_optimizations()
        
        logger.info("FlexGen++ Phase 2-3 initialization completed")
        return True
    
    def _initialize_router_optimizations(self):
        """Routeræœ€é©åŒ–ã®åˆæœŸåŒ–"""
        logger.info("Initializing Router API optimizations...")
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®åˆ†æ
        self._analyze_model_structure()
        
        # æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®š
        self._identify_optimization_points()
        
        logger.info("Router API optimizations initialized")
    
    def _analyze_model_structure(self):
        """ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®åˆ†æ"""
        layer_count = 0
        ffn_count = 0
        
        for name, module in self.model.named_modules():
            if "layer" in name.lower() and hasattr(module, 'attention'):
                layer_count += 1
            if "mlp" in name.lower() or "ffn" in name.lower():
                ffn_count += 1
        
        self.skip_engine.skip_metrics.total_layers = layer_count
        self.skip_engine.skip_metrics.total_ffns = ffn_count
        
        logger.info(f"Model structure: {layer_count} layers, {ffn_count} FFNs")
    
    def _identify_optimization_points(self):
        """æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®š"""
        # é‡è¦åº¦ã®ä½ã„å±¤ã‚’ç‰¹å®š
        low_importance_layers = []
        
        for layer_idx in range(self.skip_engine.skip_metrics.total_layers):
            importance = self.router_api.layer_analyzer.get_layer_importance(layer_idx)
            if importance < 0.3:  # é‡è¦åº¦30%æœªæº€
                low_importance_layers.append(layer_idx)
        
        logger.info(f"Identified {len(low_importance_layers)} low-importance layers for optimization")
    
    def execute_inference(self, prompt: str, max_tokens: int = 100) -> Tuple[str, PerformanceMetrics]:
        """Phase 2-3 æ¨è«–å®Ÿè¡Œ"""
        start_time = time.time()
        
        # å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(prompt, return_tensors="pt", max_length=self.config.max_sequence_length, truncation=True)
        input_length = len(inputs["input_ids"][0])
        
        # Router API ã‚’ä½¿ç”¨ã—ãŸæœ€é©åŒ–æ¨è«–
        outputs = self._execute_optimized_inference(inputs, max_tokens)
        
        # çµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_tokens = len(outputs[0]) - input_length
        
        end_time = time.time()
        inference_time = end_time - start_time
        
        # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        tokens_per_second = generated_tokens / inference_time if inference_time > 0 else 0
        
        metrics = PerformanceMetrics(
            tokens_per_second=tokens_per_second,
            latency_ms=inference_time * 1000,
            memory_usage_gb=self._get_memory_usage(),
            gpu_utilization=self._get_gpu_utilization(),
            throughput_efficiency=tokens_per_second / self.config.phase3_target_tokens_per_second
        )
        
        # Phase 2-3 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°
        self._update_phase2_3_metrics(metrics)
        
        # é©å¿œçš„æœ€é©åŒ–
        self._perform_adaptive_optimization(metrics)
        
        logger.info(f"Phase 2-3 inference: {tokens_per_second:.1f} tok/s")
        
        return response, metrics
    
    def _execute_optimized_inference(self, inputs: Dict[str, torch.Tensor], max_tokens: int) -> torch.Tensor:
        """æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–å®Ÿè¡Œ"""
        
        if self.config.enable_token_halting:
            # Token Haltingæ©Ÿèƒ½ä»˜ãç”Ÿæˆ
            outputs, halting_decisions = self.skip_engine.execute_generation_with_halting(
                self.model, self.tokenizer, inputs["input_ids"], max_tokens
            )
            
            # Haltingæ±ºå®šã®è¨˜éŒ²
            self.phase2_3_metrics["router_decisions"] += len(halting_decisions)
            
        else:
            # é€šå¸¸ã®ç”Ÿæˆï¼ˆLayer Skipã€FFN Skipã®ã¿ï¼‰
            with torch.no_grad():
                outputs = self._generate_with_layer_ffn_skip(inputs, max_tokens)
        
        return outputs
    
    def _generate_with_layer_ffn_skip(self, inputs: Dict[str, torch.Tensor], max_tokens: int) -> torch.Tensor:
        """Layer Skipã€FFN Skipæ©Ÿèƒ½ä»˜ãç”Ÿæˆ"""
        
        # ã‚«ã‚¹ã‚¿ãƒ ç”Ÿæˆãƒ«ãƒ¼ãƒ—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        input_ids = inputs["input_ids"]
        
        for step in range(max_tokens):
            # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
            outputs = self._custom_forward_pass(input_ids)
            
            # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ
            next_token_logits = outputs[:, -1, :]
            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), 1)
            
            # å…¥åŠ›ã«è¿½åŠ 
            input_ids = torch.cat([input_ids, next_token], dim=-1)
            
            # EOS ãƒˆãƒ¼ã‚¯ãƒ³ã§åœæ­¢
            if next_token.item() == self.tokenizer.eos_token_id:
                break
        
        return input_ids
    
    def _custom_forward_pass(self, input_ids: torch.Tensor) -> torch.Tensor:
        """ã‚«ã‚¹ã‚¿ãƒ é †ä¼æ’­ï¼ˆSkipæ©Ÿèƒ½ä»˜ãï¼‰"""
        
        # åŸ‹ã‚è¾¼ã¿å±¤
        hidden_states = self.model.get_input_embeddings()(input_ids)
        
        # Transformerå±¤ã®å‡¦ç†
        for layer_idx, layer in enumerate(self.model.transformer.h if hasattr(self.model, 'transformer') else []):
            
            if self.config.enable_layer_skip:
                # Layer Skipåˆ¤å®šã¨å®Ÿè¡Œ
                hidden_states = self.skip_engine.execute_layer_with_skip(
                    layer, layer_idx, hidden_states
                )
            else:
                # é€šå¸¸å®Ÿè¡Œ
                layer_output = layer(hidden_states)
                if isinstance(layer_output, tuple):
                    hidden_states = layer_output[0]
                else:
                    hidden_states = layer_output
        
        # æœ€çµ‚å±¤ã®æ­£è¦åŒ–
        if hasattr(self.model, 'ln_f'):
            hidden_states = self.model.ln_f(hidden_states)
        
        # è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰
        logits = self.model.lm_head(hidden_states)
        
        return logits
    
    def _update_phase2_3_metrics(self, metrics: PerformanceMetrics):
        """Phase 2-3 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°"""
        
        # ã‚¹ã‚­ãƒƒãƒ—åŠ¹ç‡ã®è¨ˆç®—
        skip_stats = self.skip_engine.get_skip_statistics()
        self.phase2_3_metrics["skip_efficiency"] = (
            skip_stats["skip_ratio_layer"] * 0.4 +
            skip_stats["skip_ratio_ffn"] * 0.4 +
            skip_stats["halt_ratio_token"] * 0.2
        )
        
        # å“è³ªç¶­æŒã®è©•ä¾¡
        quality_stats = self.skip_engine.quality_monitor.get_quality_statistics()
        self.phase2_3_metrics["quality_maintenance"] = quality_stats["average_quality"]
        
        # Routeræ±ºå®šæ•°ã®æ›´æ–°
        router_stats = self.router_api.get_router_statistics()
        self.phase2_3_metrics["router_decisions"] = router_stats["total_decisions"]
    
    def _perform_adaptive_optimization(self, metrics: PerformanceMetrics):
        """é©å¿œçš„æœ€é©åŒ–ã®å®Ÿè¡Œ"""
        self.optimization_counter += 1
        
        # å®šæœŸçš„ãªæœ€é©åŒ–
        if self.optimization_counter % self.config.router_update_frequency == 0:
            
            # æ€§èƒ½ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            performance_feedback = {
                "tokens_per_second": metrics.tokens_per_second,
                "quality_score": self.phase2_3_metrics["quality_maintenance"]
            }
            
            # Routeré–¾å€¤ã®é©å¿œçš„æ›´æ–°
            self.router_api.update_adaptive_thresholds(performance_feedback)
            
            # æ€§èƒ½æœ€é©åŒ–
            target_performance = self.config.phase3_target_tokens_per_second
            optimization = self.skip_engine.performance_optimizer.optimize_skip_parameters(
                metrics.tokens_per_second, target_performance
            )
            
            self.phase2_3_metrics["adaptive_optimizations"] += 1
            
            logger.debug(f"Adaptive optimization performed: {optimization}")
    
    def get_phase2_3_performance_summary(self) -> Dict[str, Any]:
        """Phase 2-3 æ€§èƒ½ã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        base_summary = super().get_phase1_performance_summary()
        
        if "error" in base_summary:
            return base_summary
        
        # Phase 2-3 å›ºæœ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ 
        phase2_3_summary = base_summary.copy()
        phase2_3_summary.update({
            "phase2_3_metrics": self.phase2_3_metrics,
            "skip_statistics": self.skip_engine.get_skip_statistics(),
            "router_statistics": self.router_api.get_router_statistics(),
            "quality_statistics": self.skip_engine.quality_monitor.get_quality_statistics(),
            "improvement_over_phase1": phase2_3_summary["average_tokens_per_second"] / 13.5,
            "phase2_target_achievement": phase2_3_summary["average_tokens_per_second"] / self.config.phase2_target_tokens_per_second,
            "phase3_target_achievement": phase2_3_summary["average_tokens_per_second"] / self.config.phase3_target_tokens_per_second
        })
        
        return phase2_3_summary

def run_phase2_3_benchmark():
    """Phase 2-3 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""
    logger.info("Starting Phase 2-3 benchmark...")
    
    # Phase 2-3 ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
    config = Phase2_3SystemConfig()
    
    # FlexGen++ Phase 2-3 ã®åˆæœŸåŒ–
    flexgen = FlexGenPlusPlusPhase2_3(config)
    
    if not flexgen.initialize_model():
        logger.error("Phase 2-3 system initialization failed")
        return
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ˆã‚Šè¤‡é›‘ã§å¤šæ§˜ï¼‰
    test_prompts = [
        "Develop a comprehensive machine learning pipeline for natural language processing tasks, including data preprocessing, model selection, training, and evaluation phases.",
        "Explain the mathematical foundations of transformer architectures, including self-attention mechanisms, positional encoding, and multi-head attention computations.",
        "Create a detailed software architecture design for a distributed AI inference system that can handle millions of requests per day with low latency requirements.",
        "äººå·¥çŸ¥èƒ½æŠ€è¡“ã®ç™ºå±•ãŒåŒ»ç™‚åˆ†é‡ã«ä¸ãˆã‚‹é©æ–°çš„ãªå½±éŸ¿ã«ã¤ã„ã¦ã€å…·ä½“çš„ãªå¿œç”¨ä¾‹ã¨å°†æ¥ã®å¯èƒ½æ€§ã‚’å«ã‚ã¦è©³ç´°ã«åˆ†æã—ã¦ãã ã•ã„ã€‚",
        "Design and implement a robust error handling and monitoring system for a production-level machine learning service, considering scalability, reliability, and observability requirements."
    ]
    
    logger.info("Executing Phase 2-3 benchmark tests...")
    
    # Phase 2 ãƒ†ã‚¹ãƒˆï¼ˆLayer Skipä¸­å¿ƒï¼‰
    logger.info("=== Phase 2 Testing (Layer Skip) ===")
    config.enable_layer_skip = True
    config.enable_ffn_skip = False
    config.enable_token_halting = False
    
    phase2_metrics = []
    for i, prompt in enumerate(test_prompts):
        logger.info(f"Phase 2 Test {i+1}/{len(test_prompts)}")
        
        try:
            response, metrics = flexgen.execute_inference(prompt, max_tokens=200)
            phase2_metrics.append(metrics)
            logger.info(f"Phase 2 Performance: {metrics.tokens_per_second:.1f} tok/s")
            
        except Exception as e:
            logger.error(f"Phase 2 Test {i+1} failed: {e}")
    
    # Phase 3 ãƒ†ã‚¹ãƒˆï¼ˆå…¨æ©Ÿèƒ½æœ‰åŠ¹ï¼‰
    logger.info("=== Phase 3 Testing (All Features) ===")
    config.enable_layer_skip = True
    config.enable_ffn_skip = True
    config.enable_token_halting = True
    
    phase3_metrics = []
    for i, prompt in enumerate(test_prompts):
        logger.info(f"Phase 3 Test {i+1}/{len(test_prompts)}")
        
        try:
            response, metrics = flexgen.execute_inference(prompt, max_tokens=200)
            phase3_metrics.append(metrics)
            logger.info(f"Phase 3 Performance: {metrics.tokens_per_second:.1f} tok/s")
            
        except Exception as e:
            logger.error(f"Phase 3 Test {i+1} failed: {e}")
    
    # çµæœã®é›†è¨ˆã¨è¡¨ç¤º
    logger.info("=== Phase 2-3 Benchmark Results ===")
    
    if phase2_metrics:
        phase2_avg = sum(m.tokens_per_second for m in phase2_metrics) / len(phase2_metrics)
        logger.info(f"Phase 2 Average: {phase2_avg:.1f} tok/s")
        logger.info(f"Phase 2 Target Achievement: {phase2_avg/config.phase2_target_tokens_per_second:.1%}")
        
        if phase2_avg >= config.phase2_target_tokens_per_second:
            logger.info("âœ… Phase 2 target achieved!")
        else:
            gap = config.phase2_target_tokens_per_second - phase2_avg
            logger.warning(f"âš ï¸ Phase 2 target not achieved. Gap: {gap:.1f} tok/s")
    
    if phase3_metrics:
        phase3_avg = sum(m.tokens_per_second for m in phase3_metrics) / len(phase3_metrics)
        logger.info(f"Phase 3 Average: {phase3_avg:.1f} tok/s")
        logger.info(f"Phase 3 Target Achievement: {phase3_avg/config.phase3_target_tokens_per_second:.1%}")
        
        if phase3_avg >= config.phase3_target_tokens_per_second:
            logger.info("âœ… Phase 3 target achieved!")
        else:
            gap = config.phase3_target_tokens_per_second - phase3_avg
            logger.warning(f"âš ï¸ Phase 3 target not achieved. Gap: {gap:.1f} tok/s")
    
    # æ”¹å–„ç‡ã®è¡¨ç¤º
    if phase2_metrics and phase3_metrics:
        phase1_baseline = 13.5  # Phase 1 ç›®æ¨™
        phase2_improvement = phase2_avg / phase1_baseline
        phase3_improvement = phase3_avg / phase1_baseline
        
        logger.info(f"Phase 2 improvement over Phase 1: {phase2_improvement:.1%}")
        logger.info(f"Phase 3 improvement over Phase 1: {phase3_improvement:.1%}")
    
    # è©³ç´°çµ±è¨ˆã®è¡¨ç¤º
    performance_summary = flexgen.get_phase2_3_performance_summary()
    logger.info("Phase 2-3 System Statistics:")
    logger.info(f"  Router decisions: {performance_summary['phase2_3_metrics']['router_decisions']}")
    logger.info(f"  Skip efficiency: {performance_summary['phase2_3_metrics']['skip_efficiency']:.1%}")
    logger.info(f"  Quality maintenance: {performance_summary['phase2_3_metrics']['quality_maintenance']:.1%}")
    logger.info(f"  Adaptive optimizations: {performance_summary['phase2_3_metrics']['adaptive_optimizations']}")
    
    logger.info("Phase 2-3 benchmark completed")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Phase 2-3: Router APIå®Ÿè£…")
    print("=" * 50)
    
    try:
        run_phase2_3_benchmark()
    except KeyboardInterrupt:
        logger.info("Phase 2-3 benchmark interrupted by user")
    except Exception as e:
        logger.error(f"Phase 2-3 benchmark failed: {e}")
        raise

if __name__ == "__main__":
    main()

