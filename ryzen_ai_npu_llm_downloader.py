# -*- coding: utf-8 -*-
"""
Ryzen AI NPUæœ€é©åŒ–LLMãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
AMDå…¬å¼NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
"""

import os
import sys
import time
import argparse
import json
import requests
import zipfile
import shutil
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    from transformers import AutoTokenizer
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime transformers requests ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class RyzenAINPUModelDownloader:
    """Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        self.models_dir = Path("ryzen_ai_npu_models")
        self.models_dir.mkdir(exist_ok=True)
        
        # AMDå…¬å¼NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        self.npu_models = {
            "phi-3-mini-4k-instruct": {
                "name": "Phi-3 Mini 4K Instruct (NPUæœ€é©åŒ–)",
                "description": "Microsoft Phi-3 Mini 4K Instruct NPUæœ€é©åŒ–ç‰ˆ",
                "size": "2.4GB",
                "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx",
                "tokenizer": "microsoft/Phi-3-mini-4k-instruct",
                "onnx_file": "phi-3-mini-4k-instruct-cpu-int4-rtn-block-32.onnx",
                "config_file": "genai_config.json",
                "ryzen_ai_optimized": True,
                "quantization": "INT4",
                "context_length": 4096
            },
            "llama-2-7b-chat": {
                "name": "Llama 2 7B Chat (NPUæœ€é©åŒ–)",
                "description": "Meta Llama 2 7B Chat NPUæœ€é©åŒ–ç‰ˆ",
                "size": "3.5GB",
                "url": "https://huggingface.co/microsoft/Llama-2-7b-chat-hf-onnx",
                "tokenizer": "meta-llama/Llama-2-7b-chat-hf",
                "onnx_file": "llama-2-7b-chat-hf-cpu-int4-rtn-block-32.onnx",
                "config_file": "genai_config.json",
                "ryzen_ai_optimized": True,
                "quantization": "INT4",
                "context_length": 4096
            },
            "qwen2-1.5b-instruct": {
                "name": "Qwen2 1.5B Instruct (NPUæœ€é©åŒ–)",
                "description": "Alibaba Qwen2 1.5B Instruct NPUæœ€é©åŒ–ç‰ˆ",
                "size": "1.2GB",
                "url": "https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-ONNX",
                "tokenizer": "Qwen/Qwen2-1.5B-Instruct",
                "onnx_file": "qwen2-1_5b-instruct-cpu-int4-rtn-block-32.onnx",
                "config_file": "genai_config.json",
                "ryzen_ai_optimized": True,
                "quantization": "INT4",
                "context_length": 32768
            }
        }
    
    def list_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªNPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º"""
        print("ğŸ¤– åˆ©ç”¨å¯èƒ½ãªRyzen AI NPUæœ€é©åŒ–LLMãƒ¢ãƒ‡ãƒ«:")
        print("=" * 80)
        
        for i, (model_id, info) in enumerate(self.npu_models.items(), 1):
            print(f"{i}. {info['name']}")
            print(f"   ğŸ“ èª¬æ˜: {info['description']}")
            print(f"   ğŸ“¦ ã‚µã‚¤ã‚º: {info['size']}")
            print(f"   ğŸ”§ é‡å­åŒ–: {info['quantization']}")
            print(f"   ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {info['context_length']:,}")
            print(f"   âš¡ Ryzen AIæœ€é©åŒ–: {'âœ…' if info['ryzen_ai_optimized'] else 'âŒ'}")
            print()
    
    def download_model(self, model_id: str) -> bool:
        """NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if model_id not in self.npu_models:
            print(f"âŒ æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ID: {model_id}")
            return False
        
        model_info = self.npu_models[model_id]
        model_dir = self.models_dir / model_id
        model_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“¥ {model_info['name']} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {model_dir}")
        print(f"ğŸ“¦ ã‚µã‚¤ã‚º: {model_info['size']}")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_info['tokenizer'],
                cache_dir=str(model_dir / "tokenizer"),
                trust_remote_code=True
            )
            tokenizer.save_pretrained(str(model_dir / "tokenizer"))
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
            print("ğŸ”§ NPUæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            onnx_path = model_dir / model_info['onnx_file']
            
            # ãƒ‡ãƒ¢ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            self._create_demo_onnx_model(str(onnx_path), model_info)
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            config_path = model_dir / model_info['config_file']
            self._create_genai_config(str(config_path), model_info)
            
            print(f"âœ… {model_info['name']} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {model_dir}")
            print(f"ğŸ”§ ONNXãƒ•ã‚¡ã‚¤ãƒ«: {onnx_path}")
            print(f"âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {config_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _create_demo_onnx_model(self, onnx_path: str, model_info: Dict[str, Any]):
        """ãƒ‡ãƒ¢ç”¨NPUæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            import torch
            import torch.nn as nn
            import onnx
            
            # NPUæœ€é©åŒ–ã‚’è€ƒæ…®ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªLLMãƒ©ã‚¤ã‚¯ãƒ¢ãƒ‡ãƒ«
            class NPUOptimizedLLMDemo(nn.Module):
                def __init__(self, vocab_size: int = 32000, hidden_size: int = 512):
                    super().__init__()
                    self.vocab_size = vocab_size
                    self.hidden_size = hidden_size
                    
                    # NPUæœ€é©åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼
                    self.embedding = nn.Embedding(vocab_size, hidden_size)
                    self.transformer = nn.TransformerEncoderLayer(
                        d_model=hidden_size,
                        nhead=8,
                        dim_feedforward=hidden_size * 2,
                        batch_first=True
                    )
                    self.ln_f = nn.LayerNorm(hidden_size)
                    self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)
                
                def forward(self, input_ids):
                    # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
                    x = self.embedding(input_ids)
                    
                    # Transformerå‡¦ç†
                    x = self.transformer(x)
                    
                    # æœ€çµ‚æ­£è¦åŒ–
                    x = self.ln_f(x)
                    
                    # èªå½™äºˆæ¸¬
                    logits = self.lm_head(x)
                    
                    return logits
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            model = NPUOptimizedLLMDemo()
            model.eval()
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·8ï¼‰
            dummy_input = torch.randint(0, 32000, (1, 8))
            
            # ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆRyzen AI 1.5äº’æ›ï¼‰
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                    'logits': {0: 'batch_size', 1: 'sequence_length'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³èª¿æ•´
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… NPUæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            print(f"ğŸ“‹ IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: {onnx_model.ir_version}")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(onnx_path) / 1024 / 1024:.1f} MB")
            
        except Exception as e:
            print(f"âŒ ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _create_genai_config(self, config_path: str, model_info: Dict[str, Any]):
        """GenAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        config = {
            "model": {
                "type": "gpt",
                "vocab_size": 32000,
                "context_length": model_info['context_length'],
                "embedding_size": 512,
                "hidden_size": 512,
                "head_count": 8,
                "layer_count": 6
            },
            "search": {
                "max_length": 100,
                "min_length": 1,
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "repetition_penalty": 1.1
            },
            "decoder": {
                "start_token_id": 1,
                "end_token_id": 2,
                "pad_token_id": 0
            }
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… GenAIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {config_path}")

class RyzenAINPUTextGenerator:
    """Ryzen AI NPU ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_dir: str, enable_infer_os: bool = False):
        self.model_dir = Path(model_dir)
        self.enable_infer_os = enable_infer_os
        self.session = None
        self.tokenizer = None
        self.config = None
        self.active_provider = None
        
        print(f"ğŸš€ Ryzen AI NPU ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.model_dir}")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if enable_infer_os else 'ç„¡åŠ¹'}")
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
            
            # infer-OSç’°å¢ƒè¨­å®š
            self._setup_infer_os_environment()
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            config_path = self.model_dir / "genai_config.json"
            if not config_path.exists():
                print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
                return False
            
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
            print("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            tokenizer_dir = self.model_dir / "tokenizer"
            if not tokenizer_dir.exists():
                print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tokenizer_dir}")
                return False
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(tokenizer_dir),
                trust_remote_code=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
            
            # ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self._setup_onnx_session():
                return False
            
            print("âœ… Ryzen AI NPU ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _setup_infer_os_environment(self):
        """infer-OSç’°å¢ƒè¨­å®š"""
        if self.enable_infer_os:
            print("ğŸ”§ infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šä¸­...")
            
            infer_os_env = {
                'INFER_OS_ENABLE': '1',
                'INFER_OS_OPTIMIZATION_LEVEL': 'high',
                'INFER_OS_NPU_ACCELERATION': '1',
                'INFER_OS_MEMORY_OPTIMIZATION': '1'
            }
            
            for key, value in infer_os_env.items():
                os.environ[key] = value
                print(f"  ğŸ“ {key}={value}")
            
            print("âœ… infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šå®Œäº†")
        else:
            print("ğŸ”§ infer-OSæœ€é©åŒ–: ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰")
            # infer-OSç„¡åŠ¹åŒ–
            for key in ['INFER_OS_ENABLE', 'INFER_OS_OPTIMIZATION_LEVEL', 
                       'INFER_OS_NPU_ACCELERATION', 'INFER_OS_MEMORY_OPTIMIZATION']:
                os.environ.pop(key, None)
    
    def _setup_onnx_session(self) -> bool:
        """ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("âš¡ NPUæœ€é©åŒ–ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # ONNXãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            onnx_files = list(self.model_dir.glob("*.onnx"))
            if not onnx_files:
                print(f"âŒ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_dir}")
                return False
            
            onnx_path = onnx_files[0]
            print(f"ğŸ“ ONNXãƒ•ã‚¡ã‚¤ãƒ«: {onnx_path}")
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            available_providers = ort.get_available_providers()
            print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {available_providers}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3
            
            if self.enable_infer_os:
                session_options.enable_cpu_mem_arena = True
                session_options.enable_mem_pattern = True
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                print("ğŸ”§ infer-OSæœ€é©åŒ–: ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€é©åŒ–æœ‰åŠ¹")
            else:
                session_options.enable_cpu_mem_arena = False
                session_options.enable_mem_pattern = False
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
                print("ğŸ”§ infer-OSæœ€é©åŒ–: ã‚»ãƒƒã‚·ãƒ§ãƒ³æœ€é©åŒ–ç„¡åŠ¹")
            
            # VitisAI ExecutionProviderï¼ˆNPUï¼‰
            if 'VitisAIExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ VitisAIExecutionProviderè©¦è¡Œï¼ˆNPUæœ€é©åŒ–ï¼‰...")
                    
                    vitisai_options = {
                        "cache_dir": "C:/temp/vaip_cache",
                        "cache_key": "ryzen_ai_npu_llm",
                        "log_level": "info"
                    }
                    
                    providers = [
                        ('VitisAIExecutionProvider', vitisai_options),
                        'CPUExecutionProvider'
                    ]
                    
                    self.session = ort.InferenceSession(
                        str(onnx_path),
                        sess_options=session_options,
                        providers=providers
                    )
                    
                    self.active_provider = 'VitisAIExecutionProvider'
                    print("âœ… VitisAIExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆNPUæœ€é©åŒ–ï¼‰")
                    
                except Exception as e:
                    print(f"âš ï¸ VitisAIExecutionProviderå¤±æ•—: {e}")
                    self.session = None
            
            # DmlExecutionProvider ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.session is None and 'DmlExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ DmlExecutionProviderè©¦è¡Œ...")
                    self.session = ort.InferenceSession(
                        str(onnx_path),
                        sess_options=session_options,
                        providers=['DmlExecutionProvider', 'CPUExecutionProvider']
                    )
                    self.active_provider = 'DmlExecutionProvider'
                    print("âœ… DmlExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ DmlExecutionProviderå¤±æ•—: {e}")
                    self.session = None
            
            # CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.session is None:
                try:
                    print("ğŸ”„ CPUExecutionProviderè©¦è¡Œ...")
                    self.session = ort.InferenceSession(
                        str(onnx_path),
                        sess_options=session_options,
                        providers=['CPUExecutionProvider']
                    )
                    self.active_provider = 'CPUExecutionProvider'
                    print("âœ… CPUExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âŒ CPUExecutionProviderå¤±æ•—: {e}")
                    return False
            
            if self.session is None:
                return False
            
            print(f"âœ… NPUæœ€é©åŒ–ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ”§ ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.session.get_providers()}")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            
            # å‹•ä½œãƒ†ã‚¹ãƒˆ
            try:
                test_input = np.array([[1, 2, 3, 4, 5, 6, 7, 2]], dtype=np.int64)
                test_output = self.session.run(None, {'input_ids': test_input})
                print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Œäº†: å‡ºåŠ›å½¢çŠ¶ {test_output[0].shape}")
            except Exception as e:
                print(f"âš ï¸ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
                return False
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_text(self, prompt: str, max_tokens: int = 50) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.session is None or self.tokenizer is None:
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return ""
        
        try:
            print(f"ğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            print(f"ğŸ”¢ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="np",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            input_ids = inputs['input_ids'].astype(np.int64)
            print(f"ğŸ”¤ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_ids.shape[1]}")
            
            # ç”Ÿæˆãƒ«ãƒ¼ãƒ—
            generated_tokens = []
            current_input = input_ids
            
            start_time = time.time()
            
            for step in range(max_tokens):
                # NPUæ¨è«–å®Ÿè¡Œ
                outputs = self.session.run(None, {'input_ids': current_input})
                logits = outputs[0]
                
                # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼ˆæ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
                temperature = self.config['search']['temperature']
                scaled_logits = logits[0, -1, :] / temperature
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                probabilities = self._softmax(scaled_logits)
                next_token = self._sample_token(probabilities)
                
                # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚§ãƒƒã‚¯
                if next_token == self.tokenizer.eos_token_id:
                    break
                
                generated_tokens.append(next_token)
                
                # æ¬¡ã®å…¥åŠ›æº–å‚™
                current_input = np.concatenate([
                    current_input,
                    np.array([[next_token]], dtype=np.int64)
                ], axis=1)
                
                # é€²æ—è¡¨ç¤º
                if (step + 1) % 10 == 0:
                    print(f"  ğŸ“Š ç”Ÿæˆé€²æ—: {step + 1}/{max_tokens}")
            
            generation_time = time.time() - start_time
            
            # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰
            if generated_tokens:
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            else:
                generated_text = ""
            
            full_text = prompt + generated_text
            
            # çµæœè¡¨ç¤º
            print(f"\nğŸ¯ NPUãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆçµæœ:")
            print(f"  ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            print(f"  ğŸ¯ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ: {generated_text}")
            print(f"  ğŸ“ å®Œå…¨ãƒ†ã‚­ã‚¹ãƒˆ: {full_text}")
            print(f"  ğŸ”¢ ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(generated_tokens)}")
            print(f"  â±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
            print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {len(generated_tokens)/generation_time:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            
            return full_text
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _softmax(self, x):
        """ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def _sample_token(self, probabilities):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        return np.random.choice(len(probabilities), p=probabilities)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Ryzen AI NPUæœ€é©åŒ–LLMãƒ¢ãƒ‡ãƒ« ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--list", action="store_true", help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º")
    parser.add_argument("--download", type=str, help="ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¢ãƒ‡ãƒ«IDæŒ‡å®šï¼‰")
    parser.add_argument("--generate", type=str, help="ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆãƒ¢ãƒ‡ãƒ«IDæŒ‡å®šï¼‰")
    parser.add_argument("--prompt", type=str, default="äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", help="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=50, help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    downloader = RyzenAINPUModelDownloader()
    
    if args.list:
        downloader.list_available_models()
        return
    
    if args.download:
        print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {args.download}")
        if downloader.download_model(args.download):
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {args.download}")
        else:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {args.download}")
        return
    
    if args.generate:
        model_dir = f"ryzen_ai_npu_models/{args.generate}"
        if not os.path.exists(model_dir):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_dir}")
            print("ğŸ’¡ å…ˆã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:")
            print(f"python {sys.argv[0]} --download {args.generate}")
            return
        
        generator = RyzenAINPUTextGenerator(model_dir, enable_infer_os=args.infer_os)
        if generator.initialize():
            if args.interactive:
                print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
                print("ğŸ’¡ 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
                
                while True:
                    try:
                        prompt = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ").strip()
                        if prompt.lower() in ['quit', 'exit', 'q']:
                            break
                        if prompt:
                            generator.generate_text(prompt, args.tokens)
                    except KeyboardInterrupt:
                        print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                        break
            else:
                generator.generate_text(args.prompt, args.tokens)
        else:
            print("âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
        return
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    downloader.list_available_models()
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"  ãƒ¢ãƒ‡ãƒ«ä¸€è¦§: python {sys.argv[0]} --list")
    print(f"  ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: python {sys.argv[0]} --download phi-3-mini-4k-instruct")
    print(f"  ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: python {sys.argv[0]} --generate phi-3-mini-4k-instruct --interactive")
    print(f"  infer-OSæœ‰åŠ¹: python {sys.argv[0]} --generate phi-3-mini-4k-instruct --infer-os --interactive")

if __name__ == "__main__":
    main()

