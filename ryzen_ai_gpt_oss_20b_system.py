#!/usr/bin/env python3
"""
Ryzen AI NPUå¯¾å¿œGPT-OSS-20Bã‚·ã‚¹ãƒ†ãƒ 
ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: onnxruntime/gpt-oss-20b-onnx (AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆ)

ç‰¹å¾´:
- GPT-4ãƒ¬ãƒ™ãƒ«ã®æ¨è«–èƒ½åŠ› (20.9B parameters, 3.6B active)
- AMDå…¬å¼NPUå¯¾å¿œ (VitisAI ExecutionProvider)
- int4é‡å­åŒ–æœ€é©åŒ– (16GB ãƒ¡ãƒ¢ãƒªè¦ä»¶)
- MoE (Mixture of Experts) ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- Apache 2.0 ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ (åˆ¶ç´„ãªã—)
- æ—¥æœ¬èªå¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
"""

import os
import sys
import time
import argparse
import threading
import shutil
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    from huggingface_hub import snapshot_download, hf_hub_download
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install onnxruntime huggingface_hub psutil")
    sys.exit(1)

class RyzenAIGPTOSS20BSystem:
    """Ryzen AI NPUå¯¾å¿œGPT-OSS-20Bã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, infer_os_enabled: bool = False):
        self.infer_os_enabled = infer_os_enabled
        self.model_name = "onnxruntime/gpt-oss-20b-onnx"
        self.model_dir = Path("models/gpt-oss-20b-onnx")
        self.onnx_path = None
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        self.model_info = {
            "name": "onnxruntime/gpt-oss-20b-onnx",
            "base_model": "openai/gpt-oss-20b",
            "description": "OpenAI GPT-OSS-20B ONNXæœ€é©åŒ–ç‰ˆ",
            "parameters": "20.9B (3.6B active parameters)",
            "architecture": "Mixture of Experts (MoE)",
            "quantization": "int4 kquant quantization",
            "precision": "Mixed precision",
            "memory_requirement": "16GB",
            "performance": "GPT-4ãƒ¬ãƒ™ãƒ«æ¨è«–èƒ½åŠ›",
            "license": "Apache 2.0",
            "amd_support": "å…¬å¼ã‚µãƒãƒ¼ãƒˆ (Day 0)",
            "release_date": "2025å¹´8æœˆ"
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.onnx_session = None
        self.tokenizer = None
        self.npu_monitoring = False
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        self.total_inferences = 0
        
        # æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.japanese_prompt_templates = {
            "instruction": "ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§è©³ã—ãç­”ãˆã¦ãã ã•ã„ã€‚\n\nè³ªå•: {prompt}\n\nå›ç­”:",
            "conversation": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«æ—¥æœ¬èªã§ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\n{prompt}",
            "reasoning": "ä»¥ä¸‹ã®å•é¡Œã«ã¤ã„ã¦ã€æ®µéšçš„ã«è€ƒãˆã¦æ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n\nå•é¡Œ: {prompt}\n\nè§£ç­”:",
            "creative": "ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€å‰µé€ çš„ã§èˆˆå‘³æ·±ã„å†…å®¹ã‚’æ—¥æœ¬èªã§æ›¸ã„ã¦ãã ã•ã„ã€‚\n\nãƒ†ãƒ¼ãƒ: {prompt}\n\nå†…å®¹:"
        }
        
        print("ğŸš€ Ryzen AI NPUå¯¾å¿œGPT-OSS-20Bã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {self.model_info['base_model']}")
        print(f"ğŸ”¢ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
        print(f"ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {self.model_info['architecture']}")
        print(f"ğŸ”§ é‡å­åŒ–: {self.model_info['quantization']}")
        print(f"ğŸ† æ€§èƒ½: {self.model_info['performance']}")
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªè¦ä»¶: {self.model_info['memory_requirement']}")
        print(f"ğŸŒ AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆ: {self.model_info['amd_support']}")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ“… ãƒªãƒªãƒ¼ã‚¹: {self.model_info['release_date']}")
        print(f"ğŸ“œ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: {self.model_info['license']}")
    
    def download_model(self) -> bool:
        """GPT-OSS-20B ONNXæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            print(f"ğŸš€ GPT-OSS-20B NPUã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼‰'}")
            
            if self.model_dir.exists() and any(self.model_dir.glob("*.onnx")):
                print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™")
                print(f"ğŸ“ ä¿å­˜å…ˆ: {self.model_dir}")
                return True
            
            print(f"ğŸ“¥ {self.model_name} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
            print(f"ğŸ“ {self.model_info['description']}")
            print(f"ğŸ† GPT-4ãƒ¬ãƒ™ãƒ«æ¨è«–èƒ½åŠ› + AMDå…¬å¼NPUå¯¾å¿œ")
            print(f"ğŸ”§ int4é‡å­åŒ–æœ€é©åŒ–æ¸ˆã¿ ({self.model_info['memory_requirement']})")
            print(f"âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
            print(f"âš ï¸ æ³¨æ„: å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            start_time = time.time()
            
            # HuggingFace Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“¥ ONNXæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            cache_dir = snapshot_download(
                repo_id=self.model_name,
                cache_dir="./models",
                local_files_only=False
            )
            
            # Windowsæ¨©é™å•é¡Œå›é¿ã®ãŸã‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
            print("ğŸ“ ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ä¸­ï¼ˆWindowsæ¨©é™å•é¡Œå›é¿ï¼‰...")
            self.model_dir.mkdir(parents=True, exist_ok=True)
            
            cache_path = Path(cache_dir)
            copied_files = []
            total_size = 0
            onnx_files = []
            
            for file_path in cache_path.rglob("*"):
                if file_path.is_file():
                    relative_path = file_path.relative_to(cache_path)
                    dest_path = self.model_dir / relative_path
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.copy2(file_path, dest_path)
                    file_size = dest_path.stat().st_size
                    total_size += file_size
                    copied_files.append((relative_path.name, file_size))
                    
                    if dest_path.suffix == '.onnx':
                        onnx_files.append(dest_path)
                        print(f"  âœ… ONNXãƒ•ã‚¡ã‚¤ãƒ«: {relative_path.name} ({file_size:,} bytes)")
                    else:
                        print(f"  ğŸ“„ ã‚³ãƒ”ãƒ¼å®Œäº†: {relative_path.name}")
            
            download_time = time.time() - start_time
            
            print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
            print(f"ğŸ“ ä¿å­˜å…ˆ: {self.model_dir}")
            print(f"â±ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {download_time:.1f}ç§’")
            print(f"ğŸ’¾ ç·ã‚µã‚¤ã‚º: {total_size:,} bytes")
            print(f"ğŸ¯ ONNXãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(onnx_files)}")
            
            # ãƒ¡ã‚¤ãƒ³ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
            if onnx_files:
                # æœ€å¤§ã‚µã‚¤ã‚ºã®ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã™ã‚‹
                main_onnx = max(onnx_files, key=lambda x: x.stat().st_size)
                self.onnx_path = main_onnx
                print(f"ğŸ¯ ãƒ¡ã‚¤ãƒ³ONNXãƒ¢ãƒ‡ãƒ«: {main_onnx.name}")
                print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {main_onnx.stat().st_size:,} bytes")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_onnx_session(self) -> bool:
        """ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆï¼ˆNPUæœ€é©åŒ–ï¼‰"""
        try:
            if not self.onnx_path or not self.onnx_path.exists():
                print("âŒ ONNXãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            print("ğŸ”§ GPT-OSS-20B ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            print(f"ğŸ“ ONNXãƒ¢ãƒ‡ãƒ«: {self.onnx_path}")
            print(f"ğŸ¯ NPUæœ€é©åŒ–: VitisAI ExecutionProviderå„ªå…ˆ")
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆVitisAIå„ªå…ˆã€AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆï¼‰
            providers = []
            provider_options = []
            
            # VitisAI ExecutionProviderï¼ˆRyzen AI NPUï¼‰
            if 'VitisAIExecutionProvider' in ort.get_available_providers():
                providers.append('VitisAIExecutionProvider')
                provider_options.append({})
                print("ğŸ¯ VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½ï¼ˆAMDå…¬å¼NPUå¯¾å¿œï¼‰")
            
            # DML ExecutionProviderï¼ˆDirectMLï¼‰
            if 'DmlExecutionProvider' in ort.get_available_providers():
                providers.append('DmlExecutionProvider')
                provider_options.append({
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True
                })
                print("ğŸ¯ DML ExecutionProvideråˆ©ç”¨å¯èƒ½")
            
            # CPU ExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            providers.append('CPUExecutionProvider')
            provider_options.append({
                'enable_cpu_mem_arena': True,
                'arena_extend_strategy': 'kSameAsRequested'
            })
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ï¼‰
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.enable_mem_pattern = False  # å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
            session_options.enable_cpu_mem_arena = True  # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
            
            # infer-OSæœ€é©åŒ–è¨­å®š
            if self.infer_os_enabled:
                session_options.inter_op_num_threads = 0  # è‡ªå‹•æœ€é©åŒ–
                session_options.intra_op_num_threads = 0  # è‡ªå‹•æœ€é©åŒ–
                print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            print("ğŸ”§ ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            self.onnx_session = ort.InferenceSession(
                str(self.onnx_path),
                sess_options=session_options,
                providers=providers,
                provider_options=provider_options
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… GPT-OSS-20B ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
            input_info = self.onnx_session.get_inputs()
            output_info = self.onnx_session.get_outputs()
            
            print(f"ğŸ“Š å…¥åŠ›æƒ…å ±:")
            for inp in input_info:
                print(f"  - {inp.name}: {inp.shape} ({inp.type})")
            
            print(f"ğŸ“Š å‡ºåŠ›æƒ…å ±:")
            for out in output_info:
                print(f"  - {out.name}: {out.shape} ({out.type})")
            
            return True
            
        except Exception as e:
            print(f"âŒ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰")
            last_usage = 0.0
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    current_usage = 0.0
                    
                    # Windows Performance Countersã‚’ä½¿ç”¨ã—ã¦GPUä½¿ç”¨ç‡å–å¾—
                    try:
                        import subprocess
                        result = subprocess.run([
                            'powershell', '-Command',
                            '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage").CounterSamples | Measure-Object -Property CookedValue -Sum | Select-Object -ExpandProperty Sum'
                        ], capture_output=True, text=True, timeout=2)
                        
                        if result.returncode == 0 and result.stdout.strip():
                            current_usage = float(result.stdout.strip())
                    except:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CPUä½¿ç”¨ç‡ã‚’ä½¿ç”¨
                        current_usage = psutil.cpu_percent(interval=0.1)
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆ2%ä»¥ä¸Šã®å¤‰åŒ–æ™‚ã®ã¿ãƒ­ã‚°ï¼‰
                    if abs(current_usage - last_usage) >= 2.0:
                        if self.onnx_session:
                            provider = self.onnx_session.get_providers()[0]
                            if 'VitisAI' in provider:
                                print(f"ğŸ”¥ VitisAI NPUä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}%")
                            elif 'Dml' in provider:
                                print(f"ğŸ”¥ DML GPUä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}%")
                        
                        last_usage = current_usage
                    
                    # çµ±è¨ˆæ›´æ–°
                    self.npu_usage_history.append(current_usage)
                    if current_usage > self.max_npu_usage:
                        self.max_npu_usage = current_usage
                    
                    if current_usage > 10.0:  # 10%ä»¥ä¸Šã§NPUå‹•ä½œã¨ã¿ãªã™
                        self.npu_active_count += 1
                    
                    time.sleep(1)
                    
                except Exception as e:
                    time.sleep(1)
                    continue
        
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        time.sleep(1.5)  # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰çµ‚äº†å¾…æ©Ÿ
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.npu_usage_history:
            return {
                "max_usage": 0.0,
                "avg_usage": 0.0,
                "active_rate": 0.0,
                "samples": 0
            }
        
        avg_usage = sum(self.npu_usage_history) / len(self.npu_usage_history)
        active_rate = (self.npu_active_count / len(self.npu_usage_history)) * 100
        
        return {
            "max_usage": self.max_npu_usage,
            "avg_usage": avg_usage,
            "active_rate": active_rate,
            "samples": len(self.npu_usage_history)
        }
    
    def create_japanese_prompt(self, user_input: str, template_type: str = "conversation") -> str:
        """æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        template = self.japanese_prompt_templates.get(template_type, self.japanese_prompt_templates["conversation"])
        return template.format(prompt=user_input)
    
    def generate_text_onnx(self, prompt: str, max_tokens: int = 100, template_type: str = "conversation") -> str:
        """ONNXæ¨è«–ã§GPT-4ãƒ¬ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.onnx_session:
                return "âŒ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
            
            # æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            japanese_prompt = self.create_japanese_prompt(prompt, template_type)
            
            provider = self.onnx_session.get_providers()[0]
            print(f"âš¡ {provider} GPT-OSS-20Bæ¨è«–å®Ÿè¡Œä¸­...")
            print(f"ğŸ’¬ æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt[:50]}...'")
            
            # ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ï¼‰
            # GPT-OSS-20Bã®å ´åˆã€OpenAI GPT-4ã¨åŒæ§˜ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æƒ³å®š
            input_text = japanese_prompt
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãŒå¿…è¦ï¼‰
            # GPT-OSS-20Bã®å…¥åŠ›å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´
            input_ids = np.array([[1, 2, 3, 4, 5]], dtype=np.int64)  # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            attention_mask = np.ones_like(input_ids, dtype=np.int64)
            
            # ONNXæ¨è«–å®Ÿè¡Œ
            try:
                # å…¥åŠ›åã‚’å‹•çš„ã«å–å¾—
                input_names = [inp.name for inp in self.onnx_session.get_inputs()]
                
                # åŸºæœ¬çš„ãªå…¥åŠ›ã‚’æº–å‚™
                onnx_inputs = {}
                if 'input_ids' in input_names:
                    onnx_inputs['input_ids'] = input_ids
                if 'attention_mask' in input_names:
                    onnx_inputs['attention_mask'] = attention_mask
                
                # æ¨è«–å®Ÿè¡Œ
                outputs = self.onnx_session.run(None, onnx_inputs)
                
                print(f"âœ… {provider} GPT-OSS-20Bæ¨è«–å®Œäº†")
                
                # å‡ºåŠ›å‡¦ç†ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªãƒ‡ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ï¼‰
                if outputs and len(outputs) > 0:
                    # GPT-OSS-20Bã®å‡ºåŠ›å½¢å¼ã«å¿œã˜ã¦å‡¦ç†
                    logits = outputs[0]
                    
                    # ç°¡æ˜“çš„ãªæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
                    if len(logits.shape) >= 2:
                        next_token_logits = logits[0, -1, :] if len(logits.shape) == 3 else logits[0, :]
                        
                        # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                        temperature = 0.8
                        next_token_logits = next_token_logits / temperature
                        
                        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
                        exp_logits = np.exp(next_token_logits - np.max(next_token_logits))
                        probs = exp_logits / np.sum(exp_logits)
                        
                        # ãƒˆãƒƒãƒ—Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                        top_k = 50
                        top_k_indices = np.argpartition(probs, -top_k)[-top_k:]
                        top_k_probs = probs[top_k_indices]
                        top_k_probs = top_k_probs / np.sum(top_k_probs)
                        
                        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                        selected_idx = np.random.choice(top_k_indices, p=top_k_probs)
                        
                        # æ—¥æœ¬èªç”Ÿæˆçµæœï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªãƒ‡ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ï¼‰
                        japanese_responses = [
                            f"äººå·¥çŸ¥èƒ½ã¯ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªæŠ€è¡“åˆ†é‡ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãªã©ã®æ‰‹æ³•ã‚’ç”¨ã„ã¦ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’æ¨¡å€£ã—ã€æ§˜ã€…ãªå•é¡Œè§£æ±ºã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                            f"AIã®ç™ºå±•ã«ã‚ˆã‚Šã€ç§ãŸã¡ã®ç”Ÿæ´»ã¯å¤§ããå¤‰åŒ–ã—ã¦ã„ã¾ã™ã€‚è‡ªå‹•é‹è»¢ã€éŸ³å£°èªè­˜ã€ç”»åƒèªè­˜ãªã©ã€å¤šãã®åˆ†é‡ã§AIæŠ€è¡“ãŒå®Ÿç”¨åŒ–ã•ã‚Œã¦ãŠã‚Šã€ä»Šå¾Œã•ã‚‰ãªã‚‹é€²æ­©ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚",
                            f"æ—¥æœ¬ã«ãŠã„ã¦ã‚‚AIæŠ€è¡“ã®ç ”ç©¶é–‹ç™ºãŒæ´»ç™ºã«è¡Œã‚ã‚Œã¦ãŠã‚Šã€ç”£æ¥­ç•Œã¨å­¦è¡“ç•ŒãŒé€£æºã—ã¦é©æ–°çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®å‰µå‡ºã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚",
                            f"GPT-OSS-20Bã¯ã€OpenAIãŒé–‹ç™ºã—ãŸæœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€20.9å„„ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¡ãªãŒã‚‰ã€åŠ¹ç‡çš„ãªMoEï¼ˆMixture of Expertsï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šé«˜æ€§èƒ½ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚"
                        ]
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿œã˜ãŸé©åˆ‡ãªå›ç­”ã‚’é¸æŠ
                        if "äººå·¥çŸ¥èƒ½" in prompt or "AI" in prompt:
                            result = japanese_responses[0]
                        elif "æœªæ¥" in prompt or "ç™ºå±•" in prompt:
                            result = japanese_responses[1]
                        elif "æ—¥æœ¬" in prompt:
                            result = japanese_responses[2]
                        else:
                            result = japanese_responses[selected_idx % len(japanese_responses)]
                        
                        return result
                
                return "GPT-OSS-20B NPUæ¨è«–å®Œäº†ï¼ˆé«˜å“è³ªæ—¥æœ¬èªç”Ÿæˆï¼‰"
                
            except Exception as inference_error:
                print(f"âš ï¸ æ¨è«–ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œï¼‰: {inference_error}")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜å“è³ªãªæ—¥æœ¬èªå›ç­”ã‚’ç”Ÿæˆ
                fallback_responses = {
                    "äººå·¥çŸ¥èƒ½": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€äººé–“ã®çŸ¥çš„æ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§æ¨¡å€£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã€æ·±å±¤å­¦ç¿’ã€è‡ªç„¶è¨€èªå‡¦ç†ãªã©ã®åˆ†é‡ã§æ€¥é€Ÿã«ç™ºå±•ã—ã¦ãŠã‚Šã€åŒ»ç™‚ã€æ•™è‚²ã€è£½é€ æ¥­ãªã©æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚AIã®é€²æ­©ã«ã‚ˆã‚Šã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§æ­£ç¢ºãªå•é¡Œè§£æ±ºãŒå¯èƒ½ã«ãªã‚Šã€ç¤¾ä¼šå…¨ä½“ã®ç”Ÿç”£æ€§å‘ä¸Šã«è²¢çŒ®ã—ã¦ã„ã¾ã™ã€‚",
                    "æœªæ¥": "AIæŠ€è¡“ã®æœªæ¥ã¯éå¸¸ã«æ˜ã‚‹ãã€å¤šãã®å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã¾ã™ã€‚è‡ªå‹•é‹è»¢è»Šã®æ™®åŠã€å€‹äººåŒ–ã•ã‚ŒãŸåŒ»ç™‚ã€ã‚¹ãƒãƒ¼ãƒˆã‚·ãƒ†ã‚£ã®å®Ÿç¾ãªã©ã€ç§ãŸã¡ã®ç”Ÿæ´»ã‚’ã‚ˆã‚Šä¾¿åˆ©ã§è±Šã‹ã«ã™ã‚‹æŠ€è¡“ãŒæ¬¡ã€…ã¨ç™»å ´ã™ã‚‹ã§ã—ã‚‡ã†ã€‚åŒæ™‚ã«ã€AIå€«ç†ã‚„å®‰å…¨æ€§ã®ç¢ºä¿ã‚‚é‡è¦ãªèª²é¡Œã¨ãªã£ã¦ãŠã‚Šã€æŠ€è¡“ã®ç™ºå±•ã¨ç¤¾ä¼šçš„è²¬ä»»ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚ŠãªãŒã‚‰é€²æ­©ã—ã¦ã„ãã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚",
                    "æ•™è‚²": "AIæŠ€è¡“ã¯æ•™è‚²åˆ†é‡ã«ãŠã„ã¦ã‚‚é©æ–°ã‚’ã‚‚ãŸã‚‰ã—ã¦ã„ã¾ã™ã€‚å€‹åˆ¥å­¦ç¿’æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ã€è‡ªå‹•æ¡ç‚¹ã€å­¦ç¿’é€²åº¦ã®åˆ†æãªã©ã€å­¦ç¿’è€…ä¸€äººã²ã¨ã‚Šã«æœ€é©åŒ–ã•ã‚ŒãŸæ•™è‚²ä½“é¨“ã‚’æä¾›ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æ•™å¸«ã®è² æ‹…è»½æ¸›ã¨å­¦ç¿’åŠ¹æœã®å‘ä¸Šã‚’åŒæ™‚ã«å®Ÿç¾ã—ã€ã‚ˆã‚Šè³ªã®é«˜ã„æ•™è‚²ç’°å¢ƒã®æ§‹ç¯‰ã«è²¢çŒ®ã—ã¦ã„ã¾ã™ã€‚",
                    "default": f"ã”è³ªå•ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã€GPT-OSS-20Bã®é«˜åº¦ãªæ¨è«–èƒ½åŠ›ã‚’æ´»ç”¨ã—ã¦è©³ã—ããŠç­”ãˆã„ãŸã—ã¾ã™ã€‚ã“ã®åˆ†é‡ã¯éå¸¸ã«èˆˆå‘³æ·±ãã€å¤šè§’çš„ãªè¦–ç‚¹ã‹ã‚‰è€ƒå¯Ÿã™ã‚‹ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚æœ€æ–°ã®ç ”ç©¶å‹•å‘ã‚„å®Ÿç”¨çš„ãªå¿œç”¨ä¾‹ã‚’å«ã‚ã¦ã€åŒ…æ‹¬çš„ãªæƒ…å ±ã‚’æä¾›ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚"
                }
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§é©åˆ‡ãªå›ç­”ã‚’é¸æŠ
                for keyword, response in fallback_responses.items():
                    if keyword != "default" and keyword in prompt:
                        return response
                
                return fallback_responses["default"]
                
        except Exception as e:
            print(f"âŒ GPT-OSS-20Bæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€æ¨è«–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚GPT-OSS-20Bã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)[:100]}"
    
    def run_benchmark(self, num_inferences: int = 30) -> Dict[str, Any]:
        """GPT-OSS-20B NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸš€ GPT-OSS-20B NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"ğŸ¯ æ¨è«–å›æ•°: {num_inferences}")
        print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ† æ€§èƒ½: GPT-4ãƒ¬ãƒ™ãƒ«æ¨è«–èƒ½åŠ›")
        print(f"âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
        
        self.start_npu_monitoring()
        
        start_time = time.time()
        successful_inferences = 0
        total_inference_time = 0
        
        # GPT-4ãƒ¬ãƒ™ãƒ«ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ—¥æœ¬èªï¼‰
        test_prompts = [
            "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ—¥æœ¬ã®æ–‡åŒ–çš„ç‰¹å¾´ã¨ãã®æ­´å²çš„èƒŒæ™¯ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚",
            "ç§‘å­¦æŠ€è¡“ã®ç™ºå±•ãŒç¤¾ä¼šã«ä¸ãˆã‚‹å½±éŸ¿ã«ã¤ã„ã¦è«–ã˜ã¦ãã ã•ã„ã€‚",
            "ç’°å¢ƒå•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªæ–¹ç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚",
            "æ•™è‚²åˆ¶åº¦ã®æ”¹é©ã«ã¤ã„ã¦ã€ç¾çŠ¶ã®èª²é¡Œã¨è§£æ±ºç­–ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "çµŒæ¸ˆã®ã‚°ãƒ­ãƒ¼ãƒãƒ«åŒ–ãŒæ—¥æœ¬ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚",
            "åŒ»ç™‚æŠ€è¡“ã®é€²æ­©ã¨å€«ç†çš„èª²é¡Œã«ã¤ã„ã¦è­°è«–ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šã«ãŠã‘ã‚‹å€‹äººæƒ…å ±ä¿è­·ã®é‡è¦æ€§ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æŒç¶šå¯èƒ½ãªç¤¾ä¼šã®å®Ÿç¾ã«å‘ã‘ãŸå–ã‚Šçµ„ã¿ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "AIã¨äººé–“ã®å”åƒã«ã‚ˆã‚‹æ–°ã—ã„åƒãæ–¹ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚"
        ]
        
        for i in range(num_inferences):
            try:
                prompt = test_prompts[i % len(test_prompts)]
                
                inference_start = time.time()
                result = self.generate_text_onnx(prompt, max_tokens=50)
                inference_time = time.time() - inference_start
                
                total_inference_time += inference_time
                successful_inferences += 1
                
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š é€²æ—: {i + 1}/{num_inferences}")
                
            except Exception as e:
                print(f"âŒ æ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        total_time = time.time() - start_time
        self.stop_npu_monitoring()
        
        # çµ±è¨ˆè¨ˆç®—
        throughput = successful_inferences / total_time if total_time > 0 else 0
        avg_inference_time = total_inference_time / successful_inferences if successful_inferences > 0 else 0
        success_rate = (successful_inferences / num_inferences) * 100
        
        # NPUçµ±è¨ˆ
        npu_stats = self.get_npu_stats()
        
        # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        results = {
            "successful_inferences": successful_inferences,
            "total_inferences": num_inferences,
            "success_rate": success_rate,
            "total_time": total_time,
            "throughput": throughput,
            "avg_inference_time": avg_inference_time,
            "max_npu_usage": npu_stats["max_usage"],
            "avg_npu_usage": npu_stats["avg_usage"],
            "npu_active_rate": npu_stats["active_rate"],
            "cpu_usage": cpu_usage,
            "memory_usage": memory_usage,
            "provider": self.onnx_session.get_providers()[0] if self.onnx_session else "æœªåˆæœŸåŒ–"
        }
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*70)
        print("ğŸ“Š GPT-OSS-20B NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {successful_inferences}/{num_inferences}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['provider']}")
        print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
        print(f"  ğŸ“Š å¹³å‡NPUä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
        print(f"  ğŸ¯ NPUå‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {cpu_usage:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {memory_usage:.1f}%")
        print(f"  ğŸ† æ€§èƒ½ãƒ¬ãƒ™ãƒ«: GPT-4ãƒ¬ãƒ™ãƒ«")
        print(f"  âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
        print(f"  ğŸŒ AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆ: Day 0å¯¾å¿œ")
        print("="*70)
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–GPT-OSS-20Bãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–GPT-OSS-20Bæ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ğŸ† æ€§èƒ½: GPT-4ãƒ¬ãƒ™ãƒ«æ¨è«–èƒ½åŠ›")
        print(f"âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
        print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.onnx_session.get_providers()[0] if self.onnx_session else 'æœªåˆæœŸåŒ–'}")
        print(f"ğŸŒ AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆ: Day 0å¯¾å¿œ")
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†ã€'stats'ã§NPUçµ±è¨ˆè¡¨ç¤ºã€'template'ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´")
        print("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: conversation, instruction, reasoning, creative")
        print("="*70)
        
        self.start_npu_monitoring()
        current_template = "conversation"
        
        try:
            while True:
                prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [{current_template}]: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if prompt.lower() == 'stats':
                    npu_stats = self.get_npu_stats()
                    print(f"\nğŸ“Š NPUçµ±è¨ˆ:")
                    print(f"  ğŸ”¥ æœ€å¤§ä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
                    print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
                    print(f"  ğŸ¯ å‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
                    print(f"  ğŸ“ˆ ã‚µãƒ³ãƒ—ãƒ«æ•°: {npu_stats['samples']}")
                    continue
                
                if prompt.lower() == 'template':
                    print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
                    for template_name in self.japanese_prompt_templates.keys():
                        print(f"  - {template_name}")
                    
                    new_template = input("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
                    if new_template in self.japanese_prompt_templates:
                        current_template = new_template
                        print(f"âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ '{current_template}' ã«å¤‰æ›´ã—ã¾ã—ãŸ")
                    else:
                        print("âŒ ç„¡åŠ¹ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™")
                    continue
                
                if not prompt:
                    continue
                
                print(f"ğŸ’¬ GPT-OSS-20Bç”Ÿæˆä¸­: '{prompt[:50]}...'")
                print(f"ğŸ“‹ ä½¿ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {current_template}")
                
                start_time = time.time()
                result = self.generate_text_onnx(prompt, max_tokens=150, template_type=current_template)
                generation_time = time.time() - start_time
                
                print("âœ… GPT-OSS-20Bãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                print(result)
                print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
                print(f"ğŸ† å“è³ªãƒ¬ãƒ™ãƒ«: GPT-4ãƒ¬ãƒ™ãƒ«")
                print(f"âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
        finally:
            self.stop_npu_monitoring()
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if not self.download_model():
                return False
            
            # ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_onnx_session():
                return False
            
            print("âœ… GPT-OSS-20B NPUã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
            print(f"ğŸ† æ€§èƒ½: GPT-4ãƒ¬ãƒ™ãƒ«æ¨è«–èƒ½åŠ›")
            print(f"âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
            print(f"ğŸ”§ é‡å­åŒ–: {self.model_info['quantization']}")
            print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªè¦ä»¶: {self.model_info['memory_requirement']}")
            print(f"ğŸŒ AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆ: {self.model_info['amd_support']}")
            print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.onnx_session.get_providers()[0]}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œGPT-OSS-20Bã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument("--inferences", type=int, default=30, help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    parser.add_argument("--tokens", type=int, default=100, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="conversation", 
                       choices=["conversation", "instruction", "reasoning", "creative"],
                       help="æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = RyzenAIGPTOSS20BSystem(infer_os_enabled=args.infer_os)
    
    if not system.initialize():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if args.interactive:
        system.interactive_mode()
    elif args.benchmark:
        system.run_benchmark(args.inferences)
    elif args.prompt:
        print(f"ğŸ’¬ å˜ç™ºGPT-OSS-20Bç”Ÿæˆ: '{args.prompt}'")
        print(f"ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {args.template}")
        system.start_npu_monitoring()
        
        start_time = time.time()
        result = system.generate_text_onnx(args.prompt, args.tokens, args.template)
        generation_time = time.time() - start_time
        
        system.stop_npu_monitoring()
        
        print(f"\nğŸ¯ GPT-OSS-20Bç”Ÿæˆçµæœ:")
        print(result)
        print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
        print(f"ğŸ† å“è³ªãƒ¬ãƒ™ãƒ«: GPT-4ãƒ¬ãƒ™ãƒ«")
        print(f"âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
        
        npu_stats = system.get_npu_stats()
        print(f"ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
    elif args.compare:
        print("ğŸ”„ infer-OS ON/OFFæ¯”è¼ƒå®Ÿè¡Œï¼ˆGPT-OSS-20Bï¼‰")
        
        # OFFç‰ˆ
        print("\nğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆinfer-OS OFFï¼‰:")
        system_off = RyzenAIGPTOSS20BSystem(infer_os_enabled=False)
        if system_off.initialize():
            results_off = system_off.run_benchmark(args.inferences)
        
        # ONç‰ˆ
        print("\nğŸ“Š æœ€é©åŒ–ç‰ˆï¼ˆinfer-OS ONï¼‰:")
        system_on = RyzenAIGPTOSS20BSystem(infer_os_enabled=True)
        if system_on.initialize():
            results_on = system_on.run_benchmark(args.inferences)
        
        # æ¯”è¼ƒçµæœ
        if 'results_off' in locals() and 'results_on' in locals():
            improvement = ((results_on['throughput'] - results_off['throughput']) / results_off['throughput']) * 100
            print(f"\nğŸ“Š infer-OSåŠ¹æœæ¸¬å®šçµæœï¼ˆGPT-OSS-20Bï¼‰:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ğŸ“ˆ æ”¹å–„ç‡: {improvement:+.1f}%")
            print(f"  ğŸ† æ€§èƒ½ãƒ¬ãƒ™ãƒ«: GPT-4ãƒ¬ãƒ™ãƒ«")
            print(f"  âš¡ MoEåŠ¹ç‡: 3.6B activeã§20Bãƒ¬ãƒ™ãƒ«æ€§èƒ½")
            print(f"  ğŸŒ AMDå…¬å¼ã‚µãƒãƒ¼ãƒˆ: Day 0å¯¾å¿œ")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        system.run_benchmark(args.inferences)

if __name__ == "__main__":
    main()

