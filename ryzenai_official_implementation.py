#!/usr/bin/env python3
"""
AMDå…¬å¼RyzenAIå®Ÿè£…
Lemonade SDK + OGA-Hybridä½¿ç”¨
å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://ryzenai.docs.amd.com/en/latest/llm/high_level_python.html
"""

import os
import sys
import time
import subprocess
from typing import Optional, List, Dict, Any
import argparse

# Lemonade SDK ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå…¬å¼APIï¼‰
try:
    from lemonade.api import from_pretrained
    LEMONADE_AVAILABLE = True
    print("âœ… Lemonade SDK ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    LEMONADE_AVAILABLE = False
    print(f"âš ï¸ Lemonade SDK ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("  conda create -n ryzenai-llm python=3.10")
    print("  conda activate ryzenai-llm")
    print("  pip install lemonade-sdk[llm-oga-hybrid]")
    print("  lemonade-install --ryzenai hybrid")

class RyzenAIOfficialLLM:
    """AMDå…¬å¼RyzenAI LLMå®Ÿè£…"""
    
    def __init__(self, model_name: str = "amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid"):
        """
        AMDå…¬å¼RyzenAI LLMåˆæœŸåŒ–
        
        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆAMDå…¬å¼ãƒ¢ãƒ‡ãƒ«ï¼‰
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.is_initialized = False
        
        print("ğŸ‡¯ğŸ‡µ AMDå…¬å¼RyzenAI LLMåˆæœŸåŒ–é–‹å§‹")
        print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«: {model_name}")
        
        if LEMONADE_AVAILABLE:
            self._initialize_model()
        else:
            print("âŒ Lemonade SDK ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    def _initialize_model(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        try:
            print("ğŸ”§ Lemonade SDK ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
            print("âš¡ NPU + CPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–...")
            
            # å…¬å¼APIä½¿ç”¨
            self.model, self.tokenizer = from_pretrained(
                self.model_name,
                recipe="oga-hybrid"  # NPU + CPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰
            )
            
            print("âœ… ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
            print(f"  ğŸ“± ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
            print(f"  ğŸ”§ ãƒ¬ã‚·ãƒ”: oga-hybrid (NPU + CPU)")
            
            self.is_initialized = True
            
            # åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
            self._initialization_test()
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            print("  1. RyzenAI 1.5.1ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹")
            print("  2. NPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹")
            print("  3. Condaç’°å¢ƒãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹")
            self.is_initialized = False
    
    def _initialization_test(self):
        """åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        try:
            print("ğŸ§ª åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            
            test_prompt = "Hello"
            input_ids = self.tokenizer(test_prompt, return_tensors="pt").input_ids
            
            # çŸ­ã„ãƒ†ã‚¹ãƒˆç”Ÿæˆ
            response = self.model.generate(input_ids, max_new_tokens=5)
            test_output = self.tokenizer.decode(response[0])
            
            print(f"âœ… åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            print(f"  ğŸ“ ãƒ†ã‚¹ãƒˆå…¥åŠ›: {test_prompt}")
            print(f"  ğŸ“ ãƒ†ã‚¹ãƒˆå‡ºåŠ›: {test_output}")
            
        except Exception as e:
            print(f"âš ï¸ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆè­¦å‘Š: {e}")
    
    def generate_text(self, prompt: str, max_new_tokens: int = 64, **kwargs) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_new_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: è¿½åŠ ã®ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.is_initialized:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return prompt
        
        try:
            print("âš¡ RyzenAI NPUæ¨è«–å®Ÿè¡Œä¸­...")
            print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            print(f"ğŸ”¢ æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_new_tokens}")
            
            start_time = time.time()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
            print(f"ğŸ”¤ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_ids.shape[1]}")
            
            # NPUæ¨è«–å®Ÿè¡Œ
            response = self.model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                **kwargs
            )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(response[0])
            
            # çµ±è¨ˆæƒ…å ±
            total_tokens = response.shape[1]
            new_tokens = total_tokens - input_ids.shape[1]
            tokens_per_second = new_tokens / generation_time if generation_time > 0 else 0
            
            print(f"âœ… RyzenAI NPUæ¨è«–å®Œäº†")
            print(f"  â±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’")
            print(f"  ğŸ”¤ ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {new_tokens}")
            print(f"  ğŸš€ ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f}ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            
            return generated_text
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return prompt
    
    def generate_japanese(self, prompt: str, max_new_tokens: int = 100) -> str:
        """
        æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæœ€é©åŒ–è¨­å®šï¼‰
        
        Args:
            prompt: æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_new_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ
        """
        print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        
        # æ—¥æœ¬èªç”Ÿæˆã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        japanese_params = {
            'temperature': 0.7,
            'top_p': 0.9,
            'top_k': 50,
            'repetition_penalty': 1.1,
            'do_sample': True,
        }
        
        return self.generate_text(prompt, max_new_tokens, **japanese_params)
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        if not self.is_initialized:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        print("\nğŸ‡¯ğŸ‡µ AMDå…¬å¼RyzenAI ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("=" * 60)
        print("âš¡ NPU + CPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print("\nğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†")
        print("=" * 60)
        
        while True:
            try:
                prompt = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ AMDå…¬å¼RyzenAI LLMã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if not prompt:
                    print("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                print("\nğŸ”„ ç”Ÿæˆä¸­...")
                
                # æ—¥æœ¬èªåˆ¤å®šï¼ˆç°¡æ˜“ï¼‰
                if any('\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF' for char in prompt):
                    result = self.generate_japanese(prompt)
                else:
                    result = self.generate_text(prompt)
                
                print(f"\nğŸ“ ç”Ÿæˆçµæœ:")
                print("-" * 40)
                print(result)
                print("-" * 40)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ AMDå…¬å¼RyzenAI LLMã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    def get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        info = {
            'lemonade_available': LEMONADE_AVAILABLE,
            'model_initialized': self.is_initialized,
            'model_name': self.model_name,
            'recipe': 'oga-hybrid',
        }
        
        # NPUæƒ…å ±å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        try:
            # ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§NPUç¢ºèªã‚’ä¿ƒã™
            info['npu_check'] = "ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ -> ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ -> NPU0 ã§ä½¿ç”¨ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
        except:
            pass
        
        return info

def check_environment():
    """ç’°å¢ƒç¢ºèª"""
    print("ğŸ” RyzenAIç’°å¢ƒç¢ºèª")
    print("=" * 40)
    
    # Condaç’°å¢ƒç¢ºèª
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')
    print(f"ğŸ“¦ Condaç’°å¢ƒ: {conda_env}")
    
    # Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    print(f"ğŸ Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {python_version}")
    
    # Lemonade SDKç¢ºèª
    print(f"ğŸ‹ Lemonade SDK: {'âœ… åˆ©ç”¨å¯èƒ½' if LEMONADE_AVAILABLE else 'âŒ æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«'}")
    
    # ç’°å¢ƒå¤‰æ•°ç¢ºèª
    ryzenai_path = os.environ.get('RYZEN_AI_INSTALLATION_PATH', 'Not set')
    print(f"ğŸ“ RYZEN_AI_INSTALLATION_PATH: {ryzenai_path}")
    
    print("=" * 40)
    
    if not LEMONADE_AVAILABLE:
        print("\nğŸ’¡ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †:")
        print("1. conda create -n ryzenai-llm python=3.10")
        print("2. conda activate ryzenai-llm")
        print("3. pip install lemonade-sdk[llm-oga-hybrid]")
        print("4. lemonade-install --ryzenai hybrid")

def run_validation_command():
    """å…¬å¼æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ"""
    if not LEMONADE_AVAILABLE:
        print("âŒ Lemonade SDK ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return
    
    print("ğŸ§ª å…¬å¼æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ")
    
    # å…¬å¼æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰
    cmd = [
        "lemonade",
        "-i", "amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid",
        "oga-load",
        "--device", "hybrid",
        "--dtype", "int4",
        "llm-prompt",
        "--max-new-tokens", "64",
        "-p", "Hello, how are you?"
    ]
    
    try:
        print(f"ğŸ“ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            print("âœ… æ¤œè¨¼æˆåŠŸ")
            print("ğŸ“ å‡ºåŠ›:")
            print(result.stdout)
        else:
            print("âŒ æ¤œè¨¼å¤±æ•—")
            print("ğŸ“ ã‚¨ãƒ©ãƒ¼:")
            print(result.stderr)
            
    except subprocess.TimeoutExpired:
        print("â° æ¤œè¨¼ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ120ç§’ï¼‰")
    except Exception as e:
        print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="AMDå…¬å¼RyzenAI LLMãƒ‡ãƒ¢")
    parser.add_argument("--model", default="amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
    parser.add_argument("--check-env", action="store_true", help="ç’°å¢ƒç¢ºèª")
    parser.add_argument("--validate", action="store_true", help="å…¬å¼æ¤œè¨¼ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    print("ğŸ¯ AMDå…¬å¼RyzenAI LLMãƒ‡ãƒ¢")
    print("=" * 50)
    
    if args.check_env:
        check_environment()
        return
    
    if args.validate:
        run_validation_command()
        return
    
    if not LEMONADE_AVAILABLE:
        print("âš ï¸ Lemonade SDK ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        check_environment()
        return
    
    # RyzenAI LLMåˆæœŸåŒ–
    llm = RyzenAIOfficialLLM(model_name=args.model)
    
    if not llm.is_initialized:
        print("âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
    info = llm.get_system_info()
    print("\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    if args.interactive:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        llm.interactive_mode()
    elif args.prompt:
        # å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ
        print(f"\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
        print("ğŸ”„ ç”Ÿæˆä¸­...")
        
        result = llm.generate_text(args.prompt)
        
        print(f"\nğŸ“ ç”Ÿæˆçµæœ:")
        print("-" * 40)
        print(result)
        print("-" * 40)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ã‚¹ãƒˆ
        test_prompts = [
            "Hello, how are you?",
            "äººå‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
        ]
        
        for prompt in test_prompts:
            print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {prompt}")
            result = llm.generate_text(prompt, max_new_tokens=50)
            
            print(f"ğŸ“ ç”Ÿæˆçµæœ:")
            print("-" * 40)
            print(result)
            print("-" * 40)

if __name__ == "__main__":
    main()

