# -*- coding: utf-8 -*-
"""
Ryzen AI NPU ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç¢ºèª
"""

import os
import sys
import time
import argparse
import json
import threading
import psutil
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime transformers torch ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class PerformanceMonitor:
    """æ€§èƒ½ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.monitoring = False
        self.cpu_samples = []
        self.memory_samples = []
        self.monitor_thread = None
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.cpu_samples = []
        self.memory_samples = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1)
    
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                self.cpu_samples.append(cpu_percent)
                self.memory_samples.append(memory_percent)
                time.sleep(0.5)
            except:
                break
    
    def get_report(self) -> Dict[str, float]:
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        if not self.cpu_samples or not self.memory_samples:
            return {"avg_cpu": 0.0, "max_cpu": 0.0, "avg_memory": 0.0, "max_memory": 0.0}
        
        return {
            "avg_cpu": sum(self.cpu_samples) / len(self.cpu_samples),
            "max_cpu": max(self.cpu_samples),
            "avg_memory": sum(self.memory_samples) / len(self.memory_samples),
            "max_memory": max(self.memory_samples),
            "samples": len(self.cpu_samples)
        }

class RyzenAITextGenerationDemo:
    """Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, enable_infer_os: bool = False, use_npu: bool = True):
        self.enable_infer_os = enable_infer_os
        self.use_npu = use_npu
        self.model = None
        self.tokenizer = None
        self.npu_session = None
        self.active_provider = None
        self.performance_monitor = PerformanceMonitor()
        
        # Ryzen AIå®Ÿç¸¾ãƒ¢ãƒ‡ãƒ«å€™è£œ
        self.model_candidates = [
            {
                "name": "microsoft/DialoGPT-medium",
                "description": "å¯¾è©±ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆRyzen AIå®Ÿç¸¾ï¼‰",
                "size": "117M",
                "ryzen_ai_proven": True
            },
            {
                "name": "microsoft/DialoGPT-small", 
                "description": "è»½é‡å¯¾è©±ãƒ¢ãƒ‡ãƒ«ï¼ˆRyzen AIå®Ÿç¸¾ï¼‰",
                "size": "117M",
                "ryzen_ai_proven": True
            },
            {
                "name": "gpt2",
                "description": "æ±ç”¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆRyzen AIå®Ÿç¸¾ï¼‰",
                "size": "124M",
                "ryzen_ai_proven": True
            },
            {
                "name": "distilgpt2",
                "description": "è»½é‡ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆRyzen AIå®Ÿç¸¾ï¼‰",
                "size": "82M",
                "ryzen_ai_proven": True
            }
        ]
        
        print("ğŸš€ Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if enable_infer_os else 'ç„¡åŠ¹'}")
        print(f"ğŸ”§ NPUä½¿ç”¨: {'æœ‰åŠ¹' if use_npu else 'ç„¡åŠ¹'}")
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
            
            # infer-OSç’°å¢ƒè¨­å®š
            self._setup_infer_os_environment()
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.use_npu:
                self._setup_npu_session()
            
            # LLMãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if not self._load_llm_model():
                return False
            
            print("âœ… Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _setup_infer_os_environment(self):
        """infer-OSç’°å¢ƒè¨­å®š"""
        if self.enable_infer_os:
            print("ğŸ”§ infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šä¸­...")
            
            infer_os_env = {
                'INFER_OS_ENABLE': '1',
                'INFER_OS_OPTIMIZATION_LEVEL': 'high',
                'INFER_OS_NPU_ACCELERATION': '1',
                'INFER_OS_MEMORY_OPTIMIZATION': '1',
                'INFER_OS_COMPUTE_OPTIMIZATION': '1'
            }
            
            for key, value in infer_os_env.items():
                os.environ[key] = value
                print(f"  ğŸ“ {key}={value}")
            
            print("âœ… infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šå®Œäº†")
        else:
            print("ğŸ”§ infer-OSæœ€é©åŒ–: ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼‰")
            # infer-OSç„¡åŠ¹åŒ–
            for key in ['INFER_OS_ENABLE', 'INFER_OS_OPTIMIZATION_LEVEL', 
                       'INFER_OS_NPU_ACCELERATION', 'INFER_OS_MEMORY_OPTIMIZATION',
                       'INFER_OS_COMPUTE_OPTIMIZATION']:
                os.environ.pop(key, None)
    
    def _setup_npu_session(self):
        """NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"""
        try:
            print("âš¡ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªNPUãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            npu_model_path = "npu_test_model.onnx"
            if not self._create_npu_test_model(npu_model_path):
                print("âš ï¸ NPUãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return
            
            # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            available_providers = ort.get_available_providers()
            print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {available_providers}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3
            
            if self.enable_infer_os:
                session_options.enable_cpu_mem_arena = True
                session_options.enable_mem_pattern = True
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            else:
                session_options.enable_cpu_mem_arena = False
                session_options.enable_mem_pattern = False
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL
            
            # VitisAI ExecutionProviderï¼ˆNPUï¼‰
            if 'VitisAIExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ VitisAIExecutionProviderè©¦è¡Œ...")
                    
                    vitisai_options = {
                        "cache_dir": "C:/temp/vaip_cache",
                        "cache_key": "text_generation_demo",
                        "log_level": "info"
                    }
                    
                    providers = [
                        ('VitisAIExecutionProvider', vitisai_options),
                        'CPUExecutionProvider'
                    ]
                    
                    self.npu_session = ort.InferenceSession(
                        npu_model_path,
                        sess_options=session_options,
                        providers=providers
                    )
                    
                    self.active_provider = 'VitisAIExecutionProvider'
                    print("âœ… VitisAIExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                    
                    # NPUå‹•ä½œãƒ†ã‚¹ãƒˆ
                    test_input = np.random.randn(1, 256).astype(np.float32)
                    test_output = self.npu_session.run(None, {'input': test_input})
                    print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Œäº†: å‡ºåŠ›å½¢çŠ¶ {test_output[0].shape}")
                    
                except Exception as e:
                    print(f"âš ï¸ VitisAIExecutionProviderå¤±æ•—: {e}")
                    self.npu_session = None
            
            # DmlExecutionProvider ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.npu_session is None and 'DmlExecutionProvider' in available_providers:
                try:
                    print("ğŸ”„ DmlExecutionProviderè©¦è¡Œ...")
                    self.npu_session = ort.InferenceSession(
                        npu_model_path,
                        sess_options=session_options,
                        providers=['DmlExecutionProvider', 'CPUExecutionProvider']
                    )
                    self.active_provider = 'DmlExecutionProvider'
                    print("âœ… DmlExecutionProvider ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ DmlExecutionProviderå¤±æ•—: {e}")
                    self.npu_session = None
            
            if self.npu_session:
                print(f"âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            else:
                print("âš ï¸ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—ï¼ˆCPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                
        except Exception as e:
            print(f"âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_npu_test_model(self, model_path: str) -> bool:
        """NPUãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            import torch
            import torch.nn as nn
            import onnx
            
            class NPUTestModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear1 = nn.Linear(256, 512)
                    self.relu = nn.ReLU()
                    self.linear2 = nn.Linear(512, 256)
                    self.dropout = nn.Dropout(0.1)
                
                def forward(self, x):
                    x = self.linear1(x)
                    x = self.relu(x)
                    x = self.dropout(x)
                    x = self.linear2(x)
                    return x
            
            model = NPUTestModel()
            model.eval()
            
            dummy_input = torch.randn(1, 256)
            
            torch.onnx.export(
                model,
                dummy_input,
                model_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³èª¿æ•´
            onnx_model = onnx.load(model_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, model_path)
            
            print(f"âœ… NPUãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ NPUãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _load_llm_model(self) -> bool:
        """LLMãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        for candidate in self.model_candidates:
            try:
                model_name = candidate["name"]
                print(f"ğŸ”„ {candidate['description']}ã‚’è©¦è¡Œä¸­: {model_name}")
                print(f"ğŸ¯ Ryzen AIå®Ÿç¸¾: {'ã‚ã‚Š' if candidate['ryzen_ai_proven'] else 'ãªã—'}")
                
                # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
                print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True
                )
                
                # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
                print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    torch_dtype="auto",
                    device_map="auto" if self.enable_infer_os else "cpu"
                )
                
                print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {model_name}")
                print(f"ğŸ¯ Ryzen AIå®Ÿç¸¾: {'ã‚ã‚Š' if candidate['ryzen_ai_proven'] else 'ãªã—'}")
                print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {candidate['size']}")
                
                return True
                
            except Exception as e:
                print(f"âš ï¸ {model_name} èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
                continue
        
        print("âŒ å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return False
    
    def generate_text(self, prompt: str, max_tokens: int = 50, num_generations: int = 1) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.model is None or self.tokenizer is None:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        try:
            print(f"\nğŸ¯ Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            print(f"ğŸ”¢ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            print(f"ğŸ”¢ ç”Ÿæˆå›æ•°: {num_generations}")
            print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            print(f"ğŸ”§ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³: {'æœ‰åŠ¹' if self.npu_session else 'ç„¡åŠ¹'}")
            
            # æ€§èƒ½ç›£è¦–é–‹å§‹
            self.performance_monitor.start_monitoring()
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            print(f"ğŸ”¤ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {inputs['input_ids'].shape[1]}")
            
            # ç”Ÿæˆè¨­å®š
            generation_config = GenerationConfig(
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
            start_time = time.time()
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config,
                    num_return_sequences=num_generations
                )
            
            generation_time = time.time() - start_time
            
            # æ€§èƒ½ç›£è¦–åœæ­¢
            self.performance_monitor.stop_monitoring()
            performance_report = self.performance_monitor.get_report()
            
            # ç”Ÿæˆçµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_texts = []
            for i, output in enumerate(outputs):
                # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»
                generated_tokens = output[inputs['input_ids'].shape[1]:]
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                full_text = prompt + generated_text
                generated_texts.append(full_text)
                
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ {i+1}:")
                print(f"  ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
                print(f"  ğŸ¯ ç”Ÿæˆéƒ¨åˆ†: {generated_text}")
                print(f"  ğŸ“ å®Œå…¨ãƒ†ã‚­ã‚¹ãƒˆ: {full_text}")
            
            # æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
            tokens_generated = sum(len(self.tokenizer.encode(text)) - len(self.tokenizer.encode(prompt)) 
                                 for text in generated_texts)
            tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
            
            print(f"\nğŸ“Š Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ:")
            print(f"  ğŸ”¢ ç”Ÿæˆå›æ•°: {num_generations}")
            print(f"  ğŸ”¢ ç·ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {tokens_generated}")
            print(f"  â±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
            print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
            print(f"  ğŸ”§ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³: {'æœ‰åŠ¹' if self.npu_session else 'ç„¡åŠ¹'}")
            
            print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½:")
            print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {performance_report['avg_cpu']:.1f}%")
            print(f"  ğŸ’» æœ€å¤§CPUä½¿ç”¨ç‡: {performance_report['max_cpu']:.1f}%")
            print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {performance_report['avg_memory']:.1f}%")
            print(f"  ğŸ’¾ æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {performance_report['max_memory']:.1f}%")
            print(f"  ğŸ”¢ ç›£è¦–ã‚µãƒ³ãƒ—ãƒ«æ•°: {performance_report['samples']}")
            
            return generated_texts
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def run_benchmark(self, prompts: List[str], max_tokens: int = 30) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"\nğŸ¯ Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"ğŸ”¢ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(prompts)}")
        print(f"ğŸ”¢ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.enable_infer_os else 'ç„¡åŠ¹'}")
        
        results = {
            "infer_os_enabled": self.enable_infer_os,
            "npu_enabled": self.npu_session is not None,
            "total_prompts": len(prompts),
            "successful_generations": 0,
            "failed_generations": 0,
            "total_time": 0,
            "total_tokens": 0,
            "generations": []
        }
        
        start_time = time.time()
        
        for i, prompt in enumerate(prompts):
            print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é€²æ—: {i+1}/{len(prompts)}")
            
            try:
                generated_texts = self.generate_text(prompt, max_tokens, 1)
                if generated_texts:
                    results["successful_generations"] += 1
                    results["generations"].extend(generated_texts)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
                    tokens = len(self.tokenizer.encode(generated_texts[0]))
                    results["total_tokens"] += tokens
                else:
                    results["failed_generations"] += 1
                    
            except Exception as e:
                print(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ{i+1}ç”Ÿæˆå¤±æ•—: {e}")
                results["failed_generations"] += 1
        
        results["total_time"] = time.time() - start_time
        results["tokens_per_second"] = results["total_tokens"] / results["total_time"] if results["total_time"] > 0 else 0
        results["avg_time_per_generation"] = results["total_time"] / results["successful_generations"] if results["successful_generations"] > 0 else 0
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœè¡¨ç¤º
        print(f"\nğŸ¯ Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if results['infer_os_enabled'] else 'ç„¡åŠ¹'}")
        print(f"  ğŸ”§ NPUä½¿ç”¨: {'æœ‰åŠ¹' if results['npu_enabled'] else 'ç„¡åŠ¹'}")
        print(f"  âœ… æˆåŠŸç”Ÿæˆæ•°: {results['successful_generations']}")
        print(f"  âŒ å¤±æ•—ç”Ÿæˆæ•°: {results['failed_generations']}")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {results['total_time']:.3f}ç§’")
        print(f"  ğŸ”¢ ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {results['total_tokens']}")
        print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {results['tokens_per_second']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
        print(f"  â±ï¸ å¹³å‡ç”Ÿæˆæ™‚é–“: {results['avg_time_per_generation']:.3f}ç§’")
        
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Ryzen AI ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--prompt", type=str, default="äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", help="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=50, help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--generations", type=int, default=1, help="ç”Ÿæˆå›æ•°")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--no-npu", action="store_true", help="NPUä½¿ç”¨ã‚’ç„¡åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    if args.compare:
        print("ğŸ”„ infer-OS ON/OFFæ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        
        # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        test_prompts = [
            "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ·±å±¤å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æŠ€è¡“ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        ]
        
        # infer-OS OFF
        print("\n" + "="*60)
        print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰")
        print("="*60)
        demo_off = RyzenAITextGenerationDemo(enable_infer_os=False, use_npu=not args.no_npu)
        if demo_off.initialize():
            results_off = demo_off.run_benchmark(test_prompts, args.tokens)
        else:
            print("âŒ infer-OS OFF ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
            return
        
        # infer-OS ON
        print("\n" + "="*60)
        print("ğŸ“Š æœ€é©åŒ–æ¸¬å®šï¼ˆinfer-OS ONï¼‰")
        print("="*60)
        demo_on = RyzenAITextGenerationDemo(enable_infer_os=True, use_npu=not args.no_npu)
        if demo_on.initialize():
            results_on = demo_on.run_benchmark(test_prompts, args.tokens)
        else:
            print("âŒ infer-OS ON ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
            return
        
        # æ¯”è¼ƒçµæœè¡¨ç¤º
        if results_off and results_on:
            print("\n" + "="*60)
            print("ğŸ“Š infer-OS ON/OFF æ¯”è¼ƒçµæœ")
            print("="*60)
            
            speed_improvement = (results_on['tokens_per_second'] / results_off['tokens_per_second'] - 1) * 100 if results_off['tokens_per_second'] > 0 else 0
            time_improvement = (1 - results_on['avg_time_per_generation'] / results_off['avg_time_per_generation']) * 100 if results_off['avg_time_per_generation'] > 0 else 0
            
            print(f"ğŸ“Š ç”Ÿæˆé€Ÿåº¦:")
            print(f"  OFF: {results_off['tokens_per_second']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            print(f"  ON:  {results_on['tokens_per_second']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            print(f"  æ”¹å–„: {speed_improvement:+.1f}%")
            
            print(f"â±ï¸ å¹³å‡ç”Ÿæˆæ™‚é–“:")
            print(f"  OFF: {results_off['avg_time_per_generation']:.3f}ç§’")
            print(f"  ON:  {results_on['avg_time_per_generation']:.3f}ç§’")
            print(f"  æ”¹å–„: {time_improvement:+.1f}%")
            
            print(f"âœ… æˆåŠŸç‡:")
            print(f"  OFF: {results_off['successful_generations']}/{results_off['total_prompts']}")
            print(f"  ON:  {results_on['successful_generations']}/{results_on['total_prompts']}")
    
    else:
        # å˜ä¸€å®Ÿè¡Œ
        demo = RyzenAITextGenerationDemo(enable_infer_os=args.infer_os, use_npu=not args.no_npu)
        if demo.initialize():
            if args.interactive:
                print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
                print("ğŸ’¡ 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
                
                while True:
                    try:
                        prompt = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ").strip()
                        if prompt.lower() in ['quit', 'exit', 'q']:
                            break
                        if prompt:
                            demo.generate_text(prompt, args.tokens, args.generations)
                    except KeyboardInterrupt:
                        print("\nğŸ‘‹ çµ‚äº†ã—ã¾ã™")
                        break
            
            elif args.benchmark:
                test_prompts = [
                    "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
                    "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                    "æ·±å±¤å­¦ç¿’ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                    "è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
                    "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æŠ€è¡“ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
                ]
                demo.run_benchmark(test_prompts, args.tokens)
            
            else:
                demo.generate_text(args.prompt, args.tokens, args.generations)
        else:
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")

if __name__ == "__main__":
    main()

