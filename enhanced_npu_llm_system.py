# -*- coding: utf-8 -*-
"""
ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¿®æ­£ç‰ˆ+NPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ 
DialoGPTè¨­å®šä¿®æ­£ + è©³ç´°NPUå‹•ä½œç›£è¦–
"""

import os
import sys
import time
import argparse
import json
import threading
import signal
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    import torch
    import torch.nn as nn
    from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime torch transformers psutil ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class EnhancedNPULLMSystem:
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¿®æ­£ç‰ˆ+NPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = None
        self.active_provider = None
        self.model = None
        self.tokenizer = None
        self.npu_monitoring_active = False
        
        # infer-OSè¨­å®š
        self.infer_os_enabled = os.getenv('INFER_OS_ENABLED', '0') == '1'
        
        print(f"ğŸš€ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¿®æ­£ç‰ˆ+NPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {timeout}ç§’")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        # NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.start_npu_monitoring()
    
    def start_npu_monitoring(self):
        """NPUå‹•ä½œç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹"""
        self.npu_monitoring_active = True
        
        def monitor_npu():
            while self.npu_monitoring_active:
                try:
                    # ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é¢¨NPUä½¿ç”¨ç‡ç›£è¦–
                    npu_usage = self.get_npu_usage()
                    if npu_usage > 0:
                        print(f"ğŸ”¥ NPUå‹•ä½œæ¤œå‡º: ä½¿ç”¨ç‡ {npu_usage:.1f}%")
                    
                    # DML/VitisAI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å‹•ä½œç›£è¦–
                    if self.active_provider:
                        provider_status = self.check_provider_activity()
                        if provider_status:
                            print(f"âš¡ {self.active_provider} ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {provider_status}")
                    
                    time.sleep(2)  # 2ç§’é–“éš”ã§ç›£è¦–
                    
                except Exception as e:
                    # ç›£è¦–ã‚¨ãƒ©ãƒ¼ã¯é™ã‹ã«å‡¦ç†
                    pass
        
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
        print("ğŸ“Š NPUå‹•ä½œç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")
    
    def get_npu_usage(self) -> float:
        """NPUä½¿ç”¨ç‡å–å¾—ï¼ˆWindows Performance Toolkitä½¿ç”¨ï¼‰"""
        try:
            # Windows Performance CountersçµŒç”±ã§NPUä½¿ç”¨ç‡å–å¾—
            result = subprocess.run([
                'powershell', '-Command',
                'Get-Counter "\\GPU Engine(*)\\Utilization Percentage" -ErrorAction SilentlyContinue | Select-Object -ExpandProperty CounterSamples | Where-Object {$_.InstanceName -like "*NPU*"} | Measure-Object -Property CookedValue -Average | Select-Object -ExpandProperty Average'
            ], capture_output=True, text=True, timeout=2)
            
            if result.returncode == 0 and result.stdout.strip():
                return float(result.stdout.strip())
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: GPUä½¿ç”¨ç‡ã‹ã‚‰NPUæ¨å®š
            gpu_usage = psutil.virtual_memory().percent
            if gpu_usage > 50:  # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨æ™‚ã¯NPUå‹•ä½œã®å¯èƒ½æ€§
                return min(gpu_usage - 50, 100)
            
            return 0.0
            
        except Exception:
            return 0.0
    
    def check_provider_activity(self) -> str:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å‹•ä½œçŠ¶æ³ç¢ºèª"""
        try:
            if not self.session:
                return ""
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆæƒ…å ±å–å¾—
            profiling_info = ""
            
            if "DmlExecutionProvider" in self.active_provider:
                # DMLå‹•ä½œç¢ºèª
                profiling_info = "DML GPUå‡¦ç†ä¸­"
            elif "VitisAIExecutionProvider" in self.active_provider:
                # VitisAI NPUå‹•ä½œç¢ºèª
                profiling_info = "VitisAI NPUå‡¦ç†ä¸­"
            elif "CPUExecutionProvider" in self.active_provider:
                profiling_info = "CPUå‡¦ç†ä¸­"
            
            return profiling_info
            
        except Exception:
            return ""
    
    def create_ultra_lightweight_model(self) -> str:
        """è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆNPUå‹•ä½œãƒ­ã‚°ä»˜ãï¼‰"""
        try:
            print("ğŸ”§ è¶…è»½é‡ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆNPUå‹•ä½œãƒ­ã‚°ä»˜ãï¼‰...")
            
            # æœ€å°é™ã®Linearå±¤ã®ã¿ï¼ˆNPUå‹•ä½œç¢ºèªç”¨ï¼‰
            class NPUMonitoringModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # NPUå‹•ä½œãŒç¢ºèªã—ã‚„ã™ã„ã‚µã‚¤ã‚º
                    self.linear1 = nn.Linear(128, 256)
                    self.relu1 = nn.ReLU()
                    self.linear2 = nn.Linear(256, 512)
                    self.relu2 = nn.ReLU()
                    self.linear3 = nn.Linear(512, 256)
                    self.relu3 = nn.ReLU()
                    self.output = nn.Linear(256, 100)
                
                def forward(self, x):
                    print("ğŸ”¥ NPUãƒ¢ãƒ‡ãƒ« forward() å®Ÿè¡Œä¸­...")
                    x = self.relu1(self.linear1(x))
                    x = self.relu2(self.linear2(x))
                    x = self.relu3(self.linear3(x))
                    x = self.output(x)
                    print("âœ… NPUãƒ¢ãƒ‡ãƒ« forward() å®Œäº†")
                    return x
            
            model = NPUMonitoringModel()
            model.eval()
            
            # NPUå‹•ä½œãŒç¢ºèªã—ã‚„ã™ã„å…¥åŠ›ã‚µã‚¤ã‚º
            dummy_input = torch.randn(1, 128)
            
            print("ğŸ“Š NPUå‹•ä½œç¢ºèªç”¨ãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
            print(f"  å…¥åŠ›: (1, 128)")
            print(f"  Layer1: 128 â†’ 256 (ReLU)")
            print(f"  Layer2: 256 â†’ 512 (ReLU)")
            print(f"  Layer3: 512 â†’ 256 (ReLU)")
            print(f"  å‡ºåŠ›: 256 â†’ 100")
            
            # ONNX IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã§ç¢ºå®Ÿãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            onnx_path = "npu_monitoring_model.onnx"
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã«å¼·åˆ¶å¤‰æ›´ï¼ˆRyzenAI 1.5äº’æ›æ€§ï¼‰
            import onnx
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… NPUå‹•ä½œç›£è¦–ç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            print(f"ğŸ“‹ IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: 10 (RyzenAI 1.5äº’æ›)")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: NPUå‹•ä½œç¢ºèªæœ€é©åŒ–")
            
            return onnx_path
            
        except Exception as e:
            print(f"âŒ NPUç›£è¦–ç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def create_session_with_npu_logging(self, onnx_path: str) -> bool:
        """NPUå‹•ä½œãƒ­ã‚°ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        # æˆ¦ç•¥1: VitisAIExecutionProviderï¼ˆNPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ï¼‰
        print("ğŸ”§ æˆ¦ç•¥1: VitisAIExecutionProviderï¼ˆNPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ï¼‰...")
        try:
            providers = ['VitisAIExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'config_file': 'C:/Program Files/RyzenAI/1.5/voe-4.0-win_amd64/vaip_config.json',
                    'cacheDir': './vaip_cache',
                    'cacheKey': 'npu_monitoring'
                },
                {}
            ]
            
            print("ğŸ”¥ VitisAI NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            print("ğŸ“Š NPUå‹•ä½œç›£è¦–: é–‹å§‹")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            def create_session():
                session = ort.InferenceSession(
                    onnx_path,
                    providers=providers,
                    provider_options=provider_options
                )
                print("ğŸ¯ VitisAI NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼")
                return session
            
            # 45ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_result = self._run_with_timeout(create_session, 45)
            if session_result:
                self.session = session_result
                self.active_provider = self.session.get_providers()[0]
                print(f"âœ… VitisAI NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                print(f"ğŸ”¥ NPUå‹•ä½œçŠ¶æ³: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
                return True
            else:
                print("âš ï¸ VitisAI NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
        except Exception as e:
            print(f"âš ï¸ VitisAI NPUå¤±æ•—: {e}")
        
        # æˆ¦ç•¥2: DmlExecutionProviderï¼ˆGPU NPUå‹•ä½œãƒ­ã‚°ï¼‰
        print("ğŸ”§ æˆ¦ç•¥2: DmlExecutionProviderï¼ˆGPU NPUå‹•ä½œãƒ­ã‚°ï¼‰...")
        try:
            providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'device_id': 0,
                    'enable_dynamic_shapes': True,
                    'disable_metacommands': False
                },
                {}
            ]
            
            print("ğŸ”¥ DML GPU/NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            print("ğŸ“Š GPU/NPUå‹•ä½œç›£è¦–: é–‹å§‹")
            
            self.session = ort.InferenceSession(
                onnx_path,
                providers=providers,
                provider_options=provider_options
            )
            
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… DML GPU/NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"ğŸ”¥ GPU/NPUå‹•ä½œçŠ¶æ³: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
            return True
            
        except Exception as e:
            print(f"âš ï¸ DML GPU/NPUå¤±æ•—: {e}")
        
        # æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ”§ æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰...")
        try:
            providers = ['CPUExecutionProvider']
            self.session = ort.InferenceSession(onnx_path, providers=providers)
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… CPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"âš ï¸ NPUå‹•ä½œçŠ¶æ³: éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆCPUä½¿ç”¨ï¼‰")
            return True
            
        except Exception as e:
            print(f"âŒ CPUå¤±æ•—: {e}")
            return False
    
    def _run_with_timeout(self, func, timeout_seconds):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãé–¢æ•°å®Ÿè¡Œ"""
        result = [None]
        exception = [None]
        
        def target():
            try:
                result[0] = func()
            except Exception as e:
                exception[0] = e
        
        thread = threading.Thread(target=target)
        thread.daemon = True
        thread.start()
        thread.join(timeout_seconds)
        
        if thread.is_alive():
            print(f"âš ï¸ é–¢æ•°å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{timeout_seconds}ç§’ï¼‰")
            return None
        
        if exception[0]:
            raise exception[0]
        
        return result[0]
    
    def test_npu_inference_with_logging(self, num_inferences: int = 10) -> Dict[str, Any]:
        """NPUå‹•ä½œãƒ­ã‚°ä»˜ãæ¨è«–ãƒ†ã‚¹ãƒˆ"""
        if not self.session:
            raise RuntimeError("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ¯ NPUå‹•ä½œãƒ­ã‚°ä»˜ãæ¨è«–ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ{num_inferences}å›ï¼‰...")
        print(f"ğŸ”¥ NPUç›£è¦–: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–")
        print(f"ğŸ“Š ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
        
        # NPUå‹•ä½œç¢ºèªç”¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        input_data = np.random.randn(1, 128).astype(np.float32)
        input_name = self.session.get_inputs()[0].name
        
        successful_inferences = 0
        total_time = 0
        cpu_usage = []
        memory_usage = []
        npu_activity_detected = 0
        
        for i in range(num_inferences):
            try:
                # NPUå‹•ä½œå‰ã®çŠ¶æ³
                pre_npu_usage = self.get_npu_usage()
                
                # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ç›£è¦–
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                cpu_usage.append(cpu_percent)
                memory_usage.append(memory_percent)
                
                print(f"ğŸ”¥ æ¨è«– {i+1}: NPUå‹•ä½œç›£è¦–ä¸­...")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãæ¨è«–å®Ÿè¡Œ
                start_time = time.time()
                
                def run_inference():
                    print(f"âš¡ {self.active_provider} æ¨è«–å®Ÿè¡Œä¸­...")
                    result = self.session.run(None, {input_name: input_data})
                    print(f"âœ… {self.active_provider} æ¨è«–å®Œäº†")
                    return result
                
                result = self._run_with_timeout(run_inference, 15)  # 15ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result is not None:
                    inference_time = time.time() - start_time
                    total_time += inference_time
                    successful_inferences += 1
                    
                    # NPUå‹•ä½œå¾Œã®çŠ¶æ³
                    post_npu_usage = self.get_npu_usage()
                    if post_npu_usage > pre_npu_usage:
                        npu_activity_detected += 1
                        print(f"ğŸ”¥ NPUå‹•ä½œæ¤œå‡ºï¼ä½¿ç”¨ç‡: {pre_npu_usage:.1f}% â†’ {post_npu_usage:.1f}%")
                    
                    if (i + 1) % 5 == 0:
                        print(f"  âœ… æ¨è«– {i+1}/{num_inferences} å®Œäº† ({inference_time:.3f}ç§’)")
                        print(f"  ğŸ”¥ NPUå‹•ä½œæ¤œå‡ºå›æ•°: {npu_activity_detected}/{i+1}")
                else:
                    print(f"  âš ï¸ æ¨è«– {i+1} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
            except Exception as e:
                print(f"  âŒ æ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœè¨ˆç®—
        if successful_inferences > 0:
            avg_time = total_time / successful_inferences
            throughput = successful_inferences / total_time if total_time > 0 else 0
        else:
            avg_time = 0
            throughput = 0
        
        results = {
            'successful_inferences': successful_inferences,
            'total_inferences': num_inferences,
            'success_rate': successful_inferences / num_inferences * 100,
            'total_time': total_time,
            'average_time': avg_time,
            'throughput': throughput,
            'active_provider': self.active_provider,
            'avg_cpu_usage': np.mean(cpu_usage) if cpu_usage else 0,
            'avg_memory_usage': np.mean(memory_usage) if memory_usage else 0,
            'npu_activity_detected': npu_activity_detected,
            'npu_activity_rate': npu_activity_detected / successful_inferences * 100 if successful_inferences > 0 else 0
        }
        
        return results
    
    def load_fixed_text_model(self) -> bool:
        """ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        proven_models = [
            ("gpt2", "GPT-2"),
            ("distilgpt2", "DistilGPT-2"),
            ("microsoft/DialoGPT-small", "DialoGPT-Small")
        ]
        
        for model_name, display_name in proven_models:
            try:
                print(f"ğŸ¤– ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {display_name}")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
                def load_model():
                    if "gpt2" in model_name.lower():
                        # GPT-2ç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆç¢ºå®Ÿãªç”Ÿæˆï¼‰
                        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
                        tokenizer.pad_token = tokenizer.eos_token
                        
                        model = GPT2LMHeadModel.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,  # å®‰å®šæ€§é‡è¦–
                            device_map=None  # CPUä½¿ç”¨
                        )
                        
                        print(f"âœ… GPT-2ç³»ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                        
                    else:
                        # DialoGPTç³»ãƒ¢ãƒ‡ãƒ«
                        tokenizer = AutoTokenizer.from_pretrained(model_name)
                        tokenizer.pad_token = tokenizer.eos_token
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            device_map=None
                        )
                        
                        print(f"âœ… DialoGPTç³»ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                    
                    return tokenizer, model
                
                result = self._run_with_timeout(load_model, 120)  # 120ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result:
                    self.tokenizer, self.model = result
                    print(f"âœ… ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {display_name}")
                    print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èªå½™æ•°: {len(self.tokenizer)}")
                    print(f"ğŸ”§ ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³: {self.tokenizer.pad_token}")
                    return True
                else:
                    print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {display_name}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {display_name} - {e}")
                continue
        
        print("âŒ å…¨ã¦ã®ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—")
        return False
    
    def generate_text_fixed(self, prompt: str, max_tokens: int = 50) -> str:
        """ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆç¢ºå®Ÿãªå‡ºåŠ›ï¼‰"""
        if not self.model or not self.tokenizer:
            return "âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        try:
            print(f"ğŸ’¬ ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
            print(f"ğŸ¯ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            input_length = inputs.shape[1]
            
            print(f"ğŸ“Š å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_length}")
            
            # ç”Ÿæˆè¨­å®šï¼ˆç¢ºå®Ÿãªå‡ºåŠ›ã®ãŸã‚ï¼‰
            generation_config = {
                'max_new_tokens': max_tokens,  # max_lengthã§ã¯ãªãmax_new_tokensä½¿ç”¨
                'min_new_tokens': 5,  # æœ€å°ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
                'do_sample': True,
                'temperature': 0.8,
                'top_p': 0.9,
                'top_k': 50,
                'repetition_penalty': 1.1,
                'pad_token_id': self.tokenizer.eos_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'no_repeat_ngram_size': 2
            }
            
            print(f"ğŸ”§ ç”Ÿæˆè¨­å®š: {generation_config}")
            
            def generate():
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        **generation_config
                    )
                    
                    # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                    generated_text = self.tokenizer.decode(
                        outputs[0], 
                        skip_special_tokens=True
                    )
                    
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                    if generated_text.startswith(prompt):
                        generated_text = generated_text[len(prompt):].strip()
                    
                    print(f"ğŸ“Š ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {outputs.shape[1] - input_length}")
                    
                    return generated_text
            
            result = self._run_with_timeout(generate, 60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            if result and result.strip():
                print(f"âœ… ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"ğŸ“ ç”Ÿæˆæ–‡å­—æ•°: {len(result)}")
                return result
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
                print("âš ï¸ æ¨™æº–ç”ŸæˆãŒç©ºã®ãŸã‚ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚’å®Ÿè¡Œ")
                fallback_result = self.generate_fallback_text(prompt, max_tokens)
                return fallback_result
                
        except Exception as e:
            print(f"âŒ ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self.generate_fallback_text(prompt, max_tokens)
    
    def generate_fallback_text(self, prompt: str, max_tokens: int) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œä¸­...")
            
            # ã‚ˆã‚Šå˜ç´”ãªç”Ÿæˆè¨­å®š
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_tokens,
                    do_sample=False,  # æ±ºå®šçš„ç”Ÿæˆ
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
                
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                
                if generated_text.strip():
                    print("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”ŸæˆæˆåŠŸ")
                    return generated_text
                else:
                    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    return f"[{prompt}ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆä¸­...] ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ãŒå¿œç­”ã‚’æº–å‚™ã—ã¦ã„ã¾ã™ã€‚"
                    
        except Exception as e:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚'{prompt}'ã«å¯¾ã™ã‚‹å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèªä¸­ã§ã™ã€‚"
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # 1. NPUå‹•ä½œç›£è¦–ç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            onnx_path = self.create_ultra_lightweight_model()
            
            # 2. NPUå‹•ä½œãƒ­ã‚°ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_session_with_npu_logging(onnx_path):
                print("âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # 3. NPUå‹•ä½œãƒ­ã‚°ä»˜ãæ¨è«–ãƒ†ã‚¹ãƒˆ
            print("ğŸ”§ NPUå‹•ä½œãƒ­ã‚°ä»˜ãæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_result = self.test_npu_inference_with_logging(5)  # 5å›ãƒ†ã‚¹ãƒˆ
            
            if test_result['successful_inferences'] > 0:
                print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ: {test_result['successful_inferences']}/5å›æˆåŠŸ")
                print(f"ğŸ“Š æˆåŠŸç‡: {test_result['success_rate']:.1f}%")
                print(f"ğŸ”¥ NPUå‹•ä½œæ¤œå‡º: {test_result['npu_activity_detected']}/5å›")
                print(f"ğŸ“ˆ NPUå‹•ä½œç‡: {test_result['npu_activity_rate']:.1f}%")
            else:
                print("âš ï¸ NPUæ¨è«–ãƒ†ã‚¹ãƒˆã§æˆåŠŸã—ãŸæ¨è«–ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # 4. ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            if not self.load_fixed_text_model():
                print("âš ï¸ ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€NPUæ¨è«–ã¯åˆ©ç”¨å¯èƒ½ã§ã™")
            
            print("âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¿®æ­£ç‰ˆ+NPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_benchmark(self, num_inferences: int = 50) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸ“Š NPUå‹•ä½œãƒ­ã‚°ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ï¼ˆ{num_inferences}å›æ¨è«–ï¼‰...")
        
        start_time = time.time()
        results = self.test_npu_inference_with_logging(num_inferences)
        total_benchmark_time = time.time() - start_time
        
        print(f"\nğŸ¯ NPUå‹•ä½œãƒ­ã‚°ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {results['successful_inferences']}/{results['total_inferences']}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_benchmark_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {results['throughput']:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {results['average_time']*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['active_provider']}")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {results['avg_cpu_usage']:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {results['avg_memory_usage']:.1f}%")
        print(f"  ğŸ”¥ NPUå‹•ä½œæ¤œå‡ºå›æ•°: {results['npu_activity_detected']}/{results['successful_inferences']}")
        print(f"  ğŸ“ˆ NPUå‹•ä½œç‡: {results['npu_activity_rate']:.1f}%")
        print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ® ä¿®æ­£ç‰ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print("ğŸ’¡ 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
        print("ğŸ’¡ 'benchmark' ã§NPUå‹•ä½œãƒ­ã‚°ä»˜ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        print("ğŸ’¡ 'status' ã§ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ç¢ºèª")
        print("ğŸ’¡ 'npu' ã§NPUå‹•ä½œçŠ¶æ³ç¢ºèª")
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    self.npu_monitoring_active = False
                    break
                
                elif user_input.lower() == 'benchmark':
                    self.run_benchmark(30)
                
                elif user_input.lower() == 'status':
                    print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                    print(f"ğŸ¤– ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: {'åˆ©ç”¨å¯èƒ½' if self.model else 'åˆ©ç”¨ä¸å¯'}")
                    print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
                    print(f"ğŸ“Š NPUç›£è¦–: {'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if self.npu_monitoring_active else 'éã‚¢ã‚¯ãƒ†ã‚£ãƒ–'}")
                
                elif user_input.lower() == 'npu':
                    npu_usage = self.get_npu_usage()
                    provider_status = self.check_provider_activity()
                    print(f"ğŸ”¥ ç¾åœ¨ã®NPUä½¿ç”¨ç‡: {npu_usage:.1f}%")
                    print(f"âš¡ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çŠ¶æ³: {provider_status}")
                    print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                
                elif user_input:
                    if self.model:
                        print(f"ğŸ”¥ NPUå‹•ä½œç›£è¦–ä¸­...")
                        generated_text = self.generate_text_fixed(user_input, 50)
                        print(f"\nğŸ¯ ä¿®æ­£ç‰ˆç”Ÿæˆçµæœ:\n{generated_text}")
                    else:
                        print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚NPUæ¨è«–ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚")
                        # NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
                        results = self.test_npu_inference_with_logging(5)
                        print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆ: {results['successful_inferences']}/5å›æˆåŠŸ")
                        print(f"ğŸ”¥ NPUå‹•ä½œæ¤œå‡º: {results['npu_activity_detected']}/5å›")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                self.npu_monitoring_active = False
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    parser = argparse.ArgumentParser(description="ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¿®æ­£ç‰ˆ+NPUå‹•ä½œãƒ­ã‚°å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--inferences", type=int, default=30, help="æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=50, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--timeout", type=int, default=30, help="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    # infer-OSè¨­å®š
    if args.infer_os:
        os.environ['INFER_OS_ENABLED'] = '1'
    
    try:
        if args.compare:
            print("ğŸ“Š infer-OS ON/OFFæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            
            # OFFç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '0'
            print("\nğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰:")
            system_off = EnhancedNPULLMSystem(args.timeout)
            if system_off.initialize_system():
                results_off = system_off.run_benchmark(args.inferences)
                system_off.npu_monitoring_active = False
            else:
                print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã«å¤±æ•—")
                return
            
            # ONç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '1'
            print("\nâš¡ æœ€é©åŒ–ç‰ˆæ¸¬å®šï¼ˆinfer-OS ONï¼‰:")
            system_on = EnhancedNPULLMSystem(args.timeout)
            if system_on.initialize_system():
                results_on = system_on.run_benchmark(args.inferences)
                system_on.npu_monitoring_active = False
            else:
                print("âŒ æœ€é©åŒ–ç‰ˆæ¸¬å®šã«å¤±æ•—")
                return
            
            # æ¯”è¼ƒçµæœ
            print(f"\nğŸ“Š infer-OSåŠ¹æœæ¸¬å®šçµæœ:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ğŸ”¥ NPUå‹•ä½œç‡ï¼ˆOFFï¼‰: {results_off['npu_activity_rate']:.1f}%")
            print(f"  ğŸ”¥ NPUå‹•ä½œç‡ï¼ˆONï¼‰: {results_on['npu_activity_rate']:.1f}%")
            
            if results_off['throughput'] > 0:
                improvement = (results_on['throughput'] - results_off['throughput']) / results_off['throughput'] * 100
                print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„ç‡: {improvement:+.1f}%")
            
        else:
            # é€šå¸¸å®Ÿè¡Œ
            system = EnhancedNPULLMSystem(args.timeout)
            
            if not system.initialize_system():
                print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            if args.interactive:
                system.interactive_mode()
            elif args.prompt:
                if system.model:
                    print(f"ğŸ”¥ NPUå‹•ä½œç›£è¦–ä¸­...")
                    generated_text = system.generate_text_fixed(args.prompt, args.tokens)
                    print(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
                    print(f"ğŸ¯ ä¿®æ­£ç‰ˆç”Ÿæˆçµæœ:\n{generated_text}")
                else:
                    print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    results = system.test_npu_inference_with_logging(args.inferences)
                    print(f"âœ… NPUæ¨è«–ãƒ†ã‚¹ãƒˆ: {results['successful_inferences']}/{args.inferences}å›æˆåŠŸ")
                    print(f"ğŸ”¥ NPUå‹•ä½œæ¤œå‡º: {results['npu_activity_detected']}/{args.inferences}å›")
            else:
                system.run_benchmark(args.inferences)
            
            # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰åœæ­¢
            system.npu_monitoring_active = False
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

