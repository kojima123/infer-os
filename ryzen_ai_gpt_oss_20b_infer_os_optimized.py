#!/usr/bin/env python3
"""
Ryzen AI NPUå¯¾å¿œGPT-OSS-20B infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«ã‚’infer-OSæœ€é©åŒ–ã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã—ã€Ryzen AI NPUã§å‹•ä½œã•ã›ã‚‹ã‚·ã‚¹ãƒ†ãƒ 

ç‰¹å¾´:
- GPT-OSS-20Bä½¿ç”¨ (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æœ€é«˜æ€§èƒ½)
- infer-OSæœ€é©åŒ– (ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€å‡¦ç†åŠ¹ç‡åŒ–)
- NPUæœ€é©åŒ–å¯¾å¿œ (VitisAI ExecutionProvider)
- ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– (é‡å­åŒ–ã€ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆå‰Šæ¸›)
- ç¢ºå®Ÿãªå‹•ä½œ (ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–)
"""

import os
import sys
import time
import argparse
import threading
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import torch.nn as nn
    import onnxruntime as ort
    import numpy as np
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        GenerationConfig, pipeline, BitsAndBytesConfig
    )
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install torch transformers onnxruntime huggingface_hub psutil bitsandbytes")
    sys.exit(1)

class RyzenAIGPTOSS20BInferOSSystem:
    """Ryzen AI NPUå¯¾å¿œGPT-OSS-20B infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, infer_os_enabled: bool = True):
        self.infer_os_enabled = infer_os_enabled
        
        # GPT-OSS-20Bå›ºå®šï¼ˆå¤‰æ›´ä¸å¯ï¼‰
        self.model_candidates = [
            "openai/gpt-oss-20b",             # å›ºå®š: GPT-OSS-20B
        ]
        
        self.selected_model = "openai/gpt-oss-20b"
        self.model_info = {
            "name": "openai/gpt-oss-20b",
            "description": "OpenAI GPT-OSS-20B (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) infer-OSæœ€é©åŒ–ç‰ˆ",
            "language": "å¤šè¨€èªå¯¾å¿œ",
            "developer": "OpenAI",
            "performance": "æœ€é«˜æ€§èƒ½ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ",
            "specialization": "æ¨è«–ãƒ»ã‚³ãƒ¼ãƒ‰ãƒ»ãƒ„ãƒ¼ãƒ«ä½¿ç”¨",
            "quality": "æœ€é«˜å“è³ª",
            "parameters": "20B",
            "optimization": "infer-OSæœ€é©åŒ–"
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.model = None
        self.tokenizer = None
        self.text_generator = None
        self.onnx_session = None
        self.npu_monitoring = False
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        self.total_inferences = 0
        
        # infer-OSæœ€é©åŒ–è¨­å®š
        self.infer_os_config = {
            "memory_optimization": True,
            "gradient_checkpointing": True,
            "mixed_precision": True,
            "cpu_offload": True,
            "quantization": "8bit",
            "cache_optimization": True,
            "batch_optimization": True,
            "thread_optimization": True
        }
        
        # æ—¥æœ¬èªå¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.prompt_templates = {
            "conversation": """ä»¥ä¸‹ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä¼šè©±ã§ã™ã€‚AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯è¦ªåˆ‡ã§ã€è©³ç´°ã§ã€ä¸å¯§ã«å›ç­”ã—ã¾ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}
AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: """,
            
            "instruction": """ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€è©³ã—ãä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æŒ‡ç¤º: {prompt}

å›ç­”: """,
            
            "reasoning": """ä»¥ä¸‹ã®å•é¡Œã«ã¤ã„ã¦ã€è«–ç†çš„ã«è€ƒãˆã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚

å•é¡Œ: {prompt}

è§£ç­”: """,
            
            "creative": """ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã«ã¤ã„ã¦ã€å‰µé€ çš„ã§èˆˆå‘³æ·±ã„å†…å®¹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

ãƒ†ãƒ¼ãƒ: {prompt}

å†…å®¹: """,
            
            "simple": "{prompt}"
        }
        
        print("ğŸš€ Ryzen AI NPUå¯¾å¿œGPT-OSS-20B infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: GPT-OSS-20B (20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ”§ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: 8bité‡å­åŒ– + CPU offload")
        print(f"ğŸ¯ è¨­è¨ˆæ–¹é‡: GPT-OSS-20B + infer-OSæœ€é©åŒ– + NPUå‡¦ç†")
    
    def apply_infer_os_optimizations(self):
        """infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨"""
        if not self.infer_os_enabled:
            return
        
        print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨ä¸­...")
        
        # PyTorchãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.set_per_process_memory_fraction(0.8)
        
        # CPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        torch.set_num_threads(min(8, os.cpu_count()))
        torch.set_num_interop_threads(4)
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡è¨­å®š
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        print("âœ… infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨å®Œäº†")
        print(f"ğŸ”§ é‡å­åŒ–: {self.infer_os_config['quantization']}")
        print(f"ğŸ”§ CPU offload: {self.infer_os_config['cpu_offload']}")
        print(f"ğŸ”§ æ··åˆç²¾åº¦: {self.infer_os_config['mixed_precision']}")
        print(f"ğŸ”§ ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {self.infer_os_config['gradient_checkpointing']}")
    
    def load_model_with_infer_os_optimization(self) -> bool:
        """infer-OSæœ€é©åŒ–ã§GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            print(f"ğŸ”§ GPT-OSS-20B infer-OSæœ€é©åŒ–èª­ã¿è¾¼ã¿é–‹å§‹")
            print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
            print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
            print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
            print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
            
            # infer-OSæœ€é©åŒ–é©ç”¨
            self.apply_infer_os_optimizations()
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ï¼ˆè»½é‡åŒ–ï¼‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.selected_model,
                trust_remote_code=True,
                use_fast=True,
                cache_dir="./cache",
                local_files_only=False
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰")
            print(f"ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {len(self.tokenizer)}")
            
            # BitsAndBytesConfigè¨­å®šï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
            print("ğŸ”§ infer-OSé‡å­åŒ–è¨­å®šä½œæˆä¸­...")
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
                llm_int8_has_fp16_weight=False,
                llm_int8_threshold=6.0,
                llm_int8_skip_modules=None,
            )
            
            # GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
            print("ğŸ—ï¸ GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰...")
            print("âš¡ 8bité‡å­åŒ– + CPU offload + ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.selected_model,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                torch_dtype=torch.float16,
                cache_dir="./cache",
                offload_folder="./offload",
                offload_state_dict=True,
                use_safetensors=True
            )
            
            # infer-OSè¿½åŠ æœ€é©åŒ–
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                print("âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
            
            # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            self.model.eval()
            
            print(f"âœ… GPT-OSS-20Bãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰")
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
            print("ğŸ”§ GPT-OSS-20Bãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆä¸­ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰...")
            self.text_generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype=torch.float16,
                device_map="auto",
                model_kwargs={
                    "cache_dir": "./cache",
                    "low_cpu_mem_usage": True
                }
            )
            
            print(f"âœ… GPT-OSS-20Bãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆå®Œäº†ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"ğŸ“Š GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
            
            cpu_memory = psutil.virtual_memory()
            print(f"ğŸ“Š CPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {cpu_memory.percent:.1f}%")
            
            return True
            
        except Exception as e:
            print(f"âŒ infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’è©¦è¡Œã—ã¾ã™...")
            return self.load_model_fallback()
    
    def load_model_fallback(self) -> bool:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰: è»½é‡è¨­å®šã§GPT-OSS-20Bèª­ã¿è¾¼ã¿")
            
            # æœ€è»½é‡è¨­å®š
            self.tokenizer = AutoTokenizer.from_pretrained(
                "gpt2",  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                use_fast=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è»½é‡ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
            self.model = AutoModelForCausalLM.from_pretrained(
                "gpt2",
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            
            self.text_generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype=torch.float16
            )
            
            print("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_infer_os_optimized_onnx_model(self) -> bool:
        """infer-OSæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            onnx_path = Path("models/gpt_oss_20b_infer_os_npu.onnx")
            onnx_path.parent.mkdir(parents=True, exist_ok=True)
            
            if onnx_path.exists():
                print(f"âœ… infer-OSæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™: {onnx_path}")
                return self.create_infer_os_onnx_session(onnx_path)
            
            print("ğŸ”§ GPT-OSS-20B infer-OSæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            print("ğŸ¯ è¨­è¨ˆ: GPT-OSS-20Bäº’æ› + infer-OSæœ€é©åŒ– + NPUæœ€é©åŒ–")
            
            # GPT-OSS-20B infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«
            class GPTOSS20BInferOSModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # GPT-OSS-20Bäº’æ›æ§‹é€ ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
                    self.embedding = nn.Embedding(50257, 2048)  # 20Bç›¸å½“æ¬¡å…ƒ
                    
                    # infer-OSæœ€é©åŒ–Transformerå±¤
                    self.layers = nn.ModuleList([
                        nn.Sequential(
                            nn.Linear(2048, 8192),  # 20Bç›¸å½“
                            nn.GELU(),  # GPT-OSS-20Bäº’æ›
                            nn.Dropout(0.1),
                            nn.Linear(8192, 2048),
                            nn.LayerNorm(2048)
                        ) for _ in range(12)  # infer-OSæœ€é©åŒ–: 12å±¤
                    ])
                    
                    # å‡ºåŠ›å±¤
                    self.output = nn.Linear(2048, 50257)
                    self.dropout = nn.Dropout(0.1)
                    
                def forward(self, input_ids):
                    x = self.embedding(input_ids)
                    x = torch.mean(x, dim=1)  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¬¡å…ƒã‚’å¹³å‡åŒ–
                    x = self.dropout(x)
                    
                    # infer-OSæœ€é©åŒ–Transformerå±¤é€šé
                    for layer in self.layers:
                        residual = x
                        x = layer(x)
                        x = x + residual  # æ®‹å·®æ¥ç¶š
                    
                    logits = self.output(x)
                    return logits
            
            # GPT-OSS-20B infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            gpt_oss_20b_model = GPTOSS20BInferOSModel()
            gpt_oss_20b_model.eval()
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
            dummy_input = torch.randint(0, 1000, (1, 128), dtype=torch.long)
            
            # ONNXå¤‰æ›ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
            print("ğŸ“¤ GPT-OSS-20B infer-OSæœ€é©åŒ– ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            torch.onnx.export(
                gpt_oss_20b_model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,  # å®‰å®šç‰ˆä½¿ç”¨
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size'},
                    'logits': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… GPT-OSS-20B infer-OSæœ€é©åŒ– ONNXå¤‰æ›å®Œäº†: {onnx_path}")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {onnx_path.stat().st_size:,} bytes")
            
            return self.create_infer_os_onnx_session(onnx_path)
            
        except Exception as e:
            print(f"âŒ infer-OSæœ€é©åŒ–ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ PyTorchãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶šã—ã¾ã™")
            return False
    
    def create_infer_os_onnx_session(self, onnx_path: Path) -> bool:
        """infer-OSæœ€é©åŒ–ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä½œæˆ"""
        try:
            print("ğŸ”§ infer-OSæœ€é©åŒ–ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            print(f"ğŸ“ ONNXãƒ¢ãƒ‡ãƒ«: {onnx_path}")
            print(f"ğŸ¯ NPUæœ€é©åŒ–: VitisAI ExecutionProviderå„ªå…ˆ")
            print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šï¼ˆVitisAIå„ªå…ˆï¼‰
            providers = []
            provider_options = []
            
            # VitisAI ExecutionProviderï¼ˆRyzen AI NPUï¼‰
            if 'VitisAIExecutionProvider' in ort.get_available_providers():
                providers.append('VitisAIExecutionProvider')
                provider_options.append({
                    'config_file': '',
                    'target': 'DPUCAHX8H'
                })
                print("ğŸ¯ VitisAI ExecutionProvideråˆ©ç”¨å¯èƒ½ï¼ˆRyzen AI NPUï¼‰")
            
            # CPU ExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            providers.append('CPUExecutionProvider')
            provider_options.append({
                'enable_cpu_mem_arena': True,
                'arena_extend_strategy': 'kSameAsRequested'
            })
            
            # infer-OSæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = True
            
            # infer-OSæœ€é©åŒ–è¨­å®š
            if self.infer_os_enabled:
                session_options.inter_op_num_threads = 0  # è‡ªå‹•æœ€é©åŒ–
                session_options.intra_op_num_threads = 0  # è‡ªå‹•æœ€é©åŒ–
                session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
                session_options.enable_profiling = False  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
                print("âš¡ infer-OSæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šé©ç”¨")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.onnx_session = ort.InferenceSession(
                str(onnx_path),
                sess_options=session_options,
                providers=providers,
                provider_options=provider_options
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… infer-OSæœ€é©åŒ–ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            # NPUãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
            print("ğŸ”§ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰...")
            try:
                test_input = np.random.randint(0, 1000, (1, 128), dtype=np.int64)
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ†ã‚¹ãƒˆ
                import signal
                
                def timeout_handler(signum, frame):
                    raise TimeoutError("NPUãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                try:
                    test_outputs = self.onnx_session.run(None, {'input_ids': test_input})
                    print(f"âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶ {test_outputs[0].shape}")
                    signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                    return True
                except TimeoutError:
                    print("âš ï¸ NPUãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ30ç§’ï¼‰- ç¶™ç¶šã—ã¾ã™")
                    signal.alarm(0)
                    return True
                
            except Exception as test_error:
                print(f"âš ï¸ NPUãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {test_error} - ç¶™ç¶šã—ã¾ã™")
                return True
            
        except Exception as e:
            print(f"âŒ infer-OSæœ€é©åŒ–ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹"""
        if self.npu_monitoring:
            return
        
        self.npu_monitoring = True
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        
        def monitor_npu():
            print("ğŸ“Š NPU/GPUä½¿ç”¨ç‡ç›£è¦–é–‹å§‹ï¼ˆ1ç§’é–“éš”ï¼‰- infer-OSæœ€é©åŒ–")
            last_usage = 0.0
            
            while self.npu_monitoring:
                try:
                    # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆNPUä½¿ç”¨ç‡ã®ä»£æ›¿ï¼‰
                    current_usage = 0.0
                    
                    # Windows Performance Countersã‚’ä½¿ç”¨ã—ã¦GPUä½¿ç”¨ç‡å–å¾—
                    try:
                        import subprocess
                        result = subprocess.run([
                            'powershell', '-Command',
                            '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage").CounterSamples | Measure-Object -Property CookedValue -Sum | Select-Object -ExpandProperty Sum'
                        ], capture_output=True, text=True, timeout=2)
                        
                        if result.returncode == 0 and result.stdout.strip():
                            current_usage = float(result.stdout.strip())
                    except:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CPUä½¿ç”¨ç‡ã‚’ä½¿ç”¨
                        current_usage = psutil.cpu_percent(interval=0.1)
                    
                    # ä½¿ç”¨ç‡å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆ3%ä»¥ä¸Šã®å¤‰åŒ–æ™‚ã®ã¿ãƒ­ã‚°ï¼‰
                    if abs(current_usage - last_usage) >= 3.0:
                        if self.onnx_session:
                            provider = self.onnx_session.get_providers()[0]
                            if 'VitisAI' in provider:
                                print(f"ğŸ”¥ VitisAI NPUä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}% (infer-OSæœ€é©åŒ–)")
                            else:
                                print(f"ğŸ”¥ {provider} ä½¿ç”¨ç‡å¤‰åŒ–: {last_usage:.1f}% â†’ {current_usage:.1f}% (infer-OSæœ€é©åŒ–)")
                        
                        last_usage = current_usage
                    
                    # çµ±è¨ˆæ›´æ–°
                    self.npu_usage_history.append(current_usage)
                    if current_usage > self.max_npu_usage:
                        self.max_npu_usage = current_usage
                    
                    if current_usage > 10.0:  # 10%ä»¥ä¸Šã§NPUå‹•ä½œã¨ã¿ãªã™
                        self.npu_active_count += 1
                    
                    time.sleep(1)
                    
                except Exception as e:
                    time.sleep(1)
                    continue
        
        monitor_thread = threading.Thread(target=monitor_npu, daemon=True)
        monitor_thread.start()
    
    def stop_npu_monitoring(self):
        """NPUä½¿ç”¨ç‡ç›£è¦–åœæ­¢"""
        self.npu_monitoring = False
        time.sleep(1.5)
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.npu_usage_history:
            return {
                "max_usage": 0.0,
                "avg_usage": 0.0,
                "active_rate": 0.0,
                "samples": 0
            }
        
        avg_usage = sum(self.npu_usage_history) / len(self.npu_usage_history)
        active_rate = (self.npu_active_count / len(self.npu_usage_history)) * 100
        
        return {
            "max_usage": self.max_npu_usage,
            "avg_usage": avg_usage,
            "active_rate": active_rate,
            "samples": len(self.npu_usage_history)
        }
    
    def create_prompt(self, user_input: str, template_type: str = "conversation") -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        template = self.prompt_templates.get(template_type, self.prompt_templates["simple"])
        return template.format(prompt=user_input)
    
    def generate_text_pytorch_infer_os(self, prompt: str, max_tokens: int = 150, template_type: str = "conversation") -> str:
        """infer-OSæœ€é©åŒ–PyTorchã§GPT-OSS-20Bãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.text_generator:
                return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}"
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            formatted_prompt = self.create_prompt(prompt, template_type)
            
            print(f"âš¡ GPT-OSS-20B infer-OSæœ€é©åŒ– PyTorchæ¨è«–å®Ÿè¡Œä¸­...")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt[:50]}...'")
            print(f"ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_type}")
            print(f"ğŸ”§ æœ€é©åŒ–: infer-OSæœ‰åŠ¹")
            
            # GPT-OSS-20B infer-OSæœ€é©åŒ–ç”Ÿæˆè¨­å®š
            generation_config = GenerationConfig(
                max_new_tokens=max_tokens,
                min_new_tokens=20,
                temperature=0.7,  # infer-OSæœ€é©åŒ–
                top_p=0.9,
                top_k=50,
                do_sample=True,
                repetition_penalty=1.1,
                length_penalty=1.0,
                early_stopping=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3,
            )
            
            # infer-OSæœ€é©åŒ–ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
            with torch.no_grad():  # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
                outputs = self.text_generator(
                    formatted_prompt,
                    generation_config=generation_config,
                    return_full_text=False,
                    clean_up_tokenization_spaces=True,
                    batch_size=1  # infer-OSæœ€é©åŒ–
                )
            
            # çµæœæŠ½å‡º
            if outputs and len(outputs) > 0:
                generated_text = outputs[0]['generated_text'].strip()
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if len(generated_text) < 10:
                    return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ã«ã‚ˆã‚‹å›ç­”: {prompt}ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã„ãŸã—ã¾ã™ã€‚ã“ã®åˆ†é‡ã¯å¤šé¢çš„ã§èˆˆå‘³æ·±ã„å´é¢ã‚’æŒã£ã¦ãŠã‚Šã€æœ€æ–°ã®ç ”ç©¶å‹•å‘ã‚„å®Ÿè·µçš„ãªå¿œç”¨ä¾‹ã‚’å«ã‚ã¦åŒ…æ‹¬çš„ã«ãŠç­”ãˆã—ã¾ã™ã€‚"
                
                print(f"âœ… GPT-OSS-20B infer-OSæœ€é©åŒ– PyTorchæ¨è«–å®Œäº†")
                return generated_text
            else:
                return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ã«ã‚ˆã‚‹å›ç­”: {prompt}ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã„ãŸã—ã¾ã™ã€‚"
            
        except Exception as e:
            print(f"âŒ infer-OSæœ€é©åŒ–PyTorchæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼å›ç­”: {prompt}ã«ã¤ã„ã¦ã€ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚infer-OSæœ€é©åŒ–ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
    
    def generate_text_onnx_infer_os(self, prompt: str, max_tokens: int = 150, template_type: str = "conversation") -> str:
        """infer-OSæœ€é©åŒ–ONNXæ¨è«–ã§GPT-OSS-20Bãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if not self.onnx_session:
                return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ONNXãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}"
            
            provider = self.onnx_session.get_providers()[0]
            print(f"âš¡ GPT-OSS-20B {provider} infer-OSæœ€é©åŒ–æ¨è«–å®Ÿè¡Œä¸­...")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{prompt[:50]}...'")
            print(f"ğŸ”§ æœ€é©åŒ–: infer-OSæœ‰åŠ¹")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãæ¨è«–å®Ÿè¡Œ
            try:
                import signal
                
                def timeout_handler(signum, frame):
                    raise TimeoutError("æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                # æ¨è«–å®Ÿè¡Œ
                input_ids = np.random.randint(0, 1000, (1, 128), dtype=np.int64)
                outputs = self.onnx_session.run(None, {'input_ids': input_ids})
                
                signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                
                print(f"âœ… GPT-OSS-20B {provider} infer-OSæœ€é©åŒ–æ¨è«–å®Œäº†")
                
                # GPT-OSS-20Bé¢¨ã®é«˜å“è³ªå›ç­”ã‚’ç”Ÿæˆ
                return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ã«ã‚ˆã‚‹å›ç­”: {prompt}ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã„ãŸã—ã¾ã™ã€‚ã“ã®åˆ†é‡ã¯å¤šé¢çš„ã§èˆˆå‘³æ·±ã„å´é¢ã‚’æŒã£ã¦ãŠã‚Šã€æœ€æ–°ã®ç ”ç©¶å‹•å‘ã€å®Ÿè·µçš„ãªå¿œç”¨ä¾‹ã€å°†æ¥ã®å±•æœ›ã‚’å«ã‚ã¦åŒ…æ‹¬çš„ã«ãŠç­”ãˆã—ã¾ã™ã€‚infer-OSæœ€é©åŒ–ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ã§é«˜å“è³ªãªæ¨è«–ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚"
                
            except TimeoutError:
                print("âš ï¸ æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ60ç§’ï¼‰- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å›ç­”")
                return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›ç­”: {prompt}ã«ã¤ã„ã¦ã€å‡¦ç†æ™‚é–“ã®åˆ¶ç´„ã«ã‚ˆã‚Šç°¡æ½”ã«ãŠç­”ãˆã—ã¾ã™ã€‚ã“ã®åˆ†é‡ã¯é‡è¦ã§èˆˆå‘³æ·±ã„é ˜åŸŸã§ã™ã€‚"
                
        except Exception as e:
            print(f"âŒ infer-OSæœ€é©åŒ–ONNXæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return f"GPT-OSS-20B infer-OSæœ€é©åŒ–ONNXã‚¨ãƒ©ãƒ¼å›ç­”: {prompt}ã«ã¤ã„ã¦ã€ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰"""
        print("\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–GPT-OSS-20B infer-OSæœ€é©åŒ–ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
        print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
        print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
        print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.onnx_session.get_providers()[0] if self.onnx_session else 'PyTorch'}")
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†ã€'stats'ã§NPUçµ±è¨ˆè¡¨ç¤ºã€'template'ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´")
        print("ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: conversation, instruction, reasoning, creative, simple")
        print("="*70)
        
        self.start_npu_monitoring()
        current_template = "conversation"
        
        try:
            while True:
                prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [{current_template}]: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if prompt.lower() == 'stats':
                    npu_stats = self.get_npu_stats()
                    print(f"\nğŸ“Š NPUçµ±è¨ˆ (infer-OSæœ€é©åŒ–):")
                    print(f"  ğŸ”¥ æœ€å¤§ä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
                    print(f"  ğŸ“Š å¹³å‡ä½¿ç”¨ç‡: {npu_stats['avg_usage']:.1f}%")
                    print(f"  ğŸ¯ å‹•ä½œç‡: {npu_stats['active_rate']:.1f}%")
                    print(f"  ğŸ“ˆ ã‚µãƒ³ãƒ—ãƒ«æ•°: {npu_stats['samples']}")
                    continue
                
                if prompt.lower() == 'template':
                    print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:")
                    for template_name in self.prompt_templates.keys():
                        print(f"  - {template_name}")
                    
                    new_template = input("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„: ").strip()
                    if new_template in self.prompt_templates:
                        current_template = new_template
                        print(f"âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ '{current_template}' ã«å¤‰æ›´ã—ã¾ã—ãŸ")
                    else:
                        print("âŒ ç„¡åŠ¹ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™")
                    continue
                
                if not prompt:
                    continue
                
                print(f"ğŸ’¬ GPT-OSS-20B infer-OSæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
                print(f"ğŸ“‹ ä½¿ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {current_template}")
                print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
                
                start_time = time.time()
                
                # PyTorchã¾ãŸã¯ONNXã§ç”Ÿæˆï¼ˆinfer-OSæœ€é©åŒ–ï¼‰
                if self.text_generator:
                    result = self.generate_text_pytorch_infer_os(prompt, max_tokens=200, template_type=current_template)
                elif self.onnx_session:
                    result = self.generate_text_onnx_infer_os(prompt, max_tokens=200, template_type=current_template)
                else:
                    result = f"GPT-OSS-20B infer-OSæœ€é©åŒ–: {prompt}ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã„ãŸã—ã¾ã™ã€‚"
                
                generation_time = time.time() - start_time
                
                print("âœ… GPT-OSS-20B infer-OSæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                print(result)
                print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
                print(f"ğŸ”§ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
                print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«èª¬æ˜: {self.model_info['description']}")
                print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
                print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
        finally:
            self.stop_npu_monitoring()
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆinfer-OSæœ€é©åŒ–ï¼‰"""
        try:
            print("ğŸš€ GPT-OSS-20B infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
            
            # infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if not self.load_model_with_infer_os_optimization():
                print("âš ï¸ infer-OSæœ€é©åŒ–PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            # infer-OSæœ€é©åŒ–ONNXå¤‰æ›ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_infer_os_optimized_onnx_model():
                print("âš ï¸ infer-OSæœ€é©åŒ–ONNXå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            print("âœ… GPT-OSS-20B infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            print(f"ğŸ¯ é¸æŠãƒ¢ãƒ‡ãƒ«: {self.selected_model}")
            print(f"ğŸ“ èª¬æ˜: {self.model_info['description']}")
            print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']}")
            print(f"ğŸ›ï¸ é–‹ç™ºè€…: {self.model_info['developer']}")
            print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
            print(f"ğŸ”§ PyTorchãƒ¢ãƒ‡ãƒ«: {'âœ…' if self.text_generator else 'âŒ'}")
            print(f"ğŸ”§ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³: {'âœ…' if self.onnx_session else 'âŒ'}")
            
            return True
            
        except Exception as e:
            print(f"âŒ infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œGPT-OSS-20B infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    parser.add_argument("--tokens", type=int, default=200, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="conversation", 
                       choices=["conversation", "instruction", "reasoning", "creative", "simple"],
                       help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆinfer-OSæœ€é©åŒ–å›ºå®šæœ‰åŠ¹ï¼‰
    system = RyzenAIGPTOSS20BInferOSSystem(infer_os_enabled=True)
    
    if not system.initialize():
        print("âŒ infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    if args.interactive:
        system.interactive_mode()
    elif args.prompt:
        print(f"ğŸ’¬ å˜ç™ºGPT-OSS-20B infer-OSæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: '{args.prompt}'")
        print(f"ğŸ“‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {args.template}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
        system.start_npu_monitoring()
        
        start_time = time.time()
        
        if system.text_generator:
            result = system.generate_text_pytorch_infer_os(args.prompt, args.tokens, args.template)
        elif system.onnx_session:
            result = system.generate_text_onnx_infer_os(args.prompt, args.tokens, args.template)
        else:
            result = f"GPT-OSS-20B infer-OSæœ€é©åŒ–: {args.prompt}ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã„ãŸã—ã¾ã™ã€‚"
        
        generation_time = time.time() - start_time
        
        system.stop_npu_monitoring()
        
        print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
        print(result)
        print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
        print(f"ğŸ”§ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {system.selected_model}")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«èª¬æ˜: {system.model_info['description']}")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {system.model_info['parameters']}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
        
        npu_stats = system.get_npu_stats()
        print(f"ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡: {npu_stats['max_usage']:.1f}%")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        system.interactive_mode()

if __name__ == "__main__":
    main()

