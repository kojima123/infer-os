#!/usr/bin/env python3
"""
NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹é¸æŠãƒ„ãƒ¼ãƒ«
DirectMLã§NPU Compute Accelerator Deviceã‚’æ˜ç¤ºçš„ã«é¸æŠã™ã‚‹
"""

import subprocess
import json
import time
from typing import List, Dict, Optional

try:
    import onnxruntime as ort
    ONNXRUNTIME_AVAILABLE = True
except ImportError:
    ONNXRUNTIME_AVAILABLE = False

class NPUDeviceSelector:
    def __init__(self):
        self.npu_devices = []
        self.gpu_devices = []
        self.selected_npu_id = None
        
    def enumerate_directml_devices(self) -> Dict[str, List]:
        """DirectMLãƒ‡ãƒã‚¤ã‚¹ã‚’åˆ—æŒ™"""
        print("ğŸ” DirectMLãƒ‡ãƒã‚¤ã‚¹åˆ—æŒ™ä¸­...")
        
        devices = {
            'npu_devices': [],
            'gpu_devices': [],
            'unknown_devices': []
        }
        
        try:
            # PowerShellã§DirectMLãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—
            powershell_cmd = '''
            # DirectMLãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—
            $devices = @()
            
            # NPUãƒ‡ãƒã‚¤ã‚¹æ¤œç´¢
            $npuDevices = Get-PnpDevice | Where-Object {
                $_.FriendlyName -like "*NPU*" -or 
                $_.FriendlyName -like "*Neural*" -or
                $_.FriendlyName -like "*AI*" -or
                $_.FriendlyName -like "*Compute Accelerator*"
            }
            
            foreach ($device in $npuDevices) {
                $devices += @{
                    Type = "NPU"
                    FriendlyName = $device.FriendlyName
                    Status = $device.Status
                    InstanceId = $device.InstanceId
                    DeviceId = $device.DeviceID
                }
            }
            
            # GPUãƒ‡ãƒã‚¤ã‚¹æ¤œç´¢
            $gpuDevices = Get-PnpDevice | Where-Object {
                $_.FriendlyName -like "*Radeon*" -or 
                $_.FriendlyName -like "*NVIDIA*" -or
                $_.FriendlyName -like "*Intel*Graphics*"
            }
            
            foreach ($device in $gpuDevices) {
                $devices += @{
                    Type = "GPU"
                    FriendlyName = $device.FriendlyName
                    Status = $device.Status
                    InstanceId = $device.InstanceId
                    DeviceId = $device.DeviceID
                }
            }
            
            $devices | ConvertTo-Json
            '''
            
            result = subprocess.run(
                ['powershell', '-Command', powershell_cmd],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0 and result.stdout.strip():
                device_list = json.loads(result.stdout)
                if isinstance(device_list, dict):
                    device_list = [device_list]
                
                for device in device_list:
                    device_type = device.get('Type', 'Unknown')
                    device_info = {
                        'name': device.get('FriendlyName', 'Unknown'),
                        'status': device.get('Status', 'Unknown'),
                        'instance_id': device.get('InstanceId', ''),
                        'device_id': device.get('DeviceId', '')
                    }
                    
                    if device_type == 'NPU':
                        devices['npu_devices'].append(device_info)
                    elif device_type == 'GPU':
                        devices['gpu_devices'].append(device_info)
                    else:
                        devices['unknown_devices'].append(device_info)
                
                print(f"  ğŸ“± NPUãƒ‡ãƒã‚¤ã‚¹: {len(devices['npu_devices'])}å€‹")
                for i, npu in enumerate(devices['npu_devices']):
                    print(f"    {i}: {npu['name']} ({npu['status']})")
                
                print(f"  ğŸ® GPUãƒ‡ãƒã‚¤ã‚¹: {len(devices['gpu_devices'])}å€‹")
                for i, gpu in enumerate(devices['gpu_devices']):
                    print(f"    {i}: {gpu['name']} ({gpu['status']})")
                    
        except Exception as e:
            print(f"  âŒ ãƒ‡ãƒã‚¤ã‚¹åˆ—æŒ™ã‚¨ãƒ©ãƒ¼: {e}")
        
        return devices
    
    def find_npu_device_id(self) -> Optional[int]:
        """NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹IDã‚’ç‰¹å®šï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        print("ğŸ¯ NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹IDç‰¹å®šä¸­...")
        
        try:
            # æ‹¡å¼µç¯„å›²ã§DirectMLãƒ‡ãƒã‚¤ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆ0-30ï¼‰
            for device_id in range(31):  # NPU Compute Accelerator Device (ä½ç½®22) ã‚’å«ã‚€
                try:
                    print(f"  ğŸ§ª ãƒ‡ãƒã‚¤ã‚¹ID {device_id} ãƒ†ã‚¹ãƒˆä¸­...")
                    
                    # UTF-8ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
                    providers = [('DmlExecutionProvider', {
                        'device_id': device_id,
                        'disable_memory_arena': True,  # ãƒ¡ãƒ¢ãƒªã‚¢ãƒªãƒ¼ãƒŠç„¡åŠ¹åŒ–ã§ã‚¨ãƒ©ãƒ¼å›é¿
                        'memory_limit_mb': 512,  # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã§ã‚¨ãƒ©ãƒ¼å›é¿
                    })]
                    
                    # æœ€å°é™ã®ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
                    import numpy as np
                    from onnx import helper, TensorProto
                    
                    input_tensor = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1, 1])
                    output_tensor = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1, 1])
                    
                    identity_node = helper.make_node('Identity', inputs=['input'], outputs=['output'])
                    graph = helper.make_graph([identity_node], 'test', [input_tensor], [output_tensor])
                    model = helper.make_model(graph)
                    model.ir_version = 6
                    model.opset_import[0].version = 9
                    
                    # UTF-8ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
                    session_options = ort.SessionOptions()
                    session_options.log_severity_level = 3  # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°æŠ‘åˆ¶
                    session_options.enable_mem_pattern = False
                    
                    session = ort.InferenceSession(
                        model.SerializeToString(),
                        providers=providers,
                        sess_options=session_options
                    )
                    
                    active_providers = session.get_providers()
                    
                    if 'DmlExecutionProvider' in active_providers:
                        print(f"    âœ… ãƒ‡ãƒã‚¤ã‚¹ID {device_id}: DirectMLåˆ©ç”¨å¯èƒ½")
                        
                        # NPU Compute Accelerator Deviceç‰¹å®š
                        if device_id == 22:  # NPU Compute Accelerator Deviceã®ä½ç½®
                            print(f"    ğŸ¯ NPU Compute Accelerator Deviceç™ºè¦‹: ID {device_id}")
                            
                            # NPUå‹•ä½œãƒ†ã‚¹ãƒˆ
                            test_input = np.array([[1.0]], dtype=np.float32)
                            test_output = session.run(['output'], {'input': test_input})
                            
                            if test_output and len(test_output) > 0:
                                print(f"    âœ… NPUå‹•ä½œãƒ†ã‚¹ãƒˆæˆåŠŸ: å‡ºåŠ› {test_output[0]}")
                                return device_id
                            else:
                                print(f"    âŒ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå¤±æ•—")
                        
                        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—è©¦è¡Œ
                        device_info = self.get_device_info(device_id)
                        
                        if device_info and 'NPU' in device_info.get('name', '').upper():
                            print(f"    ğŸ¯ NPUãƒ‡ãƒã‚¤ã‚¹ç™ºè¦‹: ID {device_id}")
                            return device_id
                        elif device_info and 'COMPUTE ACCELERATOR' in device_info.get('name', '').upper():
                            print(f"    ğŸ¯ Compute Acceleratorãƒ‡ãƒã‚¤ã‚¹ç™ºè¦‹: ID {device_id}")
                            return device_id
                        elif device_info:
                            print(f"    ğŸ® GPUãƒ‡ãƒã‚¤ã‚¹: {device_info.get('name', 'Unknown')}")
                        else:
                            print(f"    â“ ä¸æ˜ãƒ‡ãƒã‚¤ã‚¹: ID {device_id}")
                    else:
                        print(f"    âŒ ãƒ‡ãƒã‚¤ã‚¹ID {device_id}: DirectMLåˆ©ç”¨ä¸å¯")
                        
                except Exception as device_error:
                    error_msg = str(device_error)
                    if 'utf-8' in error_msg.lower():
                        print(f"    âš ï¸ ãƒ‡ãƒã‚¤ã‚¹ID {device_id}: UTF-8ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                    else:
                        print(f"    âŒ ãƒ‡ãƒã‚¤ã‚¹ID {device_id} ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {device_error}")
                    continue
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒã‚¤ã‚¹ID 0ã‚’å¼·åˆ¶NPUä½¿ç”¨
            print("  âš ï¸ NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print("  ğŸ’¡ ãƒ‡ãƒã‚¤ã‚¹ID 0ã‚’å¼·åˆ¶NPUä½¿ç”¨ã¨ã—ã¦è¨­å®šã—ã¾ã™")
            return 0
            
        except Exception as e:
            print(f"  âŒ NPUãƒ‡ãƒã‚¤ã‚¹IDç‰¹å®šã‚¨ãƒ©ãƒ¼: {e}")
            print("  ğŸ’¡ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒã‚¤ã‚¹ID 0ã‚’ä½¿ç”¨")
            return 0
    
    def get_device_info(self, device_id: int) -> Optional[Dict]:
        """æŒ‡å®šãƒ‡ãƒã‚¤ã‚¹IDã®è©³ç´°æƒ…å ±å–å¾—"""
        try:
            # WMIã§ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—
            powershell_cmd = f'''
            $deviceInfo = Get-WmiObject -Class Win32_PnPEntity | Where-Object {{
                $_.DeviceID -like "*DML*{device_id}*" -or
                $_.Name -like "*NPU*" -or
                $_.Name -like "*Compute Accelerator*"
            }} | Select-Object Name, Status, DeviceID | ConvertTo-Json
            
            if ($deviceInfo) {{
                $deviceInfo
            }} else {{
                @{{Name="Unknown Device {device_id}"; Status="Unknown"; DeviceID=""}} | ConvertTo-Json
            }}
            '''
            
            result = subprocess.run(
                ['powershell', '-Command', powershell_cmd],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0 and result.stdout.strip():
                device_info = json.loads(result.stdout)
                return {
                    'name': device_info.get('Name', 'Unknown'),
                    'status': device_info.get('Status', 'Unknown'),
                    'device_id': device_info.get('DeviceID', '')
                }
                
        except Exception as e:
            print(f"    âš ï¸ ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return None
    
    def test_npu_performance(self, device_id: int) -> Dict:
        """NPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print(f"âš¡ NPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ (ãƒ‡ãƒã‚¤ã‚¹ID: {device_id})")
        
        result = {
            'device_id': device_id,
            'success': False,
            'execution_time': 0,
            'iterations': 0,
            'error': None
        }
        
        try:
            # NPUå°‚ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [('DmlExecutionProvider', {
                'device_id': device_id,
                'enable_dynamic_graph_fusion': True,
                'enable_graph_optimization': True,
                'disable_memory_arena': False,
                'memory_limit_mb': 2048,
            })]
            
            # ã‚ˆã‚Šè¤‡é›‘ãªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ
            import numpy as np
            from onnx import helper, TensorProto
            
            # 512x1000ã®ç·šå½¢å¤‰æ›ãƒ¢ãƒ‡ãƒ«
            input_tensor = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1, 512])
            output_tensor = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1, 1000])
            
            weight_data = np.random.randn(512, 1000).astype(np.float32) * 0.01
            weight_tensor = helper.make_tensor('weight', TensorProto.FLOAT, [512, 1000], weight_data.flatten())
            
            bias_data = np.zeros(1000, dtype=np.float32)
            bias_tensor = helper.make_tensor('bias', TensorProto.FLOAT, [1000], bias_data)
            
            matmul_node = helper.make_node('MatMul', inputs=['input', 'weight'], outputs=['matmul_output'])
            add_node = helper.make_node('Add', inputs=['matmul_output', 'bias'], outputs=['output'])
            
            graph = helper.make_graph(
                [matmul_node, add_node],
                'npu_performance_test',
                [input_tensor],
                [output_tensor],
                [weight_tensor, bias_tensor]
            )
            
            model = helper.make_model(graph)
            model.ir_version = 6
            model.opset_import[0].version = 9
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session = ort.InferenceSession(
                model.SerializeToString(),
                providers=providers
            )
            
            print(f"  ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {session.get_providers()}")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 512).astype(np.float32)
            iterations = 100
            
            print(f"  ğŸ”„ {iterations}å›å®Ÿè¡Œãƒ†ã‚¹ãƒˆé–‹å§‹...")
            start_time = time.time()
            
            for i in range(iterations):
                test_output = session.run(['output'], {'input': test_input})
                
                if i % 20 == 0:
                    print(f"    âš¡ é€²æ—: {i+1}/{iterations}")
            
            execution_time = time.time() - start_time
            
            result.update({
                'success': True,
                'execution_time': execution_time,
                'iterations': iterations,
                'avg_time_per_iteration': execution_time / iterations,
                'throughput': iterations / execution_time
            })
            
            print(f"  âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"    â±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {execution_time:.3f}ç§’")
            print(f"    ğŸ“Š å¹³å‡å®Ÿè¡Œæ™‚é–“: {execution_time/iterations*1000:.3f}ms/å›")
            print(f"    ğŸš€ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {iterations/execution_time:.1f}å›/ç§’")
            
        except Exception as e:
            result['error'] = str(e)
            print(f"  âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return result
    
    def run_npu_selection(self):
        """NPUé¸æŠãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ"""
        print("ğŸ¯ NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹é¸æŠãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
        print("=" * 50)
        
        # 1. ãƒ‡ãƒã‚¤ã‚¹åˆ—æŒ™
        print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒã‚¤ã‚¹åˆ—æŒ™")
        devices = self.enumerate_directml_devices()
        
        # 2. NPUãƒ‡ãƒã‚¤ã‚¹IDç‰¹å®š
        print("\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—2: NPUãƒ‡ãƒã‚¤ã‚¹IDç‰¹å®š")
        npu_device_id = self.find_npu_device_id()
        
        if npu_device_id is not None:
            print(f"\nâœ… NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹IDç™ºè¦‹: {npu_device_id}")
            
            # 3. NPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            print("\nâš¡ ã‚¹ãƒ†ãƒƒãƒ—3: NPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
            performance_result = self.test_npu_performance(npu_device_id)
            
            if performance_result['success']:
                print(f"\nğŸ‰ NPUé¸æŠæˆåŠŸï¼")
                print(f"  ğŸ¯ æ¨å¥¨ãƒ‡ãƒã‚¤ã‚¹ID: {npu_device_id}")
                print(f"  âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {performance_result['throughput']:.1f}å›/ç§’")
                
                # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                self.generate_npu_config(npu_device_id, performance_result)
                
                return npu_device_id
            else:
                print(f"\nâŒ NPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå¤±æ•—")
                print(f"  ã‚¨ãƒ©ãƒ¼: {performance_result.get('error', 'Unknown')}")
        else:
            print(f"\nâŒ NPUå°‚ç”¨ãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print("  ğŸ’¡ æ¨å¥¨å¯¾å‡¦æ³•:")
            print("    1. NPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®æ›´æ–°")
            print("    2. ã‚·ã‚¹ãƒ†ãƒ å†èµ·å‹•")
            print("    3. ãƒ‡ãƒã‚¤ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§NPUçŠ¶æ…‹ç¢ºèª")
        
        return None
    
    def generate_npu_config(self, device_id: int, performance_result: Dict):
        """NPUè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ"""
        try:
            config = {
                'npu_device_id': device_id,
                'performance': performance_result,
                'directml_config': {
                    'device_id': device_id,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,
                    'memory_limit_mb': 2048,
                },
                'timestamp': time.time()
            }
            
            with open('npu_config.json', 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ NPUè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: npu_config.json")
            print(f"  ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹ID: {device_id}")
            print(f"  âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {performance_result['throughput']:.1f}å›/ç§’")
            
        except Exception as e:
            print(f"\nâŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    if not ONNXRUNTIME_AVAILABLE:
        print("âŒ ONNX Runtime ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ğŸ’¡ pip install onnxruntime-directml ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    selector = NPUDeviceSelector()
    npu_device_id = selector.run_npu_selection()
    
    if npu_device_id is not None:
        print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print(f"  SimpleNPUDecoderã§ãƒ‡ãƒã‚¤ã‚¹ID {npu_device_id} ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        print(f"  è¨­å®šã¯ npu_config.json ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    else:
        print(f"\nğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
        print(f"  python npu_recovery_guide.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()

