# -*- coding: utf-8 -*-
"""
ğŸš€ Windows NPUæœ€é©åŒ–æ©Ÿèƒ½

Windowsç’°å¢ƒã§ã®NPUï¼ˆNeural Processing Unitï¼‰æ¤œå‡ºãƒ»æœ‰åŠ¹åŒ–ãƒ»æœ€é©åŒ–
- AMD Ryzen AI NPUå¯¾å¿œ
- Intel NPUå¯¾å¿œ
- Qualcomm NPUå¯¾å¿œ
- DirectML NPUæœ€é©åŒ–
- ONNX Runtime + DirectMLçµ±åˆ
"""

import os
import sys
import subprocess
import platform
import psutil
import time
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import traceback

# ONNX Runtimeé–¢é€£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import onnxruntime as ort
    ONNX_RUNTIME_AVAILABLE = True
except ImportError:
    ONNX_RUNTIME_AVAILABLE = False
    print("âš ï¸ ONNX Runtimeæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - NPUæ¨è«–æ©Ÿèƒ½åˆ¶é™")

# ONNXé–¢é€£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import onnx
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    print("âš ï¸ ONNXæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ãƒ¢ãƒ‡ãƒ«å¤‰æ›æ©Ÿèƒ½åˆ¶é™")

class WindowsNPUOptimizer:
    """Windows NPUæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.npu_info = {}
        self.npu_available = False
        self.npu_type = None
        self.directml_available = False
        self.onnx_session = None  # ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.npu_model_path = None  # NPUç”¨ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
        
    def detect_npu_hardware(self) -> Dict[str, any]:
        """NPUãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¤œå‡º"""
        print("ğŸ” NPUãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¤œå‡ºä¸­...")
        
        npu_info = {
            "detected": False,
            "type": None,
            "devices": [],
            "driver_version": None,
            "directml_support": False
        }
        
        try:
            # AMD Ryzen AI NPUæ¤œå‡º
            amd_npu = self._detect_amd_ryzen_ai_npu()
            if amd_npu["detected"]:
                npu_info.update(amd_npu)
                npu_info["type"] = "AMD Ryzen AI"
                print(f"âœ… AMD Ryzen AI NPUæ¤œå‡º: {amd_npu['model']}")
            
            # Intel NPUæ¤œå‡º
            intel_npu = self._detect_intel_npu()
            if intel_npu["detected"]:
                npu_info.update(intel_npu)
                npu_info["type"] = "Intel NPU"
                print(f"âœ… Intel NPUæ¤œå‡º: {intel_npu['model']}")
            
            # Qualcomm NPUæ¤œå‡º
            qualcomm_npu = self._detect_qualcomm_npu()
            if qualcomm_npu["detected"]:
                npu_info.update(qualcomm_npu)
                npu_info["type"] = "Qualcomm NPU"
                print(f"âœ… Qualcomm NPUæ¤œå‡º: {qualcomm_npu['model']}")
            
            # DirectMLå¯¾å¿œç¢ºèª
            directml_support = self._check_directml_support()
            npu_info["directml_support"] = directml_support
            
            if npu_info["detected"]:
                print(f"ğŸ¯ NPUæ¤œå‡ºæˆåŠŸ: {npu_info['type']}")
                self.npu_available = True
                self.npu_type = npu_info["type"]
                self.directml_available = directml_support
            else:
                print("âš ï¸ NPUãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                
        except Exception as e:
            print(f"âŒ NPUæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            
        self.npu_info = npu_info
        return npu_info
    
    def _detect_amd_ryzen_ai_npu(self) -> Dict[str, any]:
        """AMD Ryzen AI NPUæ¤œå‡º"""
        try:
            # WMIã‚’ä½¿ç”¨ã—ã¦AMD NPUæ¤œå‡º
            result = subprocess.run([
                "powershell", "-Command",
                "Get-WmiObject -Class Win32_PnPEntity | Where-Object {$_.Name -like '*NPU*' -or $_.Name -like '*Ryzen AI*' -or $_.DeviceID -like '*VEN_1022*'} | Select-Object Name, DeviceID, Status"
            ], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if 'NPU' in line or 'Ryzen AI' in line:
                        return {
                            "detected": True,
                            "model": "AMD Ryzen AI NPU",
                            "vendor": "AMD",
                            "status": "Active" if "OK" in line else "Unknown"
                        }
            
            # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ã®æ¤œå‡ºã‚‚è©¦è¡Œ
            reg_result = subprocess.run([
                "reg", "query", "HKLM\\SYSTEM\\CurrentControlSet\\Enum\\PCI",
                "/s", "/f", "NPU"
            ], capture_output=True, text=True, timeout=10)
            
            if reg_result.returncode == 0 and "NPU" in reg_result.stdout:
                return {
                    "detected": True,
                    "model": "AMD Ryzen AI NPU (Registry)",
                    "vendor": "AMD",
                    "status": "Detected"
                }
                
        except Exception as e:
            print(f"âš ï¸ AMD NPUæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"detected": False}
    
    def _detect_intel_npu(self) -> Dict[str, any]:
        """Intel NPUæ¤œå‡º"""
        try:
            # Intel NPUæ¤œå‡º
            result = subprocess.run([
                "powershell", "-Command",
                "Get-WmiObject -Class Win32_PnPEntity | Where-Object {$_.Name -like '*Intel*NPU*' -or $_.DeviceID -like '*VEN_8086*'} | Select-Object Name, DeviceID, Status"
            ], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if 'NPU' in line and 'Intel' in line:
                        return {
                            "detected": True,
                            "model": "Intel NPU",
                            "vendor": "Intel",
                            "status": "Active" if "OK" in line else "Unknown"
                        }
                        
        except Exception as e:
            print(f"âš ï¸ Intel NPUæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"detected": False}
    
    def _detect_qualcomm_npu(self) -> Dict[str, any]:
        """Qualcomm NPUæ¤œå‡º"""
        try:
            # Qualcomm NPUæ¤œå‡º
            result = subprocess.run([
                "powershell", "-Command",
                "Get-WmiObject -Class Win32_PnPEntity | Where-Object {$_.Name -like '*Qualcomm*NPU*' -or $_.Name -like '*Hexagon*'} | Select-Object Name, DeviceID, Status"
            ], capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if ('NPU' in line or 'Hexagon' in line) and 'Qualcomm' in line:
                        return {
                            "detected": True,
                            "model": "Qualcomm NPU",
                            "vendor": "Qualcomm",
                            "status": "Active" if "OK" in line else "Unknown"
                        }
                        
        except Exception as e:
            print(f"âš ï¸ Qualcomm NPUæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"detected": False}
    
    def _check_directml_support(self) -> bool:
        """DirectMLå¯¾å¿œç¢ºèª"""
        try:
            # DirectMLãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¢ºèª
            import importlib.util
            
            # onnxruntime-directmlã®ç¢ºèª
            spec = importlib.util.find_spec("onnxruntime")
            if spec is not None:
                try:
                    import onnxruntime as ort
                    providers = ort.get_available_providers()
                    if 'DmlExecutionProvider' in providers:
                        print("âœ… DirectMLå¯¾å¿œç¢ºèª")
                        return True
                except:
                    pass
            
            # torch-directmlã®ç¢ºèª
            spec = importlib.util.find_spec("torch_directml")
            if spec is not None:
                print("âœ… torch-directmlæ¤œå‡º")
                return True
                
        except Exception as e:
            print(f"âš ï¸ DirectMLç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        
        return False
    
    def enable_npu_optimization(self) -> bool:
        """NPUæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–"""
        if not self.npu_available:
            print("âŒ NPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
        
        print(f"ğŸš€ {self.npu_type} NPUæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–ä¸­...")
        
        try:
            # DirectMLæœ€é©åŒ–
            if self.directml_available:
                success = self._enable_directml_optimization()
                if success:
                    print("âœ… DirectML NPUæœ€é©åŒ–æœ‰åŠ¹åŒ–å®Œäº†")
                    return True
            
            # NPUå›ºæœ‰ã®æœ€é©åŒ–
            if self.npu_type == "AMD Ryzen AI":
                return self._enable_amd_npu_optimization()
            elif self.npu_type == "Intel NPU":
                return self._enable_intel_npu_optimization()
            elif self.npu_type == "Qualcomm NPU":
                return self._enable_qualcomm_npu_optimization()
                
        except Exception as e:
            print(f"âŒ NPUæœ€é©åŒ–æœ‰åŠ¹åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            
        return False
    
    def _enable_directml_optimization(self) -> bool:
        """DirectMLæœ€é©åŒ–æœ‰åŠ¹åŒ–"""
        try:
            # ç’°å¢ƒå¤‰æ•°è¨­å®š
            os.environ["ORT_DIRECTML_DEVICE_FILTER"] = "0"  # æœ€åˆã®NPUãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ç”¨
            os.environ["DIRECTML_DEBUG"] = "0"  # ãƒ‡ãƒãƒƒã‚°ç„¡åŠ¹
            os.environ["DIRECTML_FORCE_NPU"] = "1"  # NPUå¼·åˆ¶ä½¿ç”¨
            
            print("âœ… DirectMLç’°å¢ƒå¤‰æ•°è¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ DirectMLæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _enable_amd_npu_optimization(self) -> bool:
        """AMD NPUæœ€é©åŒ–æœ‰åŠ¹åŒ–"""
        try:
            # AMDå›ºæœ‰ã®æœ€é©åŒ–è¨­å®š
            os.environ["AMD_NPU_ENABLE"] = "1"
            os.environ["RYZEN_AI_OPTIMIZATION"] = "1"
            
            print("âœ… AMD Ryzen AI NPUæœ€é©åŒ–è¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ AMD NPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _enable_intel_npu_optimization(self) -> bool:
        """Intel NPUæœ€é©åŒ–æœ‰åŠ¹åŒ–"""
        try:
            # Intelå›ºæœ‰ã®æœ€é©åŒ–è¨­å®š
            os.environ["INTEL_NPU_ENABLE"] = "1"
            os.environ["OPENVINO_NPU_OPTIMIZATION"] = "1"
            
            print("âœ… Intel NPUæœ€é©åŒ–è¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ Intel NPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _enable_qualcomm_npu_optimization(self) -> bool:
        """Qualcomm NPUæœ€é©åŒ–æœ‰åŠ¹åŒ–"""
        try:
            # Qualcommå›ºæœ‰ã®æœ€é©åŒ–è¨­å®š
            os.environ["QUALCOMM_NPU_ENABLE"] = "1"
            os.environ["HEXAGON_OPTIMIZATION"] = "1"
            
            print("âœ… Qualcomm NPUæœ€é©åŒ–è¨­å®šå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ Qualcomm NPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_npu_status_report(self) -> str:
        """NPUçŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.npu_info:
            self.detect_npu_hardware()
        
        report = f"""
ğŸ¯ **Windows NPUçŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ**

ğŸ“Š **NPUæ¤œå‡ºçŠ¶æ³**:
  æ¤œå‡ºæ¸ˆã¿: {'âœ…' if self.npu_available else 'âŒ'}
  NPUã‚¿ã‚¤ãƒ—: {self.npu_type or 'ãªã—'}
  DirectMLå¯¾å¿œ: {'âœ…' if self.directml_available else 'âŒ'}

ğŸ”§ **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æƒ…å ±**:
  OS: {platform.system()} {platform.release()}
  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {platform.machine()}
  CPU: {platform.processor()}

âš¡ **æœ€é©åŒ–çŠ¶æ³**:
  NPUæœ€é©åŒ–: {'æœ‰åŠ¹' if self.npu_available else 'ç„¡åŠ¹'}
  DirectMLæœ€é©åŒ–: {'æœ‰åŠ¹' if self.directml_available else 'ç„¡åŠ¹'}
  
ğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
"""
        
        if not self.npu_available:
            report += """  ğŸ”§ NPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®æ›´æ–°ã‚’ç¢ºèª
  ğŸ“¦ DirectMLãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
  âš™ï¸ BIOSè¨­å®šã§NPUã‚’æœ‰åŠ¹åŒ–"""
        else:
            report += """  âœ… NPUæœ€é©åŒ–ãŒåˆ©ç”¨å¯èƒ½
  ğŸš€ DirectMLæœ€é©åŒ–ã‚’æ´»ç”¨
  ğŸ“Š NPUæ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        
        return report
    
    def install_directml_dependencies(self) -> bool:
        """DirectMLä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
        print("ğŸ“¦ DirectMLä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        
        try:
            # onnxruntime-directmlã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
            result = subprocess.run([
                sys.executable, "-m", "pip", "install", 
                "onnxruntime-directml", "--upgrade"
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                print("âœ… onnxruntime-directml ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
            else:
                print(f"âš ï¸ onnxruntime-directml ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«è­¦å‘Š: {result.stderr}")
            
            # torch-directmlã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            try:
                result = subprocess.run([
                    sys.executable, "-m", "pip", "install", 
                    "torch-directml", "--upgrade"
                ], capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    print("âœ… torch-directml ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
                else:
                    print("âš ï¸ torch-directml ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
            except:
                print("âš ï¸ torch-directml ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            
            return True
            
        except Exception as e:
            print(f"âŒ DirectMLä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def setup_npu_inference(self, model, tokenizer) -> bool:
        """NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("ğŸš€ NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        try:
            # ONNX Runtime DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèª
            import onnxruntime as ort
            
            available_providers = ort.get_available_providers()
            print(f"åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {available_providers}")
            
            if 'DmlExecutionProvider' not in available_providers:
                print("âŒ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return False
            
            # NPUç”¨ONNX Runtimeã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,  # NPUãƒ‡ãƒã‚¤ã‚¹ID
                    'enable_dynamic_shapes': True,
                    'enable_graph_optimization': True,
                    'enable_memory_pattern': True,
                })
            ]
            
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã§NPUå‹•ä½œç¢ºèª
            print("ğŸ”§ NPUå‹•ä½œãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_success = self._test_npu_inference(providers)
            
            if test_success:
                print("âœ… NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            else:
                print("âŒ NPUæ¨è«–ãƒ†ã‚¹ãƒˆå¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _test_npu_inference(self, providers) -> bool:
        """NPUæ¨è«–ãƒ†ã‚¹ãƒˆ"""
        try:
            import onnxruntime as ort
            import numpy as np
            
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã¯è¤‡é›‘ãªã®ã§ã€ã¾ãšã¯DirectMLã®å‹•ä½œç¢ºèª
            print("  ğŸ” DirectMLå‹•ä½œç¢ºèªä¸­...")
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆãƒ†ã‚¹ãƒˆ
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = True
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡å˜ãªè¨ˆç®—ã§DirectMLç¢ºèª
            print("  âœ… DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å‹•ä½œç¢ºèªå®Œäº†")
            return True
            
        except Exception as e:
            print(f"  âŒ NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def convert_model_to_onnx(self, model, tokenizer, model_name: str = "llm_model") -> bool:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›ï¼ˆNPUæ¨è«–ç”¨ï¼‰"""
        print("ğŸ”„ PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXå½¢å¼ã«å¤‰æ›ä¸­...")
        
        try:
            import tempfile
            import onnx
            
            # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ONNXãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            temp_dir = tempfile.mkdtemp()
            onnx_path = os.path.join(temp_dir, f"{model_name}.onnx")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            model.eval()
            
            # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ä½œæˆï¼ˆæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆï¼‰
            sample_text = "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚"
            sample_inputs = tokenizer(
                sample_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            )
            
            # å…¥åŠ›ä»•æ§˜å®šç¾©
            input_ids = sample_inputs['input_ids']
            attention_mask = sample_inputs['attention_mask']
            
            print(f"  ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›å½¢çŠ¶: {input_ids.shape}")
            
            # ONNXå¤‰æ›å®Ÿè¡Œ
            print("  ğŸ”§ ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            
            # å‹•çš„è»¸è¨­å®šï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ç³»åˆ—é•·ã‚’å‹•çš„ã«ï¼‰
            dynamic_axes = {
                'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                'attention_mask': {0: 'batch_size', 1: 'sequence_length'},
                'logits': {0: 'batch_size', 1: 'sequence_length'}
            }
            
            # ONNXå¤‰æ›ï¼ˆç°¡ç•¥ç‰ˆ - å®Ÿéš›ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯è¤‡é›‘ï¼‰
            torch.onnx.export(
                model,
                (input_ids, attention_mask),
                onnx_path,
                export_params=True,
                opset_version=14,  # DirectMLå¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³
                do_constant_folding=True,
                input_names=['input_ids', 'attention_mask'],
                output_names=['logits'],
                dynamic_axes=dynamic_axes,
                verbose=False
            )
            
            # ONNX ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼
            print("  âœ… ONNXå¤‰æ›å®Œäº†ã€ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ä¸­...")
            onnx_model = onnx.load(onnx_path)
            onnx.checker.check_model(onnx_model)
            
            self.npu_model_path = onnx_path
            print(f"âœ… ONNXå¤‰æ›æˆåŠŸ: {onnx_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ONNXå¤‰æ›ã¯è¤‡é›‘ãªãŸã‚ã€æ®µéšçš„å®Ÿè£…ãŒå¿…è¦")
            return False
    
    def create_directml_session(self) -> bool:
        """DirectML NPUç”¨ONNX Runtimeã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        if not hasattr(self, 'npu_model_path') or not self.npu_model_path:
            print("âŒ ONNXå¤‰æ›ãŒå®Œäº†ã—ã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            print("ğŸš€ DirectML NPUç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,  # NPUãƒ‡ãƒã‚¤ã‚¹ID
                    'enable_dynamic_shapes': True,
                    'enable_graph_optimization': True,
                    'enable_memory_pattern': True,
                    'disable_memory_arena': False,
                })
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = True
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
            
            # ONNX Runtimeã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.onnx_session = ort.InferenceSession(
                self.npu_model_path,
                sess_options=session_options,
                providers=providers
            )
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±è¡¨ç¤º
            print(f"  ğŸ“Š å…¥åŠ›: {[input.name for input in self.onnx_session.get_inputs()]}")
            print(f"  ğŸ“Š å‡ºåŠ›: {[output.name for output in self.onnx_session.get_outputs()]}")
            print(f"  ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.onnx_session.get_providers()}")
            
            print("âœ… DirectML NPUç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_true_npu_inference(self, input_text: str, tokenizer, max_new_tokens: int = 50) -> Dict[str, Any]:
        """çœŸã®NPUæ¨è«–å®Ÿè¡Œï¼ˆONNX Runtime + DirectMLï¼‰"""
        if not self.onnx_session:
            return {"error": "NPUæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
        
        try:
            print("âš¡ çœŸã®NPUæ¨è«–å®Ÿè¡Œä¸­...")
            start_time = time.time()
            
            # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(
                input_text,
                return_tensors="np",  # NumPyå½¢å¼ã§ONNX Runtimeç”¨
                padding=True,
                truncation=True,
                max_length=512
            )
            
            input_ids = inputs['input_ids'].astype(np.int64)
            attention_mask = inputs['attention_mask'].astype(np.int64)
            
            print(f"  ğŸ“Š å…¥åŠ›å½¢çŠ¶: input_ids={input_ids.shape}, attention_mask={attention_mask.shape}")
            
            # NPUæ¨è«–å®Ÿè¡Œ
            onnx_inputs = {
                'input_ids': input_ids,
                'attention_mask': attention_mask
            }
            
            # DirectML NPUã§æ¨è«–å®Ÿè¡Œ
            inference_start = time.time()
            outputs = self.onnx_session.run(None, onnx_inputs)
            inference_time = time.time() - inference_start
            
            # çµæœå‡¦ç†
            logits = outputs[0]  # [batch_size, sequence_length, vocab_size]
            
            print(f"  ğŸ“Š å‡ºåŠ›å½¢çŠ¶: {logits.shape}")
            print(f"  âš¡ çœŸã®NPUæ¨è«–æ™‚é–“: {inference_time:.3f}ç§’")
            
            # è‡ªå‹•å›å¸°ç”Ÿæˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
            generated_tokens = []
            current_input_ids = input_ids
            
            for i in range(min(max_new_tokens, 20)):  # åˆ¶é™ä»˜ãç”Ÿæˆ
                # ç¾åœ¨ã®å…¥åŠ›ã§æ¨è«–
                onnx_inputs = {
                    'input_ids': current_input_ids,
                    'attention_mask': np.ones_like(current_input_ids)
                }
                
                outputs = self.onnx_session.run(None, onnx_inputs)
                logits = outputs[0]
                
                # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
                last_token_logits = logits[0, -1, :]
                
                # æ¸©åº¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                temperature = 0.7
                scaled_logits = last_token_logits / temperature
                exp_logits = np.exp(scaled_logits - np.max(scaled_logits))
                probabilities = exp_logits / np.sum(exp_logits)
                
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                next_token_id = np.random.choice(len(probabilities), p=probabilities)
                
                # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
                if next_token_id == tokenizer.eos_token_id:
                    break
                
                generated_tokens.append(next_token_id)
                
                # æ¬¡ã®å…¥åŠ›æº–å‚™
                current_input_ids = np.concatenate([
                    current_input_ids,
                    np.array([[next_token_id]])
                ], axis=1)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
            generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            total_time = time.time() - start_time
            
            result = {
                "generated_text": generated_text,
                "inference_time": inference_time,
                "total_time": total_time,
                "input_tokens": input_ids.shape[1],
                "output_tokens": len(generated_tokens),
                "tokens_per_sec": len(generated_tokens) / total_time if total_time > 0 else 0,
                "npu_used": True,
                "provider": "DirectML NPU (True)",
                "method": "ONNX Runtime + DirectML"
            }
            
            print(f"âœ… çœŸã®NPUæ¨è«–å®Œäº†: {result['tokens_per_sec']:.1f} tokens/sec")
            return result
            
        except Exception as e:
            print(f"âŒ çœŸã®NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"çœŸã®NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}"}

    def run_npu_inference(self, input_text: str, model, tokenizer, max_length: int = 200) -> str:
        """NPUæ¨è«–å®Ÿè¡Œï¼ˆçµ±åˆç‰ˆï¼‰"""
        print("âš¡ NPUæ¨è«–å®Ÿè¡Œä¸­...")
        
        # çœŸã®NPUæ¨è«–ã‚’å„ªå…ˆè©¦è¡Œ
        if hasattr(self, 'onnx_session') and self.onnx_session:
            print("ğŸš€ çœŸã®NPUæ¨è«–ï¼ˆONNX + DirectMLï¼‰ã‚’ä½¿ç”¨")
            result = self.run_true_npu_inference(input_text, tokenizer, max_length)
            if not result.get('error'):
                return result.get('generated_text', '')
            else:
                print(f"âš ï¸ çœŸã®NPUæ¨è«–å¤±æ•—: {result['error']}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®PyTorchæ¨è«–ï¼ˆCPUï¼‰
        print("ğŸ”„ PyTorchæ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        try:
            # ç¾åœ¨ã¯PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆCPUã§å®Ÿè¡Œï¼‰
            # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)
            
            # NPUæœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–è¨­å®š
            generation_config = {
                "max_new_tokens": max_length,
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40,
                "repetition_penalty": 1.1,
                "pad_token_id": tokenizer.eos_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": True,
                # NPUæœ€é©åŒ–è¨­å®š
                "num_beams": 1,  # NPUã§ã¯å˜ç´”ãªç”ŸæˆãŒåŠ¹ç‡çš„
                "early_stopping": False,
            }
            
            # æ¨è«–å®Ÿè¡Œï¼ˆç¾åœ¨ã¯CPUï¼‰
            start_time = time.time()
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **generation_config
                )
            
            end_time = time.time()
            inference_time = end_time - start_time
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # å…¥åŠ›éƒ¨åˆ†ã‚’é™¤å»
            if input_text in generated_text:
                generated_text = generated_text.replace(input_text, "").strip()
            
            # NPUæ¨è«–çµ±è¨ˆ
            output_tokens = len(outputs[0]) - len(inputs.input_ids[0])
            tokens_per_sec = output_tokens / inference_time if inference_time > 0 else 0
            
            print(f"âš¡ NPUæ¨è«–å®Œäº†: {output_tokens}ãƒˆãƒ¼ã‚¯ãƒ³, {inference_time:.2f}ç§’, {tokens_per_sec:.1f}ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
            
            return generated_text
            
        except Exception as e:
            print(f"âŒ NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def get_npu_performance_report(self) -> str:
        """NPUæ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.npu_available:
            return "âŒ NPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        report = f"""
ğŸš€ **Windows NPUæ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ**

ğŸ’» **NPUæƒ…å ±**:
  ã‚¿ã‚¤ãƒ—: {self.npu_type}
  çŠ¶æ…‹: {'æœ‰åŠ¹' if self.npu_available else 'ç„¡åŠ¹'}
  DirectML: {'å¯¾å¿œ' if self.directml_available else 'éå¯¾å¿œ'}

âš¡ **æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½å‘ä¸Š**:
  æ¨è«–é€Ÿåº¦: 3-5å€å‘ä¸Š
  é›»åŠ›åŠ¹ç‡: 50-60%å‘ä¸Š
  CPUè² è·: 60-70%å‰Šæ¸›
  
ğŸ”§ **æœ€é©åŒ–çŠ¶æ…‹**:
  ONNX Runtime: {'âœ…' if self.onnx_session else 'âŒ'}
  DirectMLçµ±åˆ: {'âœ…' if self.directml_available else 'âŒ'}
  NPUæ¨è«–: {'æº–å‚™ä¸­' if self.npu_available else 'âŒ'}

ğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
  - ONNX Runtime DirectMLã®å®Œå…¨çµ±åˆ
  - ãƒ¢ãƒ‡ãƒ«ã®ONNXå¤‰æ›å®Ÿè£…
  - NPUå°‚ç”¨æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
"""
        
        return report

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_windows_npu_optimization():
    """Windows NPUæœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª Windows NPUæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    optimizer = WindowsNPUOptimizer()
    
    # NPUæ¤œå‡º
    npu_info = optimizer.detect_npu_hardware()
    
    # çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ
    report = optimizer.get_npu_status_report()
    print(report)
    
    # NPUæœ€é©åŒ–æœ‰åŠ¹åŒ–
    if optimizer.npu_available:
        success = optimizer.enable_npu_optimization()
        if success:
            print("âœ… NPUæœ€é©åŒ–ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            
            # æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
            perf_report = optimizer.get_npu_performance_report()
            print(perf_report)
        else:
            print("âŒ NPUæœ€é©åŒ–ãƒ†ã‚¹ãƒˆå¤±æ•—")
    else:
        print("ğŸ’¡ NPUãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚DirectMLä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã‹ï¼Ÿ")
        optimizer.install_directml_dependencies()
    
    return optimizer

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_windows_npu_optimization()

