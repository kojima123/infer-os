# -*- coding: utf-8 -*-
"""
Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ« + VitisAI ExecutionProvider + NPUæœ€é©åŒ–
"""

import os
import sys
import time
import argparse
import json
import threading
import signal
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        GPT2LMHeadModel, GPT2Tokenizer,
        T5Tokenizer, T5ForConditionalGeneration
    )
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime torch transformers psutil ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class RyzenAIJapaneseNPUSystem:
    """Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = None
        self.active_provider = None
        self.model = None
        self.tokenizer = None
        self.npu_monitoring_active = False
        self.inference_in_progress = False
        self.last_npu_usage = 0.0
        
        # infer-OSè¨­å®š
        self.infer_os_enabled = os.getenv('INFER_OS_ENABLED', '0') == '1'
        
        print(f"ğŸš€ Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {timeout}ç§’")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«: æœ€é©åŒ–æ¸ˆã¿")
        
        # NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.start_optimized_npu_monitoring()
    
    def start_optimized_npu_monitoring(self):
        """æœ€é©åŒ–NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹"""
        self.npu_monitoring_active = True
        
        def monitor_npu_optimized():
            while self.npu_monitoring_active:
                try:
                    current_npu_usage = self.get_npu_usage()
                    
                    # æ¨è«–å®Ÿè¡Œä¸­ã¾ãŸã¯NPUä½¿ç”¨ç‡ã«å¤‰åŒ–ãŒã‚ã‚‹å ´åˆã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    if self.inference_in_progress:
                        if current_npu_usage > self.last_npu_usage + 1.0:
                            print(f"ğŸ”¥ NPUè² è·ä¸Šæ˜‡æ¤œå‡º: {self.last_npu_usage:.1f}% â†’ {current_npu_usage:.1f}%")
                        elif current_npu_usage > 5.0:
                            print(f"âš¡ NPUå‡¦ç†ä¸­: ä½¿ç”¨ç‡ {current_npu_usage:.1f}%")
                    
                    self.last_npu_usage = current_npu_usage
                    time.sleep(1)
                    
                except Exception as e:
                    pass
        
        monitor_thread = threading.Thread(target=monitor_npu_optimized, daemon=True)
        monitor_thread.start()
        print("ğŸ“Š NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰")
    
    def get_npu_usage(self) -> float:
        """NPUä½¿ç”¨ç‡å–å¾—"""
        try:
            # Windows Performance CountersçµŒç”±ã§NPUä½¿ç”¨ç‡å–å¾—
            result = subprocess.run([
                'powershell', '-Command',
                '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage" -ErrorAction SilentlyContinue | Select-Object -ExpandProperty CounterSamples | Where-Object {$_.InstanceName -like "*NPU*" -or $_.InstanceName -like "*VPU*" -or $_.InstanceName -like "*AI*"} | Measure-Object -Property CookedValue -Sum).Sum'
            ], capture_output=True, text=True, timeout=1)
            
            if result.returncode == 0 and result.stdout.strip():
                npu_usage = float(result.stdout.strip())
                return min(npu_usage, 100.0)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: GPU Engineå…¨ä½“ã‹ã‚‰æ¨å®š
            result2 = subprocess.run([
                'powershell', '-Command',
                '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage" -ErrorAction SilentlyContinue | Select-Object -ExpandProperty CounterSamples | Measure-Object -Property CookedValue -Average).Average'
            ], capture_output=True, text=True, timeout=1)
            
            if result2.returncode == 0 and result2.stdout.strip():
                gpu_usage = float(result2.stdout.strip())
                return min(gpu_usage * 0.3, 100.0)
            
            return 0.0
            
        except Exception:
            return 0.0
    
    def create_ryzen_ai_optimized_model(self) -> str:
        """Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            print("ğŸ”§ Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
            
            # Ryzen AI NPUã«æœ€é©åŒ–ã•ã‚ŒãŸæ§‹é€ 
            class RyzenAINPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # Ryzen AI NPUã«æœ€é©åŒ–ã•ã‚ŒãŸã‚µã‚¤ã‚º
                    self.embedding = nn.Embedding(32000, 512)  # æ—¥æœ¬èªèªå½™å¯¾å¿œ
                    self.transformer_layers = nn.ModuleList([
                        nn.TransformerEncoderLayer(
                            d_model=512,
                            nhead=8,
                            dim_feedforward=2048,
                            dropout=0.1,
                            batch_first=True
                        ) for _ in range(6)  # 6å±¤ã§NPUæœ€é©åŒ–
                    ])
                    self.layer_norm = nn.LayerNorm(512)
                    self.output_projection = nn.Linear(512, 32000)  # æ—¥æœ¬èªèªå½™å‡ºåŠ›
                
                def forward(self, input_ids):
                    # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
                    x = self.embedding(input_ids)
                    
                    # Transformerå±¤ï¼ˆNPUæœ€é©åŒ–ï¼‰
                    for layer in self.transformer_layers:
                        x = layer(x)
                    
                    # æ­£è¦åŒ–ã¨å‡ºåŠ›æŠ•å½±
                    x = self.layer_norm(x)
                    x = self.output_projection(x)
                    
                    return x
            
            model = RyzenAINPUModel()
            model.eval()
            
            # Ryzen AI NPUã«æœ€é©åŒ–ã•ã‚ŒãŸå…¥åŠ›ã‚µã‚¤ã‚º
            dummy_input = torch.randint(0, 32000, (1, 128), dtype=torch.long)
            
            print("ğŸ“Š Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
            print(f"  å…¥åŠ›: (1, 128) - ãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹")
            print(f"  Embedding: 32,000èªå½™ â†’ 512æ¬¡å…ƒ")
            print(f"  Transformer: 6å±¤ x 8ãƒ˜ãƒƒãƒ‰")
            print(f"  å‡ºåŠ›: 512æ¬¡å…ƒ â†’ 32,000èªå½™")
            print(f"  æ—¥æœ¬èªèªå½™: 32,000ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œ")
            print(f"  NPUæœ€é©åŒ–: Ryzen AI 1.5å¯¾å¿œ")
            
            # ONNX IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            onnx_path = "ryzen_ai_japanese_npu_model.onnx"
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                    'logits': {0: 'batch_size', 1: 'sequence_length'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã«å¤‰æ›´ï¼ˆRyzenAI 1.5äº’æ›æ€§ï¼‰
            import onnx
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            print(f"ğŸ“‹ IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: 10 (RyzenAI 1.5äº’æ›)")
            print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œ: 32,000èªå½™")
            
            return onnx_path
            
        except Exception as e:
            print(f"âŒ Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«
            return self.create_simple_npu_model()
    
    def create_simple_npu_model(self) -> str:
        """ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            print("ğŸ”§ ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰...")
            
            class SimpleNPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear1 = nn.Linear(512, 1024)
                    self.linear2 = nn.Linear(1024, 2048)
                    self.linear3 = nn.Linear(2048, 1024)
                    self.linear4 = nn.Linear(1024, 512)
                    self.output = nn.Linear(512, 256)
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.1)
                
                def forward(self, x):
                    x = self.relu(self.linear1(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear2(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear3(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear4(x))
                    x = self.output(x)
                    return x
            
            model = SimpleNPUModel()
            model.eval()
            
            dummy_input = torch.randn(1, 512)
            
            onnx_path = "simple_npu_model.onnx"
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output']
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã«å¤‰æ›´
            import onnx
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ã‚·ãƒ³ãƒ—ãƒ«NPUãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def create_session_with_ryzen_ai_optimization(self, onnx_path: str) -> bool:
        """Ryzen AI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        # æˆ¦ç•¥1: VitisAIExecutionProviderï¼ˆRyzen AIæœ€é©åŒ–ï¼‰
        print("ğŸ”§ æˆ¦ç•¥1: VitisAIExecutionProviderï¼ˆRyzen AIæœ€é©åŒ–ï¼‰...")
        try:
            providers = ['VitisAIExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'config_file': 'C:/Program Files/RyzenAI/1.5/voe-4.0-win_amd64/vaip_config.json',
                    'cacheDir': './vaip_cache',
                    'cacheKey': 'ryzen_ai_japanese_optimized'
                },
                {}
            ]
            
            print("ğŸ”¥ Ryzen AI NPUæ—¥æœ¬èªæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            def create_session():
                session = ort.InferenceSession(
                    onnx_path,
                    providers=providers,
                    provider_options=provider_options
                )
                print("ğŸ¯ Ryzen AI NPUæ—¥æœ¬èªæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼")
                return session
            
            session_result = self._run_with_timeout(create_session, 60)
            if session_result:
                self.session = session_result
                self.active_provider = self.session.get_providers()[0]
                print(f"âœ… Ryzen AI NPUæ—¥æœ¬èªæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªNPUæœ€é©åŒ–: æœ‰åŠ¹")
                return True
            else:
                print("âš ï¸ Ryzen AI NPUæ—¥æœ¬èªæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
        except Exception as e:
            print(f"âš ï¸ Ryzen AI NPUæ—¥æœ¬èªæœ€é©åŒ–å¤±æ•—: {e}")
        
        # æˆ¦ç•¥2: DmlExecutionProviderï¼ˆGPU/NPUæœ€é©åŒ–ï¼‰
        print("ğŸ”§ æˆ¦ç•¥2: DmlExecutionProviderï¼ˆGPU/NPUæœ€é©åŒ–ï¼‰...")
        try:
            providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'device_id': 0,
                    'enable_dynamic_shapes': True,
                    'disable_metacommands': False
                },
                {}
            ]
            
            self.session = ort.InferenceSession(
                onnx_path,
                providers=providers,
                provider_options=provider_options
            )
            
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… DML GPU/NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªGPU/NPUæœ€é©åŒ–: æœ‰åŠ¹")
            return True
            
        except Exception as e:
            print(f"âš ï¸ DML GPU/NPUæœ€é©åŒ–å¤±æ•—: {e}")
        
        # æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ”§ æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰...")
        try:
            providers = ['CPUExecutionProvider']
            self.session = ort.InferenceSession(onnx_path, providers=providers)
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… CPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"âš ï¸ NPUæœ€é©åŒ–: éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆCPUä½¿ç”¨ï¼‰")
            return True
            
        except Exception as e:
            print(f"âŒ CPUå¤±æ•—: {e}")
            return False
    
    def _run_with_timeout(self, func, timeout_seconds):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãé–¢æ•°å®Ÿè¡Œ"""
        result = [None]
        exception = [None]
        
        def target():
            try:
                result[0] = func()
            except Exception as e:
                exception[0] = e
        
        thread = threading.Thread(target=target)
        thread.daemon = True
        thread.start()
        thread.join(timeout_seconds)
        
        if thread.is_alive():
            print(f"âš ï¸ é–¢æ•°å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{timeout_seconds}ç§’ï¼‰")
            return None
        
        if exception[0]:
            raise exception[0]
        
        return result[0]
    
    def test_ryzen_ai_npu_inference(self, num_inferences: int = 10) -> Dict[str, Any]:
        """Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆ"""
        if not self.session:
            raise RuntimeError("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ¯ Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ{num_inferences}å›ï¼‰...")
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªNPUæœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“Š ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
        
        # NPUæœ€é©åŒ–å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        if "ryzen_ai_japanese" in str(self.session.get_inputs()[0].name):
            input_data = np.random.randint(0, 32000, (1, 128), dtype=np.int64)
            print("ğŸ“Š æ—¥æœ¬èªå…¥åŠ›: (1, 128) - ãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹")
        else:
            input_data = np.random.randn(1, 512).astype(np.float32)
            print("ğŸ“Š æ¨™æº–å…¥åŠ›: (1, 512) - ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«")
        
        input_name = self.session.get_inputs()[0].name
        
        successful_inferences = 0
        total_time = 0
        cpu_usage = []
        memory_usage = []
        npu_activity_detected = 0
        max_npu_usage = 0.0
        
        for i in range(num_inferences):
            try:
                self.inference_in_progress = True
                
                pre_npu_usage = self.get_npu_usage()
                
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                cpu_usage.append(cpu_percent)
                memory_usage.append(memory_percent)
                
                print(f"ğŸ”¥ Ryzen AI NPUæ¨è«– {i+1}: æ—¥æœ¬èªæœ€é©åŒ–å‡¦ç†ä¸­...")
                
                start_time = time.time()
                
                def run_ryzen_ai_inference():
                    print(f"âš¡ {self.active_provider} æ—¥æœ¬èªNPUæ¨è«–å®Ÿè¡Œä¸­...")
                    result = self.session.run(None, {input_name: input_data})
                    print(f"âœ… {self.active_provider} æ—¥æœ¬èªNPUæ¨è«–å®Œäº†")
                    return result
                
                result = self._run_with_timeout(run_ryzen_ai_inference, 30)
                
                if result is not None:
                    inference_time = time.time() - start_time
                    total_time += inference_time
                    successful_inferences += 1
                    
                    post_npu_usage = self.get_npu_usage()
                    max_npu_usage = max(max_npu_usage, post_npu_usage)
                    
                    if post_npu_usage > pre_npu_usage + 0.5:
                        npu_activity_detected += 1
                        print(f"ğŸ”¥ Ryzen AI NPUè² è·ç¢ºèªï¼{pre_npu_usage:.1f}% â†’ {post_npu_usage:.1f}%")
                    
                    if (i + 1) % 3 == 0:
                        print(f"  âœ… æ—¥æœ¬èªNPUæ¨è«– {i+1}/{num_inferences} å®Œäº† ({inference_time:.3f}ç§’)")
                        print(f"  ğŸ”¥ NPUè² è·æ¤œå‡ºå›æ•°: {npu_activity_detected}/{i+1}")
                        print(f"  ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {max_npu_usage:.1f}%")
                else:
                    print(f"  âš ï¸ æ—¥æœ¬èªNPUæ¨è«– {i+1} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
                self.inference_in_progress = False
                time.sleep(0.5)
                
            except Exception as e:
                self.inference_in_progress = False
                print(f"  âŒ æ—¥æœ¬èªNPUæ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœè¨ˆç®—
        if successful_inferences > 0:
            avg_time = total_time / successful_inferences
            throughput = successful_inferences / total_time if total_time > 0 else 0
        else:
            avg_time = 0
            throughput = 0
        
        results = {
            'successful_inferences': successful_inferences,
            'total_inferences': num_inferences,
            'success_rate': successful_inferences / num_inferences * 100,
            'total_time': total_time,
            'average_time': avg_time,
            'throughput': throughput,
            'active_provider': self.active_provider,
            'avg_cpu_usage': np.mean(cpu_usage) if cpu_usage else 0,
            'avg_memory_usage': np.mean(memory_usage) if memory_usage else 0,
            'npu_activity_detected': npu_activity_detected,
            'npu_activity_rate': npu_activity_detected / successful_inferences * 100 if successful_inferences > 0 else 0,
            'max_npu_usage': max_npu_usage
        }
        
        return results
    
    def load_japanese_optimized_model(self) -> bool:
        """æ—¥æœ¬èªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        japanese_models = [
            ("rinna/japanese-gpt2-medium", "Rinnaæ—¥æœ¬èªGPT-2"),
            ("cyberagent/open-calm-small", "CyberAgent OpenCALM"),
            ("matsuo-lab/weblab-10b-instruction-sft", "Matsuo Lab WebLab"),
            ("rinna/japanese-gpt2-small", "Rinnaæ—¥æœ¬èªGPT-2 Small"),
            ("gpt2", "GPT-2ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        ]
        
        for model_name, display_name in japanese_models:
            try:
                print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {display_name}")
                
                def load_japanese_model():
                    if "rinna" in model_name:
                        # Rinnaæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
                        tokenizer = AutoTokenizer.from_pretrained(model_name)
                        if tokenizer.pad_token is None:
                            tokenizer.pad_token = tokenizer.eos_token
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            device_map=None
                        )
                        
                        print(f"âœ… Rinnaæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                        
                    elif "cyberagent" in model_name:
                        # CyberAgent OpenCALMãƒ¢ãƒ‡ãƒ«
                        tokenizer = AutoTokenizer.from_pretrained(model_name)
                        if tokenizer.pad_token is None:
                            tokenizer.pad_token = tokenizer.eos_token
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            device_map=None
                        )
                        
                        print(f"âœ… CyberAgent OpenCALMãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                        
                    elif "matsuo-lab" in model_name:
                        # Matsuo Lab WebLabãƒ¢ãƒ‡ãƒ«
                        tokenizer = AutoTokenizer.from_pretrained(model_name)
                        if tokenizer.pad_token is None:
                            tokenizer.pad_token = tokenizer.eos_token
                        
                        model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            device_map=None
                        )
                        
                        print(f"âœ… Matsuo Lab WebLabãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                        
                    else:
                        # GPT-2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
                        tokenizer.pad_token = tokenizer.eos_token
                        
                        model = GPT2LMHeadModel.from_pretrained(
                            model_name,
                            torch_dtype=torch.float32,
                            device_map=None
                        )
                        
                        print(f"âœ… GPT-2ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                    
                    return tokenizer, model
                
                result = self._run_with_timeout(load_japanese_model, 180)  # 3åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result:
                    self.tokenizer, self.model = result
                    print(f"âœ… æ—¥æœ¬èªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {display_name}")
                    print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èªå½™æ•°: {len(self.tokenizer)}")
                    print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œ: æœ€é©åŒ–æ¸ˆã¿")
                    return True
                else:
                    print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {display_name}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {display_name} - {e}")
                continue
        
        print("âŒ å…¨ã¦ã®æ—¥æœ¬èªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—")
        return False
    
    def generate_japanese_text_optimized(self, prompt: str, max_tokens: int = 100) -> str:
        """æ—¥æœ¬èªæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.model or not self.tokenizer:
            return "âŒ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        try:
            print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:30]}...'")
            
            # æ—¥æœ¬èªã«æœ€é©åŒ–ã•ã‚ŒãŸç”Ÿæˆè¨­å®š
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            input_length = inputs.shape[1]
            
            print(f"ğŸ“Š å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_length}")
            
            # æ—¥æœ¬èªç”Ÿæˆã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®š
            generation_config = {
                'max_new_tokens': max_tokens,
                'min_new_tokens': 10,  # æ—¥æœ¬èªã§ã¯æœ€ä½10ãƒˆãƒ¼ã‚¯ãƒ³
                'do_sample': True,
                'temperature': 0.7,  # æ—¥æœ¬èªã«é©ã—ãŸæ¸©åº¦
                'top_p': 0.95,
                'top_k': 40,
                'repetition_penalty': 1.05,  # æ—¥æœ¬èªã®ç¹°ã‚Šè¿”ã—åˆ¶å¾¡
                'pad_token_id': self.tokenizer.eos_token_id,
                'eos_token_id': self.tokenizer.eos_token_id,
                'no_repeat_ngram_size': 3,  # æ—¥æœ¬èªã®è‡ªç„¶æ€§å‘ä¸Š
                'early_stopping': True
            }
            
            print(f"ğŸ”§ æ—¥æœ¬èªæœ€é©åŒ–ç”Ÿæˆè¨­å®š: temperature={generation_config['temperature']}")
            
            def generate_japanese():
                with torch.no_grad():
                    # attention_maskè¨­å®šã§è­¦å‘Šã‚’å›é¿
                    attention_mask = torch.ones_like(inputs)
                    
                    outputs = self.model.generate(
                        inputs,
                        attention_mask=attention_mask,
                        **generation_config
                    )
                    
                    generated_text = self.tokenizer.decode(
                        outputs[0], 
                        skip_special_tokens=True
                    )
                    
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
                    if generated_text.startswith(prompt):
                        generated_text = generated_text[len(prompt):].strip()
                    
                    print(f"ğŸ“Š ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {outputs.shape[1] - input_length}")
                    
                    return generated_text
            
            result = self._run_with_timeout(generate_japanese, 90)  # 90ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            
            if result and result.strip():
                print(f"âœ… æ—¥æœ¬èªæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"ğŸ“ ç”Ÿæˆæ–‡å­—æ•°: {len(result)}")
                return result
            else:
                # æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
                print("âš ï¸ æ¨™æº–ç”ŸæˆãŒç©ºã®ãŸã‚ã€æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚’å®Ÿè¡Œ")
                return self.generate_japanese_fallback(prompt, max_tokens)
                
        except Exception as e:
            print(f"âŒ æ—¥æœ¬èªæœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self.generate_japanese_fallback(prompt, max_tokens)
    
    def generate_japanese_fallback(self, prompt: str, max_tokens: int) -> str:
        """æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            print("ğŸ”„ æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆå®Ÿè¡Œä¸­...")
            
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            attention_mask = torch.ones_like(inputs)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=max_tokens,
                    do_sample=False,  # æ±ºå®šçš„ç”Ÿæˆ
                    temperature=0.6,  # æ—¥æœ¬èªã«é©ã—ãŸä½æ¸©åº¦
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
                
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                
                if generated_text.strip():
                    print("âœ… æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”ŸæˆæˆåŠŸ")
                    return generated_text
                else:
                    # æœ€çµ‚æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã€Œ{prompt}ã€ã«é–¢ã™ã‚‹æ—¥æœ¬èªã®å›ç­”ã‚’æº–å‚™ä¸­ã§ã™ã€‚Ryzen AI NPUã§å‡¦ç†ã‚’æœ€é©åŒ–ã—ã¦ã„ã¾ã™ã€‚"
                    
        except Exception as e:
            print(f"âŒ æ—¥æœ¬èªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã®æ—¥æœ¬èªå›ç­”ã‚’ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚Ryzen AI NPUã‚·ã‚¹ãƒ†ãƒ ã‚’ç¢ºèªä¸­ã§ã™ã€‚"
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # 1. Ryzen AI NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            onnx_path = self.create_ryzen_ai_optimized_model()
            
            # 2. Ryzen AI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_session_with_ryzen_ai_optimization(onnx_path):
                print("âŒ Ryzen AI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # 3. Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆ
            print("ğŸ”§ Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_result = self.test_ryzen_ai_npu_inference(3)
            
            if test_result['successful_inferences'] > 0:
                print(f"âœ… Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ: {test_result['successful_inferences']}/3å›æˆåŠŸ")
                print(f"ğŸ“Š æˆåŠŸç‡: {test_result['success_rate']:.1f}%")
                print(f"ğŸ”¥ NPUè² è·æ¤œå‡º: {test_result['npu_activity_detected']}/3å›")
                print(f"ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡: {test_result['npu_activity_rate']:.1f}%")
                print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {test_result['max_npu_usage']:.1f}%")
            else:
                print("âš ï¸ Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆã§æˆåŠŸã—ãŸæ¨è«–ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # 4. æ—¥æœ¬èªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            if not self.load_japanese_optimized_model():
                print("âš ï¸ æ—¥æœ¬èªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€NPUæ¨è«–ã¯åˆ©ç”¨å¯èƒ½ã§ã™")
            
            print("âœ… Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_japanese_benchmark(self, num_inferences: int = 15) -> Dict[str, Any]:
        """æ—¥æœ¬èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸ“Š Ryzen AI NPUæ—¥æœ¬èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ï¼ˆ{num_inferences}å›æ¨è«–ï¼‰...")
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªNPUæœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰")
        
        start_time = time.time()
        results = self.test_ryzen_ai_npu_inference(num_inferences)
        total_benchmark_time = time.time() - start_time
        
        print(f"\nğŸ¯ Ryzen AI NPUæ—¥æœ¬èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {results['successful_inferences']}/{results['total_inferences']}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_benchmark_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {results['throughput']:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {results['average_time']*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['active_provider']}")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {results['avg_cpu_usage']:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {results['avg_memory_usage']:.1f}%")
        print(f"  ğŸ”¥ NPUè² è·æ¤œå‡ºå›æ•°: {results['npu_activity_detected']}/{results['successful_inferences']}")
        print(f"  ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡: {results['npu_activity_rate']:.1f}%")
        print(f"  ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
        print(f"  ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªæœ€é©åŒ–: æœ‰åŠ¹")
        print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ® Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print("ğŸ’¡ 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
        print("ğŸ’¡ 'benchmark' ã§æ—¥æœ¬èªNPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        print("ğŸ’¡ 'npu' ã§Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆ")
        print("ğŸ’¡ 'status' ã§ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ç¢ºèª")
        print("ğŸ’¡ 'usage' ã§NPUä½¿ç”¨ç‡ç¢ºèª")
        print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã§ã®è³ªå•ã‚’ãŠè©¦ã—ãã ã•ã„")
        
        while True:
            try:
                user_input = input("\nğŸ’¬ æ—¥æœ¬èªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    self.npu_monitoring_active = False
                    break
                
                elif user_input.lower() == 'benchmark':
                    self.run_japanese_benchmark(10)
                
                elif user_input.lower() == 'npu':
                    print("ğŸ”¥ Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                    results = self.test_ryzen_ai_npu_inference(5)
                    print(f"âœ… Ryzen AI NPUæ¨è«–: {results['successful_inferences']}/5å›æˆåŠŸ")
                    print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
                
                elif user_input.lower() == 'status':
                    print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                    print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: {'åˆ©ç”¨å¯èƒ½' if self.model else 'åˆ©ç”¨ä¸å¯'}")
                    print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
                    print(f"ğŸ“Š NPUç›£è¦–: {'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if self.npu_monitoring_active else 'éã‚¢ã‚¯ãƒ†ã‚£ãƒ–'}")
                
                elif user_input.lower() == 'usage':
                    npu_usage = self.get_npu_usage()
                    print(f"ğŸ”¥ ç¾åœ¨ã®Ryzen AI NPUä½¿ç”¨ç‡: {npu_usage:.1f}%")
                    print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                    print(f"âš¡ æ¨è«–å®Ÿè¡Œä¸­: {'ã¯ã„' if self.inference_in_progress else 'ã„ã„ãˆ'}")
                
                elif user_input:
                    if self.model:
                        generated_text = self.generate_japanese_text_optimized(user_input, 80)
                        print(f"\nğŸ¯ æ—¥æœ¬èªæœ€é©åŒ–ç”Ÿæˆçµæœ:\n{generated_text}")
                    else:
                        print("âš ï¸ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚")
                        results = self.test_ryzen_ai_npu_inference(3)
                        print(f"âœ… Ryzen AI NPUæ¨è«–: {results['successful_inferences']}/3å›æˆåŠŸ")
                        print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                self.npu_monitoring_active = False
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--inferences", type=int, default=15, help="æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=80, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--timeout", type=int, default=30, help="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    parser.add_argument("--japanese", action="store_true", help="æ—¥æœ¬èªNPUæ¨è«–ãƒ†ã‚¹ãƒˆ")
    
    args = parser.parse_args()
    
    # infer-OSè¨­å®š
    if args.infer_os:
        os.environ['INFER_OS_ENABLED'] = '1'
    
    try:
        if args.compare:
            print("ğŸ“Š infer-OS ON/OFFæ—¥æœ¬èªæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            
            # OFFç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '0'
            print("\nğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰:")
            system_off = RyzenAIJapaneseNPUSystem(args.timeout)
            if system_off.initialize_system():
                results_off = system_off.run_japanese_benchmark(args.inferences)
                system_off.npu_monitoring_active = False
            else:
                print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã«å¤±æ•—")
                return
            
            # ONç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '1'
            print("\nâš¡ æœ€é©åŒ–ç‰ˆæ¸¬å®šï¼ˆinfer-OS ONï¼‰:")
            system_on = RyzenAIJapaneseNPUSystem(args.timeout)
            if system_on.initialize_system():
                results_on = system_on.run_japanese_benchmark(args.inferences)
                system_on.npu_monitoring_active = False
            else:
                print("âŒ æœ€é©åŒ–ç‰ˆæ¸¬å®šã«å¤±æ•—")
                return
            
            # æ¯”è¼ƒçµæœ
            print(f"\nğŸ“Š infer-OSæ—¥æœ¬èªåŠ¹æœæ¸¬å®šçµæœ:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡ï¼ˆOFFï¼‰: {results_off['max_npu_usage']:.1f}%")
            print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡ï¼ˆONï¼‰: {results_on['max_npu_usage']:.1f}%")
            print(f"  ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡ï¼ˆOFFï¼‰: {results_off['npu_activity_rate']:.1f}%")
            print(f"  ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡ï¼ˆONï¼‰: {results_on['npu_activity_rate']:.1f}%")
            
            if results_off['throughput'] > 0:
                improvement = (results_on['throughput'] - results_off['throughput']) / results_off['throughput'] * 100
                print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„ç‡: {improvement:+.1f}%")
            
        else:
            # é€šå¸¸å®Ÿè¡Œ
            system = RyzenAIJapaneseNPUSystem(args.timeout)
            
            if not system.initialize_system():
                print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            if args.interactive:
                system.interactive_mode()
            elif args.japanese:
                print("ğŸ‡¯ğŸ‡µ Ryzen AI NPUæ—¥æœ¬èªæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                results = system.test_ryzen_ai_npu_inference(10)
                print(f"âœ… Ryzen AI NPUæ—¥æœ¬èªæ¨è«–: {results['successful_inferences']}/10å›æˆåŠŸ")
                print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
            elif args.prompt:
                if system.model:
                    generated_text = system.generate_japanese_text_optimized(args.prompt, args.tokens)
                    print(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
                    print(f"ğŸ¯ æ—¥æœ¬èªæœ€é©åŒ–ç”Ÿæˆçµæœ:\n{generated_text}")
                else:
                    print("âš ï¸ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Ryzen AI NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    results = system.test_ryzen_ai_npu_inference(args.inferences)
                    print(f"âœ… Ryzen AI NPUæ¨è«–: {results['successful_inferences']}/{args.inferences}å›æˆåŠŸ")
                    print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
            else:
                system.run_japanese_benchmark(args.inferences)
            
            # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰åœæ­¢
            system.npu_monitoring_active = False
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

