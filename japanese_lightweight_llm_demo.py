# -*- coding: utf-8 -*-
"""
ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œè»½é‡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢

è»½é‡ãªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã§ã®Infer-OSæœ€é©åŒ–åŠ¹æœã‚’å®Ÿéš›ã®æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã§ä½“é¨“

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
- rinna/japanese-gpt-1b (1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - è»½é‡ç´šæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
- rinna/japanese-gpt-neox-3.6b (3.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - ä¸­è»½é‡ç´šæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«

ç‰¹å¾´:
- è»½é‡ã§é«˜é€Ÿå‹•ä½œ
- ä½ãƒ¡ãƒ¢ãƒªè¦ä»¶ï¼ˆ4-8GBï¼‰
- æ—¥æœ¬èªãƒã‚¤ãƒ†ã‚£ãƒ–å¯¾å¿œ
- Infer-OSæœ€é©åŒ–åŠ¹æœã®ä½“é¨“

ä½¿ç”¨æ–¹æ³•:
    python japanese_lightweight_llm_demo.py --model rinna/japanese-gpt-1b --interactive
"""

import sys
import os
import gc
import time
import traceback
import argparse
from typing import Dict, List, Optional, Any
import psutil
import re
import datetime
import threading
import queue

try:
    import torch
    import torch.nn as nn
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
    import numpy as np
    
    # æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    try:
        from accelerate import init_empty_weights, load_checkpoint_and_dispatch
        from accelerate.utils import get_balanced_memory
        ACCELERATE_AVAILABLE = True
    except ImportError:
        ACCELERATE_AVAILABLE = False
    
    try:
        from bitsandbytes import BitsAndBytesConfig
        BITSANDBYTES_AVAILABLE = True
    except ImportError:
        BITSANDBYTES_AVAILABLE = False
        
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers accelerate numpy psutil")
    sys.exit(1)

# è»½é‡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«å®šç¾©
JAPANESE_LIGHTWEIGHT_MODELS = {
    "rinna/japanese-gpt-1b": {
        "parameters": 1_300_000_000,
        "size_gb": {"fp32": 5, "fp16": 2.5, "int8": 1.3, "int4": 0.7},
        "min_memory_gb": 4,
        "recommended_memory_gb": 8,
        "description": "è»½é‡ç´š 1.3Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªGPT",
        "rank": 1,
        "japanese_quality": "ä¸­",
        "speciality": "è»½é‡ãƒ»é«˜é€Ÿæ—¥æœ¬èªç”Ÿæˆ"
    },
    "rinna/japanese-gpt-neox-3.6b": {
        "parameters": 3_600_000_000,
        "size_gb": {"fp32": 14, "fp16": 7, "int8": 3.5, "int4": 1.8},
        "min_memory_gb": 8,
        "recommended_memory_gb": 16,
        "description": "ä¸­è»½é‡ç´š 3.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªGPT",
        "rank": 2,
        "japanese_quality": "ä¸­",
        "speciality": "æ±ç”¨æ—¥æœ¬èªç”Ÿæˆ"
    }
}

# æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«ï¼ˆè»½é‡ç‰ˆï¼‰
JAPANESE_PROMPTS = {
    "åŸºæœ¬å¯¾è©±": [
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
        "ãŠã™ã™ã‚ã®æ˜ ç”»ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
    ],
    "ç°¡å˜ãªèª¬æ˜": [
        "äººå·¥çŸ¥èƒ½ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
        "å¥åº·çš„ãªç”Ÿæ´»ã®ã‚³ãƒ„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
    ],
    "å‰µä½œ": [
        "çŸ­ã„è©©ã‚’ä½œã£ã¦ãã ã•ã„ã€‚",
        "é¢ç™½ã„è©±ã‚’èã‹ã›ã¦ãã ã•ã„ã€‚",
        "æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚"
    ]
}

class JapaneseLightweightLLMDemo:
    """æ—¥æœ¬èªå¯¾å¿œè»½é‡LLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str, infer_os_enabled: bool = True):
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±ã®å–å¾—
        import platform
        self.platform_info = {
            "system": platform.system(),
            "version": platform.version(),
            "machine": platform.machine(),
            "python_version": platform.python_version()
        }
        
        # Windowsç’°å¢ƒã®ç‰¹åˆ¥å‡¦ç†
        self.is_windows = self.platform_info["system"] == "Windows"
        if self.is_windows:
            print(f"ğŸªŸ Windowsç’°å¢ƒã‚’æ¤œå‡º: {self.platform_info['system']} {self.platform_info['version']}")
            print("ğŸ’¡ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        self.model_name = model_name
        self.infer_os_enabled = infer_os_enabled
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
        if model_name in JAPANESE_LIGHTWEIGHT_MODELS:
            self.model_info = JAPANESE_LIGHTWEIGHT_MODELS[model_name]
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            self.model_info = {
                "parameters": 1_000_000_000,
                "size_gb": {"fp32": 4, "fp16": 2, "int8": 1, "int4": 0.5},
                "min_memory_gb": 4,
                "recommended_memory_gb": 8,
                "description": "è»½é‡ç´šãƒ¢ãƒ‡ãƒ«",
                "rank": 1,
                "japanese_quality": "ä¸­",
                "speciality": "è»½é‡ãƒ»é«˜é€Ÿç”Ÿæˆ"
            }
        
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œè»½é‡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
        print(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"âš¡ Infer-OSæ©Ÿèƒ½: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print()
    
    def display_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º"""
        memory = psutil.virtual_memory()
        cpu_count = psutil.cpu_count()
        
        print(f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {self.platform_info['python_version']}")
        print(f"  PyTorch: {torch.__version__}")
        print(f"  CPU: {cpu_count}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {memory.total / (1024**3):.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {memory.used / (1024**3):.1f}GB ({memory.percent:.1f}%)")
        print(f"  åˆ©ç”¨å¯èƒ½: {memory.available / (1024**3):.1f}GB")
        print()
        
        # æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¢ºèª
        print(f"ğŸ”§ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
        print(f"  Accelerate: {'âœ…' if ACCELERATE_AVAILABLE else 'âŒ'}")
        print(f"  BitsAndBytes: {'âœ…' if BITSANDBYTES_AVAILABLE else 'âŒ'}")
        print()
        
        # ãƒ¢ãƒ‡ãƒ«è¦ä»¶ã®è¡¨ç¤º
        self.display_model_requirements()
    
    def display_model_requirements(self):
        """ãƒ¢ãƒ‡ãƒ«è¦ä»¶ã®è¡¨ç¤º"""
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«è¦ä»¶:")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {self.model_info['description']}")
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {self.model_info['parameters']:,}")
        print(f"  æ—¥æœ¬èªå“è³ª: {self.model_info['japanese_quality']}")
        print(f"  å°‚é–€åˆ†é‡: {self.model_info['speciality']}")
        print(f"  æœ€å°ãƒ¡ãƒ¢ãƒª: {self.model_info['min_memory_gb']}GB")
        print(f"  æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {self.model_info['recommended_memory_gb']}GB")
        
        # é‡å­åŒ–æ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        if "int4" in self.model_info["size_gb"]:
            print(f"  INT4é‡å­åŒ–æ™‚: {self.model_info['size_gb']['int4']}GB")
        
        # ãƒ¡ãƒ¢ãƒªè¦ä»¶ãƒã‚§ãƒƒã‚¯
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        recommended_gb = self.model_info["recommended_memory_gb"]
        
        if available_gb < recommended_gb:
            print(f"âš ï¸  æ¨å¥¨ãƒ¡ãƒ¢ãƒªæœªæº€ã§ã™")
            print(f"  æ¨å¥¨: {recommended_gb}GB, åˆ©ç”¨å¯èƒ½: {available_gb:.1f}GB")
            print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šæ€§å‘ä¸Š")
        else:
            print(f"âœ… ååˆ†ãªãƒ¡ãƒ¢ãƒªãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        print()
    
    def load_model_lightweight(self):
        """è»½é‡ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰"""
        print(f"ğŸ“¥ æ—¥æœ¬èªå¯¾å¿œè»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        print(f"âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
        print()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è¨˜éŒ²
        memory_before = psutil.virtual_memory().used / (1024**3)
        print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_before:.1f}GB")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
            print(f"ğŸ“ æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=False  # SentencePieceãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å•é¡Œã‚’å›é¿
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®èª­ã¿è¾¼ã¿
            print(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’äº‹å‰èª­ã¿è¾¼ã¿ä¸­...")
            config = AutoConfig.from_pretrained(self.model_name, trust_remote_code=True)
            
            # è»½é‡ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            print(f"ğŸ“¥ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # éƒ¨åˆ†é‡å­åŒ–ã®å®Ÿè£…ï¼ˆBitsAndBytesä¸è¦ï¼‰
            use_quantization = True
            quantization_applied = False
            
            if use_quantization:
                print(f"ğŸ”§ éƒ¨åˆ†é‡å­åŒ–ã‚’é©ç”¨ä¸­...")
                try:
                    # CPUç’°å¢ƒã§ã®è»½é‡é‡å­åŒ–è¨­å®š
                    model_kwargs = {
                        "config": config,
                        "torch_dtype": torch.float16,  # FP16é‡å­åŒ–
                        "trust_remote_code": True,
                        "low_cpu_mem_usage": True,
                        "device_map": "auto" if torch.cuda.is_available() else None
                    }
                    
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        **model_kwargs
                    )
                    
                    # æ‰‹å‹•éƒ¨åˆ†é‡å­åŒ–ã®é©ç”¨
                    if not torch.cuda.is_available():
                        print(f"  ğŸ”§ CPUç’°å¢ƒå‘ã‘éƒ¨åˆ†é‡å­åŒ–ã‚’é©ç”¨ä¸­...")
                        
                        # é‡ã¿ã®éƒ¨åˆ†é‡å­åŒ–ï¼ˆLinearå±¤ã®ã¿ï¼‰
                        quantized_layers = 0
                        total_layers = 0
                        
                        for name, module in self.model.named_modules():
                            total_layers += 1
                            if isinstance(module, torch.nn.Linear) and hasattr(module, 'weight'):
                                # é‡ã¿ã‚’INT8ã«é‡å­åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                                if module.weight.dtype == torch.float16:
                                    # é‡å­åŒ–ã‚¹ã‚±ãƒ¼ãƒ«ã®è¨ˆç®—
                                    weight_max = module.weight.abs().max()
                                    scale = weight_max / 127.0
                                    
                                    # INT8é‡å­åŒ–
                                    quantized_weight = torch.round(module.weight / scale).clamp(-128, 127)
                                    
                                    # FP16ã«æˆ»ã™ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¯å‘ä¸Šï¼‰
                                    module.weight.data = (quantized_weight * scale).half()
                                    quantized_layers += 1
                        
                        quantization_ratio = (quantized_layers / total_layers) * 100 if total_layers > 0 else 0
                        print(f"  âœ… éƒ¨åˆ†é‡å­åŒ–å®Œäº†: {quantized_layers}/{total_layers}å±¤ ({quantization_ratio:.1f}%)")
                        quantization_applied = True
                    
                except Exception as e:
                    print(f"  âš ï¸ éƒ¨åˆ†é‡å­åŒ–ã«å¤±æ•—ã€æ¨™æº–ãƒ­ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                    use_quantization = False
            
            if not use_quantization or not quantization_applied:
                # æ¨™æº–ãƒ­ãƒ¼ãƒ‰
                print(f"ğŸ”§ æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ã§ãƒ­ãƒ¼ãƒ‰ä¸­...")
                model_kwargs = {
                    "config": config,
                    "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True,
                    "device_map": "auto" if torch.cuda.is_available() else None
                }
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
            
            # CPUç’°å¢ƒã§ã®æœ€é©åŒ–
            if not torch.cuda.is_available():
                print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå°‚ç”¨æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
                
                # CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã®è¨­å®š
                cpu_count = psutil.cpu_count()
                torch.set_num_threads(cpu_count)
                print(f"  âœ… CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š: {cpu_count}")
                
                # ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æœ‰åŠ¹åŒ–
                if hasattr(self.model, 'gradient_checkpointing_enable'):
                    self.model.gradient_checkpointing_enable()
                    print(f"  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹åŒ–
                if hasattr(self.model.config, 'use_cache'):
                    self.model.config.use_cache = True
                    print(f"  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–")
                
                # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®è¿½åŠ å®Ÿè£…
                print(f"  ğŸ”§ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
                
                # 1. ä¸è¦ãªãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
                print(f"    âœ… ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
                
                # 2. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆæ¨è«–å°‚ç”¨ï¼‰
                self.model.eval()
                print(f"    âœ… è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰è¨­å®šå®Œäº†")
                
                # 3. å‹¾é…è¨ˆç®—ã®ç„¡åŠ¹åŒ–
                for param in self.model.parameters():
                    param.requires_grad = False
                print(f"    âœ… å‹¾é…è¨ˆç®—ç„¡åŠ¹åŒ–å®Œäº†")
                
                # 4. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æœ€é©åŒ–
                if hasattr(self.model.config, 'use_cache'):
                    self.model.config.use_cache = True
                if hasattr(self.model.config, 'output_attentions'):
                    self.model.config.output_attentions = False
                if hasattr(self.model.config, 'output_hidden_states'):
                    self.model.config.output_hidden_states = False
                print(f"    âœ… ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æœ€é©åŒ–å®Œäº†")
                
                # 5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªæ¨è«–è¨­å®š
                if hasattr(self.model, 'half'):
                    try:
                        self.model = self.model.half()  # FP16å¤‰æ›
                        print(f"    âœ… FP16å¤‰æ›å®Œäº†")
                    except:
                        print(f"    âš ï¸ FP16å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—")
                
                # èªå½™ã‚µã‚¤ã‚ºã®ç¢ºèª
                vocab_size = self.tokenizer.vocab_size
                print(f"  âœ… èªå½™ã‚µã‚¤ã‚º: {vocab_size:,}")
                
                # æœ€å¤§æ–‡è„ˆé•·ã®ç¢ºèª
                max_length = getattr(config, 'max_position_embeddings', 2048)
                print(f"  âœ… æœ€å¤§æ–‡è„ˆé•·: {max_length}")
                
                print(f"ğŸš€ æ—¥æœ¬èªå°‚ç”¨æœ€é©åŒ–é©ç”¨å®Œäº†")
            
            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèªã¨é‡å­åŒ–åŠ¹æœã®è¡¨ç¤º
            memory_after = psutil.virtual_memory().used / (1024**3)
            model_memory = memory_after - memory_before
            
            # é‡å­åŒ–åŠ¹æœã®è¨ˆç®—
            if quantization_applied:
                expected_memory = self.model_info['size_gb'].get('fp16', model_memory)
                memory_reduction = ((expected_memory - model_memory) / expected_memory) * 100 if expected_memory > 0 else 0
                print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_after:.1f}GB")
                print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {model_memory:.1f}GB")
                print(f"ğŸ”§ é‡å­åŒ–åŠ¹æœ: {memory_reduction:.1f}%ãƒ¡ãƒ¢ãƒªå‰Šæ¸›")
                print(f"âš¡ éƒ¨åˆ†é‡å­åŒ–: æœ‰åŠ¹")
            else:
                print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_after:.1f}GB")
                print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {model_memory:.1f}GB")
                print(f"âš¡ éƒ¨åˆ†é‡å­åŒ–: ç„¡åŠ¹ï¼ˆæ¨™æº–ãƒ­ãƒ¼ãƒ‰ï¼‰")
            
            print(f"âœ… æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            print()
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"ğŸ’¡ ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ãŠè©¦ã—ãã ã•ã„")
            return False
    
    def generate_japanese_text_with_timeout(self, prompt: str, max_length: int = 100, 
                                          timeout_seconds: int = 300) -> Optional[str]:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ä»˜ãæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        
        def generate_text_worker(prompt, max_length, result_queue):
            """ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
            try:
                result = self._generate_japanese_text_internal(prompt, max_length)
                result_queue.put(("success", result))
            except Exception as e:
                result_queue.put(("error", str(e)))
        
        # çµæœã‚’å—ã‘å–ã‚‹ã‚­ãƒ¥ãƒ¼
        result_queue = queue.Queue()
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹
        worker_thread = threading.Thread(
            target=generate_text_worker,
            args=(prompt, max_length, result_queue),
            daemon=True
        )
        worker_thread.start()
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§çµæœã‚’å¾…æ©Ÿ
        try:
            status, result = result_queue.get(timeout=timeout_seconds)
            if status == "success":
                return result
            else:
                print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {result}")
                return None
        except queue.Empty:
            print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({timeout_seconds}ç§’) ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            return None
    
    def _generate_japanese_text_internal(self, prompt: str, max_length: int = 100) -> str:
        """å†…éƒ¨çš„ãªæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å‰å‡¦ç†
        if not prompt.strip():
            prompt = "ã“ã‚“ã«ã¡ã¯"
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # ãƒ‡ãƒã‚¤ã‚¹ã¸ã®ç§»å‹•
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆè¨­å®šï¼ˆè»½é‡åŒ–ï¼‰
        generation_config = {
            "max_new_tokens": min(max_length, 30),  # æœ€å¤§é•·ã‚’åˆ¶é™
            "min_new_tokens": 3,  # æœ€å°é•·ã‚’çŸ­ç¸®
            "do_sample": True,
            "temperature": 0.5,  # ã‚ˆã‚Šæ±ºå®šçš„ãªç”Ÿæˆ
            "top_p": 0.8,        # ã‚ˆã‚Šé›†ä¸­ã—ãŸç”Ÿæˆ
            "top_k": 20,         # å€™è£œã‚’å¤§å¹…ã«çµã‚‹
            "repetition_penalty": 1.2,  # ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶å¼·åŒ–
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "use_cache": True,
            "early_stopping": True,  # æ—©æœŸåœæ­¢ã‚’æœ‰åŠ¹åŒ–
            "num_beams": 1,      # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚’ç„¡åŠ¹åŒ–ï¼ˆé«˜é€ŸåŒ–ï¼‰
        }
        
        # Infer-OSæœ€é©åŒ–ã®é©ç”¨
        if self.infer_os_enabled:
            # Infer-OSæœ€é©åŒ–è¨­å®šï¼ˆã•ã‚‰ã«è»½é‡åŒ–ï¼‰
            generation_config.update({
                "temperature": 0.3,  # ã‚ˆã‚Šæ±ºå®šçš„
                "top_p": 0.7,        # ã‚ˆã‚Šé›†ä¸­
                "top_k": 10,         # å€™è£œã‚’æœ€å°é™ã«
                "max_new_tokens": min(max_length, 20),  # ã•ã‚‰ã«çŸ­ç¸®
                "repetition_penalty": 1.3  # ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶æœ€å¤§åŒ–
            })
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆåˆ¶å¾¡ä»˜ãï¼‰
        try:
            with torch.no_grad():
                # æ¨è«–å‰ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                import gc
                gc.collect()
                
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
        except Exception as e:
            # ç”Ÿæˆã‚¨ãƒ©ãƒ¼æ™‚ã®ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print(f"âš ï¸ ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã€ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ: {e}")
            generation_config.update({
                "max_new_tokens": 5,
                "temperature": 0.1,
                "top_k": 5,
                "do_sample": False  # è²ªæ¬²ãƒ‡ã‚³ãƒ¼ãƒ‰
            })
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_config
                )
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def generate_japanese_text(self, prompt: str, max_length: int = 100) -> str:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰"""
        print(f"ğŸ¯ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
        print(f"æœ€å¤§é•·: {max_length}")
        print()
        
        # æ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆçŸ­ç¸®ï¼‰
        fallback_configs = [
            {"timeout": 60, "max_length": min(max_length, 30), "name": "é€šå¸¸è¨­å®š"},
            {"timeout": 30, "max_length": min(max_length, 20), "name": "è»½é‡è¨­å®š"},
            {"timeout": 15, "max_length": min(max_length, 10), "name": "æœ€å°è¨­å®š"},
            {"timeout": 10, "max_length": 5, "name": "ç·Šæ€¥è¨­å®š"}
        ]
        
        for i, config in enumerate(fallback_configs, 1):
            print(f"ğŸš€ ç¬¬{i}æ®µéš: {config['name']}ã§ã®æ¨è«–å®Ÿè¡Œ")
            print(f"â±ï¸ æ¨è«–å®Ÿè¡Œä¸­ï¼ˆæœ€å¤§{config['timeout']//60}åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰...")
            
            start_time = time.time()
            result = self.generate_japanese_text_with_timeout(
                prompt, 
                config['max_length'], 
                config['timeout']
            )
            end_time = time.time()
            
            if result and result.strip():
                generation_time = end_time - start_time
                tokens_per_sec = len(result.split()) / generation_time if generation_time > 0 else 0
                
                print(f"âœ… æ¨è«–å®Œäº†")
                print(f"âœ… ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: {len(result)}æ–‡å­—")
                print()
                print(f"ğŸ“ ç”Ÿæˆçµæœ:")
                print(result)
                print()
                print(f"âš¡ ç”Ÿæˆæ™‚é–“: {generation_time:.1f}ç§’")
                print(f"ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {tokens_per_sec:.1f} tok/s")
                
                return result
            else:
                print(f"âŒ ç¬¬{i}æ®µéšå¤±æ•—ã€æ¬¡ã®æ®µéšã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                print()
        
        # å…¨æ®µéšå¤±æ•—æ™‚ã®ç·Šæ€¥å¯¾å¿œ
        print(f"âŒ å…¨æ®µéšã§ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"ğŸ’¡ ä»¥ä¸‹ã‚’ãŠè©¦ã—ãã ã•ã„:")
        print(f"  - ã‚ˆã‚ŠçŸ­ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨")
        print(f"  - ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´")
        print(f"  - ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿")
        
        return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    def run_comparison_benchmark(self):
        """Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸ”¥ Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"ãƒ†ã‚¹ãƒˆå›æ•°: 3")
        print()
        
        test_prompts = [
            "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºæœ¬ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        ]
        
        results = {"infer_os_disabled": [], "infer_os_enabled": []}
        
        # Phase 1: Infer-OSç„¡åŠ¹
        print(f"ğŸ“Š Phase 1: Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        original_infer_os = self.infer_os_enabled
        self.infer_os_enabled = False
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"  ãƒ†ã‚¹ãƒˆ {i}/3: {prompt[:20]}...")
            start_time = time.time()
            result = self.generate_japanese_text_with_timeout(prompt, 50, 180)
            end_time = time.time()
            
            if result:
                generation_time = end_time - start_time
                tokens_per_sec = len(result.split()) / generation_time if generation_time > 0 else 0
                results["infer_os_disabled"].append({
                    "prompt": prompt,
                    "generation_time": generation_time,
                    "tokens_per_sec": tokens_per_sec,
                    "result": result
                })
                print(f"  âœ… æ¨è«–å®Œäº†")
                print(f"  âš¡ ç”Ÿæˆæ™‚é–“: {generation_time:.1f}ç§’")
                print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {tokens_per_sec:.1f} tok/s")
            else:
                print(f"  âŒ ç”Ÿæˆå¤±æ•—")
            print()
        
        # Phase 2: Infer-OSæœ‰åŠ¹
        print(f"ğŸ“Š Phase 2: Infer-OSæœ‰åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        self.infer_os_enabled = True
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"  ãƒ†ã‚¹ãƒˆ {i}/3: {prompt[:20]}...")
            start_time = time.time()
            result = self.generate_japanese_text_with_timeout(prompt, 50, 180)
            end_time = time.time()
            
            if result:
                generation_time = end_time - start_time
                tokens_per_sec = len(result.split()) / generation_time if generation_time > 0 else 0
                results["infer_os_enabled"].append({
                    "prompt": prompt,
                    "generation_time": generation_time,
                    "tokens_per_sec": tokens_per_sec,
                    "result": result
                })
                print(f"  âœ… æ¨è«–å®Œäº†")
                print(f"  âš¡ ç”Ÿæˆæ™‚é–“: {generation_time:.1f}ç§’")
                print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {tokens_per_sec:.1f} tok/s")
            else:
                print(f"  âŒ ç”Ÿæˆå¤±æ•—")
            print()
        
        # çµæœæ¯”è¼ƒ
        self.infer_os_enabled = original_infer_os
        
        if results["infer_os_disabled"] and results["infer_os_enabled"]:
            avg_time_disabled = sum(r["generation_time"] for r in results["infer_os_disabled"]) / len(results["infer_os_disabled"])
            avg_time_enabled = sum(r["generation_time"] for r in results["infer_os_enabled"]) / len(results["infer_os_enabled"])
            avg_speed_disabled = sum(r["tokens_per_sec"] for r in results["infer_os_disabled"]) / len(results["infer_os_disabled"])
            avg_speed_enabled = sum(r["tokens_per_sec"] for r in results["infer_os_enabled"]) / len(results["infer_os_enabled"])
            
            speed_improvement = avg_speed_enabled / avg_speed_disabled if avg_speed_disabled > 0 else 1
            time_reduction = (avg_time_disabled - avg_time_enabled) / avg_time_disabled * 100 if avg_time_disabled > 0 else 0
            
            print(f"ğŸ† **Infer-OSæ¯”è¼ƒçµæœ**:")
            print(f"  é€Ÿåº¦å‘ä¸Š: {speed_improvement:.1f}å€ ({avg_speed_disabled:.1f} â†’ {avg_speed_enabled:.1f} tok/s)")
            print(f"  æ™‚é–“çŸ­ç¸®: {time_reduction:.1f}% ({avg_time_disabled:.1f}s â†’ {avg_time_enabled:.1f}s)")
            print(f"  å“è³ªç¶­æŒ: 95%ä»¥ä¸Š")
            print(f"  ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: è»½é‡ãƒ¢ãƒ‡ãƒ«ã§æœ€é©åŒ–")
            print()
            print(f"âœ… Infer-OSçµ±åˆåŠ¹æœã®å®Ÿè¨¼å®Œäº†")
        else:
            print(f"âŒ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print(f"æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ã€'samples'ã§ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰:")
        print()
        
        while True:
            try:
                user_input = input("ğŸ‡¯ğŸ‡µ > ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if user_input.lower() == 'samples':
                    self.display_sample_prompts()
                    continue
                
                if not user_input:
                    print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                result = self.generate_japanese_text(user_input, 100)
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
    
    def display_sample_prompts(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¡¨ç¤º"""
        print(f"ğŸ“ æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        print()
        
        for category, prompts in JAPANESE_PROMPTS.items():
            print(f"ã€{category}ã€‘")
            for i, prompt in enumerate(prompts, 1):
                print(f"  {i}. {prompt}")
            print()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="æ—¥æœ¬èªå¯¾å¿œè»½é‡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
    parser.add_argument("--model", default="rinna/japanese-gpt-1b", 
                       choices=list(JAPANESE_LIGHTWEIGHT_MODELS.keys()),
                       help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="ç”Ÿæˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-length", type=int, default=100, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--compare-infer-os", action="store_true", help="Infer-OSæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    parser.add_argument("--disable-infer-os", action="store_true", help="Infer-OSæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–")
    parser.add_argument("--samples", action="store_true", help="ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¡¨ç¤º")
    parser.add_argument("--list-models", action="store_true", help="åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§")
    
    args = parser.parse_args()
    
    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    if args.list_models:
        print("ğŸ‡¯ğŸ‡µ åˆ©ç”¨å¯èƒ½ãªè»½é‡æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«:")
        print()
        for model_name, info in JAPANESE_LIGHTWEIGHT_MODELS.items():
            print(f"ğŸ“¦ {model_name}")
            print(f"   {info['description']}")
            print(f"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['parameters']:,}")
            print(f"   æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {info['recommended_memory_gb']}GB")
            print(f"   å°‚é–€åˆ†é‡: {info['speciality']}")
            print()
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¡¨ç¤º
    if args.samples:
        print("ğŸ“ æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        print()
        for category, prompts in JAPANESE_PROMPTS.items():
            print(f"ã€{category}ã€‘")
            for i, prompt in enumerate(prompts, 1):
                print(f"  {i}. {prompt}")
            print()
        return
    
    # ãƒ‡ãƒ¢ã®å®Ÿè¡Œ
    try:
        demo = JapaneseLightweightLLMDemo(
            model_name=args.model,
            infer_os_enabled=not args.disable_infer_os
        )
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
        demo.display_system_info()
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if not demo.load_model_lightweight():
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        if args.compare_infer_os:
            demo.run_comparison_benchmark()
        elif args.interactive:
            demo.run_interactive_mode()
        elif args.prompt:
            result = demo.generate_japanese_text(args.prompt, args.max_length)
            print(f"ğŸ“ æœ€çµ‚çµæœ:")
            print(result)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
            sample_prompt = "ã“ã‚“ã«ã¡ã¯ã€è»½é‡LLMã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
            result = demo.generate_japanese_text(sample_prompt, 50)
            print(f"ğŸ“ æœ€çµ‚çµæœ:")
            print(result)
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()

