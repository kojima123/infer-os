#!/usr/bin/env python3
"""
RyzenAIçµ±åˆæ—¥æœ¬èªLLMãƒ‡ãƒ¢
RyzenAI 1.5.1 + rinna/youri-7b-chatçµ±åˆå®Ÿè£…
"""

import os
import sys
import time
import torch
import numpy as np
from typing import Optional, List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse

# RyzenAI NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ryzenai_npu_engine import RyzenAINPUEngine, RYZENAI_AVAILABLE

class RyzenAIJapaneseLLM:
    """RyzenAIçµ±åˆæ—¥æœ¬èªLLM"""
    
    def __init__(self, model_name: str = "rinna/youri-7b-chat"):
        """
        RyzenAIçµ±åˆæ—¥æœ¬èªLLMåˆæœŸåŒ–
        
        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.model_name = model_name
        self.tokenizer = None
        self.pytorch_model = None
        self.ryzenai_engine = None
        self.npu_layers = None
        
        print("ğŸ‡¯ğŸ‡µ RyzenAIçµ±åˆæ—¥æœ¬èªLLMåˆæœŸåŒ–é–‹å§‹")
        print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«: {model_name}")
        
        # åˆæœŸåŒ–å®Ÿè¡Œ
        self._initialize_components()
    
    def _initialize_components(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        try:
            # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–å®Œäº†")
            
            # 2. PyTorchãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            print("ğŸ§  PyTorchãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
            self.pytorch_model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="cpu",  # CPUã§åˆæœŸåŒ–
                trust_remote_code=True
            )
            
            print("âœ… PyTorchãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
            
            # 3. RyzenAI NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
            print("âš¡ RyzenAI NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ä¸­...")
            self.ryzenai_engine = RyzenAINPUEngine()
            
            if self.ryzenai_engine.npu_available:
                print("âœ… RyzenAI NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–æˆåŠŸ")
                
                # NPUç”¨LLMæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ
                self._setup_npu_layers()
            else:
                print("âš ï¸ RyzenAI NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                print("ğŸ’¡ CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
            
        except Exception as e:
            print(f"âŒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _setup_npu_layers(self):
        """NPUç”¨LLMæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("ğŸ”§ NPUç”¨LLMæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šå–å¾—
            config = self.pytorch_model.config
            vocab_size = config.vocab_size
            hidden_size = config.hidden_size
            
            print(f"  ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {vocab_size}")
            print(f"  ğŸ“Š éš ã‚Œå±¤ã‚µã‚¤ã‚º: {hidden_size}")
            
            # RyzenAIç”¨LLMæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ
            self.npu_layers = self.ryzenai_engine.create_simple_llm_inference(
                vocab_size=vocab_size,
                hidden_dim=hidden_size
            )
            
            if self.npu_layers:
                print("âœ… NPUç”¨LLMæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            else:
                print("âŒ NPUç”¨LLMæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ NPUæ¨è«–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            self.npu_layers = None
    
    def generate_with_ryzenai(self, prompt: str, max_length: int = 300) -> str:
        """
        RyzenAI NPUä½¿ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            max_length: æœ€å¤§ç”Ÿæˆé•·
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.ryzenai_engine.npu_available or not self.npu_layers:
            print("âš ï¸ RyzenAI NPUæ¨è«–ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return self._generate_cpu_fallback(prompt, max_length)
        
        try:
            print("âš¡ RyzenAI NPUæ¨è«–ã«ã‚ˆã‚‹æ—¥æœ¬èªç”Ÿæˆé–‹å§‹")
            print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(prompt, return_tensors="pt")
            input_ids = inputs["input_ids"]
            
            print(f"ğŸ”¤ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_ids.shape[1]}")
            
            generated_tokens = []
            current_ids = input_ids
            
            start_time = time.time()
            
            # é€æ¬¡ç”Ÿæˆ
            for step in range(max_length):
                # PyTorchãƒ¢ãƒ‡ãƒ«ã§éš ã‚ŒçŠ¶æ…‹å–å¾—
                with torch.no_grad():
                    outputs = self.pytorch_model(current_ids, output_hidden_states=True)
                    hidden_states = outputs.hidden_states[-1]  # æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹
                    last_hidden = hidden_states[:, -1, :].cpu().numpy()  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹
                
                # RyzenAI NPUã§æ¨è«–å®Ÿè¡Œ
                if step % 10 == 0:
                    print(f"  ğŸ”„ ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ— {step+1}/{max_length}")
                
                # NPUæ¨è«–
                npu_logits = self._npu_inference(last_hidden)
                
                if npu_logits is not None:
                    # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
                    next_token_id = self._sample_next_token(npu_logits)
                    
                    # ç”Ÿæˆçµ‚äº†åˆ¤å®š
                    if next_token_id == self.tokenizer.eos_token_id:
                        print("ğŸ ç”Ÿæˆçµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³æ¤œå‡º")
                        break
                    
                    generated_tokens.append(next_token_id)
                    
                    # æ¬¡ã®å…¥åŠ›æº–å‚™
                    next_token_tensor = torch.tensor([[next_token_id]])
                    current_ids = torch.cat([current_ids, next_token_tensor], dim=1)
                    
                    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™
                    if current_ids.shape[1] > 512:
                        current_ids = current_ids[:, -256:]  # å¾ŒåŠ256ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿æŒ
                else:
                    print("âš ï¸ NPUæ¨è«–å¤±æ•—ã€CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    return self._generate_cpu_fallback(prompt, max_length)
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # ç”Ÿæˆçµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            if generated_tokens:
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                full_text = prompt + generated_text
                
                print(f"âœ… RyzenAI NPUç”Ÿæˆå®Œäº†")
                print(f"  â±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’")
                print(f"  ğŸ”¤ ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(generated_tokens)}")
                print(f"  ğŸš€ ç”Ÿæˆé€Ÿåº¦: {len(generated_tokens)/generation_time:.1f}ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
                
                return full_text
            else:
                print("âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãªã—")
                return prompt
                
        except Exception as e:
            print(f"âŒ RyzenAI NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_cpu_fallback(prompt, max_length)
    
    def _npu_inference(self, hidden_state: np.ndarray) -> Optional[np.ndarray]:
        """NPUæ¨è«–å®Ÿè¡Œ"""
        try:
            # RMSNormå®Ÿè¡Œ
            normalized = self.npu_layers['rms_norm'](hidden_state)
            
            # Linearå±¤ï¼ˆè¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ï¼‰å®Ÿè¡Œ
            logits = self.npu_layers['lm_head'](normalized)
            
            return logits
            
        except Exception as e:
            print(f"âŒ NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _sample_next_token(self, logits: np.ndarray, temperature: float = 0.7) -> int:
        """æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        try:
            # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            logits = logits / temperature
            
            # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
            exp_logits = np.exp(logits - np.max(logits))
            probs = exp_logits / np.sum(exp_logits)
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            next_token_id = np.random.choice(len(probs), p=probs)
            
            return int(next_token_id)
            
        except Exception as e:
            print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return self.tokenizer.eos_token_id
    
    def _generate_cpu_fallback(self, prompt: str, max_length: int = 300) -> str:
        """CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ"""
        try:
            print("ğŸ–¥ï¸ CPUæ¨è«–ã«ã‚ˆã‚‹æ—¥æœ¬èªç”Ÿæˆ")
            
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.pytorch_model.generate(
                    inputs["input_ids"],
                    max_length=max_length,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return generated_text
            
        except Exception as e:
            print(f"âŒ CPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return prompt
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ‡¯ğŸ‡µ RyzenAIçµ±åˆæ—¥æœ¬èªLLMã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("=" * 60)
        
        # NPUçŠ¶æ…‹è¡¨ç¤º
        if self.ryzenai_engine.npu_available:
            status = self.ryzenai_engine.get_npu_status()
            print("âš¡ NPUçŠ¶æ…‹:")
            print(f"  ğŸ“± ãƒ‡ãƒã‚¤ã‚¹: {status.get('device', 'Unknown')}")
            print(f"  ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {status.get('performance_stats', {}).get('throughput', 'Unknown')}å›/ç§’")
        else:
            print("ğŸ–¥ï¸ CPUæ¨è«–ãƒ¢ãƒ¼ãƒ‰")
        
        print("\nğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†")
        print("=" * 60)
        
        while True:
            try:
                prompt = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ RyzenAIçµ±åˆæ—¥æœ¬èªLLMã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if not prompt:
                    print("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                print("\nğŸ”„ ç”Ÿæˆä¸­...")
                
                # RyzenAI NPUç”Ÿæˆå®Ÿè¡Œ
                result = self.generate_with_ryzenai(prompt, max_length=300)
                
                print(f"\nğŸ“ ç”Ÿæˆçµæœ:")
                print("-" * 40)
                print(result)
                print("-" * 40)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ RyzenAIçµ±åˆæ—¥æœ¬èªLLMã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="RyzenAIçµ±åˆæ—¥æœ¬èªLLMãƒ‡ãƒ¢")
    parser.add_argument("--model", default="rinna/youri-7b-chat", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    print("ğŸ¯ RyzenAIçµ±åˆæ—¥æœ¬èªLLMãƒ‡ãƒ¢")
    print("=" * 50)
    
    if not RYZENAI_AVAILABLE:
        print("âš ï¸ RyzenAI SDK ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        print("ğŸ’¡ RyzenAI 1.5.1 SDKã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print("ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•:")
        print("  pip install ryzenai")
        print("  ã¾ãŸã¯ AMDå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰SDKã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        return
    
    # RyzenAIçµ±åˆæ—¥æœ¬èªLLMåˆæœŸåŒ–
    llm = RyzenAIJapaneseLLM(model_name=args.model)
    
    if args.interactive:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        llm.interactive_mode()
    elif args.prompt:
        # å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ
        print(f"\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
        print("ğŸ”„ ç”Ÿæˆä¸­...")
        
        result = llm.generate_with_ryzenai(args.prompt)
        
        print(f"\nğŸ“ ç”Ÿæˆçµæœ:")
        print("-" * 40)
        print(result)
        print("-" * 40)
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ã‚¹ãƒˆ
        test_prompt = "äººå‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {test_prompt}")
        
        result = llm.generate_with_ryzenai(test_prompt)
        
        print(f"\nğŸ“ ç”Ÿæˆçµæœ:")
        print("-" * 40)
        print(result)
        print("-" * 40)

if __name__ == "__main__":
    main()

