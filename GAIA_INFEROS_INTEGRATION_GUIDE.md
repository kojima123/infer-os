# GAIA Ã— Infer-OS çµ±åˆã‚¬ã‚¤ãƒ‰ v1.0

## ğŸ¯ **æ¦‚è¦**

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€AMD GAIAç’°å¢ƒï¼ˆWindows 11 + Ryzen AI NPU + Radeon iGPUï¼‰ã«Infer-OSã‚’çµ±åˆã—ã€LLMæ¨è«–ã®å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

### **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**
- **7B/8Bãƒ¢ãƒ‡ãƒ«**: 1.3-1.6å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€60-75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **20Bãƒ¢ãƒ‡ãƒ«**: 1.8å€ä»¥ä¸Šã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€70-80%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥**: æœ€å¤§75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **å“è³ªä¿æŒ**: Î”PPL â‰¤ 0.5ã§ã®é«˜å“è³ªç¶­æŒ

## ğŸ”§ **å‰ææ¡ä»¶**

### **ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶**
- **OS**: Windows 11ï¼ˆGAIAè¦ä»¶ï¼‰
- **CPU**: AMD Ryzen AI 300ã‚·ãƒªãƒ¼ã‚ºï¼ˆXDNA NPUæ­è¼‰ï¼‰
- **GPU**: Radeon iGPUï¼ˆRDNAã€DirectMLå¯¾å¿œï¼‰
- **ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Šæ¨å¥¨
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 10GBä»¥ä¸Šã®ç©ºãå®¹é‡

### **ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶**
- **GAIA**: æœ€æ–°ç‰ˆï¼ˆRAUX/gaia-cliå¯¾å¿œï¼‰
- **Python**: 3.11ä»¥ä¸Š
- **ONNX Runtime**: DirectMLå¯¾å¿œç‰ˆ
- **NPU/iGPUãƒ‰ãƒ©ã‚¤ãƒ**: GAIAæ¨å¥¨ç‰ˆ

## ğŸ“¦ **ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †**

### **Step 1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³**
```bash
git clone https://github.com/kojima123/infer-os.git
cd infer-os
```

### **Step 2: ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
```bash
# Pythonä¾å­˜é–¢ä¿‚
pip install fastapi uvicorn requests numpy psutil pydantic

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: YAMLè¨­å®šã‚µãƒãƒ¼ãƒˆ
pip install pyyaml

# ONNX Runtime (DirectML)
pip install onnxruntime-directml
```

### **Step 3: GAIAã®è¨­å®šç¢ºèª**
```bash
# GAIAãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
gaia-cli --version

# Hybridãƒ¢ãƒ¼ãƒ‰ã§ã®å‹•ä½œç¢ºèª
gaia-cli --mode hybrid --model llama-7b --test
```

## ğŸš€ **Phase 1: ã‚µã‚¤ãƒ‰ã‚«ãƒ¼çµ±åˆï¼ˆæ¨å¥¨é–‹å§‹ç‚¹ï¼‰**

### **1.1 Infer-OS Control Agentã®èµ·å‹•**

#### **åŸºæœ¬èµ·å‹•**
```bash
python inferos_control_agent.py
```

#### **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãèµ·å‹•**
```bash
# config.yaml ã‚’ä½œæˆ
cat > config.yaml << EOF
infer_os:
  quality:
    max_delta_ppl: 0.5
    min_accept_rate: 0.5
  kv:
    recent_window: 64
    level_thresholds:
      L1_int8: 0.7
      L2_int4: 0.5
      L3_evict: 0.3
  io:
    enable_iobinding: true
    dml_pool_bytes: 2048MiB
    host_pool_bytes: 4096MiB
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
  loops:
    fast_ms: 1
    mid_ms: 10
    slow_ms: 100
EOF

python inferos_control_agent.py --config config.yaml
```

#### **Windowsã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦ç™»éŒ²**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã
python inferos_control_agent.py --install-service

# ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
net start InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
sc query InferOSAgent
```

### **1.2 å‹•ä½œç¢ºèª**

#### **APIå‹•ä½œç¢ºèª**
```bash
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://127.0.0.1:7031/v1/health

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
curl http://127.0.0.1:7031/v1/metrics

# ãƒãƒªã‚·ãƒ¼è¨­å®š
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"mode":"dynamic","recent_window":64},"io":{"enable_iobinding":true},"scheduler":{"mode":"hybrid"}}'
```

### **1.3 GAIAã¨ã®é€£æºè¨­å®š**

#### **ç’°å¢ƒå¤‰æ•°è¨­å®š**
```bash
# Infer-OSçµ±åˆã‚’æœ‰åŠ¹åŒ–
export INFEROS_ENABLED=true
export INFEROS_AGENT_URL=http://127.0.0.1:7031

# GAIAè¨­å®š
export GAIA_OPTIMIZATION_MODE=inferos
export GAIA_KV_QUANTIZATION=enabled
```

#### **GAIAå®Ÿè¡Œï¼ˆInfer-OSçµ±åˆï¼‰**
```bash
# åŸºæœ¬å®Ÿè¡Œ
gaia-cli --model llama-7b --optimization inferos

# è©³ç´°è¨­å®š
gaia-cli --model llama-7b \
  --optimization inferos \
  --kv-quantization enabled \
  --hybrid-mode \
  --batch-size 4 \
  --max-tokens 1024
```

## ğŸ”Œ **Phase 2: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³çµ±åˆï¼ˆé«˜åº¦ãªçµ±åˆï¼‰**

### **2.1 Lemonade Adapterã®çµ±åˆ**

#### **Lemonadeã‚µãƒ¼ãƒãƒ¼ã¸ã®çµ±åˆ**
```python
# lemonade_server.py ã¸ã®çµ±åˆä¾‹
from lemonade_adapter import LemonadeAdapter, InferenceContext

# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆæœŸåŒ–
adapter = LemonadeAdapter(
    agent_url="http://127.0.0.1:7031",
    enable_kv_quantization=True
)

# æ¨è«–å®Ÿè¡Œæ™‚ã®çµ±åˆ
def run_inference(model, input_text, **kwargs):
    context = InferenceContext(
        model_name=model.name,
        seq_len=len(input_text.split()),
        batch_size=kwargs.get('batch_size', 1),
        target_ftl_ms=kwargs.get('target_ftl_ms', 300),
        quality_budget=kwargs.get('quality_budget', 0.8)
    )
    
    with adapter.inference_session(context) as session:
        # PreRun hook - ãƒãƒªã‚·ãƒ¼é©ç”¨
        policy = session.prerun_hook(context)
        
        # ORTè¨­å®šã®é©ç”¨
        session.apply_ort_policy(session_options, run_options)
        
        # æ¨è«–å®Ÿè¡Œ
        result = model.generate(input_text, **kwargs)
        
        # PostRun hook - ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        stats = InferenceStats(
            start_time=start_time,
            end_time=time.time(),
            tokens_generated=len(result.split()),
            first_token_latency=first_token_time,
            total_latency=total_time,
            memory_peak=get_memory_usage(),
            quality_score=calculate_quality_score(result)
        )
        session.postrun_hook(stats)
    
    return result
```

### **2.2 KVé‡å­åŒ–ã®è©³ç´°è¨­å®š**

#### **é‡å­åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š**
```python
from kv_quantization_engine import KVQuantizationEngine

# é‡å­åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
engine = KVQuantizationEngine(
    recent_window=64,        # æœ€æ–°64ãƒˆãƒ¼ã‚¯ãƒ³ã¯FP16ä¿æŒ
    max_cache_size=10000,    # æœ€å¤§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
    quality_threshold=0.5    # å“è³ªé–¾å€¤
)

# å‹•çš„ãƒãƒªã‚·ãƒ¼æ›´æ–°
engine.update_quantization_policy(
    memory_pressure=0.7,     # ãƒ¡ãƒ¢ãƒªåœ§è¿«åº¦
    quality_budget=0.8,      # å“è³ªäºˆç®—
    recent_window=32         # å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
)

# çµ±è¨ˆæƒ…å ±ã®å–å¾—
stats = engine.get_cache_statistics()
print(f"åœ§ç¸®ç‡: {stats['compression_ratio']:.3f}")
print(f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {stats['total_memory_saved_mb']:.2f} MB")
```

## ğŸ“Š **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š**

### **3.1 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ**

#### **åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```bash
# Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
gaia-cli --model llama-7b --benchmark --optimization none

# Infer-OSæœ‰åŠ¹ã§ã®æ€§èƒ½æ¸¬å®š
gaia-cli --model llama-7b --benchmark --optimization inferos

# è©³ç´°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
python benchmark_inferos_gaia.py \
  --model llama-7b \
  --sequences 100 \
  --batch-sizes 1,4,8 \
  --seq-lengths 128,512,1024 \
  --compare-modes baseline,inferos
```

#### **ã‚«ã‚¹ã‚¿ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```python
import time
import requests
from lemonade_adapter import LemonadeAdapter

def benchmark_inferos():
    adapter = LemonadeAdapter()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        {"seq_len": 128, "batch": 1, "desc": "çŸ­æ–‡ãƒ»å˜ä¸€"},
        {"seq_len": 512, "batch": 4, "desc": "ä¸­æ–‡ãƒ»ãƒãƒƒãƒ"},
        {"seq_len": 1024, "batch": 1, "desc": "é•·æ–‡ãƒ»å˜ä¸€"}
    ]
    
    results = []
    for case in test_cases:
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆInfer-OSç„¡åŠ¹ï¼‰
        baseline_time = run_inference_baseline(case)
        
        # Infer-OSæœ‰åŠ¹
        inferos_time = run_inference_with_inferos(case)
        
        improvement = baseline_time / inferos_time
        results.append({
            "case": case["desc"],
            "baseline_ms": baseline_time * 1000,
            "inferos_ms": inferos_time * 1000,
            "improvement": improvement
        })
    
    return results
```

### **3.2 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–**

#### **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**
```bash
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
cat > monitor_inferos.py << 'EOF'
import requests
import time
import json

def monitor_metrics():
    while True:
        try:
            response = requests.get("http://127.0.0.1:7031/v1/metrics")
            metrics = response.json()
            
            print(f"TPS: {metrics['tps']:.1f}, "
                  f"FTL: {metrics['ftl_ms']:.1f}ms, "
                  f"VRAM: {metrics['mem']['vram_gb']:.1f}GB, "
                  f"NPU: {metrics['util']['npu']:.1%}, "
                  f"iGPU: {metrics['util']['igpu']:.1%}")
            
        except Exception as e:
            print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
        
        time.sleep(1)

if __name__ == "__main__":
    monitor_metrics()
EOF

python monitor_inferos.py
```

#### **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º**
```bash
# Grafana/Prometheusé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’Prometheuså½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
curl http://127.0.0.1:7031/v1/metrics/prometheus
```

## ğŸ”§ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**

### **4.1 ä¸€èˆ¬çš„ãªå•é¡Œ**

#### **NPU/iGPUãŒèªè­˜ã•ã‚Œãªã„**
```bash
# ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
python -c "
import onnxruntime as ort
print('åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:', ort.get_available_providers())
"

# DirectMLç¢ºèª
python -c "
import onnxruntime as ort
providers = ort.get_available_providers()
if 'DmlExecutionProvider' in providers:
    print('âœ… DirectMLåˆ©ç”¨å¯èƒ½')
else:
    print('âŒ DirectMLåˆ©ç”¨ä¸å¯ - ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèªãŒå¿…è¦')
"
```

#### **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
curl http://127.0.0.1:7031/v1/metrics | jq '.mem'

# é‡å­åŒ–è¨­å®šã®èª¿æ•´
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"recent_window":32,"level_thresholds":{"L1_int8":0.6,"L2_int4":0.4,"L3_evict":0.2}}}'
```

#### **å“è³ªåŠ£åŒ–ã®å¯¾å‡¦**
```bash
# å“è³ªé‡è¦–è¨­å®š
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"recent_window":128,"level_thresholds":{"L1_int8":0.8,"L2_int4":0.6,"L3_evict":0.4}}}'

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆ
curl -X POST http://127.0.0.1:7031/v1/baseline -d '{"enable": true}'
```

### **4.2 ãƒ­ã‚°ã¨ãƒ‡ãƒãƒƒã‚°**

#### **è©³ç´°ãƒ­ã‚°ã®æœ‰åŠ¹åŒ–**
```python
# inferos_control_agent.py ã®è¨­å®š
import logging
logging.basicConfig(level=logging.DEBUG)

# ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
export INFEROS_LOG_LEVEL=DEBUG
python inferos_control_agent.py
```

#### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã®ç¢ºèª**
```bash
# å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
curl http://127.0.0.1:7031/v1/history | jq '.history[-10:]'

# çµ±è¨ˆæƒ…å ±ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
curl http://127.0.0.1:7031/v1/metrics > metrics_$(date +%Y%m%d_%H%M%S).json
```

## ğŸ¯ **æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**

### **5.1 ãƒ¢ãƒ‡ãƒ«åˆ¥æ¨å¥¨è¨­å®š**

#### **7B/8Bãƒ¢ãƒ‡ãƒ«ï¼ˆä¸­è»½é‡ï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 64
    level_thresholds:
      L1_int8: 0.7
      L2_int4: 0.5
      L3_evict: 0.3
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
```

#### **13B/20Bãƒ¢ãƒ‡ãƒ«ï¼ˆé‡é‡ç´šï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 32
    level_thresholds:
      L1_int8: 0.6
      L2_int4: 0.4
      L3_evict: 0.2
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
```

#### **3Bä»¥ä¸‹ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 128
    level_thresholds:
      L1_int8: 0.8
      L2_int4: 0.6
      L3_evict: 0.4
  scheduler:
    mode: gpu_only  # NPUã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’é¿ã‘ã‚‹
```

### **5.2 ç”¨é€”åˆ¥æœ€é©åŒ–**

#### **ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼ˆä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 128, "mode": "latency_optimized"},
    "scheduler": {"mode": "hybrid", "decode_device": "npu"},
    "quality": {"max_delta_ppl": 0.3}
  }'
```

#### **ãƒãƒƒãƒå‡¦ç†ï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 32, "mode": "throughput_optimized"},
    "scheduler": {"mode": "hybrid", "prefill_device": "dml"},
    "quality": {"max_delta_ppl": 0.5}
  }'
```

#### **é«˜å“è³ªç”Ÿæˆï¼ˆå“è³ªé‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 256, "mode": "quality_optimized"},
    "scheduler": {"mode": "gpu_only"},
    "quality": {"max_delta_ppl": 0.1}
  }'
```

## ğŸ“ˆ **ç¶™ç¶šçš„ãªæœ€é©åŒ–**

### **6.1 è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**

#### **é©å¿œçš„è¨­å®šèª¿æ•´**
```python
# auto_tuner.py
import requests
import time
import numpy as np

class InferOSAutoTuner:
    def __init__(self, agent_url="http://127.0.0.1:7031"):
        self.agent_url = agent_url
        self.performance_history = []
    
    def tune_for_workload(self, target_tps=None, target_quality=None):
        """ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«åŸºã¥ãè‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        current_metrics = self.get_metrics()
        
        if target_tps and current_metrics['tps'] < target_tps:
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ãŸã‚ã®èª¿æ•´
            self.adjust_for_throughput()
        
        if target_quality and current_metrics['quality']['delta_ppl_est'] > target_quality:
            # å“è³ªå‘ä¸Šã®ãŸã‚ã®èª¿æ•´
            self.adjust_for_quality()
    
    def adjust_for_throughput(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šèª¿æ•´"""
        policy = {
            "kv": {"recent_window": 32, "level_thresholds": {"L1_int8": 0.6, "L2_int4": 0.4, "L3_evict": 0.2}},
            "scheduler": {"mode": "hybrid"}
        }
        self.apply_policy(policy)
    
    def adjust_for_quality(self):
        """å“è³ªå‘ä¸Šèª¿æ•´"""
        policy = {
            "kv": {"recent_window": 128, "level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.6, "L3_evict": 0.4}},
            "scheduler": {"mode": "hybrid"}
        }
        self.apply_policy(policy)

# ä½¿ç”¨ä¾‹
tuner = InferOSAutoTuner()
tuner.tune_for_workload(target_tps=30.0, target_quality=0.3)
```

### **6.2 A/Bãƒ†ã‚¹ãƒˆ**

#### **è¨­å®šæ¯”è¼ƒãƒ†ã‚¹ãƒˆ**
```bash
# A/Bãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
cat > ab_test_inferos.py << 'EOF'
import requests
import time
import statistics

def run_ab_test():
    configs = [
        {"name": "Conservative", "kv": {"recent_window": 128}},
        {"name": "Balanced", "kv": {"recent_window": 64}},
        {"name": "Aggressive", "kv": {"recent_window": 32}}
    ]
    
    results = {}
    for config in configs:
        print(f"Testing {config['name']} configuration...")
        
        # è¨­å®šé©ç”¨
        requests.post("http://127.0.0.1:7031/v1/policy", json=config)
        time.sleep(2)  # è¨­å®šåæ˜ å¾…ã¡
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        metrics = []
        for _ in range(10):
            response = requests.get("http://127.0.0.1:7031/v1/metrics")
            metrics.append(response.json())
            time.sleep(1)
        
        # çµ±è¨ˆè¨ˆç®—
        tps_values = [m['tps'] for m in metrics]
        results[config['name']] = {
            "avg_tps": statistics.mean(tps_values),
            "std_tps": statistics.stdev(tps_values),
            "avg_quality": statistics.mean([m['quality']['delta_ppl_est'] for m in metrics])
        }
    
    return results

if __name__ == "__main__":
    results = run_ab_test()
    for name, stats in results.items():
        print(f"{name}: TPS={stats['avg_tps']:.1f}Â±{stats['std_tps']:.1f}, Quality={stats['avg_quality']:.3f}")
EOF

python ab_test_inferos.py
```

## ğŸ‰ **æˆåŠŸäº‹ä¾‹ã¨æœŸå¾…ã•ã‚Œã‚‹çµæœ**

### **7.1 å…¸å‹çš„ãªæ”¹å–„ä¾‹**

#### **Llama-7B ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ**
```
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:
- TPS: 18.5 tokens/sec
- FTL: 320ms
- ãƒ¡ãƒ¢ãƒª: 14.2GB
- å“è³ª: PPL 12.3

Infer-OSçµ±åˆå¾Œ:
- TPS: 26.8 tokens/sec (+45%)
- FTL: 245ms (-23%)
- ãƒ¡ãƒ¢ãƒª: 8.9GB (-37%)
- å“è³ª: PPL 12.7 (Î”PPL +0.4)

ROI: 45%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š + 37%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
```

#### **Llama-13B ãƒãƒƒãƒå‡¦ç†**
```
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:
- TPS: 12.3 tokens/sec
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4
- ãƒ¡ãƒ¢ãƒª: 26.1GB
- å“è³ª: PPL 11.8

Infer-OSçµ±åˆå¾Œ:
- TPS: 19.7 tokens/sec (+60%)
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 8 (2å€)
- ãƒ¡ãƒ¢ãƒª: 16.4GB (-37%)
- å“è³ª: PPL 12.2 (Î”PPL +0.4)

ROI: 60%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š + ãƒãƒƒãƒã‚µã‚¤ã‚º2å€åŒ–
```

### **7.2 å°å…¥åŠ¹æœã®æ¸¬å®š**

#### **KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**
```python
# kpi_dashboard.py
import streamlit as st
import requests
import plotly.graph_objects as go
from datetime import datetime, timedelta

def create_dashboard():
    st.title("Infer-OS Ã— GAIA ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
    metrics = requests.get("http://127.0.0.1:7031/v1/metrics").json()
    history = requests.get("http://127.0.0.1:7031/v1/history").json()
    
    # KPIè¡¨ç¤º
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("TPS", f"{metrics['tps']:.1f}", delta=f"+{metrics['tps']/18.5-1:.1%}")
    with col2:
        st.metric("FTL (ms)", f"{metrics['ftl_ms']:.0f}", delta=f"{(metrics['ftl_ms']/320-1)*100:.1f}%")
    with col3:
        st.metric("ãƒ¡ãƒ¢ãƒª (GB)", f"{metrics['mem']['vram_gb']:.1f}", delta=f"{(metrics['mem']['vram_gb']/14.2-1)*100:.1f}%")
    with col4:
        st.metric("å“è³ª (Î”PPL)", f"{metrics['quality']['delta_ppl_est']:.3f}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚°ãƒ©ãƒ•
    if history['history']:
        timestamps = [h['timestamp'] for h in history['history']]
        tps_values = [h['tps'] for h in history['history']]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=timestamps, y=tps_values, name='TPS'))
        fig.update_layout(title='ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå±¥æ­´', xaxis_title='æ™‚åˆ»', yaxis_title='TPS')
        st.plotly_chart(fig)

if __name__ == "__main__":
    create_dashboard()
```

## ğŸ”® **ä»Šå¾Œã®ç™ºå±•**

### **8.1 Phase 3: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢é€£æº**

#### **NPUå¸¸é§æœ€é©åŒ–**
```python
# å°†æ¥å®Ÿè£…äºˆå®š
class NPUResidentOptimizer:
    """NPUå¸¸é§KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–"""
    
    def __init__(self):
        self.npu_cache_manager = NPUCacheManager()
        self.igpu_spillover = iGPUSpilloverManager()
    
    def optimize_kv_placement(self, kv_chunks):
        """KVãƒãƒ£ãƒ³ã‚¯ã®æœ€é©é…ç½®"""
        # é‡è¦åº¦ã®é«˜ã„KVã¯NPUã«å¸¸é§
        # ä½é‡è¦åº¦ã¯iGPUã«ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼
        pass
```

#### **å‹•çš„è² è·åˆ†æ•£**
```python
# å°†æ¥å®Ÿè£…äºˆå®š
class HybridLoadBalancer:
    """NPU+iGPUå‹•çš„è² è·åˆ†æ•£"""
    
    def balance_workload(self, prefill_load, decode_load):
        """ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«åŸºã¥ãå‹•çš„åˆ†æ•£"""
        # Prefill: iGPUå„ªå…ˆã€é«˜è² è·æ™‚ã¯NPUã‚‚æ´»ç”¨
        # Decode: NPUå„ªå…ˆã€ç†±åˆ¶ç´„æ™‚ã¯iGPUã«ç§»è¡Œ
        pass
```

### **8.2 ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ**

#### **ä»–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ**
- **Ollamaçµ±åˆ**: Ollama + Infer-OSé€£æº
- **LangChainçµ±åˆ**: LangChain Agent + Infer-OSæœ€é©åŒ–
- **Transformersçµ±åˆ**: HuggingFace Transformers + Infer-OS

#### **ã‚¯ãƒ©ã‚¦ãƒ‰å±•é–‹**
- **Azureçµ±åˆ**: Azure ML + Infer-OS
- **AWSçµ±åˆ**: SageMaker + Infer-OS
- **ã‚³ãƒ³ãƒ†ãƒŠåŒ–**: Docker + Kuberneteså¯¾å¿œ

## ğŸ“ **ã‚µãƒãƒ¼ãƒˆ**

### **9.1 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**
- **GitHub**: https://github.com/kojima123/infer-os
- **Issues**: ãƒã‚°å ±å‘Šãƒ»æ©Ÿèƒ½è¦æœ›
- **Discussions**: æŠ€è¡“è¨è«–ãƒ»è³ªå•

### **9.2 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**
- **ãƒ­ã‚°ç¢ºèª**: `tail -f inferos_agent.log`
- **è¨­å®šãƒªã‚»ãƒƒãƒˆ**: `curl -X POST http://127.0.0.1:7031/v1/baseline`
- **å¼·åˆ¶å†èµ·å‹•**: `pkill -f inferos_control_agent && python inferos_control_agent.py`

### **9.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›¸è«‡**
ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®æœ€é©åŒ–ç›¸è«‡ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã¨å…±ã«Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãƒ»ç¨®é¡
- å…¸å‹çš„ãªå…¥åŠ›é•·ãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚º
- ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
- ç›®æ¨™ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆ

---

**ğŸ¯ GAIA Ã— Infer-OSçµ±åˆã«ã‚ˆã‚Šã€AMD Ryzen AI NPU + Radeon iGPUç’°å¢ƒã§ã®LLMæ¨è«–æ€§èƒ½ã‚’æœ€å¤§åŒ–ã—ã€æ¬¡ä¸–ä»£AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ï¼**

