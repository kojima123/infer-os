# GAIA Ã— Infer-OS çµ±åˆã‚¬ã‚¤ãƒ‰ v1.0

## ğŸ¯ **æ¦‚è¦**

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€AMD GAIAç’°å¢ƒï¼ˆWindows 11 + Ryzen AI NPU + Radeon iGPUï¼‰ã«Infer-OSã‚’çµ±åˆã—ã€LLMæ¨è«–ã®å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

### **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**
- **7B/8Bãƒ¢ãƒ‡ãƒ«**: 1.3-1.6å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€60-75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **20Bãƒ¢ãƒ‡ãƒ«**: 1.8å€ä»¥ä¸Šã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€70-80%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥**: æœ€å¤§75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **å“è³ªä¿æŒ**: Î”PPL â‰¤ 0.5ã§ã®é«˜å“è³ªç¶­æŒ

## ğŸ”§ **å‰ææ¡ä»¶**

### **ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶**
- **OS**: Windows 11ï¼ˆGAIAè¦ä»¶ï¼‰
- **CPU**: AMD Ryzen AI 300ã‚·ãƒªãƒ¼ã‚ºï¼ˆXDNA NPUæ­è¼‰ï¼‰
- **GPU**: Radeon iGPUï¼ˆRDNAã€DirectMLå¯¾å¿œï¼‰
- **ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Šæ¨å¥¨
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 10GBä»¥ä¸Šã®ç©ºãå®¹é‡

### **ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶**
- **GAIA**: æœ€æ–°ç‰ˆï¼ˆRAUX/gaia-cliå¯¾å¿œï¼‰
- **Python**: 3.11ä»¥ä¸Š
- **ONNX Runtime**: DirectMLå¯¾å¿œç‰ˆ
- **NPU/iGPUãƒ‰ãƒ©ã‚¤ãƒ**: GAIAæ¨å¥¨ç‰ˆ

## ğŸ“¦ **ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †**

### **Step 1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³**
```bash
git clone https://github.com/kojima123/infer-os.git
cd infer-os
```

### **Step 2: ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
```bash
# Pythonä¾å­˜é–¢ä¿‚
pip install fastapi uvicorn requests numpy psutil pydantic

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: YAMLè¨­å®šã‚µãƒãƒ¼ãƒˆ
pip install pyyaml

# ONNX Runtime (DirectML)
pip install onnxruntime-directml
```

### **Step 3: GAIAã®è¨­å®šç¢ºèª**
```bash
# GAIAãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
gaia-cli --version

# Hybridãƒ¢ãƒ¼ãƒ‰ã§ã®å‹•ä½œç¢ºèª
gaia-cli --mode hybrid --model llama-7b --test
```

## ğŸ“‹ **CLIãƒãƒƒãƒ”ãƒ³ã‚°è¡¨**

### **GAIA CLI ãƒ•ãƒ©ã‚°å¯¾å¿œè¡¨**

| æ¦‚å¿µ | å…¬å¼CLIãƒ•ãƒ©ã‚° | ä½¿ç”¨ä¾‹ | å‚™è€ƒ |
|------|---------------|--------|------|
| æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰: Infer-OS | `--optimization inferos` | `gaia-cli --model llama-7b --optimization inferos` | Infer-OSçµ±åˆã‚’æœ‰åŠ¹åŒ– |
| ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œ | `--mode hybrid` | `gaia-cli --mode hybrid` | NPU+iGPUå”èª¿å‡¦ç† |
| KVé‡å­åŒ–ON | `--kv-quantization enabled` | `gaia-cli --kv-quantization enabled` | KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ– |
| ãƒãƒƒãƒã‚µã‚¤ã‚º | `--batch-size <N>` | `--batch-size 4` | åŒæ™‚å‡¦ç†æ•° |
| æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ | `--max-tokens <N>` | `--max-tokens 1024` | ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ |
| å“è³ªé–¾å€¤ | `--quality-threshold <F>` | `--quality-threshold 0.5` | Î”PPLè¨±å®¹å€¤ |
| ãƒ¡ãƒ¢ãƒªåˆ¶é™ | `--memory-limit <SIZE>` | `--memory-limit 16GB` | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ä¸Šé™ |
| ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ | `--debug` | `gaia-cli --debug` | è©³ç´°ãƒ­ã‚°å‡ºåŠ› |

> **æ³¨æ„**: å®Ÿéš›ã®CLIãƒ•ãƒ©ã‚°åã¯ GAIA ã®å®Ÿè£…ã«ä¾å­˜ã—ã¾ã™ã€‚ä¸Šè¨˜ã¯è¨­è¨ˆä»•æ§˜ã§ã‚ã‚Šã€å®Ÿè£…æ™‚ã«èª¿æ•´ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚

### **å‹•ä½œç¢ºèªã‚³ãƒãƒ³ãƒ‰**
```bash
# GAIAåŸºæœ¬å‹•ä½œç¢ºèª
gaia-cli --version
gaia-cli --help | grep -E "(optimization|mode|kv-quantization)"

# Infer-OSçµ±åˆãƒ†ã‚¹ãƒˆ
gaia-cli --model llama-7b --optimization inferos --test --dry-run
```

## ğŸ”Œ **APIä»•æ§˜ï¼ˆOpenAPIï¼‰**

### **Infer-OS Control Agent API v1.0**

#### **ãƒ™ãƒ¼ã‚¹URL**: `http://127.0.0.1:7031/v1`

#### **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§**

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ | èªè¨¼ |
|---------------|----------|------|------|
| `/health` | GET | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | ä¸è¦ |
| `/metrics` | GET | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ä¸è¦ |
| `/policy` | GET/POST | æœ€é©åŒ–ãƒãƒªã‚·ãƒ¼å–å¾—/è¨­å®š | ä¸è¦ |
| `/baseline` | POST | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ | ä¸è¦ |
| `/history` | GET | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ | ä¸è¦ |
| `/config` | GET/POST | è¨­å®šå–å¾—/æ›´æ–° | ä¸è¦ |

#### **API ã‚¹ã‚­ãƒ¼ãƒå®šç¾©**

##### **POST /v1/policy**
```json
{
  "type": "object",
  "properties": {
    "kv": {
      "type": "object",
      "properties": {
        "mode": {
          "type": "string",
          "enum": ["dynamic", "latency_optimized", "throughput_optimized", "quality_optimized"],
          "default": "dynamic"
        },
        "recent_window": {
          "type": "integer",
          "minimum": 16,
          "maximum": 512,
          "default": 64
        },
        "level_thresholds": {
          "type": "object",
          "properties": {
            "L1_int8": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.7},
            "L2_int4": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.5},
            "L3_evict": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.3}
          }
        }
      }
    },
    "io": {
      "type": "object",
      "properties": {
        "enable_iobinding": {"type": "boolean", "default": true},
        "dml_pool_bytes": {"type": "integer", "minimum": 1073741824, "default": 2147483648},
        "host_pool_bytes": {"type": "integer", "minimum": 1073741824, "default": 4294967296}
      }
    },
    "scheduler": {
      "type": "object",
      "properties": {
        "mode": {
          "type": "string",
          "enum": ["hybrid", "gpu_only", "npu_only"],
          "default": "hybrid"
        },
        "prefill_device": {
          "type": "string",
          "enum": ["dml", "npu", "cpu"],
          "default": "dml"
        },
        "decode_device": {
          "type": "string",
          "enum": ["npu", "dml", "cpu"],
          "default": "npu"
        }
      }
    },
    "quality": {
      "type": "object",
      "properties": {
        "max_delta_ppl": {"type": "number", "minimum": 0.0, "maximum": 2.0, "default": 0.5},
        "min_accept_rate": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.5}
      }
    }
  }
}
```

##### **GET /v1/metrics ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹**
```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "tps": 26.8,
  "ftl_ms": 245,
  "mem": {
    "vram_gb": 8.9,
    "system_gb": 16.2,
    "kv_cache_gb": 2.1
  },
  "util": {
    "npu": 0.75,
    "igpu": 0.45,
    "cpu": 0.32
  },
  "quality": {
    "delta_ppl_est": 0.42,
    "accept_rate": 0.87
  },
  "kv_stats": {
    "total_chunks": 1250,
    "level_distribution": {
      "L0": 320,
      "L1": 450,
      "L2": 380,
      "L3": 100
    },
    "compression_ratio": 0.35,
    "hit_rate": 0.92
  }
}
```

##### **ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹**
```json
{
  "error": {
    "code": 400,
    "message": "Invalid policy configuration",
    "details": "recent_window must be between 16 and 512"
  }
}
```

## ğŸš€ **Phase 1: ã‚µã‚¤ãƒ‰ã‚«ãƒ¼çµ±åˆï¼ˆæ¨å¥¨é–‹å§‹ç‚¹ï¼‰**

### **1.1 Infer-OS Control Agentã®èµ·å‹•**

#### **åŸºæœ¬èµ·å‹•**
```bash
python inferos_control_agent.py
```

#### **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãèµ·å‹•**
```bash
# config.yaml ã‚’ä½œæˆ
cat > config.yaml << EOF
infer_os:
  quality:
    max_delta_ppl: 0.5
    min_accept_rate: 0.5
  kv:
    recent_window: 64
    level_thresholds:
      L1_int8: 0.7
      L2_int4: 0.5
      L3_evict: 0.3
  io:
    enable_iobinding: true
    dml_pool_bytes: 2048MiB
    host_pool_bytes: 4096MiB
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
  loops:
    fast_ms: 1
    mid_ms: 10
    slow_ms: 100
EOF

python inferos_control_agent.py --config config.yaml
```

#### **Windowsã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦ç™»éŒ²ï¼ˆæ¨å¥¨ï¼‰**

##### **æ–¹æ³•1: NSSM (Non-Sucking Service Manager) ä½¿ç”¨**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã

# NSSMã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://nssm.cc/download ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# nssm.exe ã‚’ C:\Windows\System32 ã«ã‚³ãƒ”ãƒ¼

# ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
nssm install InferOSAgent

# NSSM GUIãŒé–‹ãã®ã§ä»¥ä¸‹ã‚’è¨­å®š:
# Application tab:
#   Path: C:\Python311\python.exe
#   Startup directory: C:\path\to\infer-os
#   Arguments: inferos_control_agent.py --config config.yaml

# Details tab:
#   Display name: Infer-OS Control Agent
#   Description: AI inference optimization service for GAIA integration

# Log on tab:
#   This account: .\InferOSService (å°‚ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨å¥¨)

# I/O tab:
#   Output (stdout): C:\ProgramData\InferOS\logs\stdout.log
#   Error (stderr): C:\ProgramData\InferOS\logs\stderr.log

# Rotation tab:
#   Replace existing Output/Error files: ãƒã‚§ãƒƒã‚¯
#   Rotate files: ãƒã‚§ãƒƒã‚¯
#   Restrict rotation to files older than: 7 days

# Install service ã‚’ã‚¯ãƒªãƒƒã‚¯
```

##### **æ–¹æ³•2: sc create ã‚³ãƒãƒ³ãƒ‰ä½¿ç”¨**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir C:\ProgramData\InferOS\logs

# ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
sc create InferOSAgent ^
  binPath= "C:\Python311\python.exe C:\path\to\infer-os\inferos_control_agent.py --service" ^
  DisplayName= "Infer-OS Control Agent" ^
  Description= "AI inference optimization service for GAIA integration" ^
  start= auto ^
  obj= ".\InferOSService" ^
  password= "YourServicePassword"

# ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
sc config InferOSAgent depend= "Tcpip/Afd"
sc failure InferOSAgent reset= 86400 actions= restart/5000/restart/10000/restart/30000

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š
netsh advfirewall firewall add rule name="InferOS Agent" dir=in action=allow protocol=TCP localport=7031
```

##### **æ–¹æ³•3: ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ä½¿ç”¨**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã

# ã‚¿ã‚¹ã‚¯ä½œæˆ
schtasks /create /tn "InferOS Agent" /tr "C:\Python311\python.exe C:\path\to\infer-os\inferos_control_agent.py" /sc onstart /ru "SYSTEM" /rl highest

# ã‚¿ã‚¹ã‚¯è¨­å®šã®è©³ç´°åŒ–
schtasks /change /tn "InferOS Agent" /st 00:00 /ri 1 /du 9999:59

# ã‚¿ã‚¹ã‚¯é–‹å§‹
schtasks /run /tn "InferOS Agent"
```

#### **ã‚µãƒ¼ãƒ“ã‚¹é‹ç”¨ç®¡ç†**

##### **ã‚µãƒ¼ãƒ“ã‚¹åˆ¶å¾¡ã‚³ãƒãƒ³ãƒ‰**
```cmd
# ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
net start InferOSAgent
# ã¾ãŸã¯
sc start InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
net stop InferOSAgent
# ã¾ãŸã¯
sc stop InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
sc query InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šç¢ºèª
sc qc InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹å‰Šé™¤ï¼ˆã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ï¼‰
sc delete InferOSAgent
```

##### **ãƒ­ã‚°ç®¡ç†è¨­å®š**
```cmd
# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ
C:\ProgramData\InferOS\
â”œâ”€â”€ logs\
â”‚   â”œâ”€â”€ stdout.log          # æ¨™æº–å‡ºåŠ›
â”‚   â”œâ”€â”€ stderr.log          # ã‚¨ãƒ©ãƒ¼å‡ºåŠ›
â”‚   â”œâ”€â”€ inferos_agent.log   # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
â”‚   â””â”€â”€ archived\           # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ­ã‚°
â”œâ”€â”€ config\
â”‚   â”œâ”€â”€ config.yaml         # ãƒ¡ã‚¤ãƒ³è¨­å®š
â”‚   â””â”€â”€ policy.json         # ãƒãƒªã‚·ãƒ¼è¨­å®š
â””â”€â”€ temp\
    â””â”€â”€ onnx_cache\         # ONNXå¤‰æ›ã‚­ãƒ£ãƒƒã‚·ãƒ¥

# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆPowerShellï¼‰
$logConfig = @"
<configuration>
  <appender name="RollingFile" type="log4net.Appender.RollingFileAppender">
    <file value="C:\ProgramData\InferOS\logs\inferos_agent.log" />
    <appendToFile value="true" />
    <rollingStyle value="Size" />
    <maxSizeRollBackups value="10" />
    <maximumFileSize value="10MB" />
    <staticLogFileName value="true" />
    <layout type="log4net.Layout.PatternLayout">
      <conversionPattern value="%date [%thread] %-5level %logger - %message%newline" />
    </layout>
  </appender>
</configuration>
"@

$logConfig | Out-File -FilePath "C:\ProgramData\InferOS\config\log4net.config" -Encoding UTF8
```

#### **æ¨©é™ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š**

##### **å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ**
```cmd
# å°‚ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
net user InferOSService "ComplexPassword123!" /add /comment:"Infer-OS Service Account"

# å¿…è¦ãªæ¨©é™ä»˜ä¸
ntrights +r SeServiceLogonRight -u InferOSService
ntrights +r SeIncreaseQuotaPrivilege -u InferOSService
ntrights +r SeAssignPrimaryTokenPrivilege -u InferOSService

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®š
icacls "C:\ProgramData\InferOS" /grant InferOSService:(OI)(CI)F
icacls "C:\path\to\infer-os" /grant InferOSService:(OI)(CI)RX
```

##### **ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š**
```cmd
# Infer-OS Agent API ãƒãƒ¼ãƒˆè¨±å¯
netsh advfirewall firewall add rule name="InferOS Agent API" dir=in action=allow protocol=TCP localport=7031 profile=private

# ç‰¹å®šIPã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã®ã¿è¨±å¯ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
netsh advfirewall firewall add rule name="InferOS Agent API Restricted" dir=in action=allow protocol=TCP localport=7031 remoteip=127.0.0.1,192.168.1.0/24

# ãƒ­ã‚°æœ‰åŠ¹åŒ–
netsh advfirewall set allprofiles logging filename C:\ProgramData\InferOS\logs\firewall.log
netsh advfirewall set allprofiles logging maxfilesize 4096
netsh advfirewall set allprofiles logging droppedconnections enable
```

#### **ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š**

##### **Windows ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°çµ±åˆ**
```python
# inferos_control_agent.py ã§ã®å®Ÿè£…ä¾‹
import logging
import logging.handlers

def setup_windows_event_logging():
    """Windowsã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°è¨­å®š"""
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        event_handler = logging.handlers.NTEventLogHandler(
            appname="InferOS Agent",
            dllname=None,
            logtype="Application"
        )
        event_handler.setLevel(logging.WARNING)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        event_handler.setFormatter(formatter)
        
        # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã«è¿½åŠ 
        logging.getLogger().addHandler(event_handler)
        
    except Exception as e:
        print(f"ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
```

##### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ç™»éŒ²**
```cmd
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ä½œæˆ
lodctr /R

# ã‚«ã‚¹ã‚¿ãƒ ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
echo [info] > inferos_counters.ini
echo drivername=InferOS Agent >> inferos_counters.ini
echo symbolfile=inferos_counters.h >> inferos_counters.ini
echo [objects] >> inferos_counters.ini
echo INFEROS_OBJECT_1_009_NAME=InferOS Performance >> inferos_counters.ini

# ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ç™»éŒ²
lodctr inferos_counters.ini
```

#### **ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †**

##### **å®Œå…¨å‰Šé™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```cmd
@echo off
echo Infer-OS Agent ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...

# ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
net stop InferOSAgent 2>nul
sc delete InferOSAgent 2>nul

# ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‹ã‚‰å‰Šé™¤
schtasks /delete /tn "InferOS Agent" /f 2>nul

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ãƒ«ãƒ¼ãƒ«å‰Šé™¤
netsh advfirewall firewall delete rule name="InferOS Agent API" 2>nul
netsh advfirewall firewall delete rule name="InferOS Agent API Restricted" 2>nul

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå‰Šé™¤
net user InferOSService /delete 2>nul

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ï¼ˆãƒ‡ãƒ¼ã‚¿ä¿æŒç¢ºèªï¼‰
set /p KEEP_DATA="ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¾ã™ã‹ï¼Ÿ (Y/N): "
if /i "%KEEP_DATA%"=="N" (
    rmdir /s /q "C:\ProgramData\InferOS" 2>nul
)

# ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
reg delete "HKLM\SYSTEM\CurrentControlSet\Services\InferOSAgent" /f 2>nul
reg delete "HKLM\SOFTWARE\InferOS" /f 2>nul

echo ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†
pause
```

### **1.2 å‹•ä½œç¢ºèª**

#### **APIå‹•ä½œç¢ºèª**
```bash
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://127.0.0.1:7031/v1/health

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
curl http://127.0.0.1:7031/v1/metrics

# ãƒãƒªã‚·ãƒ¼è¨­å®š
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"mode":"dynamic","recent_window":64},"io":{"enable_iobinding":true},"scheduler":{"mode":"hybrid"}}'
```

### **1.3 GAIAã¨ã®é€£æºè¨­å®š**

#### **ç’°å¢ƒå¤‰æ•°è¨­å®š**
```bash
# Infer-OSçµ±åˆã‚’æœ‰åŠ¹åŒ–
export INFEROS_ENABLED=true
export INFEROS_AGENT_URL=http://127.0.0.1:7031

# GAIAè¨­å®š
export GAIA_OPTIMIZATION_MODE=inferos
export GAIA_KV_QUANTIZATION=enabled
```

#### **GAIAå®Ÿè¡Œï¼ˆInfer-OSçµ±åˆï¼‰**
```bash
# åŸºæœ¬å®Ÿè¡Œ
gaia-cli --model llama-7b --optimization inferos

# è©³ç´°è¨­å®š
gaia-cli --model llama-7b \
  --optimization inferos \
  --kv-quantization enabled \
  --hybrid-mode \
  --batch-size 4 \
  --max-tokens 1024
```

## ğŸ”Œ **Phase 2: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³çµ±åˆï¼ˆé«˜åº¦ãªçµ±åˆï¼‰**

### **2.1 Lemonade Adapterã®çµ±åˆ**

#### **Lemonadeã‚µãƒ¼ãƒãƒ¼ã¸ã®çµ±åˆ**
```python
# lemonade_server.py ã¸ã®çµ±åˆä¾‹
from lemonade_adapter import LemonadeAdapter, InferenceContext

# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆæœŸåŒ–
adapter = LemonadeAdapter(
    agent_url="http://127.0.0.1:7031",
    enable_kv_quantization=True
)

# æ¨è«–å®Ÿè¡Œæ™‚ã®çµ±åˆ
def run_inference(model, input_text, **kwargs):
    context = InferenceContext(
        model_name=model.name,
        seq_len=len(input_text.split()),
        batch_size=kwargs.get('batch_size', 1),
        target_ftl_ms=kwargs.get('target_ftl_ms', 300),
        quality_budget=kwargs.get('quality_budget', 0.8)
    )
    
    with adapter.inference_session(context) as session:
        # PreRun hook - ãƒãƒªã‚·ãƒ¼é©ç”¨
        policy = session.prerun_hook(context)
        
        # ORTè¨­å®šã®é©ç”¨
        session.apply_ort_policy(session_options, run_options)
        
        # æ¨è«–å®Ÿè¡Œ
        result = model.generate(input_text, **kwargs)
        
        # PostRun hook - ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        stats = InferenceStats(
            start_time=start_time,
            end_time=time.time(),
            tokens_generated=len(result.split()),
            first_token_latency=first_token_time,
            total_latency=total_time,
            memory_peak=get_memory_usage(),
            quality_score=calculate_quality_score(result)
        )
        session.postrun_hook(stats)
    
    return result
```

### **2.2 KVé‡å­åŒ–ã®è©³ç´°è¨­å®š**

#### **é‡å­åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š**
```python
from kv_quantization_engine import KVQuantizationEngine

# é‡å­åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
engine = KVQuantizationEngine(
    recent_window=64,        # æœ€æ–°64ãƒˆãƒ¼ã‚¯ãƒ³ã¯FP16ä¿æŒ
    max_cache_size=10000,    # æœ€å¤§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
    quality_threshold=0.5    # å“è³ªé–¾å€¤
)

# å‹•çš„ãƒãƒªã‚·ãƒ¼æ›´æ–°
engine.update_quantization_policy(
    memory_pressure=0.7,     # ãƒ¡ãƒ¢ãƒªåœ§è¿«åº¦
    quality_budget=0.8,      # å“è³ªäºˆç®—
    recent_window=32         # å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
)

# çµ±è¨ˆæƒ…å ±ã®å–å¾—
stats = engine.get_cache_statistics()
print(f"åœ§ç¸®ç‡: {stats['compression_ratio']:.3f}")
print(f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {stats['total_memory_saved_mb']:.2f} MB")
```

## ğŸ“Š **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š**

### **3.1 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å†ç¾æ€§**

#### **å›ºå®šæ¸¬å®šæ¡ä»¶**

##### **ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆå³é¸2ç¨®ï¼‰**
| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ç”¨é€” | æ¨å¥¨ãƒ¡ãƒ¢ãƒª |
|--------|-------------|------|-----------|
| `microsoft/DialoGPT-medium` | 355M | è»½é‡ãƒ†ã‚¹ãƒˆ | 4GB |
| `microsoft/DialoGPT-large` | 762M | æ¨™æº–ãƒ†ã‚¹ãƒˆ | 8GB |

##### **å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**
```python
# benchmark_prompts.py
BENCHMARK_PROMPTS = {
    "conversation": [
        "Hello, how are you today?",
        "What is artificial intelligence?",
        "Can you explain machine learning?",
        "Tell me about the weather.",
        "What are your hobbies?"
    ],
    "reasoning": [
        "If I have 5 apples and give away 2, how many do I have left?",
        "What is the capital of France?",
        "Explain the difference between AI and ML.",
        "How does photosynthesis work?",
        "What causes seasons to change?"
    ]
}

# å›ºå®šç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GENERATION_CONFIG = {
    "max_new_tokens": 128,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
    "pad_token_id": 50256,
    "eos_token_id": 50256,
    "seed": 42  # å†ç¾æ€§ã®ãŸã‚å›ºå®š
}

# æ¸¬å®šæ¡ä»¶
BENCHMARK_CONFIG = {
    "warmup_runs": 3,
    "measurement_runs": 10,
    "concurrent_sessions": 1,
    "timeout_seconds": 300,
    "quality_baseline_runs": 5
}
```

#### **å†ç¾å¯èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```python
# reproducible_benchmark.py
import time
import json
import statistics
from datetime import datetime
from typing import Dict, List, Tuple

class ReproducibleBenchmark:
    def __init__(self, model_name: str, config: Dict):
        self.model_name = model_name
        self.config = config
        self.results = []
        
    def run_baseline_benchmark(self) -> Dict:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆInfer-OSç„¡åŠ¹ï¼‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸ”„ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {self.model_name}")
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for i in range(self.config["warmup_runs"]):
            self._single_inference(BENCHMARK_PROMPTS["conversation"][0], warmup=True)
        
        # æ¸¬å®šå®Ÿè¡Œ
        results = []
        for prompt_category in BENCHMARK_PROMPTS:
            for prompt in BENCHMARK_PROMPTS[prompt_category]:
                for run in range(self.config["measurement_runs"]):
                    result = self._single_inference(prompt)
                    results.append(result)
        
        return self._calculate_metrics(results, "baseline")
    
    def run_inferos_benchmark(self) -> Dict:
        """Infer-OSæœ‰åŠ¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸš€ Infer-OSãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {self.model_name}")
        
        # Infer-OSè¨­å®šé©ç”¨
        self._apply_inferos_config()
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for i in range(self.config["warmup_runs"]):
            self._single_inference(BENCHMARK_PROMPTS["conversation"][0], warmup=True)
        
        # æ¸¬å®šå®Ÿè¡Œ
        results = []
        for prompt_category in BENCHMARK_PROMPTS:
            for prompt in BENCHMARK_PROMPTS[prompt_category]:
                for run in range(self.config["measurement_runs"]):
                    result = self._single_inference(prompt)
                    results.append(result)
        
        return self._calculate_metrics(results, "inferos")
    
    def _single_inference(self, prompt: str, warmup: bool = False) -> Dict:
        """å˜ä¸€æ¨è«–å®Ÿè¡Œ"""
        start_time = time.perf_counter()
        
        # å®Ÿéš›ã®æ¨è«–å®Ÿè¡Œï¼ˆå®Ÿè£…ä¾å­˜ï¼‰
        response = self._execute_inference(prompt)
        
        end_time = time.perf_counter()
        
        result = {
            "prompt": prompt,
            "response": response,
            "latency_ms": (end_time - start_time) * 1000,
            "tokens_generated": len(response.split()),
            "timestamp": datetime.now().isoformat(),
            "warmup": warmup
        }
        
        if not warmup:
            # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            result["quality_score"] = self._calculate_quality_score(prompt, response)
        
        return result
    
    def _calculate_metrics(self, results: List[Dict], mode: str) -> Dict:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        latencies = [r["latency_ms"] for r in results if not r["warmup"]]
        token_counts = [r["tokens_generated"] for r in results if not r["warmup"]]
        quality_scores = [r["quality_score"] for r in results if not r["warmup"] and "quality_score" in r]
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        total_tokens = sum(token_counts)
        total_time_s = sum(latencies) / 1000
        tps = total_tokens / total_time_s if total_time_s > 0 else 0
        
        # First Token Latency (FTL) æ¨å®š
        ftl_ms = statistics.mean(latencies) * 0.3  # æ¨å®šå€¤
        
        metrics = {
            "mode": mode,
            "model": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "latency": {
                "mean_ms": statistics.mean(latencies),
                "median_ms": statistics.median(latencies),
                "p95_ms": self._percentile(latencies, 95),
                "p99_ms": self._percentile(latencies, 99),
                "std_ms": statistics.stdev(latencies) if len(latencies) > 1 else 0
            },
            "throughput": {
                "tokens_per_second": tps,
                "requests_per_second": len(latencies) / total_time_s if total_time_s > 0 else 0
            },
            "first_token_latency_ms": ftl_ms,
            "quality": {
                "mean_score": statistics.mean(quality_scores) if quality_scores else 0,
                "std_score": statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0
            },
            "tokens": {
                "mean_per_response": statistics.mean(token_counts),
                "total_generated": total_tokens
            },
            "config": self.config
        }
        
        return metrics
    
    def generate_comparison_report(self, baseline: Dict, inferos: Dict) -> Dict:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        improvement = {
            "tps_improvement": (inferos["throughput"]["tokens_per_second"] / baseline["throughput"]["tokens_per_second"] - 1) * 100,
            "latency_reduction": (1 - inferos["latency"]["mean_ms"] / baseline["latency"]["mean_ms"]) * 100,
            "ftl_reduction": (1 - inferos["first_token_latency_ms"] / baseline["first_token_latency_ms"]) * 100,
            "quality_change": inferos["quality"]["mean_score"] - baseline["quality"]["mean_score"]
        }
        
        report = {
            "summary": {
                "model": self.model_name,
                "test_date": datetime.now().isoformat(),
                "baseline_tps": baseline["throughput"]["tokens_per_second"],
                "inferos_tps": inferos["throughput"]["tokens_per_second"],
                "improvement_percent": improvement["tps_improvement"],
                "quality_delta": improvement["quality_change"]
            },
            "detailed_metrics": {
                "baseline": baseline,
                "inferos": inferos,
                "improvements": improvement
            },
            "test_conditions": {
                "prompts_tested": len(BENCHMARK_PROMPTS["conversation"]) + len(BENCHMARK_PROMPTS["reasoning"]),
                "runs_per_prompt": self.config["measurement_runs"],
                "total_inferences": len(BENCHMARK_PROMPTS["conversation"]) * self.config["measurement_runs"] + len(BENCHMARK_PROMPTS["reasoning"]) * self.config["measurement_runs"],
                "generation_config": GENERATION_CONFIG
            }
        }
        
        return report

# ä½¿ç”¨ä¾‹
def run_reproducible_benchmark():
    """å†ç¾å¯èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    benchmark = ReproducibleBenchmark("microsoft/DialoGPT-medium", BENCHMARK_CONFIG)
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
    baseline_results = benchmark.run_baseline_benchmark()
    
    # Infer-OSæ¸¬å®š
    inferos_results = benchmark.run_inferos_benchmark()
    
    # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = benchmark.generate_comparison_report(baseline_results, inferos_results)
    
    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"benchmark_report_{timestamp}.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: benchmark_report_{timestamp}.json")
    return report
```

#### **ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ**
```python
# ablation_study.py
class AblationStudy:
    def __init__(self):
        self.configurations = {
            "baseline": {
                "kv_quantization": False,
                "io_binding": False,
                "hybrid_scheduling": False,
                "description": "æ¨™æº–PyTorchæ¨è«–"
            },
            "kv_only": {
                "kv_quantization": True,
                "io_binding": False,
                "hybrid_scheduling": False,
                "description": "KVé‡å­åŒ–ã®ã¿"
            },
            "io_only": {
                "kv_quantization": False,
                "io_binding": True,
                "hybrid_scheduling": False,
                "description": "IOBindingã®ã¿"
            },
            "hybrid_only": {
                "kv_quantization": False,
                "io_binding": False,
                "hybrid_scheduling": True,
                "description": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ã¿"
            },
            "kv_io": {
                "kv_quantization": True,
                "io_binding": True,
                "hybrid_scheduling": False,
                "description": "KVé‡å­åŒ– + IOBinding"
            },
            "full_inferos": {
                "kv_quantization": True,
                "io_binding": True,
                "hybrid_scheduling": True,
                "description": "Infer-OSå®Œå…¨ç‰ˆ"
            }
        }
    
    def run_ablation_study(self, model_name: str) -> Dict:
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶å®Ÿè¡Œ"""
        results = {}
        
        for config_name, config in self.configurations.items():
            print(f"ğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ: {config['description']}")
            
            # è¨­å®šé©ç”¨
            self._apply_configuration(config)
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            benchmark = ReproducibleBenchmark(model_name, BENCHMARK_CONFIG)
            result = benchmark.run_inferos_benchmark()
            result["config_name"] = config_name
            result["config_description"] = config["description"]
            
            results[config_name] = result
        
        # ç›¸ä¹—åŠ¹æœåˆ†æ
        analysis = self._analyze_synergy(results)
        
        return {
            "results": results,
            "analysis": analysis
        }
    
    def _analyze_synergy(self, results: Dict) -> Dict:
        """ç›¸ä¹—åŠ¹æœåˆ†æ"""
        baseline_tps = results["baseline"]["throughput"]["tokens_per_second"]
        
        individual_effects = {
            "kv_quantization": (results["kv_only"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100,
            "io_binding": (results["io_only"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100,
            "hybrid_scheduling": (results["hybrid_only"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100
        }
        
        combined_effects = {
            "kv_io": (results["kv_io"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100,
            "full_inferos": (results["full_inferos"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100
        }
        
        # ç›¸ä¹—åŠ¹æœè¨ˆç®—
        expected_kv_io = individual_effects["kv_quantization"] + individual_effects["io_binding"]
        actual_kv_io = combined_effects["kv_io"]
        synergy_kv_io = actual_kv_io - expected_kv_io
        
        expected_full = sum(individual_effects.values())
        actual_full = combined_effects["full_inferos"]
        synergy_full = actual_full - expected_full
        
        return {
            "individual_effects": individual_effects,
            "combined_effects": combined_effects,
            "synergy_analysis": {
                "kv_io_synergy": synergy_kv_io,
                "full_synergy": synergy_full,
                "synergy_interpretation": self._interpret_synergy(synergy_full)
            }
        }
    
    def _interpret_synergy(self, synergy: float) -> str:
        """ç›¸ä¹—åŠ¹æœã®è§£é‡ˆ"""
        if synergy > 5:
            return "å¼·ã„æ­£ã®ç›¸ä¹—åŠ¹æœ"
        elif synergy > 0:
            return "å¼±ã„æ­£ã®ç›¸ä¹—åŠ¹æœ"
        elif synergy > -5:
            return "ç›¸ä¹—åŠ¹æœãªã—"
        else:
            return "è² ã®ç›¸äº’ä½œç”¨"
```

### **3.2 å“è³ªç®¡ç†**

#### **åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```bash
# Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
gaia-cli --model llama-7b --benchmark --optimization none

# Infer-OSæœ‰åŠ¹ã§ã®æ€§èƒ½æ¸¬å®š
gaia-cli --model llama-7b --benchmark --optimization inferos

# è©³ç´°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
python benchmark_inferos_gaia.py \
  --model llama-7b \
  --sequences 100 \
  --batch-sizes 1,4,8 \
  --seq-lengths 128,512,1024 \
  --compare-modes baseline,inferos
```

#### **ã‚«ã‚¹ã‚¿ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```python
import time
import requests
from lemonade_adapter import LemonadeAdapter

def benchmark_inferos():
    adapter = LemonadeAdapter()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        {"seq_len": 128, "batch": 1, "desc": "çŸ­æ–‡ãƒ»å˜ä¸€"},
        {"seq_len": 512, "batch": 4, "desc": "ä¸­æ–‡ãƒ»ãƒãƒƒãƒ"},
        {"seq_len": 1024, "batch": 1, "desc": "é•·æ–‡ãƒ»å˜ä¸€"}
    ]
    
    results = []
    for case in test_cases:
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆInfer-OSç„¡åŠ¹ï¼‰
        baseline_time = run_inference_baseline(case)
        
        # Infer-OSæœ‰åŠ¹
        inferos_time = run_inference_with_inferos(case)
        
        improvement = baseline_time / inferos_time
        results.append({
            "case": case["desc"],
            "baseline_ms": baseline_time * 1000,
            "inferos_ms": inferos_time * 1000,
            "improvement": improvement
        })
    
    return results
```

### **3.2 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–**

#### **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**
```bash
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
cat > monitor_inferos.py << 'EOF'
import requests
import time
import json

def monitor_metrics():
    while True:
        try:
            response = requests.get("http://127.0.0.1:7031/v1/metrics")
            metrics = response.json()
            
            print(f"TPS: {metrics['tps']:.1f}, "
                  f"FTL: {metrics['ftl_ms']:.1f}ms, "
                  f"VRAM: {metrics['mem']['vram_gb']:.1f}GB, "
                  f"NPU: {metrics['util']['npu']:.1%}, "
                  f"iGPU: {metrics['util']['igpu']:.1%}")
            
        except Exception as e:
            print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
        
        time.sleep(1)

if __name__ == "__main__":
    monitor_metrics()
EOF

python monitor_inferos.py
```

#### **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º**
```bash
# Grafana/Prometheusé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’Prometheuså½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
curl http://127.0.0.1:7031/v1/metrics/prometheus
```

## ğŸ”§ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**

### **4.1 ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½**

#### **è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¡¨**

| äº‹è±¡ | æ¤œå‡ºæ–¹æ³• | ç›´ã¡ã«è¡Œã†å‡¦ç½® | è¿½éšå‡¦ç½® | å¾©æ—§æ¡ä»¶ |
|------|----------|----------------|----------|----------|
| DMLæœªæ¤œå‡º | `ort.get_available_providers()` | CPU EPã¸åˆ‡æ›¿ã€KVã—ãã„å€¤ã‚’ä¿å®ˆåŒ– | ç®¡ç†è€…ã¸é€šçŸ¥ã€ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª | DMLå†æ¤œå‡ºå¾Œ5åˆ† |
| NPUæœªæ¤œå‡º/æ¸©åº¦è¶…é | ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸©åº¦/åˆ©ç”¨ç‡ | decodeâ†’DMLã¸ç§»è­² | 5åˆ†æ¯ã«å†è©¦è¡Œ | NPUæ¸©åº¦æ­£å¸¸åŒ– |
| Î”PPLè¶…é | `/v1/metrics` | KVã‚’ INT4â†’INT8â†’FP16 ã¸æ®µéšå¾©å…ƒ | é–¾å€¤ã‚’ä¸€æ®µä¿å®ˆåŒ– | å“è³ªå®‰å®šå¾Œ10åˆ† |
| ãƒ¡ãƒ¢ãƒªåœ§è¿« | `/v1/metrics.mem` | recent_windowâ†“ ç­‰ã§åœ§ç¸®å¼·åŒ– | çµ±è¨ˆã§æœ€é©ç‚¹å†æ¢ç´¢ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡<80% |
| APIå¿œç­”ãªã— | HTTP timeout | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã¸åˆ‡æ›¿ | Agentå†èµ·å‹• | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ­£å¸¸ |

#### **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šä¾‹**

##### **DML/NPUæœªæ¤œå‡ºæ™‚ã®è‡ªå‹•åˆ‡æ›¿**
```python
# inferos_control_agent.py ã§ã®å®Ÿè£…ä¾‹
import onnxruntime as ort

class DeviceManager:
    def __init__(self):
        self.available_providers = ort.get_available_providers()
        self.current_config = self.detect_optimal_config()
    
    def detect_optimal_config(self):
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹æ§‹æˆã‚’æ¤œå‡º"""
        config = {
            "prefill_device": "cpu",
            "decode_device": "cpu",
            "kv_policy": "conservative"
        }
        
        if "DmlExecutionProvider" in self.available_providers:
            config["prefill_device"] = "dml"
            config["kv_policy"] = "balanced"
            
            # NPUæ¤œå‡ºï¼ˆDirectMLçµŒç”±ï¼‰
            if self.detect_npu_capability():
                config["decode_device"] = "npu"
                config["kv_policy"] = "aggressive"
        
        return config
    
    def apply_fallback_policy(self, error_type):
        """ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        fallback_policies = {
            "dml_unavailable": {
                "prefill_device": "cpu",
                "decode_device": "cpu",
                "kv": {"recent_window": 128, "level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.6, "L3_evict": 0.4}}
            },
            "npu_thermal": {
                "decode_device": "dml",
                "kv": {"recent_window": 96, "level_thresholds": {"L1_int8": 0.75, "L2_int4": 0.55, "L3_evict": 0.35}}
            },
            "quality_degradation": {
                "kv": {"recent_window": 256, "level_thresholds": {"L1_int8": 0.9, "L2_int4": 0.7, "L3_evict": 0.5}}
            },
            "memory_pressure": {
                "kv": {"recent_window": 32, "level_thresholds": {"L1_int8": 0.6, "L2_int4": 0.4, "L3_evict": 0.2}}
            }
        }
        
        return fallback_policies.get(error_type, fallback_policies["dml_unavailable"])
```

##### **å“è³ªåŠ£åŒ–æ¤œå‡ºã¨è‡ªå‹•å¾©æ—§**
```python
class QualityMonitor:
    def __init__(self, max_delta_ppl=0.5, window_size=10):
        self.max_delta_ppl = max_delta_ppl
        self.window_size = window_size
        self.quality_history = []
        self.degradation_count = 0
    
    def check_quality(self, current_ppl, baseline_ppl):
        """å“è³ªãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•èª¿æ•´"""
        delta_ppl = current_ppl - baseline_ppl
        self.quality_history.append(delta_ppl)
        
        if len(self.quality_history) > self.window_size:
            self.quality_history.pop(0)
        
        avg_delta = sum(self.quality_history) / len(self.quality_history)
        
        if avg_delta > self.max_delta_ppl:
            self.degradation_count += 1
            if self.degradation_count >= 3:  # 3å›é€£ç¶šã§åŠ£åŒ–
                return self.trigger_quality_recovery()
        else:
            self.degradation_count = 0
        
        return None
    
    def trigger_quality_recovery(self):
        """å“è³ªå›å¾©å‡¦ç†"""
        recovery_steps = [
            {"kv": {"level_thresholds": {"L2_int4": 0.6, "L3_evict": 0.4}}},  # INT4ã‚’æ¸›ã‚‰ã™
            {"kv": {"level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.7}}},   # INT8ã‚’å¢—ã‚„ã™
            {"kv": {"recent_window": 128}},                                    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ‹¡å¤§
            {"scheduler": {"mode": "gpu_only"}}                               # NPUç„¡åŠ¹åŒ–
        ]
        
        return recovery_steps[min(self.degradation_count - 3, len(recovery_steps) - 1)]
```

#### **ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®è©³ç´°**

##### **DirectMLé‡å­åŒ–æ¼”ç®—ã‚µãƒãƒ¼ãƒˆç¢ºèª**
```python
def check_quantization_support():
    """DirectMLã§ã®é‡å­åŒ–æ¼”ç®—ã‚µãƒãƒ¼ãƒˆç¢ºèª"""
    try:
        import onnxruntime as ort
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªONNXãƒ¢ãƒ‡ãƒ«ã§é‡å­åŒ–æ¼”ç®—ã‚’ç¢ºèª
        test_model_path = create_test_quantized_model()
        
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        providers = [
            ('DmlExecutionProvider', {
                'device_id': 0,
                'enable_dynamic_graph_fusion': True
            })
        ]
        
        session = ort.InferenceSession(test_model_path, session_options, providers=providers)
        
        # é‡å­åŒ–æ¼”ç®—ã®ã‚µãƒãƒ¼ãƒˆç¢ºèª
        supported_ops = get_supported_quantization_ops(session)
        
        return {
            "w4_support": "QLinearConv" in supported_ops,
            "w8_support": "QLinearMatMul" in supported_ops,
            "kv_int8_support": "QuantizeLinear" in supported_ops,
            "kv_int4_support": "DequantizeLinear" in supported_ops
        }
        
    except Exception as e:
        logger.warning(f"é‡å­åŒ–ã‚µãƒãƒ¼ãƒˆç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return {"w4_support": False, "w8_support": False, "kv_int8_support": False, "kv_int4_support": False}

def apply_quantization_fallback(support_info):
    """é‡å­åŒ–ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    if not support_info["kv_int4_support"]:
        # INT4æœªã‚µãƒãƒ¼ãƒˆã®å ´åˆã¯INT8ã®ã¿ä½¿ç”¨
        return {
            "kv": {
                "level_thresholds": {
                    "L1_int8": 0.5,
                    "L2_int4": 1.0,  # INT4ã‚’ç„¡åŠ¹åŒ–
                    "L3_evict": 0.3
                }
            }
        }
    
    if not support_info["kv_int8_support"]:
        # INT8æœªã‚µãƒãƒ¼ãƒˆã®å ´åˆã¯é‡å­åŒ–ç„¡åŠ¹
        return {
            "kv": {
                "level_thresholds": {
                    "L1_int8": 1.0,  # INT8ã‚’ç„¡åŠ¹åŒ–
                    "L2_int4": 1.0,  # INT4ã‚’ç„¡åŠ¹åŒ–
                    "L3_evict": 0.7  # åœ§ç¸®ã®ã¿ä½¿ç”¨
                }
            }
        }
    
    return None  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸è¦
```

### **4.2 ä¸€èˆ¬çš„ãªå•é¡Œ**

#### **NPU/iGPUãŒèªè­˜ã•ã‚Œãªã„**
```bash
# ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
python -c "
import onnxruntime as ort
print('åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:', ort.get_available_providers())
"

# DirectMLç¢ºèª
python -c "
import onnxruntime as ort
providers = ort.get_available_providers()
if 'DmlExecutionProvider' in providers:
    print('âœ… DirectMLåˆ©ç”¨å¯èƒ½')
else:
    print('âŒ DirectMLåˆ©ç”¨ä¸å¯ - ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèªãŒå¿…è¦')
"
```

#### **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
curl http://127.0.0.1:7031/v1/metrics | jq '.mem'

# é‡å­åŒ–è¨­å®šã®èª¿æ•´
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"recent_window":32,"level_thresholds":{"L1_int8":0.6,"L2_int4":0.4,"L3_evict":0.2}}}'
```

#### **å“è³ªåŠ£åŒ–ã®å¯¾å‡¦**
```bash
# å“è³ªé‡è¦–è¨­å®š
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"recent_window":128,"level_thresholds":{"L1_int8":0.8,"L2_int4":0.6,"L3_evict":0.4}}}'

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆ
curl -X POST http://127.0.0.1:7031/v1/baseline -d '{"enable": true}'
```

### **4.2 ãƒ­ã‚°ã¨ãƒ‡ãƒãƒƒã‚°**

#### **è©³ç´°ãƒ­ã‚°ã®æœ‰åŠ¹åŒ–**
```python
# inferos_control_agent.py ã®è¨­å®š
import logging
logging.basicConfig(level=logging.DEBUG)

# ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
export INFEROS_LOG_LEVEL=DEBUG
python inferos_control_agent.py
```

#### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã®ç¢ºèª**
```bash
# å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
curl http://127.0.0.1:7031/v1/history | jq '.history[-10:]'

# çµ±è¨ˆæƒ…å ±ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
curl http://127.0.0.1:7031/v1/metrics > metrics_$(date +%Y%m%d_%H%M%S).json
```

## ğŸ¯ **æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**

### **5.1 ãƒ¢ãƒ‡ãƒ«åˆ¥æ¨å¥¨è¨­å®š**

#### **7B/8Bãƒ¢ãƒ‡ãƒ«ï¼ˆä¸­è»½é‡ï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 64
    level_thresholds:
      L1_int8: 0.7
      L2_int4: 0.5
      L3_evict: 0.3
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
```

#### **13B/20Bãƒ¢ãƒ‡ãƒ«ï¼ˆé‡é‡ç´šï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 32
    level_thresholds:
      L1_int8: 0.6
      L2_int4: 0.4
      L3_evict: 0.2
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
```

#### **3Bä»¥ä¸‹ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 128
    level_thresholds:
      L1_int8: 0.8
      L2_int4: 0.6
      L3_evict: 0.4
  scheduler:
    mode: gpu_only  # NPUã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’é¿ã‘ã‚‹
```

### **5.2 ç”¨é€”åˆ¥æœ€é©åŒ–**

#### **ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼ˆä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 128, "mode": "latency_optimized"},
    "scheduler": {"mode": "hybrid", "decode_device": "npu"},
    "quality": {"max_delta_ppl": 0.3}
  }'
```

#### **ãƒãƒƒãƒå‡¦ç†ï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 32, "mode": "throughput_optimized"},
    "scheduler": {"mode": "hybrid", "prefill_device": "dml"},
    "quality": {"max_delta_ppl": 0.5}
  }'
```

#### **é«˜å“è³ªç”Ÿæˆï¼ˆå“è³ªé‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 256, "mode": "quality_optimized"},
    "scheduler": {"mode": "gpu_only"},
    "quality": {"max_delta_ppl": 0.1}
  }'
```

## ğŸ“ˆ **ç¶™ç¶šçš„ãªæœ€é©åŒ–**

### **6.1 è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**

#### **é©å¿œçš„è¨­å®šèª¿æ•´**
```python
# auto_tuner.py
import requests
import time
import numpy as np

class InferOSAutoTuner:
    def __init__(self, agent_url="http://127.0.0.1:7031"):
        self.agent_url = agent_url
        self.performance_history = []
    
    def tune_for_workload(self, target_tps=None, target_quality=None):
        """ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«åŸºã¥ãè‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        current_metrics = self.get_metrics()
        
        if target_tps and current_metrics['tps'] < target_tps:
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ãŸã‚ã®èª¿æ•´
            self.adjust_for_throughput()
        
        if target_quality and current_metrics['quality']['delta_ppl_est'] > target_quality:
            # å“è³ªå‘ä¸Šã®ãŸã‚ã®èª¿æ•´
            self.adjust_for_quality()
    
    def adjust_for_throughput(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šèª¿æ•´"""
        policy = {
            "kv": {"recent_window": 32, "level_thresholds": {"L1_int8": 0.6, "L2_int4": 0.4, "L3_evict": 0.2}},
            "scheduler": {"mode": "hybrid"}
        }
        self.apply_policy(policy)
    
    def adjust_for_quality(self):
        """å“è³ªå‘ä¸Šèª¿æ•´"""
        policy = {
            "kv": {"recent_window": 128, "level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.6, "L3_evict": 0.4}},
            "scheduler": {"mode": "hybrid"}
        }
        self.apply_policy(policy)

# ä½¿ç”¨ä¾‹
tuner = InferOSAutoTuner()
tuner.tune_for_workload(target_tps=30.0, target_quality=0.3)
```

### **6.2 A/Bãƒ†ã‚¹ãƒˆ**

#### **è¨­å®šæ¯”è¼ƒãƒ†ã‚¹ãƒˆ**
```bash
# A/Bãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
cat > ab_test_inferos.py << 'EOF'
import requests
import time
import statistics

def run_ab_test():
    configs = [
        {"name": "Conservative", "kv": {"recent_window": 128}},
        {"name": "Balanced", "kv": {"recent_window": 64}},
        {"name": "Aggressive", "kv": {"recent_window": 32}}
    ]
    
    results = {}
    for config in configs:
        print(f"Testing {config['name']} configuration...")
        
        # è¨­å®šé©ç”¨
        requests.post("http://127.0.0.1:7031/v1/policy", json=config)
        time.sleep(2)  # è¨­å®šåæ˜ å¾…ã¡
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        metrics = []
        for _ in range(10):
            response = requests.get("http://127.0.0.1:7031/v1/metrics")
            metrics.append(response.json())
            time.sleep(1)
        
        # çµ±è¨ˆè¨ˆç®—
        tps_values = [m['tps'] for m in metrics]
        results[config['name']] = {
            "avg_tps": statistics.mean(tps_values),
            "std_tps": statistics.stdev(tps_values),
            "avg_quality": statistics.mean([m['quality']['delta_ppl_est'] for m in metrics])
        }
    
    return results

if __name__ == "__main__":
    results = run_ab_test()
    for name, stats in results.items():
        print(f"{name}: TPS={stats['avg_tps']:.1f}Â±{stats['std_tps']:.1f}, Quality={stats['avg_quality']:.3f}")
EOF

python ab_test_inferos.py
```

## ğŸ‰ **æˆåŠŸäº‹ä¾‹ã¨æœŸå¾…ã•ã‚Œã‚‹çµæœ**

### **7.1 å…¸å‹çš„ãªæ”¹å–„ä¾‹**

#### **Llama-7B ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ**
```
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:
- TPS: 18.5 tokens/sec
- FTL: 320ms
- ãƒ¡ãƒ¢ãƒª: 14.2GB
- å“è³ª: PPL 12.3

Infer-OSçµ±åˆå¾Œ:
- TPS: 26.8 tokens/sec (+45%)
- FTL: 245ms (-23%)
- ãƒ¡ãƒ¢ãƒª: 8.9GB (-37%)
- å“è³ª: PPL 12.7 (Î”PPL +0.4)

ROI: 45%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š + 37%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
```

#### **Llama-13B ãƒãƒƒãƒå‡¦ç†**
```
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:
- TPS: 12.3 tokens/sec
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4
- ãƒ¡ãƒ¢ãƒª: 26.1GB
- å“è³ª: PPL 11.8

Infer-OSçµ±åˆå¾Œ:
- TPS: 19.7 tokens/sec (+60%)
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 8 (2å€)
- ãƒ¡ãƒ¢ãƒª: 16.4GB (-37%)
- å“è³ª: PPL 12.2 (Î”PPL +0.4)

ROI: 60%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š + ãƒãƒƒãƒã‚µã‚¤ã‚º2å€åŒ–
```

### **7.2 å°å…¥åŠ¹æœã®æ¸¬å®š**

#### **KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**
```python
# kpi_dashboard.py
import streamlit as st
import requests
import plotly.graph_objects as go
from datetime import datetime, timedelta

def create_dashboard():
    st.title("Infer-OS Ã— GAIA ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
    metrics = requests.get("http://127.0.0.1:7031/v1/metrics").json()
    history = requests.get("http://127.0.0.1:7031/v1/history").json()
    
    # KPIè¡¨ç¤º
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("TPS", f"{metrics['tps']:.1f}", delta=f"+{metrics['tps']/18.5-1:.1%}")
    with col2:
        st.metric("FTL (ms)", f"{metrics['ftl_ms']:.0f}", delta=f"{(metrics['ftl_ms']/320-1)*100:.1f}%")
    with col3:
        st.metric("ãƒ¡ãƒ¢ãƒª (GB)", f"{metrics['mem']['vram_gb']:.1f}", delta=f"{(metrics['mem']['vram_gb']/14.2-1)*100:.1f}%")
    with col4:
        st.metric("å“è³ª (Î”PPL)", f"{metrics['quality']['delta_ppl_est']:.3f}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚°ãƒ©ãƒ•
    if history['history']:
        timestamps = [h['timestamp'] for h in history['history']]
        tps_values = [h['tps'] for h in history['history']]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=timestamps, y=tps_values, name='TPS'))
        fig.update_layout(title='ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå±¥æ­´', xaxis_title='æ™‚åˆ»', yaxis_title='TPS')
        st.plotly_chart(fig)

if __name__ == "__main__":
    create_dashboard()
```

## ğŸ”® **ä»Šå¾Œã®ç™ºå±•**

### **8.1 Phase 3: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢é€£æº**

#### **NPUå¸¸é§æœ€é©åŒ–**
```python
# å°†æ¥å®Ÿè£…äºˆå®š
class NPUResidentOptimizer:
    """NPUå¸¸é§KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–"""
    
    def __init__(self):
        self.npu_cache_manager = NPUCacheManager()
        self.igpu_spillover = iGPUSpilloverManager()
    
    def optimize_kv_placement(self, kv_chunks):
        """KVãƒãƒ£ãƒ³ã‚¯ã®æœ€é©é…ç½®"""
        # é‡è¦åº¦ã®é«˜ã„KVã¯NPUã«å¸¸é§
        # ä½é‡è¦åº¦ã¯iGPUã«ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼
        pass
```

#### **å‹•çš„è² è·åˆ†æ•£**
```python
# å°†æ¥å®Ÿè£…äºˆå®š
class HybridLoadBalancer:
    """NPU+iGPUå‹•çš„è² è·åˆ†æ•£"""
    
    def balance_workload(self, prefill_load, decode_load):
        """ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«åŸºã¥ãå‹•çš„åˆ†æ•£"""
        # Prefill: iGPUå„ªå…ˆã€é«˜è² è·æ™‚ã¯NPUã‚‚æ´»ç”¨
        # Decode: NPUå„ªå…ˆã€ç†±åˆ¶ç´„æ™‚ã¯iGPUã«ç§»è¡Œ
        pass
```

### **8.2 ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ**

#### **ä»–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ**
- **Ollamaçµ±åˆ**: Ollama + Infer-OSé€£æº
- **LangChainçµ±åˆ**: LangChain Agent + Infer-OSæœ€é©åŒ–
- **Transformersçµ±åˆ**: HuggingFace Transformers + Infer-OS

#### **ã‚¯ãƒ©ã‚¦ãƒ‰å±•é–‹**
- **Azureçµ±åˆ**: Azure ML + Infer-OS
- **AWSçµ±åˆ**: SageMaker + Infer-OS
- **ã‚³ãƒ³ãƒ†ãƒŠåŒ–**: Docker + Kuberneteså¯¾å¿œ

## ğŸ“ **ã‚µãƒãƒ¼ãƒˆ**

### **9.1 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**
- **GitHub**: https://github.com/kojima123/infer-os
- **Issues**: ãƒã‚°å ±å‘Šãƒ»æ©Ÿèƒ½è¦æœ›
- **Discussions**: æŠ€è¡“è¨è«–ãƒ»è³ªå•

### **9.2 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**
- **ãƒ­ã‚°ç¢ºèª**: `tail -f inferos_agent.log`
- **è¨­å®šãƒªã‚»ãƒƒãƒˆ**: `curl -X POST http://127.0.0.1:7031/v1/baseline`
- **å¼·åˆ¶å†èµ·å‹•**: `pkill -f inferos_control_agent && python inferos_control_agent.py`

### **9.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›¸è«‡**
ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®æœ€é©åŒ–ç›¸è«‡ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã¨å…±ã«Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãƒ»ç¨®é¡
- å…¸å‹çš„ãªå…¥åŠ›é•·ãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚º
- ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
- ç›®æ¨™ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆ

---

**ğŸ¯ GAIA Ã— Infer-OSçµ±åˆã«ã‚ˆã‚Šã€AMD Ryzen AI NPU + Radeon iGPUç’°å¢ƒã§ã®LLMæ¨è«–æ€§èƒ½ã‚’æœ€å¤§åŒ–ã—ã€æ¬¡ä¸–ä»£AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ï¼**

