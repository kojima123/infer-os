# GAIA Ã— Infer-OS çµ±åˆã‚¬ã‚¤ãƒ‰ v1.0

## ğŸ¯ **æ¦‚è¦**

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€AMD GAIAç’°å¢ƒï¼ˆWindows 11 + Ryzen AI NPU + Radeon iGPUï¼‰ã«Infer-OSã‚’çµ±åˆã—ã€LLMæ¨è«–ã®å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

### **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**
- **7B/8Bãƒ¢ãƒ‡ãƒ«**: 1.3-1.6å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€60-75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **20Bãƒ¢ãƒ‡ãƒ«**: 1.8å€ä»¥ä¸Šã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€70-80%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥**: æœ€å¤§75%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- **å“è³ªä¿æŒ**: Î”PPL â‰¤ 0.5ã§ã®é«˜å“è³ªç¶­æŒ

## ğŸ”§ **å‰ææ¡ä»¶**

### **ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶**
- **OS**: Windows 11ï¼ˆGAIAè¦ä»¶ï¼‰
- **CPU**: AMD Ryzen AI 300ã‚·ãƒªãƒ¼ã‚ºï¼ˆXDNA NPUæ­è¼‰ï¼‰
- **GPU**: Radeon iGPUï¼ˆRDNAã€DirectMLå¯¾å¿œï¼‰
- **ãƒ¡ãƒ¢ãƒª**: 16GBä»¥ä¸Šæ¨å¥¨
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 10GBä»¥ä¸Šã®ç©ºãå®¹é‡

### **ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢è¦ä»¶**
- **GAIA**: æœ€æ–°ç‰ˆï¼ˆRAUX/gaia-cliå¯¾å¿œï¼‰
- **Python**: 3.11ä»¥ä¸Š
- **ONNX Runtime**: DirectMLå¯¾å¿œç‰ˆ
- **NPU/iGPUãƒ‰ãƒ©ã‚¤ãƒ**: GAIAæ¨å¥¨ç‰ˆ

## ğŸ“¦ **ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †**

### **Step 1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³**
```bash
git clone https://github.com/kojima123/infer-os.git
cd infer-os
```

### **Step 2: ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
```bash
# Pythonä¾å­˜é–¢ä¿‚
pip install fastapi uvicorn requests numpy psutil pydantic

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: YAMLè¨­å®šã‚µãƒãƒ¼ãƒˆ
pip install pyyaml

# ONNX Runtime (DirectML)
pip install onnxruntime-directml
```

### **Step 3: GAIAã®è¨­å®šç¢ºèª**
```bash
# GAIAãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
gaia-cli --version

# Hybridãƒ¢ãƒ¼ãƒ‰ã§ã®å‹•ä½œç¢ºèª
gaia-cli --mode hybrid --model llama-7b --test
```

## ğŸ“‹ **CLIãƒãƒƒãƒ”ãƒ³ã‚°è¡¨**

### **GAIA CLI ãƒ•ãƒ©ã‚°å¯¾å¿œè¡¨**

| æ¦‚å¿µ | å…¬å¼CLIãƒ•ãƒ©ã‚° | ä½¿ç”¨ä¾‹ | å‚™è€ƒ |
|------|---------------|--------|------|
| æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰: Infer-OS | `--optimization inferos` | `gaia-cli --model llama-7b --optimization inferos` | Infer-OSçµ±åˆã‚’æœ‰åŠ¹åŒ– |
| ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œ | `--mode hybrid` | `gaia-cli --mode hybrid` | NPU+iGPUå”èª¿å‡¦ç† |
| KVé‡å­åŒ–ON | `--kv-quantization enabled` | `gaia-cli --kv-quantization enabled` | KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ– |
| ãƒãƒƒãƒã‚µã‚¤ã‚º | `--batch-size <N>` | `--batch-size 4` | åŒæ™‚å‡¦ç†æ•° |
| æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³ | `--max-tokens <N>` | `--max-tokens 1024` | ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ |
| å“è³ªé–¾å€¤ | `--quality-threshold <F>` | `--quality-threshold 0.5` | Î”PPLè¨±å®¹å€¤ |
| ãƒ¡ãƒ¢ãƒªåˆ¶é™ | `--memory-limit <SIZE>` | `--memory-limit 16GB` | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ä¸Šé™ |
| ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ | `--debug` | `gaia-cli --debug` | è©³ç´°ãƒ­ã‚°å‡ºåŠ› |

> **æ³¨æ„**: å®Ÿéš›ã®CLIãƒ•ãƒ©ã‚°åã¯ GAIA ã®å®Ÿè£…ã«ä¾å­˜ã—ã¾ã™ã€‚ä¸Šè¨˜ã¯è¨­è¨ˆä»•æ§˜ã§ã‚ã‚Šã€å®Ÿè£…æ™‚ã«èª¿æ•´ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚

### **å‹•ä½œç¢ºèªã‚³ãƒãƒ³ãƒ‰**
```bash
# GAIAåŸºæœ¬å‹•ä½œç¢ºèª
gaia-cli --version
gaia-cli --help | grep -E "(optimization|mode|kv-quantization)"

# Infer-OSçµ±åˆãƒ†ã‚¹ãƒˆ
gaia-cli --model llama-7b --optimization inferos --test --dry-run
```

## ğŸ”Œ **APIä»•æ§˜ï¼ˆOpenAPIï¼‰**

### **Infer-OS Control Agent API v1.0**

#### **ãƒ™ãƒ¼ã‚¹URL**: `http://127.0.0.1:7031/v1`
> **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ›ã‚¹ãƒˆå°‚ç”¨ãƒã‚¤ãƒ³ãƒ‰ï¼ˆ127.0.0.1ï¼‰ã€èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å¿…é ˆ

#### **èªè¨¼è¨­å®š**

##### **ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š**
```cmd
# Windows PowerShell
$token = [System.Guid]::NewGuid().ToString()
[Environment]::SetEnvironmentVariable("INFEROS_AGENT_TOKEN", $token, "User")
echo "Generated token: $token"

# Linux/macOS
export INFEROS_AGENT_TOKEN=$(uuidgen)
echo "Generated token: $INFEROS_AGENT_TOKEN"
```

##### **èªè¨¼ãƒ˜ãƒƒãƒ€ãƒ¼**
```http
# èªè¨¼ãŒå¿…è¦ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆPOST/PUT/DELETEï¼‰
X-Inferos-Token: your-generated-token-here

# ä¾‹
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "X-Inferos-Token: $INFEROS_AGENT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"kv": {"mode": "dynamic"}}'
```

#### **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä¸€è¦§**

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ | èªè¨¼ | ãƒã‚¤ãƒ³ãƒ‰ |
|---------------|----------|------|------|----------|
| `/health` | GET | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | ä¸è¦ | 127.0.0.1 |
| `/metrics` | GET | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ä¸è¦ | 127.0.0.1 |
| `/policy` | GET | æœ€é©åŒ–ãƒãƒªã‚·ãƒ¼å–å¾— | ä¸è¦ | 127.0.0.1 |
| `/policy` | POST | æœ€é©åŒ–ãƒãƒªã‚·ãƒ¼è¨­å®š | **å¿…é ˆ** | 127.0.0.1 |
| `/baseline` | POST | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ | **å¿…é ˆ** | 127.0.0.1 |
| `/config` | GET | è¨­å®šå–å¾— | ä¸è¦ | 127.0.0.1 |
| `/config` | POST | è¨­å®šæ›´æ–° | **å¿…é ˆ** | 127.0.0.1 |
| `/history` | GET | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ | ä¸è¦ | 127.0.0.1 |

#### **API ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…ä¾‹**

##### **èªè¨¼ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢**
```python
# inferos_control_agent.py
import os
import secrets
from fastapi import FastAPI, Header, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="Infer-OS Control Agent", version="1.0.0")

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
AGENT_TOKEN = os.getenv("INFEROS_AGENT_TOKEN")
if not AGENT_TOKEN:
    # é–‹ç™ºç’°å¢ƒç”¨ã®è‡ªå‹•ç”Ÿæˆï¼ˆæœ¬ç•ªã§ã¯ç’°å¢ƒå¤‰æ•°å¿…é ˆï¼‰
    AGENT_TOKEN = secrets.token_urlsafe(32)
    print(f"âš ï¸  è‡ªå‹•ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³: {AGENT_TOKEN}")
    print("æœ¬ç•ªç’°å¢ƒã§ã¯ INFEROS_AGENT_TOKEN ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")

# CORSç„¡åŠ¹åŒ–ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å°‚ç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://127.0.0.1", "http://localhost"],
    allow_methods=["GET", "POST"],
    allow_headers=["X-Inferos-Token", "Content-Type"],
)

def require_auth(x_inferos_token: str | None = Header(None)):
    """èªè¨¼ãŒå¿…è¦ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç”¨"""
    if not x_inferos_token or x_inferos_token != AGENT_TOKEN:
        raise HTTPException(
            status_code=401,
            detail="Unauthorized: Valid X-Inferos-Token header required"
        )

@app.middleware("http")
async def security_headers(request: Request, call_next):
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ """
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    return response

# èªè¨¼ä¸è¦ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/v1/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/v1/metrics")
async def get_metrics():
    return get_current_metrics()

# èªè¨¼å¿…é ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/v1/policy")
async def set_policy(
    policy: PolicyConfig,
    _: None = Depends(require_auth)
):
    return apply_policy_config(policy)

@app.post("/v1/config")
async def update_config(
    config: AgentConfig,
    _: None = Depends(require_auth)
):
    return update_agent_config(config)
```

##### **ãƒãƒ¼ãƒˆè¨­å®šã®æŸ”è»ŸåŒ–**
```python
# è¨­å®šå¯èƒ½ãƒãƒ¼ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 7031ï¼‰
DEFAULT_PORT = 7031
AGENT_PORT = int(os.getenv("INFEROS_AGENT_PORT", DEFAULT_PORT))

# ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
if os.getenv("INFEROS_RANDOM_PORT", "false").lower() == "true":
    import socket
    with socket.socket() as s:
        s.bind(('', 0))
        AGENT_PORT = s.getsockname()[1]
    print(f"ğŸ”’ ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒ¼ãƒˆä½¿ç”¨: {AGENT_PORT}")

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="127.0.0.1",  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ›ã‚¹ãƒˆå°‚ç”¨
        port=AGENT_PORT,
        log_level="info"
    )
```

#### **API ã‚¹ã‚­ãƒ¼ãƒå®šç¾©**

##### **POST /v1/policyï¼ˆèªè¨¼å¿…é ˆï¼‰**
```json
{
  "type": "object",
  "properties": {
    "kv": {
      "type": "object",
      "properties": {
        "mode": {
          "type": "string",
          "enum": ["dynamic", "latency_optimized", "throughput_optimized", "quality_optimized"],
          "default": "dynamic"
        },
        "recent_window": {
          "type": "integer",
          "minimum": 16,
          "maximum": 512,
          "default": 64
        },
        "level_thresholds": {
          "type": "object",
          "properties": {
            "L1_int8": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.7},
            "L2_int4": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.5},
            "L3_evict": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.3}
          }
        }
      }
    },
    "io": {
      "type": "object",
      "properties": {
        "enable_iobinding": {"type": "boolean", "default": true},
        "dml_pool_bytes": {
          "oneOf": [
            {"type": "integer", "minimum": 1073741824},
            {"type": "string", "pattern": "^\\d+(\\.\\d+)?(B|KB|MB|GB|MiB|GiB)$"}
          ],
          "default": 2147483648,
          "description": "ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆæ•´æ•°ã¾ãŸã¯äººé–“å¯èª­æ–‡å­—åˆ—ï¼‰"
        },
        "host_pool_bytes": {
          "oneOf": [
            {"type": "integer", "minimum": 1073741824},
            {"type": "string", "pattern": "^\\d+(\\.\\d+)?(B|KB|MB|GB|MiB|GiB)$"}
          ],
          "default": 4294967296,
          "description": "ãƒ›ã‚¹ãƒˆãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆæ•´æ•°ã¾ãŸã¯äººé–“å¯èª­æ–‡å­—åˆ—ï¼‰"
        }
      }
    },
    "scheduler": {
      "type": "object",
      "properties": {
        "mode": {
          "type": "string",
          "enum": ["hybrid", "gpu_only", "npu_only"],
          "default": "hybrid"
        },
        "prefill_device": {
          "type": "string",
          "enum": ["dml", "npu", "cpu"],
          "default": "dml"
        },
        "decode_device": {
          "type": "string",
          "enum": ["npu", "dml", "cpu"],
          "default": "npu"
        }
      }
    },
    "quality": {
      "type": "object",
      "properties": {
        "max_delta_ppl": {"type": "number", "minimum": 0.0, "maximum": 2.0, "default": 0.5},
        "min_accept_rate": {"type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.5}
      }
    }
  }
}
```

##### **å˜ä½å¤‰æ›ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£**
```python
def parse_memory_size(value) -> int:
    """äººé–“å¯èª­ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’ãƒã‚¤ãƒˆæ•´æ•°ã«å¤‰æ›"""
    if isinstance(value, int):
        return value
    
    if isinstance(value, str):
        import re
        match = re.match(r'^(\d+(?:\.\d+)?)(B|KB|MB|GB|MiB|GiB)$', value.upper())
        if not match:
            raise ValueError(f"Invalid memory size format: {value}")
        
        size, unit = match.groups()
        size = float(size)
        
        multipliers = {
            'B': 1,
            'KB': 1000,
            'MB': 1000**2,
            'GB': 1000**3,
            'MIB': 1024**2,
            'GIB': 1024**3
        }
        
        return int(size * multipliers[unit])
    
    raise ValueError(f"Unsupported memory size type: {type(value)}")

# ä½¿ç”¨ä¾‹
policy_data = {
    "io": {
        "dml_pool_bytes": "2048MiB",  # â†’ 2147483648
        "host_pool_bytes": "4GiB"     # â†’ 4294967296
    }
}

# æ­£è¦åŒ–
policy_data["io"]["dml_pool_bytes"] = parse_memory_size(policy_data["io"]["dml_pool_bytes"])
policy_data["io"]["host_pool_bytes"] = parse_memory_size(policy_data["io"]["host_pool_bytes"])
```

##### **GET /v1/metrics ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹**
```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "measurement_window": {
    "duration_seconds": 60,
    "sample_count": 150,
    "description": "ç›´è¿‘60ç§’é–“ã®ç§»å‹•å¹³å‡ï¼ˆ150ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰"
  },
  "throughput": {
    "tokens_per_second": 45.7,
    "requests_per_second": 2.3,
    "calculation": "total_tokens / total_time_seconds",
    "note": "TPS = ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³ç·æ•° / å®Ÿæ™‚é–“, RPS = ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° / å®Ÿæ™‚é–“"
  },
  "latency": {
    "mean_ms": 1250.5,
    "median_ms": 1180.0,
    "p95_ms": 2100.0,
    "p99_ms": 3200.0,
    "std_ms": 450.2,
    "calculation": "end_time - start_time per request"
  },
  "first_token_latency": {
    "mean_ms": 380.2,
    "median_ms": 350.0,
    "p95_ms": 650.0,
    "measurement_method": "å®Ÿæ¸¬ï¼ˆfirst_token_emit_time - request_start_timeï¼‰",
    "note": "æ¨å®šå€¤ã§ã¯ãªãã€å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé–‹å§‹æ™‚åˆ»ã‚’è¨ˆæ¸¬"
  },
  "quality": {
    "delta_ppl_est": {
      "current": 0.12,
      "baseline": 2.45,
      "delta": 0.12,
      "measurement": {
        "method": "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç°¡æ˜“æ¨å®šï¼ˆNLLè¿‘ä¼¼ï¼‰",
        "window_size": 100,
        "update_frequency": "ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¯",
        "last_full_evaluation": "2024-01-15T09:00:00Z"
      }
    },
    "accept_rate": {
      "current": 0.78,
      "target": 0.75,
      "measurement": {
        "method": "ã‚¹ãƒšã‚­ãƒ¥ãƒ¬ã‚¤ãƒ†ã‚£ãƒ–ç”Ÿæˆå—è«¾ç‡",
        "window_size": 50,
        "description": "ç›´è¿‘50å›ã®æ¨è«–ã§ã®å—è«¾ãƒˆãƒ¼ã‚¯ãƒ³ç‡"
      }
    },
    "full_evaluation": {
      "last_run": "2024-01-15T06:00:00Z",
      "next_scheduled": "2024-01-16T06:00:00Z",
      "dataset": "validation_set_500samples",
      "baseline_ppl": 2.45,
      "current_ppl": 2.57,
      "delta_ppl": 0.12,
      "status": "within_threshold"
    }
  },
  "memory": {
    "kv_cache_usage_mb": 1024.5,
    "kv_cache_hit_rate": 0.85,
    "total_allocated_mb": 3072.0,
    "peak_usage_mb": 3456.0
  },
  "device_utilization": {
    "npu_utilization": 0.67,
    "dml_utilization": 0.45,
    "cpu_utilization": 0.23
  },
  "optimization_status": {
    "kv_quantization_active": true,
    "io_binding_active": true,
    "hybrid_scheduling_active": true,
    "current_policy": "dynamic"
  }
}
```

#### **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®—å‡ºå®šç¾©**

##### **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—**
```python
class MetricsCalculator:
    def __init__(self, window_seconds: int = 60):
        self.window_seconds = window_seconds
        self.request_history = []
        
    def calculate_throughput(self) -> Dict[str, float]:
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ï¼ˆæ™‚é–“çª“ãƒ™ãƒ¼ã‚¹ï¼‰"""
        now = time.time()
        cutoff = now - self.window_seconds
        
        # æ™‚é–“çª“å†…ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã¿
        recent_requests = [
            req for req in self.request_history 
            if req['end_time'] >= cutoff
        ]
        
        if not recent_requests:
            return {"tokens_per_second": 0.0, "requests_per_second": 0.0}
        
        total_tokens = sum(req['tokens_generated'] for req in recent_requests)
        total_time = now - min(req['start_time'] for req in recent_requests)
        
        return {
            "tokens_per_second": total_tokens / total_time if total_time > 0 else 0.0,
            "requests_per_second": len(recent_requests) / total_time if total_time > 0 else 0.0
        }
```

##### **First Token Latencyå®Ÿæ¸¬**
```python
class FTLMeasurement:
    def __init__(self):
        self.ftl_samples = []
        
    def measure_generation(self, prompt: str, model, tokenizer):
        """FTLå®Ÿæ¸¬ä»˜ãç”Ÿæˆ"""
        start_time = time.perf_counter()
        first_token_time = None
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã§FTLè¨ˆæ¸¬
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            for i, output_ids in enumerate(model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹åŒ–
                output_scores=True,
                return_dict_in_generate=True
            )):
                if i == 0:  # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³
                    first_token_time = time.perf_counter()
                    break
        
        end_time = time.perf_counter()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        if first_token_time:
            ftl_ms = (first_token_time - start_time) * 1000
            total_latency_ms = (end_time - start_time) * 1000
            
            self.ftl_samples.append({
                "ftl_ms": ftl_ms,
                "total_latency_ms": total_latency_ms,
                "timestamp": time.time()
            })
            
            return {
                "first_token_latency_ms": ftl_ms,
                "total_latency_ms": total_latency_ms,
                "response": tokenizer.decode(output_ids[0], skip_special_tokens=True)
            }
```

##### **å“è³ªæ¸¬å®šï¼ˆPPLï¼‰**
```python
class QualityMeasurement:
    def __init__(self, validation_dataset: List[str]):
        self.validation_dataset = validation_dataset
        self.baseline_ppl = None
        self.online_nll_buffer = []
        
    def calculate_full_ppl(self, model, tokenizer) -> float:
        """å®Œå…¨PPLè¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰"""
        total_nll = 0.0
        total_tokens = 0
        
        model.eval()
        with torch.no_grad():
            for text in self.validation_dataset:
                inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
                
                # è² ã®å¯¾æ•°å°¤åº¦è¨ˆç®—
                outputs = model(**inputs, labels=inputs["input_ids"])
                nll = outputs.loss.item() * inputs["input_ids"].size(1)
                
                total_nll += nll
                total_tokens += inputs["input_ids"].size(1)
        
        ppl = math.exp(total_nll / total_tokens)
        return ppl
    
    def estimate_online_ppl(self, logits: torch.Tensor, targets: torch.Tensor) -> float:
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç°¡æ˜“PPLæ¨å®š"""
        # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        
        # NLLãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        self.online_nll_buffer.append(ce_loss.item())
        
        # çª“ã‚µã‚¤ã‚ºç¶­æŒ
        if len(self.online_nll_buffer) > 100:
            self.online_nll_buffer.pop(0)
        
        # å¹³å‡NLLã‹ã‚‰PPLæ¨å®š
        avg_nll = sum(self.online_nll_buffer) / len(self.online_nll_buffer)
        estimated_ppl = math.exp(avg_nll)
        
        return estimated_ppl
    
    def get_quality_metrics(self) -> Dict:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        current_ppl_est = self.estimate_online_ppl() if self.online_nll_buffer else 0.0
        
        return {
            "delta_ppl_est": {
                "current": current_ppl_est,
                "baseline": self.baseline_ppl or 0.0,
                "delta": current_ppl_est - (self.baseline_ppl or 0.0),
                "measurement": {
                    "method": "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç°¡æ˜“æ¨å®šï¼ˆNLLè¿‘ä¼¼ï¼‰",
                    "window_size": len(self.online_nll_buffer),
                    "update_frequency": "ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¯"
                }
            }
        }
```

#### **å“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ **

##### **å®šæœŸè©•ä¾¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©**
```python
import schedule
import threading
from datetime import datetime, timedelta

class QualityScheduler:
    def __init__(self, quality_measurement: QualityMeasurement):
        self.quality_measurement = quality_measurement
        self.scheduler_thread = None
        self.running = False
        
    def start_scheduler(self):
        """å“è³ªè©•ä¾¡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©é–‹å§‹"""
        # æ¯æ—¥6æ™‚ã«å®Œå…¨è©•ä¾¡
        schedule.every().day.at("06:00").do(self.run_full_evaluation)
        
        # 4æ™‚é–“æ¯ã«è»½é‡è©•ä¾¡
        schedule.every(4).hours.do(self.run_light_evaluation)
        
        self.running = True
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
        self.scheduler_thread.start()
        
    def _scheduler_loop(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¼ãƒ—"""
        while self.running:
            schedule.run_pending()
            time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯
    
    def run_full_evaluation(self):
        """å®Œå…¨å“è³ªè©•ä¾¡å®Ÿè¡Œ"""
        print(f"ğŸ” å®Œå…¨å“è³ªè©•ä¾¡é–‹å§‹: {datetime.now()}")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å–å¾—
            model, tokenizer = self._get_current_model()
            
            # å®Œå…¨PPLè¨ˆç®—
            current_ppl = self.quality_measurement.calculate_full_ppl(model, tokenizer)
            baseline_ppl = self.quality_measurement.baseline_ppl or current_ppl
            
            # å“è³ªåŠ£åŒ–ãƒã‚§ãƒƒã‚¯
            delta_ppl = current_ppl - baseline_ppl
            if delta_ppl > 0.5:  # é–¾å€¤è¶…é
                self._trigger_quality_alert(delta_ppl, current_ppl, baseline_ppl)
            
            # çµæœè¨˜éŒ²
            self._record_evaluation_result({
                "timestamp": datetime.now().isoformat(),
                "type": "full_evaluation",
                "current_ppl": current_ppl,
                "baseline_ppl": baseline_ppl,
                "delta_ppl": delta_ppl,
                "status": "alert" if delta_ppl > 0.5 else "normal"
            })
            
            print(f"âœ… å®Œå…¨å“è³ªè©•ä¾¡å®Œäº†: Î”PPL={delta_ppl:.3f}")
            
        except Exception as e:
            print(f"âŒ å®Œå…¨å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            self._record_evaluation_result({
                "timestamp": datetime.now().isoformat(),
                "type": "full_evaluation",
                "status": "error",
                "error": str(e)
            })
    
    def _trigger_quality_alert(self, delta_ppl: float, current_ppl: float, baseline_ppl: float):
        """å“è³ªåŠ£åŒ–ã‚¢ãƒ©ãƒ¼ãƒˆ"""
        alert_message = f"""
ğŸš¨ å“è³ªåŠ£åŒ–ã‚¢ãƒ©ãƒ¼ãƒˆ
æ™‚åˆ»: {datetime.now()}
Î”PPL: {delta_ppl:.3f} (é–¾å€¤: 0.5)
ç¾åœ¨PPL: {current_ppl:.3f}
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³PPL: {baseline_ppl:.3f}
æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: KVé‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã®ç·©å’Œã‚’æ¤œè¨
"""
        
        print(alert_message)
        
        # ãƒ­ã‚°è¨˜éŒ²
        with open("quality_alerts.log", "a") as f:
            f.write(f"{datetime.now().isoformat()}: QUALITY_ALERT delta_ppl={delta_ppl:.3f}\n")
        
        # è‡ªå‹•å›å¾©è©¦è¡Œ
        self._attempt_quality_recovery()
    
    def _attempt_quality_recovery(self):
        """å“è³ªè‡ªå‹•å›å¾©"""
        print("ğŸ”„ å“è³ªè‡ªå‹•å›å¾©ã‚’è©¦è¡Œä¸­...")
        
        # KVé‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã‚’æ®µéšçš„ã«ç·©å’Œ
        recovery_policies = [
            {"kv": {"mode": "quality_optimized"}},
            {"kv": {"level_thresholds": {"L2_int4": 0.3, "L1_int8": 0.5}}},
            {"kv": {"mode": "latency_optimized"}}  # æœ€çµ‚æ‰‹æ®µ
        ]
        
        for i, policy in enumerate(recovery_policies):
            try:
                # ãƒãƒªã‚·ãƒ¼é©ç”¨
                self._apply_recovery_policy(policy)
                
                # çŸ­æ™‚é–“å¾…æ©Ÿ
                time.sleep(30)
                
                # è»½é‡å“è³ªãƒã‚§ãƒƒã‚¯
                if self._quick_quality_check():
                    print(f"âœ… å“è³ªå›å¾©æˆåŠŸ: ãƒãƒªã‚·ãƒ¼{i+1}")
                    return True
                    
            except Exception as e:
                print(f"âš ï¸  å›å¾©ãƒãƒªã‚·ãƒ¼{i+1}å¤±æ•—: {e}")
        
        print("âŒ è‡ªå‹•å“è³ªå›å¾©å¤±æ•—: æ‰‹å‹•ä»‹å…¥ãŒå¿…è¦")
        return False
#### **é‹ç”¨è€æ€§ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–**

##### **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã¨ãƒãƒƒã‚¯ã‚ªãƒ•**
```python
import time
import random
from enum import Enum
from typing import Dict, List, Optional

class FallbackReason(Enum):
    QUALITY_DEGRADATION = "quality_degradation"
    DEVICE_ERROR = "device_error"
    MEMORY_PRESSURE = "memory_pressure"
    TIMEOUT = "timeout"
    UNKNOWN = "unknown"

class BackoffStrategy:
    def __init__(self, initial_delay: float = 1.0, max_delay: float = 60.0, multiplier: float = 2.0):
        self.initial_delay = initial_delay
        self.max_delay = max_delay
        self.multiplier = multiplier
        self.current_delay = initial_delay
        self.attempt_count = 0
    
    def get_delay(self) -> float:
        """ãƒãƒƒã‚¯ã‚ªãƒ•é…å»¶æ™‚é–“å–å¾—"""
        if self.attempt_count == 0:
            delay = self.initial_delay
        else:
            # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + ã‚¸ãƒƒã‚¿ãƒ¼
            delay = min(self.current_delay * self.multiplier, self.max_delay)
            jitter = random.uniform(0.1, 0.3) * delay
            delay = delay + jitter
        
        self.current_delay = delay
        self.attempt_count += 1
        return delay
    
    def reset(self):
        """ãƒãƒƒã‚¯ã‚ªãƒ•çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        self.current_delay = self.initial_delay
        self.attempt_count = 0

class RobustFallbackManager:
    def __init__(self):
        self.fallback_history = []
        self.backoff_strategies = {
            FallbackReason.QUALITY_DEGRADATION: BackoffStrategy(2.0, 120.0, 1.5),
            FallbackReason.DEVICE_ERROR: BackoffStrategy(5.0, 300.0, 2.0),
            FallbackReason.MEMORY_PRESSURE: BackoffStrategy(1.0, 60.0, 1.8),
            FallbackReason.TIMEOUT: BackoffStrategy(3.0, 180.0, 2.2),
        }
        self.max_consecutive_fallbacks = 5
        self.notification_thresholds = {
            FallbackReason.QUALITY_DEGRADATION: 3,
            FallbackReason.DEVICE_ERROR: 2,
            FallbackReason.MEMORY_PRESSURE: 4,
            FallbackReason.TIMEOUT: 3,
        }
    
    def handle_fallback(self, reason: FallbackReason, context: Dict) -> Dict:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†"""
        timestamp = time.time()
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±¥æ­´è¨˜éŒ²
        fallback_event = {
            "timestamp": timestamp,
            "reason": reason,
            "context": context,
            "attempt_count": self.backoff_strategies[reason].attempt_count + 1
        }
        self.fallback_history.append(fallback_event)
        
        # é€£ç¶šãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ•°ãƒã‚§ãƒƒã‚¯
        recent_fallbacks = [
            event for event in self.fallback_history
            if timestamp - event["timestamp"] < 300  # 5åˆ†ä»¥å†…
        ]
        
        if len(recent_fallbacks) >= self.max_consecutive_fallbacks:
            return self._handle_critical_fallback(reason, context)
        
        # é€šçŸ¥åˆ¤å®š
        reason_count = sum(1 for event in recent_fallbacks if event["reason"] == reason)
        if reason_count >= self.notification_thresholds.get(reason, 3):
            self._send_notification(reason, reason_count, context)
        
        # ãƒãƒƒã‚¯ã‚ªãƒ•é…å»¶
        delay = self.backoff_strategies[reason].get_delay()
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
        fallback_result = self._execute_fallback(reason, context)
        
        return {
            "status": "fallback_executed",
            "reason": reason.value,
            "delay_seconds": delay,
            "result": fallback_result,
            "next_retry": timestamp + delay
        }
    
    def _execute_fallback(self, reason: FallbackReason, context: Dict) -> Dict:
        """å…·ä½“çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ"""
        if reason == FallbackReason.QUALITY_DEGRADATION:
            return self._quality_degradation_fallback(context)
        elif reason == FallbackReason.DEVICE_ERROR:
            return self._device_error_fallback(context)
        elif reason == FallbackReason.MEMORY_PRESSURE:
            return self._memory_pressure_fallback(context)
        elif reason == FallbackReason.TIMEOUT:
            return self._timeout_fallback(context)
        else:
            return self._generic_fallback(context)
    
    def _quality_degradation_fallback(self, context: Dict) -> Dict:
        """å“è³ªåŠ£åŒ–ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        # KVé‡å­åŒ–ãƒ¬ãƒ™ãƒ«ç·©å’Œ
        current_policy = context.get("current_policy", {})
        
        fallback_policies = [
            # Level 1: INT4é–¾å€¤ã‚’ä¸‹ã’ã‚‹
            {
                "kv": {
                    "level_thresholds": {
                        "L2_int4": max(0.2, current_policy.get("kv", {}).get("level_thresholds", {}).get("L2_int4", 0.5) - 0.1),
                        "L1_int8": max(0.4, current_policy.get("kv", {}).get("level_thresholds", {}).get("L1_int8", 0.7) - 0.1)
                    }
                }
            },
            # Level 2: å“è³ªå„ªå…ˆãƒ¢ãƒ¼ãƒ‰
            {"kv": {"mode": "quality_optimized"}},
            # Level 3: KVé‡å­åŒ–ç„¡åŠ¹
            {"kv": {"mode": "disabled"}}
        ]
        
        for i, policy in enumerate(fallback_policies):
            try:
                self._apply_policy(policy)
                return {"level": i + 1, "policy": policy, "status": "applied"}
            except Exception as e:
                continue
        
        return {"status": "all_fallbacks_failed"}
    
    def _device_error_fallback(self, context: Dict) -> Dict:
        """ãƒ‡ãƒã‚¤ã‚¹ã‚¨ãƒ©ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        # ãƒ‡ãƒã‚¤ã‚¹åˆ‡æ›¿é †åº
        device_fallback_order = ["cpu", "dml", "npu"]
        current_device = context.get("current_device", "npu")
        
        try:
            current_index = device_fallback_order.index(current_device)
            next_devices = device_fallback_order[current_index + 1:]
        except ValueError:
            next_devices = device_fallback_order
        
        for device in next_devices:
            try:
                self._switch_device(device)
                return {"fallback_device": device, "status": "switched"}
            except Exception as e:
                continue
        
        return {"status": "no_available_device"}
    
    def _memory_pressure_fallback(self, context: Dict) -> Dict:
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç­–
        reduction_strategies = [
            # Level 1: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºå‰Šæ¸›
            {"action": "reduce_kv_cache", "factor": 0.7},
            # Level 2: ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›
            {"action": "reduce_batch_size", "factor": 0.5},
            # Level 3: ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–å¼·åŒ–
            {"action": "aggressive_quantization"},
            # Level 4: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
            {"action": "force_gc"}
        ]
        
        for strategy in reduction_strategies:
            try:
                result = self._apply_memory_reduction(strategy)
                if result.get("memory_freed_mb", 0) > 100:  # 100MBä»¥ä¸Šè§£æ”¾
                    return {"strategy": strategy, "result": result, "status": "effective"}
            except Exception as e:
                continue
        
        return {"status": "memory_reduction_failed"}
    
    def _send_notification(self, reason: FallbackReason, count: int, context: Dict):
        """é‹ç”¨é€šçŸ¥é€ä¿¡"""
        notification = {
            "timestamp": time.time(),
            "severity": "warning" if count < 5 else "critical",
            "reason": reason.value,
            "count": count,
            "message": f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é »ç™º: {reason.value} ({count}å›/5åˆ†)",
            "context": context,
            "recommended_action": self._get_recommended_action(reason)
        }
        
        # ãƒ­ã‚°è¨˜éŒ²
        with open("fallback_notifications.log", "a") as f:
            import json
            f.write(f"{json.dumps(notification)}\n")
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        severity_emoji = "âš ï¸" if notification["severity"] == "warning" else "ğŸš¨"
        print(f"{severity_emoji} {notification['message']}")
        print(f"æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {notification['recommended_action']}")
    
    def _get_recommended_action(self, reason: FallbackReason) -> str:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"""
        recommendations = {
            FallbackReason.QUALITY_DEGRADATION: "ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´ã¾ãŸã¯ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¦‹ç›´ã—ã‚’æ¤œè¨",
            FallbackReason.DEVICE_ERROR: "ãƒ‡ãƒã‚¤ã‚¹ãƒ‰ãƒ©ã‚¤ãƒãƒ¼æ›´æ–°ã¾ãŸã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¨ºæ–­ã‚’å®Ÿè¡Œ",
            FallbackReason.MEMORY_PRESSURE: "ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªå¢—è¨­ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›ã‚’æ¤œè¨",
            FallbackReason.TIMEOUT: "æ¨è«–ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®è¦‹ç›´ã—ã¾ãŸã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ€§èƒ½å‘ä¸Šã‚’æ¤œè¨"
        }
        return recommendations.get(reason, "æŠ€è¡“ã‚µãƒãƒ¼ãƒˆã«é€£çµ¡")
```

##### **Windowsã‚µãƒ¼ãƒ“ã‚¹é‹ç”¨å¼·åŒ–**
```powershell
# Windows Service å®Œå…¨é‹ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

# 1. å°‚ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
$ServiceUser = "InferOSAgent"
$ServicePassword = -join ((65..90) + (97..122) + (48..57) | Get-Random -Count 16 | % {[char]$_})

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
net user $ServiceUser $ServicePassword /add /comment:"Infer-OS Control Agent Service Account"
net localgroup "Log on as a service" $ServiceUser /add

# 2. ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
$ServiceConfig = @"
[Service]
Name=InferOSControlAgent
DisplayName=Infer-OS Control Agent
Description=AI inference optimization control agent for GAIA integration
User=$ServiceUser
Password=$ServicePassword
WorkingDirectory=C:\InferOS\Agent
ExePath=C:\InferOS\Agent\venv\Scripts\python.exe
Arguments=C:\InferOS\Agent\inferos_control_agent.py
LogPath=C:\InferOS\Logs\agent.log
MaxLogSize=100MB
LogRotation=daily
RestartPolicy=always
RestartDelay=30
Environment=INFEROS_AGENT_TOKEN=$env:INFEROS_AGENT_TOKEN;INFEROS_AGENT_PORT=7031
"@

$ServiceConfig | Out-File -FilePath "C:\InferOS\Config\service.conf" -Encoding UTF8

# 3. NSSMä½¿ç”¨ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
nssm install InferOSControlAgent "C:\InferOS\Agent\venv\Scripts\python.exe"
nssm set InferOSControlAgent Arguments "C:\InferOS\Agent\inferos_control_agent.py"
nssm set InferOSControlAgent AppDirectory "C:\InferOS\Agent"
nssm set InferOSControlAgent ObjectName ".\$ServiceUser" $ServicePassword
nssm set InferOSControlAgent DisplayName "Infer-OS Control Agent"
nssm set InferOSControlAgent Description "AI inference optimization control agent for GAIA integration"

# ãƒ­ã‚°è¨­å®š
nssm set InferOSControlAgent AppStdout "C:\InferOS\Logs\agent_stdout.log"
nssm set InferOSControlAgent AppStderr "C:\InferOS\Logs\agent_stderr.log"
nssm set InferOSControlAgent AppRotateFiles 1
nssm set InferOSControlAgent AppRotateOnline 1
nssm set InferOSControlAgent AppRotateBytes 10485760  # 10MB

# å†èµ·å‹•è¨­å®š
nssm set InferOSControlAgent AppExit Default Restart
nssm set InferOSControlAgent AppRestartDelay 30000  # 30ç§’

# ç’°å¢ƒå¤‰æ•°è¨­å®š
nssm set InferOSControlAgent AppEnvironmentExtra "INFEROS_AGENT_TOKEN=$env:INFEROS_AGENT_TOKEN"

# 4. ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š
New-NetFirewallRule -DisplayName "Infer-OS Control Agent" -Direction Inbound -Protocol TCP -LocalPort 7031 -LocalAddress 127.0.0.1 -Action Allow

# 5. æ¨©é™è¨­å®š
icacls "C:\InferOS\Agent" /grant "${ServiceUser}:(OI)(CI)RX" /T
icacls "C:\InferOS\Logs" /grant "${ServiceUser}:(OI)(CI)F" /T
icacls "C:\InferOS\Config" /grant "${ServiceUser}:(OI)(CI)R" /T

# 6. ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
nssm start InferOSControlAgent

# 7. ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
$MonitorScript = @'
# Infer-OS Agent ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
$ServiceName = "InferOSControlAgent"
$LogPath = "C:\InferOS\Logs\monitor.log"
$AlertThresholds = @{
    CPUPercent = 80
    MemoryMB = 2048
    ResponseTimeMs = 5000
}

while ($true) {
    try {
        # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
        $service = Get-Service -Name $ServiceName -ErrorAction SilentlyContinue
        if ($service.Status -ne "Running") {
            $message = "$(Get-Date): ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢æ¤œå‡º - å†èµ·å‹•è©¦è¡Œ"
            Add-Content -Path $LogPath -Value $message
            Start-Service -Name $ServiceName
        }
        
        # APIå¿œç­”ãƒã‚§ãƒƒã‚¯
        $response = Invoke-WebRequest -Uri "http://127.0.0.1:7031/v1/health" -TimeoutSec 5 -ErrorAction SilentlyContinue
        if ($response.StatusCode -ne 200) {
            $message = "$(Get-Date): APIå¿œç­”ç•°å¸¸ - ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•"
            Add-Content -Path $LogPath -Value $message
            Restart-Service -Name $ServiceName
        }
        
        # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        $process = Get-Process -Name "python" | Where-Object {$_.MainWindowTitle -like "*inferos*"}
        if ($process) {
            $cpuPercent = $process.CPU
            $memoryMB = $process.WorkingSet64 / 1MB
            
            if ($cpuPercent -gt $AlertThresholds.CPUPercent) {
                $message = "$(Get-Date): é«˜CPUä½¿ç”¨ç‡è­¦å‘Š: $cpuPercent%"
                Add-Content -Path $LogPath -Value $message
            }
            
            if ($memoryMB -gt $AlertThresholds.MemoryMB) {
                $message = "$(Get-Date): é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è­¦å‘Š: $memoryMB MB"
                Add-Content -Path $LogPath -Value $message
            }
        }
        
    } catch {
        $message = "$(Get-Date): ç›£è¦–ã‚¨ãƒ©ãƒ¼: $($_.Exception.Message)"
        Add-Content -Path $LogPath -Value $message
    }
    
    Start-Sleep -Seconds 60
}
'@

$MonitorScript | Out-File -FilePath "C:\InferOS\Scripts\monitor.ps1" -Encoding UTF8

# 8. ç›£è¦–ã‚¿ã‚¹ã‚¯ä½œæˆ
$TaskAction = New-ScheduledTaskAction -Execute "PowerShell.exe" -Argument "-File C:\InferOS\Scripts\monitor.ps1"
$TaskTrigger = New-ScheduledTaskTrigger -AtStartup
$TaskSettings = New-ScheduledTaskSettingsSet -RestartCount 3 -RestartInterval (New-TimeSpan -Minutes 5)
$TaskPrincipal = New-ScheduledTaskPrincipal -UserId $ServiceUser -LogonType ServiceAccount

Register-ScheduledTask -TaskName "InferOSAgentMonitor" -Action $TaskAction -Trigger $TaskTrigger -Settings $TaskSettings -Principal $TaskPrincipal

# 9. ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
$UninstallScript = @"
# Infer-OS Agent å®Œå…¨ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
Write-Host "Infer-OS Control Agent ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹..."

# ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢ãƒ»å‰Šé™¤
nssm stop InferOSControlAgent
nssm remove InferOSControlAgent confirm

# ã‚¿ã‚¹ã‚¯å‰Šé™¤
Unregister-ScheduledTask -TaskName "InferOSAgentMonitor" -Confirm:`$false

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¦å‰‡å‰Šé™¤
Remove-NetFirewallRule -DisplayName "Infer-OS Control Agent" -ErrorAction SilentlyContinue

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å‰Šé™¤
net user $ServiceUser /delete

# ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
Remove-Item -Path "C:\InferOS" -Recurse -Force -ErrorAction SilentlyContinue

Write-Host "ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†"
"@

$UninstallScript | Out-File -FilePath "C:\InferOS\Scripts\uninstall.ps1" -Encoding UTF8

Write-Host "Infer-OS Control Agent ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šå®Œäº†"
Write-Host "ã‚µãƒ¼ãƒ“ã‚¹ãƒ¦ãƒ¼ã‚¶ãƒ¼: $ServiceUser"
Write-Host "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰: $ServicePassword"
Write-Host "ç›£è¦–: ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§è‡ªå‹•å®Ÿè¡Œ"
Write-Host "ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: C:\InferOS\Scripts\uninstall.ps1 ã‚’å®Ÿè¡Œ"
```

##### **DirectML/NPUæ¼”ç®—å­ã‚µãƒãƒ¼ãƒˆç¢ºèª**
```python
import onnxruntime as ort
from typing import Dict, List, Set

class DeviceCapabilityChecker:
    def __init__(self):
        self.dml_supported_ops = set()
        self.npu_supported_ops = set()
        self.capability_cache = {}
        
    def check_directml_quantization_support(self) -> Dict[str, bool]:
        """DirectMLé‡å­åŒ–ã‚µãƒãƒ¼ãƒˆç¢ºèª"""
        try:
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True
                })
            ]
            
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # ãƒ†ã‚¹ãƒˆç”¨ç°¡å˜ãªãƒ¢ãƒ‡ãƒ«ä½œæˆ
            test_model = self._create_quantization_test_model()
            
            # INT8ã‚µãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            int8_support = self._test_quantization_support(test_model, "int8", providers)
            
            # INT4ã‚µãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            int4_support = self._test_quantization_support(test_model, "int4", providers)
            
            # FP16ã‚µãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            fp16_support = self._test_fp16_support(providers)
            
            return {
                "int8_quantization": int8_support,
                "int4_quantization": int4_support,
                "fp16_mixed_precision": fp16_support,
                "dynamic_quantization": int8_support,  # INT8ãƒ™ãƒ¼ã‚¹
                "static_quantization": int8_support and self._test_static_quantization(providers)
            }
            
        except Exception as e:
            return {
                "int8_quantization": False,
                "int4_quantization": False,
                "fp16_mixed_precision": False,
                "dynamic_quantization": False,
                "static_quantization": False,
                "error": str(e)
            }
    
    def get_supported_operators(self, provider: str) -> Set[str]:
        """ã‚µãƒãƒ¼ãƒˆæ¼”ç®—å­ä¸€è¦§å–å¾—"""
        if provider in self.capability_cache:
            return self.capability_cache[provider]
        
        try:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®æ¼”ç®—å­ãƒªã‚¹ãƒˆå–å¾—
            if provider == "DmlExecutionProvider":
                supported_ops = self._get_dml_operators()
            elif provider == "NPUExecutionProvider":
                supported_ops = self._get_npu_operators()
            else:
                supported_ops = set()
            
            self.capability_cache[provider] = supported_ops
            return supported_ops
            
        except Exception:
            return set()
    
    def create_fallback_table(self) -> Dict[str, List[Dict]]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¡¨ä½œæˆ"""
        dml_caps = self.check_directml_quantization_support()
        
        fallback_scenarios = {
            "quantization_failure": [
                {
                    "condition": "INT4é‡å­åŒ–å¤±æ•—",
                    "action": "INT8é‡å­åŒ–ã«åˆ‡æ›¿",
                    "supported": dml_caps.get("int8_quantization", False)
                },
                {
                    "condition": "INT8é‡å­åŒ–å¤±æ•—",
                    "action": "FP16æ··åˆç²¾åº¦ã«åˆ‡æ›¿",
                    "supported": dml_caps.get("fp16_mixed_precision", False)
                },
                {
                    "condition": "FP16å¤±æ•—",
                    "action": "FP32ãƒ•ãƒ«ç²¾åº¦ã«åˆ‡æ›¿",
                    "supported": True  # å¸¸ã«ã‚µãƒãƒ¼ãƒˆ
                }
            ],
            "device_unavailable": [
                {
                    "condition": "NPUåˆ©ç”¨ä¸å¯",
                    "action": "DirectML(iGPU)ã«åˆ‡æ›¿",
                    "check_method": "self._check_dml_availability()"
                },
                {
                    "condition": "DirectMLåˆ©ç”¨ä¸å¯",
                    "action": "CPUæ¨è«–ã«åˆ‡æ›¿",
                    "check_method": "True"  # CPUå¸¸ã«åˆ©ç”¨å¯èƒ½
                }
            ],
            "memory_pressure": [
                {
                    "condition": "KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªä¸è¶³",
                    "action": "é‡å­åŒ–ãƒ¬ãƒ™ãƒ«å¼·åŒ–",
                    "parameters": {"L2_int4": 0.3, "L1_int8": 0.5}
                },
                {
                    "condition": "ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä¸è¶³",
                    "action": "ãƒ¢ãƒ‡ãƒ«åˆ†å‰²å®Ÿè¡Œ",
                    "parameters": {"chunk_size": 512, "overlap": 64}
                }
            ],
            "performance_degradation": [
                {
                    "condition": "TPS < é–¾å€¤ã®50%",
                    "action": "æœ€é©åŒ–ãƒãƒªã‚·ãƒ¼ç·©å’Œ",
                    "parameters": {"mode": "latency_optimized"}
                },
                {
                    "condition": "å“è³ªåŠ£åŒ– Î”PPL > 0.5",
                    "action": "å“è³ªå„ªå…ˆãƒ¢ãƒ¼ãƒ‰ã«åˆ‡æ›¿",
                    "parameters": {"mode": "quality_optimized"}
                }
            ]
        }
        
        return fallback_scenarios
    
    def _test_quantization_support(self, model_path: str, quant_type: str, providers: List) -> bool:
        """é‡å­åŒ–ã‚µãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        try:
            if quant_type == "int8":
                # INT8å‹•çš„é‡å­åŒ–ãƒ†ã‚¹ãƒˆ
                from onnxruntime.quantization import quantize_dynamic, QuantType
                quantized_model = f"test_model_int8.onnx"
                quantize_dynamic(model_path, quantized_model, weight_type=QuantType.QInt8)
                
            elif quant_type == "int4":
                # INT4é‡å­åŒ–ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿè£…ä¾å­˜ï¼‰
                quantized_model = f"test_model_int4.onnx"
                # INT4é‡å­åŒ–å®Ÿè£…ï¼ˆç°¡ç•¥åŒ–ï¼‰
                return self._test_int4_quantization(model_path, quantized_model)
            
            # é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆãƒ†ã‚¹ãƒˆ
            session = ort.InferenceSession(quantized_model, providers=providers)
            
            # ç°¡å˜ãªæ¨è«–ãƒ†ã‚¹ãƒˆ
            input_name = session.get_inputs()[0].name
            input_shape = session.get_inputs()[0].shape
            
            import numpy as np
            test_input = np.random.randn(*[1 if dim is None else dim for dim in input_shape]).astype(np.float32)
            
            outputs = session.run(None, {input_name: test_input})
            
            return True
            
        except Exception as e:
            print(f"é‡å­åŒ–ãƒ†ã‚¹ãƒˆå¤±æ•— ({quant_type}): {e}")
            return False
```
  "ftl_ms": 245,
  "mem": {
    "vram_gb": 8.9,
    "system_gb": 16.2,
    "kv_cache_gb": 2.1
  },
  "util": {
    "npu": 0.75,
    "igpu": 0.45,
    "cpu": 0.32
  },
  "quality": {
    "delta_ppl_est": 0.42,
    "accept_rate": 0.87
  },
  "kv_stats": {
    "total_chunks": 1250,
    "level_distribution": {
      "L0": 320,
      "L1": 450,
      "L2": 380,
      "L3": 100
    },
    "compression_ratio": 0.35,
    "hit_rate": 0.92
  }
}
```

##### **ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹**
```json
{
  "error": {
    "code": 400,
    "message": "Invalid policy configuration",
    "details": "recent_window must be between 16 and 512"
  }
}
```

## ğŸš€ **Phase 1: ã‚µã‚¤ãƒ‰ã‚«ãƒ¼çµ±åˆï¼ˆæ¨å¥¨é–‹å§‹ç‚¹ï¼‰**

### **1.1 Infer-OS Control Agentã®èµ·å‹•**

#### **åŸºæœ¬èµ·å‹•**
```bash
python inferos_control_agent.py
```

#### **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä»˜ãèµ·å‹•**
```bash
# config.yaml ã‚’ä½œæˆ
cat > config.yaml << EOF
infer_os:
  quality:
    max_delta_ppl: 0.5
    min_accept_rate: 0.5
  kv:
    recent_window: 64
    level_thresholds:
      L1_int8: 0.7
      L2_int4: 0.5
      L3_evict: 0.3
  io:
    enable_iobinding: true
    dml_pool_bytes: 2048MiB
    host_pool_bytes: 4096MiB
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
  loops:
    fast_ms: 1
    mid_ms: 10
    slow_ms: 100
EOF

python inferos_control_agent.py --config config.yaml
```

#### **Windowsã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦ç™»éŒ²ï¼ˆæ¨å¥¨ï¼‰**

##### **æ–¹æ³•1: NSSM (Non-Sucking Service Manager) ä½¿ç”¨**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã

# NSSMã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://nssm.cc/download ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# nssm.exe ã‚’ C:\Windows\System32 ã«ã‚³ãƒ”ãƒ¼

# ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
nssm install InferOSAgent

# NSSM GUIãŒé–‹ãã®ã§ä»¥ä¸‹ã‚’è¨­å®š:
# Application tab:
#   Path: C:\Python311\python.exe
#   Startup directory: C:\path\to\infer-os
#   Arguments: inferos_control_agent.py --config config.yaml

# Details tab:
#   Display name: Infer-OS Control Agent
#   Description: AI inference optimization service for GAIA integration

# Log on tab:
#   This account: .\InferOSService (å°‚ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¨å¥¨)

# I/O tab:
#   Output (stdout): C:\ProgramData\InferOS\logs\stdout.log
#   Error (stderr): C:\ProgramData\InferOS\logs\stderr.log

# Rotation tab:
#   Replace existing Output/Error files: ãƒã‚§ãƒƒã‚¯
#   Rotate files: ãƒã‚§ãƒƒã‚¯
#   Restrict rotation to files older than: 7 days

# Install service ã‚’ã‚¯ãƒªãƒƒã‚¯
```

##### **æ–¹æ³•2: sc create ã‚³ãƒãƒ³ãƒ‰ä½¿ç”¨**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir C:\ProgramData\InferOS\logs

# ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆ
sc create InferOSAgent ^
  binPath= "C:\Python311\python.exe C:\path\to\infer-os\inferos_control_agent.py --service" ^
  DisplayName= "Infer-OS Control Agent" ^
  Description= "AI inference optimization service for GAIA integration" ^
  start= auto ^
  obj= ".\InferOSService" ^
  password= "YourServicePassword"

# ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
sc config InferOSAgent depend= "Tcpip/Afd"
sc failure InferOSAgent reset= 86400 actions= restart/5000/restart/10000/restart/30000

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š
netsh advfirewall firewall add rule name="InferOS Agent" dir=in action=allow protocol=TCP localport=7031
```

##### **æ–¹æ³•3: ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ä½¿ç”¨**
```cmd
# ç®¡ç†è€…æ¨©é™ã§ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é–‹ã

# ã‚¿ã‚¹ã‚¯ä½œæˆ
schtasks /create /tn "InferOS Agent" /tr "C:\Python311\python.exe C:\path\to\infer-os\inferos_control_agent.py" /sc onstart /ru "SYSTEM" /rl highest

# ã‚¿ã‚¹ã‚¯è¨­å®šã®è©³ç´°åŒ–
schtasks /change /tn "InferOS Agent" /st 00:00 /ri 1 /du 9999:59

# ã‚¿ã‚¹ã‚¯é–‹å§‹
schtasks /run /tn "InferOS Agent"
```

#### **ã‚µãƒ¼ãƒ“ã‚¹é‹ç”¨ç®¡ç†**

##### **ã‚µãƒ¼ãƒ“ã‚¹åˆ¶å¾¡ã‚³ãƒãƒ³ãƒ‰**
```cmd
# ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹
net start InferOSAgent
# ã¾ãŸã¯
sc start InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
net stop InferOSAgent
# ã¾ãŸã¯
sc stop InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
sc query InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹è¨­å®šç¢ºèª
sc qc InferOSAgent

# ã‚µãƒ¼ãƒ“ã‚¹å‰Šé™¤ï¼ˆã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ï¼‰
sc delete InferOSAgent
```

##### **ãƒ­ã‚°ç®¡ç†è¨­å®š**
```cmd
# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ
C:\ProgramData\InferOS\
â”œâ”€â”€ logs\
â”‚   â”œâ”€â”€ stdout.log          # æ¨™æº–å‡ºåŠ›
â”‚   â”œâ”€â”€ stderr.log          # ã‚¨ãƒ©ãƒ¼å‡ºåŠ›
â”‚   â”œâ”€â”€ inferos_agent.log   # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
â”‚   â””â”€â”€ archived\           # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿ãƒ­ã‚°
â”œâ”€â”€ config\
â”‚   â”œâ”€â”€ config.yaml         # ãƒ¡ã‚¤ãƒ³è¨­å®š
â”‚   â””â”€â”€ policy.json         # ãƒãƒªã‚·ãƒ¼è¨­å®š
â””â”€â”€ temp\
    â””â”€â”€ onnx_cache\         # ONNXå¤‰æ›ã‚­ãƒ£ãƒƒã‚·ãƒ¥

# ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆPowerShellï¼‰
$logConfig = @"
<configuration>
  <appender name="RollingFile" type="log4net.Appender.RollingFileAppender">
    <file value="C:\ProgramData\InferOS\logs\inferos_agent.log" />
    <appendToFile value="true" />
    <rollingStyle value="Size" />
    <maxSizeRollBackups value="10" />
    <maximumFileSize value="10MB" />
    <staticLogFileName value="true" />
    <layout type="log4net.Layout.PatternLayout">
      <conversionPattern value="%date [%thread] %-5level %logger - %message%newline" />
    </layout>
  </appender>
</configuration>
"@

$logConfig | Out-File -FilePath "C:\ProgramData\InferOS\config\log4net.config" -Encoding UTF8
```

#### **æ¨©é™ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š**

##### **å°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ**
```cmd
# å°‚ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
net user InferOSService "ComplexPassword123!" /add /comment:"Infer-OS Service Account"

# å¿…è¦ãªæ¨©é™ä»˜ä¸
ntrights +r SeServiceLogonRight -u InferOSService
ntrights +r SeIncreaseQuotaPrivilege -u InferOSService
ntrights +r SeAssignPrimaryTokenPrivilege -u InferOSService

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¨©é™è¨­å®š
icacls "C:\ProgramData\InferOS" /grant InferOSService:(OI)(CI)F
icacls "C:\path\to\infer-os" /grant InferOSService:(OI)(CI)RX
```

##### **ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š**
```cmd
# Infer-OS Agent API ãƒãƒ¼ãƒˆè¨±å¯
netsh advfirewall firewall add rule name="InferOS Agent API" dir=in action=allow protocol=TCP localport=7031 profile=private

# ç‰¹å®šIPã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã®ã¿è¨±å¯ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ï¼‰
netsh advfirewall firewall add rule name="InferOS Agent API Restricted" dir=in action=allow protocol=TCP localport=7031 remoteip=127.0.0.1,192.168.1.0/24

# ãƒ­ã‚°æœ‰åŠ¹åŒ–
netsh advfirewall set allprofiles logging filename C:\ProgramData\InferOS\logs\firewall.log
netsh advfirewall set allprofiles logging maxfilesize 4096
netsh advfirewall set allprofiles logging droppedconnections enable
```

#### **ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š**

##### **Windows ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°çµ±åˆ**
```python
# inferos_control_agent.py ã§ã®å®Ÿè£…ä¾‹
import logging
import logging.handlers

def setup_windows_event_logging():
    """Windowsã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°è¨­å®š"""
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        event_handler = logging.handlers.NTEventLogHandler(
            appname="InferOS Agent",
            dllname=None,
            logtype="Application"
        )
        event_handler.setLevel(logging.WARNING)
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        event_handler.setFormatter(formatter)
        
        # ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼ã«è¿½åŠ 
        logging.getLogger().addHandler(event_handler)
        
    except Exception as e:
        print(f"ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
```

##### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ç™»éŒ²**
```cmd
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ä½œæˆ
lodctr /R

# ã‚«ã‚¹ã‚¿ãƒ ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
echo [info] > inferos_counters.ini
echo drivername=InferOS Agent >> inferos_counters.ini
echo symbolfile=inferos_counters.h >> inferos_counters.ini
echo [objects] >> inferos_counters.ini
echo INFEROS_OBJECT_1_009_NAME=InferOS Performance >> inferos_counters.ini

# ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ç™»éŒ²
lodctr inferos_counters.ini
```

#### **ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †**

##### **å®Œå…¨å‰Šé™¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```cmd
@echo off
echo Infer-OS Agent ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...

# ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢
net stop InferOSAgent 2>nul
sc delete InferOSAgent 2>nul

# ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‹ã‚‰å‰Šé™¤
schtasks /delete /tn "InferOS Agent" /f 2>nul

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ãƒ«ãƒ¼ãƒ«å‰Šé™¤
netsh advfirewall firewall delete rule name="InferOS Agent API" 2>nul
netsh advfirewall firewall delete rule name="InferOS Agent API Restricted" 2>nul

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå‰Šé™¤
net user InferOSService /delete 2>nul

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ï¼ˆãƒ‡ãƒ¼ã‚¿ä¿æŒç¢ºèªï¼‰
set /p KEEP_DATA="ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¾ã™ã‹ï¼Ÿ (Y/N): "
if /i "%KEEP_DATA%"=="N" (
    rmdir /s /q "C:\ProgramData\InferOS" 2>nul
)

# ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
reg delete "HKLM\SYSTEM\CurrentControlSet\Services\InferOSAgent" /f 2>nul
reg delete "HKLM\SOFTWARE\InferOS" /f 2>nul

echo ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†
pause
```

### **1.2 å‹•ä½œç¢ºèª**

#### **APIå‹•ä½œç¢ºèª**
```bash
# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl http://127.0.0.1:7031/v1/health

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
curl http://127.0.0.1:7031/v1/metrics

# ãƒãƒªã‚·ãƒ¼è¨­å®š
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"mode":"dynamic","recent_window":64},"io":{"enable_iobinding":true},"scheduler":{"mode":"hybrid"}}'
```

### **1.3 GAIAã¨ã®é€£æºè¨­å®š**

#### **ç’°å¢ƒå¤‰æ•°è¨­å®š**
```bash
# Infer-OSçµ±åˆã‚’æœ‰åŠ¹åŒ–
export INFEROS_ENABLED=true
export INFEROS_AGENT_URL=http://127.0.0.1:7031

# GAIAè¨­å®š
export GAIA_OPTIMIZATION_MODE=inferos
export GAIA_KV_QUANTIZATION=enabled
```

#### **GAIAå®Ÿè¡Œï¼ˆInfer-OSçµ±åˆï¼‰**
```bash
# åŸºæœ¬å®Ÿè¡Œ
gaia-cli --model llama-7b --optimization inferos

# è©³ç´°è¨­å®š
gaia-cli --model llama-7b \
  --optimization inferos \
  --kv-quantization enabled \
  --hybrid-mode \
  --batch-size 4 \
  --max-tokens 1024
```

## ğŸ”Œ **Phase 2: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³çµ±åˆï¼ˆé«˜åº¦ãªçµ±åˆï¼‰**

### **2.1 Lemonade Adapterã®çµ±åˆ**

#### **Lemonadeã‚µãƒ¼ãƒãƒ¼ã¸ã®çµ±åˆ**
```python
# lemonade_server.py ã¸ã®çµ±åˆä¾‹
from lemonade_adapter import LemonadeAdapter, InferenceContext

# ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åˆæœŸåŒ–
adapter = LemonadeAdapter(
    agent_url="http://127.0.0.1:7031",
    enable_kv_quantization=True
)

# æ¨è«–å®Ÿè¡Œæ™‚ã®çµ±åˆ
def run_inference(model, input_text, **kwargs):
    context = InferenceContext(
        model_name=model.name,
        seq_len=len(input_text.split()),
        batch_size=kwargs.get('batch_size', 1),
        target_ftl_ms=kwargs.get('target_ftl_ms', 300),
        quality_budget=kwargs.get('quality_budget', 0.8)
    )
    
    with adapter.inference_session(context) as session:
        # PreRun hook - ãƒãƒªã‚·ãƒ¼é©ç”¨
        policy = session.prerun_hook(context)
        
        # ORTè¨­å®šã®é©ç”¨
        session.apply_ort_policy(session_options, run_options)
        
        # æ¨è«–å®Ÿè¡Œ
        result = model.generate(input_text, **kwargs)
        
        # PostRun hook - ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        stats = InferenceStats(
            start_time=start_time,
            end_time=time.time(),
            tokens_generated=len(result.split()),
            first_token_latency=first_token_time,
            total_latency=total_time,
            memory_peak=get_memory_usage(),
            quality_score=calculate_quality_score(result)
        )
        session.postrun_hook(stats)
    
    return result
```

### **2.2 KVé‡å­åŒ–ã®è©³ç´°è¨­å®š**

#### **é‡å­åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š**
```python
from kv_quantization_engine import KVQuantizationEngine

# é‡å­åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
engine = KVQuantizationEngine(
    recent_window=64,        # æœ€æ–°64ãƒˆãƒ¼ã‚¯ãƒ³ã¯FP16ä¿æŒ
    max_cache_size=10000,    # æœ€å¤§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
    quality_threshold=0.5    # å“è³ªé–¾å€¤
)

# å‹•çš„ãƒãƒªã‚·ãƒ¼æ›´æ–°
engine.update_quantization_policy(
    memory_pressure=0.7,     # ãƒ¡ãƒ¢ãƒªåœ§è¿«åº¦
    quality_budget=0.8,      # å“è³ªäºˆç®—
    recent_window=32         # å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
)

# çµ±è¨ˆæƒ…å ±ã®å–å¾—
stats = engine.get_cache_statistics()
print(f"åœ§ç¸®ç‡: {stats['compression_ratio']:.3f}")
print(f"ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {stats['total_memory_saved_mb']:.2f} MB")
```

## ğŸ“Š **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š**

### **3.1 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å†ç¾æ€§**

#### **å›ºå®šæ¸¬å®šæ¡ä»¶**

##### **ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆå³é¸2ç¨®ï¼‰**
| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | ç”¨é€” | æ¨å¥¨ãƒ¡ãƒ¢ãƒª |
|--------|-------------|------|-----------|
| `microsoft/DialoGPT-medium` | 355M | è»½é‡ãƒ†ã‚¹ãƒˆ | 4GB |
| `microsoft/DialoGPT-large` | 762M | æ¨™æº–ãƒ†ã‚¹ãƒˆ | 8GB |

##### **å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**
```python
# benchmark_prompts.py
BENCHMARK_PROMPTS = {
    "conversation": [
        "Hello, how are you today?",
        "What is artificial intelligence?",
        "Can you explain machine learning?",
        "Tell me about the weather.",
        "What are your hobbies?"
    ],
    "reasoning": [
        "If I have 5 apples and give away 2, how many do I have left?",
        "What is the capital of France?",
        "Explain the difference between AI and ML.",
        "How does photosynthesis work?",
        "What causes seasons to change?"
    ]
}

# å›ºå®šç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GENERATION_CONFIG = {
    "max_new_tokens": 128,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
    "pad_token_id": 50256,
    "eos_token_id": 50256,
    "seed": 42  # å†ç¾æ€§ã®ãŸã‚å›ºå®š
}

# æ¸¬å®šæ¡ä»¶
BENCHMARK_CONFIG = {
    "warmup_runs": 3,
    "measurement_runs": 10,
    "concurrent_sessions": 1,
    "timeout_seconds": 300,
    "quality_baseline_runs": 5
}
```

#### **å†ç¾å¯èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```python
# reproducible_benchmark.py
import time
import json
import statistics
from datetime import datetime
from typing import Dict, List, Tuple

class ReproducibleBenchmark:
    def __init__(self, model_name: str, config: Dict):
        self.model_name = model_name
        self.config = config
        self.results = []
        
    def run_baseline_benchmark(self) -> Dict:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆInfer-OSç„¡åŠ¹ï¼‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸ”„ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {self.model_name}")
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for i in range(self.config["warmup_runs"]):
            self._single_inference(BENCHMARK_PROMPTS["conversation"][0], warmup=True)
        
        # æ¸¬å®šå®Ÿè¡Œ
        results = []
        for prompt_category in BENCHMARK_PROMPTS:
            for prompt in BENCHMARK_PROMPTS[prompt_category]:
                for run in range(self.config["measurement_runs"]):
                    result = self._single_inference(prompt)
                    results.append(result)
        
        return self._calculate_metrics(results, "baseline")
    
    def run_inferos_benchmark(self) -> Dict:
        """Infer-OSæœ‰åŠ¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"ğŸš€ Infer-OSãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {self.model_name}")
        
        # Infer-OSè¨­å®šé©ç”¨
        self._apply_inferos_config()
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for i in range(self.config["warmup_runs"]):
            self._single_inference(BENCHMARK_PROMPTS["conversation"][0], warmup=True)
        
        # æ¸¬å®šå®Ÿè¡Œ
        results = []
        for prompt_category in BENCHMARK_PROMPTS:
            for prompt in BENCHMARK_PROMPTS[prompt_category]:
                for run in range(self.config["measurement_runs"]):
                    result = self._single_inference(prompt)
                    results.append(result)
        
        return self._calculate_metrics(results, "inferos")
    
    def _single_inference(self, prompt: str, warmup: bool = False) -> Dict:
        """å˜ä¸€æ¨è«–å®Ÿè¡Œ"""
        start_time = time.perf_counter()
        
        # å®Ÿéš›ã®æ¨è«–å®Ÿè¡Œï¼ˆå®Ÿè£…ä¾å­˜ï¼‰
        response = self._execute_inference(prompt)
        
        end_time = time.perf_counter()
        
        result = {
            "prompt": prompt,
            "response": response,
            "latency_ms": (end_time - start_time) * 1000,
            "tokens_generated": len(response.split()),
            "timestamp": datetime.now().isoformat(),
            "warmup": warmup
        }
        
        if not warmup:
            # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            result["quality_score"] = self._calculate_quality_score(prompt, response)
        
        return result
    
    def _calculate_metrics(self, results: List[Dict], mode: str) -> Dict:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        latencies = [r["latency_ms"] for r in results if not r["warmup"]]
        token_counts = [r["tokens_generated"] for r in results if not r["warmup"]]
        quality_scores = [r["quality_score"] for r in results if not r["warmup"] and "quality_score" in r]
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        total_tokens = sum(token_counts)
        total_time_s = sum(latencies) / 1000
        tps = total_tokens / total_time_s if total_time_s > 0 else 0
        
        # First Token Latency (FTL) æ¨å®š
        ftl_ms = statistics.mean(latencies) * 0.3  # æ¨å®šå€¤
        
        metrics = {
            "mode": mode,
            "model": self.model_name,
            "timestamp": datetime.now().isoformat(),
            "latency": {
                "mean_ms": statistics.mean(latencies),
                "median_ms": statistics.median(latencies),
                "p95_ms": self._percentile(latencies, 95),
                "p99_ms": self._percentile(latencies, 99),
                "std_ms": statistics.stdev(latencies) if len(latencies) > 1 else 0
            },
            "throughput": {
                "tokens_per_second": tps,
                "requests_per_second": len(latencies) / total_time_s if total_time_s > 0 else 0
            },
            "first_token_latency_ms": ftl_ms,
            "quality": {
                "mean_score": statistics.mean(quality_scores) if quality_scores else 0,
                "std_score": statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0
            },
            "tokens": {
                "mean_per_response": statistics.mean(token_counts),
                "total_generated": total_tokens
            },
            "config": self.config
        }
        
        return metrics
    
    def generate_comparison_report(self, baseline: Dict, inferos: Dict) -> Dict:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        improvement = {
            "tps_improvement": (inferos["throughput"]["tokens_per_second"] / baseline["throughput"]["tokens_per_second"] - 1) * 100,
            "latency_reduction": (1 - inferos["latency"]["mean_ms"] / baseline["latency"]["mean_ms"]) * 100,
            "ftl_reduction": (1 - inferos["first_token_latency_ms"] / baseline["first_token_latency_ms"]) * 100,
            "quality_change": inferos["quality"]["mean_score"] - baseline["quality"]["mean_score"]
        }
        
        report = {
            "summary": {
                "model": self.model_name,
                "test_date": datetime.now().isoformat(),
                "baseline_tps": baseline["throughput"]["tokens_per_second"],
                "inferos_tps": inferos["throughput"]["tokens_per_second"],
                "improvement_percent": improvement["tps_improvement"],
                "quality_delta": improvement["quality_change"]
            },
            "detailed_metrics": {
                "baseline": baseline,
                "inferos": inferos,
                "improvements": improvement
            },
            "test_conditions": {
                "prompts_tested": len(BENCHMARK_PROMPTS["conversation"]) + len(BENCHMARK_PROMPTS["reasoning"]),
                "runs_per_prompt": self.config["measurement_runs"],
                "total_inferences": len(BENCHMARK_PROMPTS["conversation"]) * self.config["measurement_runs"] + len(BENCHMARK_PROMPTS["reasoning"]) * self.config["measurement_runs"],
                "generation_config": GENERATION_CONFIG
            }
        }
        
        return report

# ä½¿ç”¨ä¾‹
def run_reproducible_benchmark():
    """å†ç¾å¯èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    benchmark = ReproducibleBenchmark("microsoft/DialoGPT-medium", BENCHMARK_CONFIG)
    
    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
    baseline_results = benchmark.run_baseline_benchmark()
    
    # Infer-OSæ¸¬å®š
    inferos_results = benchmark.run_inferos_benchmark()
    
    # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = benchmark.generate_comparison_report(baseline_results, inferos_results)
    
    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"benchmark_report_{timestamp}.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: benchmark_report_{timestamp}.json")
    return report
```

#### **ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ**
```python
# ablation_study.py
class AblationStudy:
    def __init__(self):
        self.configurations = {
            "baseline": {
                "kv_quantization": False,
                "io_binding": False,
                "hybrid_scheduling": False,
                "description": "æ¨™æº–PyTorchæ¨è«–"
            },
            "kv_only": {
                "kv_quantization": True,
                "io_binding": False,
                "hybrid_scheduling": False,
                "description": "KVé‡å­åŒ–ã®ã¿"
            },
            "io_only": {
                "kv_quantization": False,
                "io_binding": True,
                "hybrid_scheduling": False,
                "description": "IOBindingã®ã¿"
            },
            "hybrid_only": {
                "kv_quantization": False,
                "io_binding": False,
                "hybrid_scheduling": True,
                "description": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ã¿"
            },
            "kv_io": {
                "kv_quantization": True,
                "io_binding": True,
                "hybrid_scheduling": False,
                "description": "KVé‡å­åŒ– + IOBinding"
            },
            "full_inferos": {
                "kv_quantization": True,
                "io_binding": True,
                "hybrid_scheduling": True,
                "description": "Infer-OSå®Œå…¨ç‰ˆ"
            }
        }
    
    def run_ablation_study(self, model_name: str) -> Dict:
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶å®Ÿè¡Œ"""
        results = {}
        
        for config_name, config in self.configurations.items():
            print(f"ğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ: {config['description']}")
            
            # è¨­å®šé©ç”¨
            self._apply_configuration(config)
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            benchmark = ReproducibleBenchmark(model_name, BENCHMARK_CONFIG)
            result = benchmark.run_inferos_benchmark()
            result["config_name"] = config_name
            result["config_description"] = config["description"]
            
            results[config_name] = result
        
        # ç›¸ä¹—åŠ¹æœåˆ†æ
        analysis = self._analyze_synergy(results)
        
        return {
            "results": results,
            "analysis": analysis
        }
    
    def _analyze_synergy(self, results: Dict) -> Dict:
        """ç›¸ä¹—åŠ¹æœåˆ†æ"""
        baseline_tps = results["baseline"]["throughput"]["tokens_per_second"]
        
        individual_effects = {
            "kv_quantization": (results["kv_only"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100,
            "io_binding": (results["io_only"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100,
            "hybrid_scheduling": (results["hybrid_only"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100
        }
        
        combined_effects = {
            "kv_io": (results["kv_io"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100,
            "full_inferos": (results["full_inferos"]["throughput"]["tokens_per_second"] / baseline_tps - 1) * 100
        }
        
        # ç›¸ä¹—åŠ¹æœè¨ˆç®—
        expected_kv_io = individual_effects["kv_quantization"] + individual_effects["io_binding"]
        actual_kv_io = combined_effects["kv_io"]
        synergy_kv_io = actual_kv_io - expected_kv_io
        
        expected_full = sum(individual_effects.values())
        actual_full = combined_effects["full_inferos"]
        synergy_full = actual_full - expected_full
        
        return {
            "individual_effects": individual_effects,
            "combined_effects": combined_effects,
            "synergy_analysis": {
                "kv_io_synergy": synergy_kv_io,
                "full_synergy": synergy_full,
                "synergy_interpretation": self._interpret_synergy(synergy_full)
            }
        }
    
    def _interpret_synergy(self, synergy: float) -> str:
        """ç›¸ä¹—åŠ¹æœã®è§£é‡ˆ"""
        if synergy > 5:
            return "å¼·ã„æ­£ã®ç›¸ä¹—åŠ¹æœ"
        elif synergy > 0:
            return "å¼±ã„æ­£ã®ç›¸ä¹—åŠ¹æœ"
        elif synergy > -5:
            return "ç›¸ä¹—åŠ¹æœãªã—"
        else:
            return "è² ã®ç›¸äº’ä½œç”¨"
```

### **3.2 å“è³ªç®¡ç†**

#### **åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```bash
# Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
gaia-cli --model llama-7b --benchmark --optimization none

# Infer-OSæœ‰åŠ¹ã§ã®æ€§èƒ½æ¸¬å®š
gaia-cli --model llama-7b --benchmark --optimization inferos

# è©³ç´°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
python benchmark_inferos_gaia.py \
  --model llama-7b \
  --sequences 100 \
  --batch-sizes 1,4,8 \
  --seq-lengths 128,512,1024 \
  --compare-modes baseline,inferos
```

#### **ã‚«ã‚¹ã‚¿ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
```python
import time
import requests
from lemonade_adapter import LemonadeAdapter

def benchmark_inferos():
    adapter = LemonadeAdapter()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        {"seq_len": 128, "batch": 1, "desc": "çŸ­æ–‡ãƒ»å˜ä¸€"},
        {"seq_len": 512, "batch": 4, "desc": "ä¸­æ–‡ãƒ»ãƒãƒƒãƒ"},
        {"seq_len": 1024, "batch": 1, "desc": "é•·æ–‡ãƒ»å˜ä¸€"}
    ]
    
    results = []
    for case in test_cases:
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆInfer-OSç„¡åŠ¹ï¼‰
        baseline_time = run_inference_baseline(case)
        
        # Infer-OSæœ‰åŠ¹
        inferos_time = run_inference_with_inferos(case)
        
        improvement = baseline_time / inferos_time
        results.append({
            "case": case["desc"],
            "baseline_ms": baseline_time * 1000,
            "inferos_ms": inferos_time * 1000,
            "improvement": improvement
        })
    
    return results
```

### **3.2 ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–**

#### **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**
```bash
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
cat > monitor_inferos.py << 'EOF'
import requests
import time
import json

def monitor_metrics():
    while True:
        try:
            response = requests.get("http://127.0.0.1:7031/v1/metrics")
            metrics = response.json()
            
            print(f"TPS: {metrics['tps']:.1f}, "
                  f"FTL: {metrics['ftl_ms']:.1f}ms, "
                  f"VRAM: {metrics['mem']['vram_gb']:.1f}GB, "
                  f"NPU: {metrics['util']['npu']:.1%}, "
                  f"iGPU: {metrics['util']['igpu']:.1%}")
            
        except Exception as e:
            print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
        
        time.sleep(1)

if __name__ == "__main__":
    monitor_metrics()
EOF

python monitor_inferos.py
```

#### **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º**
```bash
# Grafana/Prometheusé€£æºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’Prometheuså½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
curl http://127.0.0.1:7031/v1/metrics/prometheus
```

## ğŸ”§ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**

### **4.1 ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½**

#### **è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¡¨**

| äº‹è±¡ | æ¤œå‡ºæ–¹æ³• | ç›´ã¡ã«è¡Œã†å‡¦ç½® | è¿½éšå‡¦ç½® | å¾©æ—§æ¡ä»¶ |
|------|----------|----------------|----------|----------|
| DMLæœªæ¤œå‡º | `ort.get_available_providers()` | CPU EPã¸åˆ‡æ›¿ã€KVã—ãã„å€¤ã‚’ä¿å®ˆåŒ– | ç®¡ç†è€…ã¸é€šçŸ¥ã€ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª | DMLå†æ¤œå‡ºå¾Œ5åˆ† |
| NPUæœªæ¤œå‡º/æ¸©åº¦è¶…é | ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸©åº¦/åˆ©ç”¨ç‡ | decodeâ†’DMLã¸ç§»è­² | 5åˆ†æ¯ã«å†è©¦è¡Œ | NPUæ¸©åº¦æ­£å¸¸åŒ– |
| Î”PPLè¶…é | `/v1/metrics` | KVã‚’ INT4â†’INT8â†’FP16 ã¸æ®µéšå¾©å…ƒ | é–¾å€¤ã‚’ä¸€æ®µä¿å®ˆåŒ– | å“è³ªå®‰å®šå¾Œ10åˆ† |
| ãƒ¡ãƒ¢ãƒªåœ§è¿« | `/v1/metrics.mem` | recent_windowâ†“ ç­‰ã§åœ§ç¸®å¼·åŒ– | çµ±è¨ˆã§æœ€é©ç‚¹å†æ¢ç´¢ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡<80% |
| APIå¿œç­”ãªã— | HTTP timeout | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã¸åˆ‡æ›¿ | Agentå†èµ·å‹• | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ­£å¸¸ |

#### **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šä¾‹**

##### **DML/NPUæœªæ¤œå‡ºæ™‚ã®è‡ªå‹•åˆ‡æ›¿**
```python
# inferos_control_agent.py ã§ã®å®Ÿè£…ä¾‹
import onnxruntime as ort

class DeviceManager:
    def __init__(self):
        self.available_providers = ort.get_available_providers()
        self.current_config = self.detect_optimal_config()
    
    def detect_optimal_config(self):
        """æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹æ§‹æˆã‚’æ¤œå‡º"""
        config = {
            "prefill_device": "cpu",
            "decode_device": "cpu",
            "kv_policy": "conservative"
        }
        
        if "DmlExecutionProvider" in self.available_providers:
            config["prefill_device"] = "dml"
            config["kv_policy"] = "balanced"
            
            # NPUæ¤œå‡ºï¼ˆDirectMLçµŒç”±ï¼‰
            if self.detect_npu_capability():
                config["decode_device"] = "npu"
                config["kv_policy"] = "aggressive"
        
        return config
    
    def apply_fallback_policy(self, error_type):
        """ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        fallback_policies = {
            "dml_unavailable": {
                "prefill_device": "cpu",
                "decode_device": "cpu",
                "kv": {"recent_window": 128, "level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.6, "L3_evict": 0.4}}
            },
            "npu_thermal": {
                "decode_device": "dml",
                "kv": {"recent_window": 96, "level_thresholds": {"L1_int8": 0.75, "L2_int4": 0.55, "L3_evict": 0.35}}
            },
            "quality_degradation": {
                "kv": {"recent_window": 256, "level_thresholds": {"L1_int8": 0.9, "L2_int4": 0.7, "L3_evict": 0.5}}
            },
            "memory_pressure": {
                "kv": {"recent_window": 32, "level_thresholds": {"L1_int8": 0.6, "L2_int4": 0.4, "L3_evict": 0.2}}
            }
        }
        
        return fallback_policies.get(error_type, fallback_policies["dml_unavailable"])
```

##### **å“è³ªåŠ£åŒ–æ¤œå‡ºã¨è‡ªå‹•å¾©æ—§**
```python
class QualityMonitor:
    def __init__(self, max_delta_ppl=0.5, window_size=10):
        self.max_delta_ppl = max_delta_ppl
        self.window_size = window_size
        self.quality_history = []
        self.degradation_count = 0
    
    def check_quality(self, current_ppl, baseline_ppl):
        """å“è³ªãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•èª¿æ•´"""
        delta_ppl = current_ppl - baseline_ppl
        self.quality_history.append(delta_ppl)
        
        if len(self.quality_history) > self.window_size:
            self.quality_history.pop(0)
        
        avg_delta = sum(self.quality_history) / len(self.quality_history)
        
        if avg_delta > self.max_delta_ppl:
            self.degradation_count += 1
            if self.degradation_count >= 3:  # 3å›é€£ç¶šã§åŠ£åŒ–
                return self.trigger_quality_recovery()
        else:
            self.degradation_count = 0
        
        return None
    
    def trigger_quality_recovery(self):
        """å“è³ªå›å¾©å‡¦ç†"""
        recovery_steps = [
            {"kv": {"level_thresholds": {"L2_int4": 0.6, "L3_evict": 0.4}}},  # INT4ã‚’æ¸›ã‚‰ã™
            {"kv": {"level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.7}}},   # INT8ã‚’å¢—ã‚„ã™
            {"kv": {"recent_window": 128}},                                    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ‹¡å¤§
            {"scheduler": {"mode": "gpu_only"}}                               # NPUç„¡åŠ¹åŒ–
        ]
        
        return recovery_steps[min(self.degradation_count - 3, len(recovery_steps) - 1)]
```

#### **ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®è©³ç´°**

##### **DirectMLé‡å­åŒ–æ¼”ç®—ã‚µãƒãƒ¼ãƒˆç¢ºèª**
```python
def check_quantization_support():
    """DirectMLã§ã®é‡å­åŒ–æ¼”ç®—ã‚µãƒãƒ¼ãƒˆç¢ºèª"""
    try:
        import onnxruntime as ort
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªONNXãƒ¢ãƒ‡ãƒ«ã§é‡å­åŒ–æ¼”ç®—ã‚’ç¢ºèª
        test_model_path = create_test_quantized_model()
        
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        providers = [
            ('DmlExecutionProvider', {
                'device_id': 0,
                'enable_dynamic_graph_fusion': True
            })
        ]
        
        session = ort.InferenceSession(test_model_path, session_options, providers=providers)
        
        # é‡å­åŒ–æ¼”ç®—ã®ã‚µãƒãƒ¼ãƒˆç¢ºèª
        supported_ops = get_supported_quantization_ops(session)
        
        return {
            "w4_support": "QLinearConv" in supported_ops,
            "w8_support": "QLinearMatMul" in supported_ops,
            "kv_int8_support": "QuantizeLinear" in supported_ops,
            "kv_int4_support": "DequantizeLinear" in supported_ops
        }
        
    except Exception as e:
        logger.warning(f"é‡å­åŒ–ã‚µãƒãƒ¼ãƒˆç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return {"w4_support": False, "w8_support": False, "kv_int8_support": False, "kv_int4_support": False}

def apply_quantization_fallback(support_info):
    """é‡å­åŒ–ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    if not support_info["kv_int4_support"]:
        # INT4æœªã‚µãƒãƒ¼ãƒˆã®å ´åˆã¯INT8ã®ã¿ä½¿ç”¨
        return {
            "kv": {
                "level_thresholds": {
                    "L1_int8": 0.5,
                    "L2_int4": 1.0,  # INT4ã‚’ç„¡åŠ¹åŒ–
                    "L3_evict": 0.3
                }
            }
        }
    
    if not support_info["kv_int8_support"]:
        # INT8æœªã‚µãƒãƒ¼ãƒˆã®å ´åˆã¯é‡å­åŒ–ç„¡åŠ¹
        return {
            "kv": {
                "level_thresholds": {
                    "L1_int8": 1.0,  # INT8ã‚’ç„¡åŠ¹åŒ–
                    "L2_int4": 1.0,  # INT4ã‚’ç„¡åŠ¹åŒ–
                    "L3_evict": 0.7  # åœ§ç¸®ã®ã¿ä½¿ç”¨
                }
            }
        }
    
    return None  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¸è¦
```

### **4.2 ä¸€èˆ¬çš„ãªå•é¡Œ**

#### **NPU/iGPUãŒèªè­˜ã•ã‚Œãªã„**
```bash
# ãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
python -c "
import onnxruntime as ort
print('åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:', ort.get_available_providers())
"

# DirectMLç¢ºèª
python -c "
import onnxruntime as ort
providers = ort.get_available_providers()
if 'DmlExecutionProvider' in providers:
    print('âœ… DirectMLåˆ©ç”¨å¯èƒ½')
else:
    print('âŒ DirectMLåˆ©ç”¨ä¸å¯ - ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèªãŒå¿…è¦')
"
```

#### **ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**
```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
curl http://127.0.0.1:7031/v1/metrics | jq '.mem'

# é‡å­åŒ–è¨­å®šã®èª¿æ•´
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"recent_window":32,"level_thresholds":{"L1_int8":0.6,"L2_int4":0.4,"L3_evict":0.2}}}'
```

#### **å“è³ªåŠ£åŒ–ã®å¯¾å‡¦**
```bash
# å“è³ªé‡è¦–è¨­å®š
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{"kv":{"recent_window":128,"level_thresholds":{"L1_int8":0.8,"L2_int4":0.6,"L3_evict":0.4}}}'

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆ
curl -X POST http://127.0.0.1:7031/v1/baseline -d '{"enable": true}'
```

### **4.2 ãƒ­ã‚°ã¨ãƒ‡ãƒãƒƒã‚°**

#### **è©³ç´°ãƒ­ã‚°ã®æœ‰åŠ¹åŒ–**
```python
# inferos_control_agent.py ã®è¨­å®š
import logging
logging.basicConfig(level=logging.DEBUG)

# ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
export INFEROS_LOG_LEVEL=DEBUG
python inferos_control_agent.py
```

#### **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã®ç¢ºèª**
```bash
# å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
curl http://127.0.0.1:7031/v1/history | jq '.history[-10:]'

# çµ±è¨ˆæƒ…å ±ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
curl http://127.0.0.1:7031/v1/metrics > metrics_$(date +%Y%m%d_%H%M%S).json
```

## ğŸ¯ **æœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**

### **5.1 ãƒ¢ãƒ‡ãƒ«åˆ¥æ¨å¥¨è¨­å®š**

#### **7B/8Bãƒ¢ãƒ‡ãƒ«ï¼ˆä¸­è»½é‡ï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 64
    level_thresholds:
      L1_int8: 0.7
      L2_int4: 0.5
      L3_evict: 0.3
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
```

#### **13B/20Bãƒ¢ãƒ‡ãƒ«ï¼ˆé‡é‡ç´šï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 32
    level_thresholds:
      L1_int8: 0.6
      L2_int4: 0.4
      L3_evict: 0.2
  scheduler:
    mode: hybrid
    prefill_device: dml
    decode_device: npu
```

#### **3Bä»¥ä¸‹ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰**
```yaml
infer_os:
  kv:
    recent_window: 128
    level_thresholds:
      L1_int8: 0.8
      L2_int4: 0.6
      L3_evict: 0.4
  scheduler:
    mode: gpu_only  # NPUã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’é¿ã‘ã‚‹
```

### **5.2 ç”¨é€”åˆ¥æœ€é©åŒ–**

#### **ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼ˆä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 128, "mode": "latency_optimized"},
    "scheduler": {"mode": "hybrid", "decode_device": "npu"},
    "quality": {"max_delta_ppl": 0.3}
  }'
```

#### **ãƒãƒƒãƒå‡¦ç†ï¼ˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 32, "mode": "throughput_optimized"},
    "scheduler": {"mode": "hybrid", "prefill_device": "dml"},
    "quality": {"max_delta_ppl": 0.5}
  }'
```

#### **é«˜å“è³ªç”Ÿæˆï¼ˆå“è³ªé‡è¦–ï¼‰**
```bash
curl -X POST http://127.0.0.1:7031/v1/policy \
  -H "Content-Type: application/json" \
  -d '{
    "kv": {"recent_window": 256, "mode": "quality_optimized"},
    "scheduler": {"mode": "gpu_only"},
    "quality": {"max_delta_ppl": 0.1}
  }'
```

## ğŸ“ˆ **ç¶™ç¶šçš„ãªæœ€é©åŒ–**

### **6.1 è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**

#### **é©å¿œçš„è¨­å®šèª¿æ•´**
```python
# auto_tuner.py
import requests
import time
import numpy as np

class InferOSAutoTuner:
    def __init__(self, agent_url="http://127.0.0.1:7031"):
        self.agent_url = agent_url
        self.performance_history = []
    
    def tune_for_workload(self, target_tps=None, target_quality=None):
        """ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«åŸºã¥ãè‡ªå‹•ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        current_metrics = self.get_metrics()
        
        if target_tps and current_metrics['tps'] < target_tps:
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ãŸã‚ã®èª¿æ•´
            self.adjust_for_throughput()
        
        if target_quality and current_metrics['quality']['delta_ppl_est'] > target_quality:
            # å“è³ªå‘ä¸Šã®ãŸã‚ã®èª¿æ•´
            self.adjust_for_quality()
    
    def adjust_for_throughput(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šèª¿æ•´"""
        policy = {
            "kv": {"recent_window": 32, "level_thresholds": {"L1_int8": 0.6, "L2_int4": 0.4, "L3_evict": 0.2}},
            "scheduler": {"mode": "hybrid"}
        }
        self.apply_policy(policy)
    
    def adjust_for_quality(self):
        """å“è³ªå‘ä¸Šèª¿æ•´"""
        policy = {
            "kv": {"recent_window": 128, "level_thresholds": {"L1_int8": 0.8, "L2_int4": 0.6, "L3_evict": 0.4}},
            "scheduler": {"mode": "hybrid"}
        }
        self.apply_policy(policy)

# ä½¿ç”¨ä¾‹
tuner = InferOSAutoTuner()
tuner.tune_for_workload(target_tps=30.0, target_quality=0.3)
```

### **6.2 A/Bãƒ†ã‚¹ãƒˆ**

#### **è¨­å®šæ¯”è¼ƒãƒ†ã‚¹ãƒˆ**
```bash
# A/Bãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
cat > ab_test_inferos.py << 'EOF'
import requests
import time
import statistics

def run_ab_test():
    configs = [
        {"name": "Conservative", "kv": {"recent_window": 128}},
        {"name": "Balanced", "kv": {"recent_window": 64}},
        {"name": "Aggressive", "kv": {"recent_window": 32}}
    ]
    
    results = {}
    for config in configs:
        print(f"Testing {config['name']} configuration...")
        
        # è¨­å®šé©ç”¨
        requests.post("http://127.0.0.1:7031/v1/policy", json=config)
        time.sleep(2)  # è¨­å®šåæ˜ å¾…ã¡
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        metrics = []
        for _ in range(10):
            response = requests.get("http://127.0.0.1:7031/v1/metrics")
            metrics.append(response.json())
            time.sleep(1)
        
        # çµ±è¨ˆè¨ˆç®—
        tps_values = [m['tps'] for m in metrics]
        results[config['name']] = {
            "avg_tps": statistics.mean(tps_values),
            "std_tps": statistics.stdev(tps_values),
            "avg_quality": statistics.mean([m['quality']['delta_ppl_est'] for m in metrics])
        }
    
    return results

if __name__ == "__main__":
    results = run_ab_test()
    for name, stats in results.items():
        print(f"{name}: TPS={stats['avg_tps']:.1f}Â±{stats['std_tps']:.1f}, Quality={stats['avg_quality']:.3f}")
EOF

python ab_test_inferos.py
```

## ğŸ‰ **æˆåŠŸäº‹ä¾‹ã¨æœŸå¾…ã•ã‚Œã‚‹çµæœ**

### **7.1 å…¸å‹çš„ãªæ”¹å–„ä¾‹**

#### **Llama-7B ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ**
```
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:
- TPS: 18.5 tokens/sec
- FTL: 320ms
- ãƒ¡ãƒ¢ãƒª: 14.2GB
- å“è³ª: PPL 12.3

Infer-OSçµ±åˆå¾Œ:
- TPS: 26.8 tokens/sec (+45%)
- FTL: 245ms (-23%)
- ãƒ¡ãƒ¢ãƒª: 8.9GB (-37%)
- å“è³ª: PPL 12.7 (Î”PPL +0.4)

ROI: 45%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š + 37%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
```

#### **Llama-13B ãƒãƒƒãƒå‡¦ç†**
```
ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:
- TPS: 12.3 tokens/sec
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4
- ãƒ¡ãƒ¢ãƒª: 26.1GB
- å“è³ª: PPL 11.8

Infer-OSçµ±åˆå¾Œ:
- TPS: 19.7 tokens/sec (+60%)
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 8 (2å€)
- ãƒ¡ãƒ¢ãƒª: 16.4GB (-37%)
- å“è³ª: PPL 12.2 (Î”PPL +0.4)

ROI: 60%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š + ãƒãƒƒãƒã‚µã‚¤ã‚º2å€åŒ–
```

### **7.2 å°å…¥åŠ¹æœã®æ¸¬å®š**

#### **KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**
```python
# kpi_dashboard.py
import streamlit as st
import requests
import plotly.graph_objects as go
from datetime import datetime, timedelta

def create_dashboard():
    st.title("Infer-OS Ã— GAIA ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
    metrics = requests.get("http://127.0.0.1:7031/v1/metrics").json()
    history = requests.get("http://127.0.0.1:7031/v1/history").json()
    
    # KPIè¡¨ç¤º
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("TPS", f"{metrics['tps']:.1f}", delta=f"+{metrics['tps']/18.5-1:.1%}")
    with col2:
        st.metric("FTL (ms)", f"{metrics['ftl_ms']:.0f}", delta=f"{(metrics['ftl_ms']/320-1)*100:.1f}%")
    with col3:
        st.metric("ãƒ¡ãƒ¢ãƒª (GB)", f"{metrics['mem']['vram_gb']:.1f}", delta=f"{(metrics['mem']['vram_gb']/14.2-1)*100:.1f}%")
    with col4:
        st.metric("å“è³ª (Î”PPL)", f"{metrics['quality']['delta_ppl_est']:.3f}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚°ãƒ©ãƒ•
    if history['history']:
        timestamps = [h['timestamp'] for h in history['history']]
        tps_values = [h['tps'] for h in history['history']]
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=timestamps, y=tps_values, name='TPS'))
        fig.update_layout(title='ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå±¥æ­´', xaxis_title='æ™‚åˆ»', yaxis_title='TPS')
        st.plotly_chart(fig)

if __name__ == "__main__":
    create_dashboard()
```

## ğŸ”® **ä»Šå¾Œã®ç™ºå±•**

### **8.1 Phase 3: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢é€£æº**

#### **NPUå¸¸é§æœ€é©åŒ–**
```python
# å°†æ¥å®Ÿè£…äºˆå®š
class NPUResidentOptimizer:
    """NPUå¸¸é§KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–"""
    
    def __init__(self):
        self.npu_cache_manager = NPUCacheManager()
        self.igpu_spillover = iGPUSpilloverManager()
    
    def optimize_kv_placement(self, kv_chunks):
        """KVãƒãƒ£ãƒ³ã‚¯ã®æœ€é©é…ç½®"""
        # é‡è¦åº¦ã®é«˜ã„KVã¯NPUã«å¸¸é§
        # ä½é‡è¦åº¦ã¯iGPUã«ã‚¹ãƒ”ãƒ«ã‚ªãƒ¼ãƒãƒ¼
        pass
```

#### **å‹•çš„è² è·åˆ†æ•£**
```python
# å°†æ¥å®Ÿè£…äºˆå®š
class HybridLoadBalancer:
    """NPU+iGPUå‹•çš„è² è·åˆ†æ•£"""
    
    def balance_workload(self, prefill_load, decode_load):
        """ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«åŸºã¥ãå‹•çš„åˆ†æ•£"""
        # Prefill: iGPUå„ªå…ˆã€é«˜è² è·æ™‚ã¯NPUã‚‚æ´»ç”¨
        # Decode: NPUå„ªå…ˆã€ç†±åˆ¶ç´„æ™‚ã¯iGPUã«ç§»è¡Œ
        pass
```

### **8.2 ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µ**

#### **ä»–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ**
- **Ollamaçµ±åˆ**: Ollama + Infer-OSé€£æº
- **LangChainçµ±åˆ**: LangChain Agent + Infer-OSæœ€é©åŒ–
- **Transformersçµ±åˆ**: HuggingFace Transformers + Infer-OS

#### **ã‚¯ãƒ©ã‚¦ãƒ‰å±•é–‹**
- **Azureçµ±åˆ**: Azure ML + Infer-OS
- **AWSçµ±åˆ**: SageMaker + Infer-OS
- **ã‚³ãƒ³ãƒ†ãƒŠåŒ–**: Docker + Kuberneteså¯¾å¿œ

## ğŸ“ **ã‚µãƒãƒ¼ãƒˆ**

### **9.1 ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**
- **GitHub**: https://github.com/kojima123/infer-os
- **Issues**: ãƒã‚°å ±å‘Šãƒ»æ©Ÿèƒ½è¦æœ›
- **Discussions**: æŠ€è¡“è¨è«–ãƒ»è³ªå•

### **9.2 ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**
- **ãƒ­ã‚°ç¢ºèª**: `tail -f inferos_agent.log`
- **è¨­å®šãƒªã‚»ãƒƒãƒˆ**: `curl -X POST http://127.0.0.1:7031/v1/baseline`
- **å¼·åˆ¶å†èµ·å‹•**: `pkill -f inferos_control_agent && python inferos_control_agent.py`

### **9.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›¸è«‡**
ç‰¹å®šã®ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®æœ€é©åŒ–ç›¸è«‡ã¯ã€ä»¥ä¸‹ã®æƒ…å ±ã¨å…±ã«Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãƒ»ç¨®é¡
- å…¸å‹çš„ãªå…¥åŠ›é•·ãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚º
- ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
- ç›®æ¨™ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆ

---

**ğŸ¯ GAIA Ã— Infer-OSçµ±åˆã«ã‚ˆã‚Šã€AMD Ryzen AI NPU + Radeon iGPUç’°å¢ƒã§ã®LLMæ¨è«–æ€§èƒ½ã‚’æœ€å¤§åŒ–ã—ã€æ¬¡ä¸–ä»£AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºç›¤ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ï¼**

