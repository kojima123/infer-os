# -*- coding: utf-8 -*-
"""
Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ ï¼ˆbitsandbyteså•é¡Œä¿®æ­£ç‰ˆï¼‰
é€šå¸¸ã®float16ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ã§bitsandbyteså•é¡Œã‚’å›é¿
PyTorchç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ
"""

import os
import sys
import time
import argparse
import json
import threading
import psutil
import shutil
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import torch
    import onnxruntime as ort
    import numpy as np
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from huggingface_hub import snapshot_download
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("pip install torch transformers onnxruntime huggingface_hub")
    sys.exit(1)

class RyzenAIFixedLLMSystem:
    """Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ ï¼ˆbitsandbyteså•é¡Œä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, enable_infer_os: bool = False):
        self.enable_infer_os = enable_infer_os
        
        # bitsandbyteså•é¡Œå›é¿: é€šå¸¸ã®float16ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
        self.model_id = "cyberagent/open-calm-small"  # è»½é‡ç‰ˆã§å®‰å®šæ€§ç¢ºä¿
        
        # Windowsæ¨©é™å•é¡Œå›é¿: ç›´æ¥ãƒ‘ã‚¹ã‚’ä½¿ç”¨
        self.cache_dir = Path("./models")
        self.model_dir = None  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«è¨­å®š
        self.onnx_path = None  # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºå®šå¾Œã«è¨­å®š
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.pytorch_model = None
        self.tokenizer = None
        self.onnx_session = None
        self.active_provider = None
        
        # NPUç›£è¦–
        self.npu_monitoring = False
        self.npu_usage_history = []
        self.max_npu_usage = 0.0
        self.npu_active_count = 0
        self.total_inferences = 0
        
        print(f"ğŸš€ Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«è©³ç´°: CyberAgent OpenCALM-Smallï¼ˆbitsandbyteså•é¡Œå›é¿ç‰ˆï¼‰")
        print(f"ğŸŒ è¨€èª: æ—¥æœ¬èªç‰¹åŒ–")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if enable_infer_os else 'ç„¡åŠ¹'}")
        print(f"ğŸ› ï¸ bitsandbyteså•é¡Œå¯¾å¿œ: é€šå¸¸ã®float16ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨")
    
    def setup_infer_os_environment(self):
        """infer-OSç’°å¢ƒè¨­å®š"""
        if self.enable_infer_os:
            print("ğŸ”§ infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šä¸­...")
            
            infer_os_env = {
                'INFER_OS_ENABLE': '1',
                'INFER_OS_OPTIMIZATION_LEVEL': 'high',
                'INFER_OS_NPU_ACCELERATION': '1',
                'INFER_OS_MEMORY_OPTIMIZATION': '1',
                'INFER_OS_JAPANESE_OPTIMIZATION': '1'
            }
            
            for key, value in infer_os_env.items():
                os.environ[key] = value
                print(f"  ğŸ“ {key}={value}")
            
            print("âœ… infer-OSæœ€é©åŒ–ç’°å¢ƒè¨­å®šå®Œäº†")
        else:
            print("ğŸ”§ infer-OSæœ€é©åŒ–: ç„¡åŠ¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼‰")
            # infer-OSç„¡åŠ¹åŒ–
            for key in ['INFER_OS_ENABLE', 'INFER_OS_OPTIMIZATION_LEVEL', 
                       'INFER_OS_NPU_ACCELERATION', 'INFER_OS_MEMORY_OPTIMIZATION',
                       'INFER_OS_JAPANESE_OPTIMIZATION']:
                os.environ.pop(key, None)
    
    def download_model(self) -> bool:
        """æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆWindowsæ¨©é™å•é¡Œä¿®æ­£ç‰ˆï¼‰"""
        try:
            print(f"ğŸ“¥ {self.model_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
            print(f"ğŸ“ CyberAgent OpenCALM-Smallï¼ˆbitsandbyteså•é¡Œå›é¿ç‰ˆï¼‰")
            print(f"ğŸŒ æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«")
            print(f"âš ï¸ æ³¨æ„: åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            start_time = time.time()
            
            # HuggingFace Hubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯å›é¿ï¼‰
            model_path = snapshot_download(
                repo_id=self.model_id,
                cache_dir=str(self.cache_dir),
                resume_download=True,
                local_files_only=False
            )
            
            # Windowsæ¨©é™å•é¡Œå›é¿: ã‚³ãƒ”ãƒ¼ã‚’ä½¿ç”¨
            self.model_dir = self.cache_dir / "open-calm-small"
            
            if not self.model_dir.exists():
                print("ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ä¸­ï¼ˆWindowsæ¨©é™å•é¡Œå›é¿ï¼‰...")
                self.model_dir.mkdir(parents=True, exist_ok=True)
                
                # å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚³ãƒ”ãƒ¼
                source_dir = Path(model_path)
                for file_path in source_dir.iterdir():
                    if file_path.is_file():
                        dest_path = self.model_dir / file_path.name
                        if not dest_path.exists():
                            shutil.copy2(file_path, dest_path)
                            print(f"  âœ… ã‚³ãƒ”ãƒ¼å®Œäº†: {file_path.name}")
            
            # ONNXãƒ‘ã‚¹è¨­å®š
            self.onnx_path = self.model_dir / "open_calm_small_npu.onnx"
            
            download_time = time.time() - start_time
            
            print(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†!")
            print(f"ğŸ“ ä¿å­˜å…ˆ: {self.model_dir}")
            print(f"â±ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚é–“: {download_time:.1f}ç§’")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            required_files = ["config.json", "pytorch_model.bin", "tokenizer.json"]
            missing_files = []
            
            for file_name in required_files:
                file_path = self.model_dir / file_name
                if file_path.exists():
                    print(f"  âœ… {file_name}: {file_path.stat().st_size:,} bytes")
                else:
                    missing_files.append(file_name)
                    print(f"  âŒ {file_name}: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            if missing_files:
                print(f"âš ï¸ ä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«: {missing_files}")
                return False
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_pytorch_model(self) -> bool:
        """PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆbitsandbyteså•é¡Œä¿®æ­£ç‰ˆï¼‰"""
        try:
            if self.model_dir is None:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
            
            print("ğŸ“¥ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
            print(f"ğŸ”§ é‡å­åŒ–: bitsandbyteså•é¡Œå›é¿ã®ãŸã‚é€šå¸¸ã®float16ä½¿ç”¨")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_dir),
                trust_remote_code=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: èªå½™ã‚µã‚¤ã‚º {len(self.tokenizer)}")
            
            # é€šå¸¸ã®float16ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆbitsandbyteså›é¿ï¼‰
            print("ğŸ”§ float16ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ï¼ˆbitsandbyteså•é¡Œå›é¿ï¼‰...")
            print("âš ï¸ æ³¨æ„: åˆå›ãƒ­ãƒ¼ãƒ‰ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            self.pytorch_model = AutoModelForCausalLM.from_pretrained(
                str(self.model_dir),
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            print(f"âœ… PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            print(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‹: float16ï¼ˆbitsandbyteså•é¡Œå›é¿ï¼‰")
            print(f"ğŸ“Š ãƒ‡ãƒã‚¤ã‚¹: {self.pytorch_model.device}")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¡¨ç¤º
            if torch.cuda.is_available():
                memory_gb = torch.cuda.memory_allocated() / 1024**3
                print(f"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_gb:.2f} GB")
            else:
                print("ğŸ’¾ CPUä½¿ç”¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_text_pytorch(self, prompt: str, max_new_tokens: int = 50) -> str:
        """PyTorchæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰"""
        try:
            if self.pytorch_model is None or self.tokenizer is None:
                return "âŒ PyTorchãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
            
            print(f"ğŸ’¬ PyTorchæ—¥æœ¬èªç”Ÿæˆ: '{prompt}'")
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.pytorch_model.device)
            
            with torch.no_grad():
                # ç”Ÿæˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                outputs = self.pytorch_model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    min_new_tokens=5,
                    do_sample=True,
                    temperature=0.8,  # å®‰å®šã—ãŸæ¸©åº¦è¨­å®š
                    top_p=0.95,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3,
                    early_stopping=True,
                    use_cache=True
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            # ç©ºã®çµæœãƒã‚§ãƒƒã‚¯
            if not generated_text:
                generated_text = "ï¼ˆç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã—ãŸï¼‰"
            
            return generated_text
            
        except Exception as e:
            print(f"âš ï¸ PyTorchç”Ÿæˆã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡å˜ãªç”Ÿæˆ
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.pytorch_model.device)
                with torch.no_grad():
                    outputs = self.pytorch_model.generate(
                        **inputs,
                        max_new_tokens=20,
                        do_sample=False,  # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ç”Ÿæˆ
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                return generated_text if generated_text else "ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚‚å¤±æ•—ï¼‰"
            except Exception as e2:
                return f"âŒ PyTorchç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e2}"
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰"""
        print(f"\nğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ—¥æœ¬èªç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
        print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: PyTorch")
        print(f"ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰: 'quit'ã§çµ‚äº†")
        print(f"=" * 60)
        
        try:
            while True:
                prompt = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not prompt:
                    continue
                
                print(f"ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
                
                start_time = time.time()
                result = self.generate_text_pytorch(prompt, max_new_tokens=64)
                generation_time = time.time() - start_time
                
                print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                print(f"{result}")
                print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.3f}ç§’")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
    
    def run_benchmark(self, num_inferences: int = 10) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        try:
            print(f"ğŸ“Š PyTorch ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {num_inferences}å›æ¨è«–")
            print(f"ğŸ¯ ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
            print(f"ğŸ”§ ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: PyTorch")
            print(f"ğŸŒ è¨€èª: æ—¥æœ¬èª")
            
            start_time = time.time()
            successful_inferences = 0
            
            test_prompts = [
                "AIã«ã‚ˆã£ã¦ç§é”ã®æš®ã‚‰ã—ã¯ã€",
                "æ—¥æœ¬ã®æœªæ¥ã«ã¤ã„ã¦è€ƒãˆã‚‹ã¨ã€",
                "æŠ€è¡“é©æ–°ãŒç¤¾ä¼šã«ä¸ãˆã‚‹å½±éŸ¿ã¯ã€",
                "äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã‚ˆã‚Šã€",
                "ãƒ‡ã‚¸ã‚¿ãƒ«ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªã®ã¯ã€"
            ]
            
            for i in range(num_inferences):
                try:
                    prompt = test_prompts[i % len(test_prompts)]
                    result = self.generate_text_pytorch(prompt, max_new_tokens=20)
                    
                    if not result.startswith("âŒ"):
                        successful_inferences += 1
                        print(f"âœ… æ¨è«– {i+1}/{num_inferences}: æˆåŠŸ")
                    else:
                        print(f"âŒ æ¨è«– {i+1}/{num_inferences}: å¤±æ•—")
                    
                except Exception as e:
                    print(f"âŒ æ¨è«– {i+1}/{num_inferences}: ã‚¨ãƒ©ãƒ¼ - {e}")
            
            total_time = time.time() - start_time
            
            # çµ±è¨ˆè¨ˆç®—
            success_rate = (successful_inferences / num_inferences) * 100
            throughput = successful_inferences / total_time
            avg_inference_time = total_time / num_inferences * 1000  # ms
            
            results = {
                "successful_inferences": successful_inferences,
                "total_inferences": num_inferences,
                "success_rate": success_rate,
                "total_time": total_time,
                "throughput": throughput,
                "avg_inference_time": avg_inference_time,
                "active_provider": "PyTorch",
                "model_id": self.model_id
            }
            
            print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
            print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {successful_inferences}/{num_inferences}")
            print(f"  ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
            print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {avg_inference_time:.1f}ms")
            print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: PyTorch")
            
            return results
            
        except Exception as e:
            print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“åˆæœŸåŒ–"""
        try:
            print("ğŸš€ Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹")
            
            # infer-OSç’°å¢ƒè¨­å®š
            self.setup_infer_os_environment()
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if not self.download_model():
                return False
            
            # PyTorchãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if not self.load_pytorch_model():
                return False
            
            print("âœ… Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="Ryzen AI NPUå¯¾å¿œæ—¥æœ¬èªLLMã‚·ã‚¹ãƒ†ãƒ ï¼ˆbitsandbyteså•é¡Œä¿®æ­£ç‰ˆï¼‰")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true", help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument("--inferences", type=int, default=10, help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
    parser.add_argument("--tokens", type=int, default=50, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    
    args = parser.parse_args()
    
    try:
        system = RyzenAIFixedLLMSystem(enable_infer_os=args.infer_os)
        
        if not system.initialize_system():
            print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        if args.interactive:
            system.interactive_mode()
        elif args.benchmark:
            system.run_benchmark(args.inferences)
        elif args.prompt:
            print(f"ğŸ’¬ å˜ç™ºãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: '{args.prompt}'")
            result = system.generate_text_pytorch(args.prompt, args.tokens)
            print(f"ğŸ¯ ç”Ÿæˆçµæœ:")
            print(f"{result}")
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
            system.run_benchmark(5)
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

