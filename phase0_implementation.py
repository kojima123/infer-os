#!/usr/bin/env python3
"""
Phase 0: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè£…
Infer-OS & FlexGen++ NPUçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®åŸºç›¤æ§‹ç¯‰

ç›®æ¨™æ€§èƒ½: 11 tok/s
ä¸»è¦å®Ÿè£…: FlexGen++åŸºæœ¬å®Ÿè£…ã€Infer-OSåŸºæœ¬åˆ¶å¾¡æ©Ÿæ§‹

ä½œæˆè€…: Manus AI
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 1.0
"""

import os
import sys
import time
import json
import logging
import threading
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
import psutil
import numpy as np

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('phase0_implementation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class SystemConfig:
    """ã‚·ã‚¹ãƒ†ãƒ è¨­å®š"""
    model_name: str = "microsoft/Phi-3-mini-4k-instruct"
    max_sequence_length: int = 2048
    batch_size: int = 1
    target_tokens_per_second: float = 11.0
    memory_limit_gb: float = 8.0
    
    # 3å±¤ãƒ¡ãƒ¢ãƒªéšå±¤è¨­å®š
    gpu_vram_gb: float = 4.0
    ddr_memory_gb: float = 16.0
    nvme_storage_gb: float = 100.0
    
    # åˆ¶å¾¡å‘¨æœŸè¨­å®š
    control_cycle_ms: int = 10  # 10msåˆ¶å¾¡å‘¨æœŸ

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    tokens_per_second: float = 0.0
    latency_ms: float = 0.0
    memory_usage_gb: float = 0.0
    gpu_utilization: float = 0.0
    throughput_efficiency: float = 0.0

class MemoryTier(ABC):
    """ãƒ¡ãƒ¢ãƒªéšå±¤ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str, capacity_gb: float, bandwidth_gbps: float):
        self.name = name
        self.capacity_gb = capacity_gb
        self.bandwidth_gbps = bandwidth_gbps
        self.used_gb = 0.0
        self.access_count = 0
        self.total_access_time = 0.0
    
    @abstractmethod
    def allocate(self, size_gb: float) -> bool:
        """ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦"""
        pass
    
    @abstractmethod
    def deallocate(self, size_gb: float) -> bool:
        """ãƒ¡ãƒ¢ãƒªè§£æ”¾"""
        pass
    
    def get_utilization(self) -> float:
        """ä½¿ç”¨ç‡ã®å–å¾—"""
        return self.used_gb / self.capacity_gb if self.capacity_gb > 0 else 0.0
    
    def get_average_access_time(self) -> float:
        """å¹³å‡ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã®å–å¾—"""
        return self.total_access_time / self.access_count if self.access_count > 0 else 0.0

class GPUVRAMTier(MemoryTier):
    """GPU VRAMéšå±¤"""
    
    def __init__(self, capacity_gb: float):
        super().__init__("GPU_VRAM", capacity_gb, 900.0)  # 900 GB/s
        self.access_latency_ms = 0.1
    
    def allocate(self, size_gb: float) -> bool:
        if self.used_gb + size_gb <= self.capacity_gb:
            self.used_gb += size_gb
            self.access_count += 1
            self.total_access_time += self.access_latency_ms
            logger.debug(f"GPU VRAM allocated: {size_gb:.2f}GB, total: {self.used_gb:.2f}GB")
            return True
        return False
    
    def deallocate(self, size_gb: float) -> bool:
        if self.used_gb >= size_gb:
            self.used_gb -= size_gb
            logger.debug(f"GPU VRAM deallocated: {size_gb:.2f}GB, remaining: {self.used_gb:.2f}GB")
            return True
        return False

class DDRMemoryTier(MemoryTier):
    """DDR ãƒ¡ãƒ¢ãƒªéšå±¤"""
    
    def __init__(self, capacity_gb: float):
        super().__init__("DDR_MEMORY", capacity_gb, 50.0)  # 50 GB/s
        self.access_latency_ms = 0.5
    
    def allocate(self, size_gb: float) -> bool:
        if self.used_gb + size_gb <= self.capacity_gb:
            self.used_gb += size_gb
            self.access_count += 1
            self.total_access_time += self.access_latency_ms
            logger.debug(f"DDR Memory allocated: {size_gb:.2f}GB, total: {self.used_gb:.2f}GB")
            return True
        return False
    
    def deallocate(self, size_gb: float) -> bool:
        if self.used_gb >= size_gb:
            self.used_gb -= size_gb
            logger.debug(f"DDR Memory deallocated: {size_gb:.2f}GB, remaining: {self.used_gb:.2f}GB")
            return True
        return False

class NVMeStorageTier(MemoryTier):
    """NVMe ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšå±¤"""
    
    def __init__(self, capacity_gb: float):
        super().__init__("NVME_STORAGE", capacity_gb, 3.0)  # 3 GB/s
        self.access_latency_ms = 10.0
    
    def allocate(self, size_gb: float) -> bool:
        if self.used_gb + size_gb <= self.capacity_gb:
            self.used_gb += size_gb
            self.access_count += 1
            self.total_access_time += self.access_latency_ms
            logger.debug(f"NVMe Storage allocated: {size_gb:.2f}GB, total: {self.used_gb:.2f}GB")
            return True
        return False
    
    def deallocate(self, size_gb: float) -> bool:
        if self.used_gb >= size_gb:
            self.used_gb -= size_gb
            logger.debug(f"NVMe Storage deallocated: {size_gb:.2f}GB, remaining: {self.used_gb:.2f}GB")
            return True
        return False

class MemoryHierarchyManager:
    """3å±¤ãƒ¡ãƒ¢ãƒªéšå±¤ç®¡ç†"""
    
    def __init__(self, config: SystemConfig):
        self.config = config
        self.tiers = [
            GPUVRAMTier(config.gpu_vram_gb),
            DDRMemoryTier(config.ddr_memory_gb),
            NVMeStorageTier(config.nvme_storage_gb)
        ]
        self.allocation_history = []
    
    def allocate_optimal(self, size_gb: float, priority: str = "performance") -> Optional[MemoryTier]:
        """æœ€é©ãªéšå±¤ã«ãƒ¡ãƒ¢ãƒªã‚’å‰²ã‚Šå½“ã¦"""
        
        if priority == "performance":
            # æ€§èƒ½å„ªå…ˆ: é«˜é€Ÿãªéšå±¤ã‹ã‚‰é †ã«è©¦è¡Œ
            for tier in self.tiers:
                if tier.allocate(size_gb):
                    self.allocation_history.append({
                        "tier": tier.name,
                        "size_gb": size_gb,
                        "timestamp": time.time()
                    })
                    return tier
        
        elif priority == "capacity":
            # å®¹é‡å„ªå…ˆ: ä½é€Ÿã ãŒå¤§å®¹é‡ãªéšå±¤ã‹ã‚‰é †ã«è©¦è¡Œ
            for tier in reversed(self.tiers):
                if tier.allocate(size_gb):
                    self.allocation_history.append({
                        "tier": tier.name,
                        "size_gb": size_gb,
                        "timestamp": time.time()
                    })
                    return tier
        
        logger.warning(f"Failed to allocate {size_gb:.2f}GB in any tier")
        return None
    
    def get_memory_status(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®å–å¾—"""
        status = {}
        for tier in self.tiers:
            status[tier.name] = {
                "capacity_gb": tier.capacity_gb,
                "used_gb": tier.used_gb,
                "utilization": tier.get_utilization(),
                "bandwidth_gbps": tier.bandwidth_gbps,
                "avg_access_time_ms": tier.get_average_access_time()
            }
        return status
    
    def optimize_allocation(self):
        """ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã®æœ€é©åŒ–"""
        # ä½¿ç”¨ç‡ã®é«˜ã„éšå±¤ã‹ã‚‰ä½ã„éšå±¤ã¸ã®ç§»å‹•ã‚’æ¤œè¨
        for i, tier in enumerate(self.tiers[:-1]):
            if tier.get_utilization() > 0.8:  # 80%ä»¥ä¸Šã®ä½¿ç”¨ç‡
                next_tier = self.tiers[i + 1]
                if next_tier.get_utilization() < 0.6:  # 60%æœªæº€ã®ä½¿ç”¨ç‡
                    # ãƒ‡ãƒ¼ã‚¿ç§»å‹•ã®å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ï¼‰
                    logger.info(f"Optimizing allocation from {tier.name} to {next_tier.name}")

class FlexGenPlusPlus:
    """FlexGen++ åŸºæœ¬å®Ÿè£…"""
    
    def __init__(self, config: SystemConfig):
        self.config = config
        self.memory_manager = MemoryHierarchyManager(config)
        self.model = None
        self.tokenizer = None
        self.performance_metrics = PerformanceMetrics()
        
        # æ¨è«–çµ±è¨ˆ
        self.inference_count = 0
        self.total_inference_time = 0.0
        self.total_tokens_generated = 0
    
    def initialize_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        logger.info(f"Initializing model: {self.config.model_name}")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—
            model_size_gb = sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024**3)
            
            # ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦
            allocated_tier = self.memory_manager.allocate_optimal(model_size_gb, "performance")
            if allocated_tier:
                logger.info(f"Model allocated to {allocated_tier.name}: {model_size_gb:.2f}GB")
            else:
                logger.error("Failed to allocate memory for model")
                return False
            
            logger.info("Model initialization completed")
            return True
            
        except Exception as e:
            logger.error(f"Model initialization failed: {e}")
            return False
    
    def execute_inference(self, prompt: str, max_tokens: int = 100) -> Tuple[str, PerformanceMetrics]:
        """æ¨è«–ã®å®Ÿè¡Œ"""
        if not self.model or not self.tokenizer:
            raise RuntimeError("Model not initialized")
        
        start_time = time.time()
        
        # å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = self.tokenizer(prompt, return_tensors="pt", max_length=self.config.max_sequence_length, truncation=True)
        input_length = len(inputs["input_ids"][0])
        
        # æ¨è«–å®Ÿè¡Œ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )
        
        # çµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_tokens = len(outputs[0]) - input_length
        
        end_time = time.time()
        inference_time = end_time - start_time
        
        # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°
        tokens_per_second = generated_tokens / inference_time if inference_time > 0 else 0
        
        metrics = PerformanceMetrics(
            tokens_per_second=tokens_per_second,
            latency_ms=inference_time * 1000,
            memory_usage_gb=self._get_memory_usage(),
            gpu_utilization=self._get_gpu_utilization(),
            throughput_efficiency=tokens_per_second / self.config.target_tokens_per_second
        )
        
        # çµ±è¨ˆã®æ›´æ–°
        self.inference_count += 1
        self.total_inference_time += inference_time
        self.total_tokens_generated += generated_tokens
        
        logger.info(f"Inference completed: {tokens_per_second:.1f} tok/s, {inference_time*1000:.1f}ms")
        
        return response, metrics
    
    def _get_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å–å¾—"""
        process = psutil.Process()
        return process.memory_info().rss / (1024**3)  # GB
    
    def _get_gpu_utilization(self) -> float:
        """GPUä½¿ç”¨ç‡ã®å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
        if torch.cuda.is_available():
            return torch.cuda.utilization() / 100.0
        return 0.0
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """æ€§èƒ½ã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        if self.inference_count == 0:
            return {"error": "No inference executed"}
        
        avg_tokens_per_second = self.total_tokens_generated / self.total_inference_time
        avg_latency_ms = (self.total_inference_time / self.inference_count) * 1000
        
        return {
            "total_inferences": self.inference_count,
            "total_tokens_generated": self.total_tokens_generated,
            "total_time_seconds": self.total_inference_time,
            "average_tokens_per_second": avg_tokens_per_second,
            "average_latency_ms": avg_latency_ms,
            "target_achievement": avg_tokens_per_second / self.config.target_tokens_per_second,
            "memory_status": self.memory_manager.get_memory_status()
        }

class InferOSController:
    """Infer-OS åŸºæœ¬åˆ¶å¾¡æ©Ÿæ§‹"""
    
    def __init__(self, config: SystemConfig):
        self.config = config
        self.flexgen = FlexGenPlusPlus(config)
        self.control_thread = None
        self.running = False
        self.control_metrics = []
        
        # åˆ¶å¾¡çŠ¶æ…‹
        self.current_load = 0.0
        self.target_performance = config.target_tokens_per_second
        self.control_actions = []
    
    def start_control_loop(self):
        """åˆ¶å¾¡ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹"""
        if self.running:
            logger.warning("Control loop is already running")
            return
        
        self.running = True
        self.control_thread = threading.Thread(target=self._control_loop, daemon=True)
        self.control_thread.start()
        logger.info("Infer-OS control loop started")
    
    def stop_control_loop(self):
        """åˆ¶å¾¡ãƒ«ãƒ¼ãƒ—ã®åœæ­¢"""
        self.running = False
        if self.control_thread:
            self.control_thread.join()
        logger.info("Infer-OS control loop stopped")
    
    def _control_loop(self):
        """åˆ¶å¾¡ãƒ«ãƒ¼ãƒ—ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        while self.running:
            try:
                # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç›£è¦–
                system_state = self._monitor_system_state()
                
                # åˆ¶å¾¡åˆ¤æ–­ã®å®Ÿè¡Œ
                control_action = self._make_control_decision(system_state)
                
                # åˆ¶å¾¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ
                if control_action:
                    self._execute_control_action(control_action)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨˜éŒ²
                self.control_metrics.append({
                    "timestamp": time.time(),
                    "system_state": system_state,
                    "control_action": control_action
                })
                
                # åˆ¶å¾¡å‘¨æœŸã®å¾…æ©Ÿ
                time.sleep(self.config.control_cycle_ms / 1000.0)
                
            except Exception as e:
                logger.error(f"Control loop error: {e}")
    
    def _monitor_system_state(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç›£è¦–"""
        memory_status = self.flexgen.memory_manager.get_memory_status()
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # GPUä½¿ç”¨ç‡ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        gpu_utilization = 0.0
        if torch.cuda.is_available():
            gpu_utilization = torch.cuda.utilization()
        
        return {
            "cpu_percent": cpu_percent,
            "memory_percent": memory_percent,
            "gpu_utilization": gpu_utilization,
            "memory_tiers": memory_status,
            "current_load": self.current_load
        }
    
    def _make_control_decision(self, system_state: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """åˆ¶å¾¡åˆ¤æ–­ã®å®Ÿè¡Œ"""
        # ç°¡å˜ãªåˆ¶å¾¡ãƒ­ã‚¸ãƒƒã‚¯
        control_action = None
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„å ´åˆ
        if system_state["memory_percent"] > 80:
            control_action = {
                "type": "memory_optimization",
                "action": "optimize_allocation",
                "priority": "high"
            }
        
        # GPUä½¿ç”¨ç‡ãŒä½ã„å ´åˆ
        elif system_state["gpu_utilization"] < 50:
            control_action = {
                "type": "performance_optimization",
                "action": "increase_batch_size",
                "priority": "medium"
            }
        
        return control_action
    
    def _execute_control_action(self, action: Dict[str, Any]):
        """åˆ¶å¾¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ"""
        action_type = action.get("type")
        
        if action_type == "memory_optimization":
            self.flexgen.memory_manager.optimize_allocation()
            logger.info("Executed memory optimization")
        
        elif action_type == "performance_optimization":
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´ï¼ˆç°¡ç•¥åŒ–ï¼‰
            logger.info("Executed performance optimization")
        
        self.control_actions.append({
            "timestamp": time.time(),
            "action": action
        })
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        logger.info("Initializing Infer-OS system...")
        
        # FlexGen++ ã®åˆæœŸåŒ–
        if not self.flexgen.initialize_model():
            logger.error("Failed to initialize FlexGen++")
            return False
        
        # åˆ¶å¾¡ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹
        self.start_control_loop()
        
        logger.info("Infer-OS system initialization completed")
        return True
    
    def execute_inference_request(self, prompt: str, max_tokens: int = 100) -> Tuple[str, PerformanceMetrics]:
        """æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        # è² è·ã®æ›´æ–°
        self.current_load += 1.0
        
        try:
            result, metrics = self.flexgen.execute_inference(prompt, max_tokens)
            return result, metrics
        finally:
            # è² è·ã®æ¸›å°‘
            self.current_load = max(0.0, self.current_load - 1.0)
    
    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®å–å¾—"""
        return {
            "flexgen_performance": self.flexgen.get_performance_summary(),
            "control_metrics_count": len(self.control_metrics),
            "control_actions_count": len(self.control_actions),
            "current_load": self.current_load,
            "system_running": self.running
        }

def run_phase0_benchmark():
    """Phase 0 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""
    logger.info("Starting Phase 0 benchmark...")
    
    # ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
    config = SystemConfig()
    
    # Infer-OS ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    inferos = InferOSController(config)
    
    if not inferos.initialize_system():
        logger.error("System initialization failed")
        return
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "Hello, how are you today?",
        "Explain quantum computing in simple terms.",
        "Write a Python function to calculate fibonacci numbers.",
        "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
        "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
    ]
    
    logger.info("Executing benchmark tests...")
    
    total_start_time = time.time()
    all_metrics = []
    
    for i, prompt in enumerate(test_prompts):
        logger.info(f"Test {i+1}/{len(test_prompts)}: {prompt[:50]}...")
        
        try:
            response, metrics = inferos.execute_inference_request(prompt, max_tokens=100)
            all_metrics.append(metrics)
            
            logger.info(f"Response: {response[:100]}...")
            logger.info(f"Performance: {metrics.tokens_per_second:.1f} tok/s")
            
        except Exception as e:
            logger.error(f"Test {i+1} failed: {e}")
    
    total_time = time.time() - total_start_time
    
    # çµæœã®é›†è¨ˆ
    if all_metrics:
        avg_tokens_per_second = sum(m.tokens_per_second for m in all_metrics) / len(all_metrics)
        avg_latency = sum(m.latency_ms for m in all_metrics) / len(all_metrics)
        
        logger.info("=== Phase 0 Benchmark Results ===")
        logger.info(f"Total tests: {len(all_metrics)}")
        logger.info(f"Total time: {total_time:.2f} seconds")
        logger.info(f"Average tokens/second: {avg_tokens_per_second:.1f}")
        logger.info(f"Average latency: {avg_latency:.1f} ms")
        logger.info(f"Target achievement: {avg_tokens_per_second/config.target_tokens_per_second:.1%}")
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®è¡¨ç¤º
        system_status = inferos.get_system_status()
        logger.info(f"System status: {json.dumps(system_status, indent=2)}")
        
        # ç›®æ¨™é”æˆã®ç¢ºèª
        if avg_tokens_per_second >= config.target_tokens_per_second:
            logger.info("âœ… Phase 0 target achieved!")
        else:
            logger.warning(f"âš ï¸ Phase 0 target not achieved. Gap: {config.target_tokens_per_second - avg_tokens_per_second:.1f} tok/s")
    
    # ã‚·ã‚¹ãƒ†ãƒ ã®åœæ­¢
    inferos.stop_control_loop()
    logger.info("Phase 0 benchmark completed")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Phase 0: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè£…")
    print("=" * 50)
    
    try:
        run_phase0_benchmark()
    except KeyboardInterrupt:
        logger.info("Benchmark interrupted by user")
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        raise

if __name__ == "__main__":
    main()

