#!/usr/bin/env python3
"""
çµ±åˆNPU + Infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼ãƒ†ã‚¹ãƒˆ
åŒ…æ‹¬çš„ãªæ€§èƒ½æ¸¬å®šã¨æ©Ÿèƒ½æ¤œè¨¼
"""

import os
import sys
import time
import json
import traceback
import subprocess
from typing import Dict, List, Any, Optional
from pathlib import Path


class IntegratedSystemValidator:
    """çµ±åˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.test_results = {
            "system_check": {},
            "dependency_check": {},
            "model_tests": {},
            "optimization_tests": {},
            "performance_benchmarks": {},
            "integration_tests": {}
        }
        
        self.test_models = [
            "llama3-8b-amd-npu",
            "ALMA-Ja-V3-amd-npu",
            "rinna/youri-7b-chat"
        ]
        
        self.optimization_modes = [
            "full",
            "npu_only", 
            "infer_os_only",
            "balanced"
        ]
        
        self.test_prompts = [
            "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
        ]
    
    def run_full_validation(self) -> Dict[str, Any]:
        """å®Œå…¨æ¤œè¨¼å®Ÿè¡Œ"""
        print("ğŸš€ çµ±åˆNPU + Infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨æ¤œè¨¼é–‹å§‹")
        print("=" * 80)
        
        # Phase 1: ã‚·ã‚¹ãƒ†ãƒ ãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Phase 1: ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒãƒã‚§ãƒƒã‚¯")
        self._check_system_environment()
        
        # Phase 2: ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
        print("\nğŸ“¦ Phase 2: ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯")
        self._check_dependencies()
        
        # Phase 3: ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
        print("\nğŸ¤– Phase 3: ãƒ¢ãƒ‡ãƒ«æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        self._test_models()
        
        # Phase 4: æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        print("\nâš¡ Phase 4: æœ€é©åŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        self._test_optimizations()
        
        # Phase 5: æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        print("\nğŸ“Š Phase 5: æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        self._run_performance_benchmarks()
        
        # Phase 6: çµ±åˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ”— Phase 6: çµ±åˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
        self._test_integration()
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\nğŸ“‹ æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        self._display_validation_summary()
        
        return self.test_results
    
    def _check_system_environment(self):
        """ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
        print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒç¢ºèªä¸­...")
        
        # Pythonç’°å¢ƒ
        python_version = sys.version
        self.test_results["system_check"]["python_version"] = python_version
        print(f"ğŸ Python: {python_version.split()[0]}")
        
        # OSæƒ…å ±
        import platform
        os_info = platform.platform()
        self.test_results["system_check"]["os_info"] = os_info
        print(f"ğŸ’» OS: {os_info}")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        try:
            import psutil
            memory_info = psutil.virtual_memory()
            memory_gb = memory_info.total / (1024**3)
            self.test_results["system_check"]["memory_gb"] = memory_gb
            print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB")
        except ImportError:
            print("âš ï¸ psutilãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # GPU/NPUæƒ…å ±
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                self.test_results["system_check"]["gpu_available"] = True
                self.test_results["system_check"]["gpu_count"] = gpu_count
                self.test_results["system_check"]["gpu_name"] = gpu_name
                print(f"ğŸ® GPU: {gpu_name} ({gpu_count}å€‹)")
            else:
                self.test_results["system_check"]["gpu_available"] = False
                print("ğŸ® GPU: åˆ©ç”¨ä¸å¯")
        except ImportError:
            print("âš ï¸ PyTorchãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # NPUç’°å¢ƒå¤‰æ•°
        npu_env_vars = [
            "RYZEN_AI_INSTALLATION_PATH",
            "XLNX_VART_FIRMWARE", 
            "XLNX_TARGET_NAME"
        ]
        
        npu_env_status = {}
        for var in npu_env_vars:
            value = os.environ.get(var)
            npu_env_status[var] = value is not None
            status = "âœ…" if value else "âŒ"
            print(f"ğŸ”§ {var}: {status}")
        
        self.test_results["system_check"]["npu_env_vars"] = npu_env_status
    
    def _check_dependencies(self):
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        print("ğŸ“¦ ä¾å­˜é–¢ä¿‚ç¢ºèªä¸­...")
        
        # å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
        required_libraries = [
            "torch",
            "transformers", 
            "onnx",
            "onnxruntime",
            "psutil"
        ]
        
        library_status = {}
        for lib in required_libraries:
            try:
                __import__(lib)
                library_status[lib] = True
                print(f"âœ… {lib}: åˆ©ç”¨å¯èƒ½")
            except ImportError:
                library_status[lib] = False
                print(f"âŒ {lib}: åˆ©ç”¨ä¸å¯")
        
        self.test_results["dependency_check"]["libraries"] = library_status
        
        # ONNX Runtime ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        try:
            import onnxruntime as ort
            providers = ort.get_available_providers()
            self.test_results["dependency_check"]["onnx_providers"] = providers
            
            print("ğŸ”§ ONNX Runtime ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:")
            for provider in providers:
                print(f"  - {provider}")
                
            # VitisAI ExecutionProviderç¢ºèª
            vitisai_available = 'VitisAIExecutionProvider' in providers
            self.test_results["dependency_check"]["vitisai_available"] = vitisai_available
            status = "âœ…" if vitisai_available else "âŒ"
            print(f"âš¡ VitisAI ExecutionProvider: {status}")
            
        except ImportError:
            print("âŒ ONNX RuntimeãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«
        required_files = [
            "integrated_npu_infer_os.py",
            "run_integrated_demo.py",
            "npu_optimized_japanese_models.py"
        ]
        
        file_status = {}
        for file_name in required_files:
            exists = os.path.exists(file_name)
            file_status[file_name] = exists
            status = "âœ…" if exists else "âŒ"
            print(f"ğŸ“„ {file_name}: {status}")
        
        self.test_results["dependency_check"]["required_files"] = file_status
    
    def _test_models(self):
        """ãƒ¢ãƒ‡ãƒ«æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        for model_name in self.test_models:
            print(f"\nğŸ“± ãƒ†ã‚¹ãƒˆå¯¾è±¡: {model_name}")
            
            model_test_result = {
                "load_test": False,
                "generation_test": False,
                "error_messages": []
            }
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
            try:
                print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ...")
                cmd = [
                    "python", "integrated_npu_infer_os.py",
                    "--model", model_name,
                    "--prompt", "ãƒ†ã‚¹ãƒˆ",
                    "--max-tokens", "10",
                    "--disable-npu",
                    "--disable-infer-os"
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
                
                if result.returncode == 0:
                    model_test_result["load_test"] = True
                    print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                    
                    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
                    if "ğŸ“ å¿œç­”:" in result.stdout:
                        model_test_result["generation_test"] = True
                        print("âœ… ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆæˆåŠŸ")
                    else:
                        print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå¿œç­”ãªã—")
                else:
                    error_msg = result.stderr[-200:] if result.stderr else "Unknown error"
                    model_test_result["error_messages"].append(error_msg)
                    print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {error_msg}")
                    
            except subprocess.TimeoutExpired:
                model_test_result["error_messages"].append("Timeout")
                print("â° ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            except Exception as e:
                model_test_result["error_messages"].append(str(e))
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            
            self.test_results["model_tests"][model_name] = model_test_result
    
    def _test_optimizations(self):
        """æœ€é©åŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        print("âš¡ æœ€é©åŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        test_model = "rinna/youri-7b-chat"  # å®‰å®šã—ãŸãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒ‡ãƒ«
        test_prompt = "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        
        for mode in self.optimization_modes:
            print(f"\nğŸ”§ æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰: {mode}")
            
            optimization_test_result = {
                "execution_success": False,
                "execution_time": 0,
                "optimization_applied": False,
                "error_messages": []
            }
            
            try:
                cmd = [
                    "python", "integrated_npu_infer_os.py",
                    "--model", test_model,
                    "--prompt", test_prompt,
                    "--max-tokens", "50"
                ]
                
                # æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰è¨­å®š
                if mode == "npu_only":
                    cmd.append("--disable-infer-os")
                elif mode == "infer_os_only":
                    cmd.append("--disable-npu")
                elif mode == "balanced":
                    cmd.extend(["--quantization-profile", "balanced"])
                
                start_time = time.time()
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
                execution_time = time.time() - start_time
                
                optimization_test_result["execution_time"] = execution_time
                
                if result.returncode == 0:
                    optimization_test_result["execution_success"] = True
                    print(f"âœ… å®Ÿè¡ŒæˆåŠŸ ({execution_time:.1f}ç§’)")
                    
                    # æœ€é©åŒ–é©ç”¨ç¢ºèª
                    if "æœ€é©åŒ–" in result.stdout or "NPU" in result.stdout:
                        optimization_test_result["optimization_applied"] = True
                        print("âœ… æœ€é©åŒ–é©ç”¨ç¢ºèª")
                    else:
                        print("âš ï¸ æœ€é©åŒ–é©ç”¨æœªç¢ºèª")
                else:
                    error_msg = result.stderr[-200:] if result.stderr else "Unknown error"
                    optimization_test_result["error_messages"].append(error_msg)
                    print(f"âŒ å®Ÿè¡Œå¤±æ•—: {error_msg}")
                    
            except subprocess.TimeoutExpired:
                optimization_test_result["error_messages"].append("Timeout")
                print("â° æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            except Exception as e:
                optimization_test_result["error_messages"].append(str(e))
                print(f"âŒ æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            
            self.test_results["optimization_tests"][mode] = optimization_test_result
    
    def _run_performance_benchmarks(self):
        """æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸ“Š æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        
        benchmark_model = "rinna/youri-7b-chat"
        benchmark_prompts = self.test_prompts
        
        benchmark_results = {}
        
        for mode in ["infer_os_only", "full"]:  # å®‰å®šã—ãŸãƒ¢ãƒ¼ãƒ‰ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            print(f"\nâš¡ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰: {mode}")
            
            mode_results = {
                "total_time": 0,
                "average_time": 0,
                "successful_runs": 0,
                "failed_runs": 0,
                "prompt_results": []
            }
            
            total_time = 0
            successful_runs = 0
            
            for i, prompt in enumerate(benchmark_prompts, 1):
                print(f"ğŸ”„ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ {i}/{len(benchmark_prompts)}")
                
                try:
                    cmd = [
                        "python", "integrated_npu_infer_os.py",
                        "--model", benchmark_model,
                        "--prompt", prompt,
                        "--max-tokens", "100"
                    ]
                    
                    if mode == "infer_os_only":
                        cmd.append("--disable-npu")
                    
                    start_time = time.time()
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
                    execution_time = time.time() - start_time
                    
                    prompt_result = {
                        "prompt": prompt[:50] + "...",
                        "execution_time": execution_time,
                        "success": result.returncode == 0
                    }
                    
                    if result.returncode == 0:
                        successful_runs += 1
                        total_time += execution_time
                        print(f"âœ… æˆåŠŸ ({execution_time:.1f}ç§’)")
                    else:
                        print(f"âŒ å¤±æ•— ({execution_time:.1f}ç§’)")
                    
                    mode_results["prompt_results"].append(prompt_result)
                    
                except subprocess.TimeoutExpired:
                    print("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                    mode_results["prompt_results"].append({
                        "prompt": prompt[:50] + "...",
                        "execution_time": 120,
                        "success": False
                    })
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    mode_results["prompt_results"].append({
                        "prompt": prompt[:50] + "...",
                        "execution_time": 0,
                        "success": False
                    })
            
            mode_results["total_time"] = total_time
            mode_results["successful_runs"] = successful_runs
            mode_results["failed_runs"] = len(benchmark_prompts) - successful_runs
            mode_results["average_time"] = total_time / successful_runs if successful_runs > 0 else 0
            
            benchmark_results[mode] = mode_results
            
            print(f"ğŸ“Š {mode} çµæœ:")
            print(f"   æˆåŠŸ: {successful_runs}/{len(benchmark_prompts)}")
            print(f"   å¹³å‡æ™‚é–“: {mode_results['average_time']:.1f}ç§’")
        
        self.test_results["performance_benchmarks"] = benchmark_results
    
    def _test_integration(self):
        """çµ±åˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ”— çµ±åˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        integration_tests = {
            "demo_script_test": False,
            "interactive_mode_test": False,
            "comparison_test": False,
            "dependency_integration": False
        }
        
        # ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ã‚¹ãƒˆ
        try:
            print("ğŸ® ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ã‚¹ãƒˆ...")
            result = subprocess.run(
                ["python", "run_integrated_demo.py", "--check-deps"],
                capture_output=True, text=True, timeout=30
            )
            
            if result.returncode == 0 and "ä¾å­˜é–¢ä¿‚" in result.stdout:
                integration_tests["demo_script_test"] = True
                print("âœ… ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆå‹•ä½œç¢ºèª")
            else:
                print("âŒ ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆå‹•ä½œå¤±æ•—")
                
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # ä¾å­˜é–¢ä¿‚çµ±åˆãƒ†ã‚¹ãƒˆ
        try:
            print("ğŸ“¦ ä¾å­˜é–¢ä¿‚çµ±åˆãƒ†ã‚¹ãƒˆ...")
            
            # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            sys.path.append(os.getcwd())
            
            try:
                from integrated_npu_infer_os import IntegratedNPUInferOS
                integration_tests["dependency_integration"] = True
                print("âœ… çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
            except ImportError as e:
                print(f"âŒ çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")
                
        except Exception as e:
            print(f"âŒ ä¾å­˜é–¢ä¿‚çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        self.test_results["integration_tests"] = integration_tests
    
    def _display_validation_summary(self):
        """æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("=" * 80)
        print("ğŸ“‹ çµ±åˆNPU + Infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        
        # ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒ
        print("\nğŸ’» ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒ:")
        system_check = self.test_results["system_check"]
        print(f"   Python: {system_check.get('python_version', 'Unknown').split()[0]}")
        print(f"   OS: {system_check.get('os_info', 'Unknown')}")
        print(f"   ãƒ¡ãƒ¢ãƒª: {system_check.get('memory_gb', 0):.1f}GB")
        print(f"   GPU: {'âœ…' if system_check.get('gpu_available', False) else 'âŒ'}")
        
        # ä¾å­˜é–¢ä¿‚
        print("\nğŸ“¦ ä¾å­˜é–¢ä¿‚:")
        dep_check = self.test_results["dependency_check"]
        libraries = dep_check.get("libraries", {})
        for lib, status in libraries.items():
            print(f"   {lib}: {'âœ…' if status else 'âŒ'}")
        
        vitisai_status = dep_check.get("vitisai_available", False)
        print(f"   VitisAI EP: {'âœ…' if vitisai_status else 'âŒ'}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ:")
        model_tests = self.test_results["model_tests"]
        for model, result in model_tests.items():
            load_status = "âœ…" if result["load_test"] else "âŒ"
            gen_status = "âœ…" if result["generation_test"] else "âŒ"
            print(f"   {model}: ãƒ­ãƒ¼ãƒ‰{load_status} ç”Ÿæˆ{gen_status}")
        
        # æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        print("\nâš¡ æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ:")
        opt_tests = self.test_results["optimization_tests"]
        for mode, result in opt_tests.items():
            success_status = "âœ…" if result["execution_success"] else "âŒ"
            time_info = f"({result['execution_time']:.1f}ç§’)" if result["execution_time"] > 0 else ""
            print(f"   {mode}: {success_status} {time_info}")
        
        # æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        print("\nğŸ“Š æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:")
        benchmarks = self.test_results["performance_benchmarks"]
        for mode, result in benchmarks.items():
            success_rate = f"{result['successful_runs']}/{result['successful_runs'] + result['failed_runs']}"
            avg_time = f"{result['average_time']:.1f}ç§’" if result['average_time'] > 0 else "N/A"
            print(f"   {mode}: æˆåŠŸç‡{success_rate} å¹³å‡{avg_time}")
        
        # çµ±åˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ”— çµ±åˆãƒ†ã‚¹ãƒˆ:")
        integration = self.test_results["integration_tests"]
        for test, status in integration.items():
            print(f"   {test}: {'âœ…' if status else 'âŒ'}")
        
        # ç·åˆè©•ä¾¡
        print("\nğŸ† ç·åˆè©•ä¾¡:")
        
        # æˆåŠŸç‡è¨ˆç®—
        total_tests = 0
        passed_tests = 0
        
        # ä¾å­˜é–¢ä¿‚æˆåŠŸç‡
        lib_total = len(libraries)
        lib_passed = sum(libraries.values())
        total_tests += lib_total
        passed_tests += lib_passed
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆæˆåŠŸç‡
        model_total = len(model_tests) * 2  # ãƒ­ãƒ¼ãƒ‰ + ç”Ÿæˆ
        model_passed = sum(result["load_test"] + result["generation_test"] for result in model_tests.values())
        total_tests += model_total
        passed_tests += model_passed
        
        # æœ€é©åŒ–ãƒ†ã‚¹ãƒˆæˆåŠŸç‡
        opt_total = len(opt_tests)
        opt_passed = sum(result["execution_success"] for result in opt_tests.values())
        total_tests += opt_total
        passed_tests += opt_passed
        
        # çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸç‡
        int_total = len(integration)
        int_passed = sum(integration.values())
        total_tests += int_total
        passed_tests += int_passed
        
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"   ç·åˆæˆåŠŸç‡: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        
        if success_rate >= 80:
            print("   è©•ä¾¡: ğŸ‰ å„ªç§€ - çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
        elif success_rate >= 60:
            print("   è©•ä¾¡: âœ… è‰¯å¥½ - ä¸€éƒ¨ã®æ©Ÿèƒ½ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ãŒä½¿ç”¨å¯èƒ½ã§ã™")
        elif success_rate >= 40:
            print("   è©•ä¾¡: âš ï¸ æ³¨æ„ - è¤‡æ•°ã®å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä¿®æ­£ãŒå¿…è¦ã§ã™")
        else:
            print("   è©•ä¾¡: âŒ ä¸è‰¯ - é‡å¤§ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™")
        
        print("=" * 80)
    
    def save_results(self, filename: str = "validation_results.json"):
        """æ¤œè¨¼çµæœä¿å­˜"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ æ¤œè¨¼çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="çµ±åˆNPU + Infer-OSæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼")
    parser.add_argument("--save-results", action="store_true", help="çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜")
    parser.add_argument("--output-file", default="validation_results.json", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å")
    
    args = parser.parse_args()
    
    validator = IntegratedSystemValidator()
    
    try:
        results = validator.run_full_validation()
        
        if args.save_results:
            validator.save_results(args.output_file)
        
        print("\nğŸ çµ±åˆã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼å®Œäº†")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ¤œè¨¼ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()

