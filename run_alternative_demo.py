"""
ä»£æ›¿NPUãƒ‡ãƒ¢å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ONNXå¤‰æ›ãŒå›°é›£ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ä»£æ›¿NPUå‡¦ç†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ä½¿ç”¨æ–¹æ³•:
    python run_alternative_demo.py --interactive
    python run_alternative_demo.py --prompt "äººå‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
"""

import sys
import os
import argparse
import time
import traceback
from typing import Dict, Any, Optional

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
try:
    import torch
    import transformers
    import onnx
    import onnxruntime as ort
    import numpy as np
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    DEPENDENCIES_OK = False

# ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from alternative_npu_approach import AlternativeNPUEngine
    ALTERNATIVE_NPU_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
    ALTERNATIVE_NPU_AVAILABLE = False

class AlternativeNPUDemo:
    """ä»£æ›¿NPUãƒ‡ãƒ¢"""
    
    def __init__(self, model_name: str = "rinna/youri-7b-chat"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.npu_engine = None
        
        print(f"ğŸš€ ä»£æ›¿NPUãƒ‡ãƒ¢åˆæœŸåŒ–")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ğŸ¯ ONNXå¤‰æ›å›°é›£ãƒ¢ãƒ‡ãƒ«å¯¾å¿œç‰ˆ")
        
        # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
        if not DEPENDENCIES_OK:
            print("âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
            print("   pip install torch transformers onnx onnxruntime-directml accelerate")
            sys.exit(1)
    
    def setup(self) -> bool:
        """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            print("\nğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            from transformers import AutoModelForCausalLM
            
            load_start = time.time()
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            load_time = time.time() - load_start
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† ({load_time:.1f}ç§’)")
            
            # ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if ALTERNATIVE_NPU_AVAILABLE:
                print("ğŸš€ ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
                self.npu_engine = AlternativeNPUEngine(self.model, self.tokenizer)
                
                if self.npu_engine.setup_npu():
                    print("âœ… ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                    print(f"ğŸ¯ ä½¿ç”¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {self.npu_engine.npu_approach}")
                else:
                    print("âš ï¸ ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã€CPUæ¨è«–ã‚’ä½¿ç”¨")
                    self.npu_engine = None
            else:
                print("âš ï¸ ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€CPUæ¨è«–ã‚’ä½¿ç”¨")
            
            print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def generate_text(self, prompt: str, max_new_tokens: int = 50, 
                     temperature: float = 0.7) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            # ä»£æ›¿NPUæ¨è«–ã‚’å„ªå…ˆ
            if self.npu_engine and self.npu_engine.is_npu_ready:
                print("âš¡ ä»£æ›¿NPUæ¨è«–ã‚’ä½¿ç”¨ä¸­...")
                result = self.npu_engine.generate_with_npu(
                    prompt, max_new_tokens, temperature
                )
                
                if "error" not in result:
                    return result
                else:
                    print(f"âš ï¸ ä»£æ›¿NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {result['error']}")
                    print("ğŸ”„ CPUæ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
            # CPUæ¨è«–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            print("ğŸ–¥ï¸ CPUæ¨è«–ã‚’ä½¿ç”¨ä¸­...")
            return self._generate_with_cpu(prompt, max_new_tokens, temperature)
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _generate_with_cpu(self, prompt: str, max_new_tokens: int, 
                          temperature: float) -> Dict[str, Any]:
        """CPUæ¨è«–"""
        try:
            start_time = time.time()
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆè¨­å®š
            from transformers import GenerationConfig
            generation_config = GenerationConfig(
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=generation_config
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            tokens_per_sec = output_tokens / generation_time if generation_time > 0 else 0
            
            return {
                "generated_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_sec": tokens_per_sec,
                "inference_method": "CPU"
            }
            
        except Exception as e:
            print(f"âŒ CPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"CPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}"}
    
    def run_interactive(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ‡¯ğŸ‡µ ä»£æ›¿NPUãƒ‡ãƒ¢ - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print("ğŸ¯ ONNXå¤‰æ›å›°é›£ãƒ¢ãƒ‡ãƒ«å¯¾å¿œç‰ˆ")
        print("ğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†ã€'stats'ã§NPUçµ±è¨ˆè¡¨ç¤º")
        print("=" * 60)
        
        while True:
            try:
                prompt = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if not prompt:
                    print("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                if prompt.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if prompt.lower() == 'stats':
                    self._show_npu_stats()
                    continue
                
                if prompt.lower() == 'help':
                    self._show_help()
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
                print("\nğŸ”„ ç”Ÿæˆä¸­...")
                result = self.generate_text(prompt)
                
                if "error" in result:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
                    continue
                
                # çµæœè¡¨ç¤º
                print(f"\nâœ… ç”Ÿæˆå®Œäº†:")
                print(f"ğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ: {result['generated_text']}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
                print(f"ğŸ“Š å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['input_tokens']}")
                print(f"ğŸ“Š å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['output_tokens']}")
                print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
                print(f"ğŸ”§ æ¨è«–æ–¹æ³•: {result['inference_method']}")
                
                # NPUçµ±è¨ˆæƒ…å ±è¡¨ç¤º
                if 'npu_inference_count' in result:
                    print(f"âš¡ NPUæ¨è«–å›æ•°: {result['npu_inference_count']}")
                    if 'avg_npu_time' in result:
                        print(f"âš¡ å¹³å‡NPUæ™‚é–“: {result['avg_npu_time']:.3f}ç§’")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                traceback.print_exc()
    
    def _show_npu_stats(self):
        """NPUçµ±è¨ˆæƒ…å ±è¡¨ç¤º"""
        print("\nğŸ“Š ä»£æ›¿NPUçµ±è¨ˆæƒ…å ±:")
        
        if self.npu_engine:
            stats = self.npu_engine.get_npu_stats()
            print(f"  ğŸ¯ NPUæº–å‚™çŠ¶æ…‹: {'âœ… æº–å‚™å®Œäº†' if stats['is_npu_ready'] else 'âŒ æœªæº–å‚™'}")
            print(f"  ğŸ”§ NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {stats['npu_approach']}")
            print(f"  âš¡ NPUæ¨è«–å›æ•°: {stats['npu_inference_count']}")
            print(f"  â±ï¸ ç·NPUæ™‚é–“: {stats['total_npu_time']:.3f}ç§’")
            print(f"  ğŸ“Š å¹³å‡NPUæ™‚é–“: {stats['avg_npu_time']:.3f}ç§’")
            print(f"  ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹ID: {stats['device_id']}")
        else:
            print("  âš ï¸ ä»£æ›¿NPUã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    def _show_help(self):
        """ãƒ˜ãƒ«ãƒ—è¡¨ç¤º"""
        print("\nğŸ“– ä»£æ›¿NPUãƒ‡ãƒ¢ãƒ˜ãƒ«ãƒ—:")
        print("  - æ—¥æœ¬èªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("  - 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†")
        print("  - 'stats'ã§NPUçµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º")
        print("  - 'help'ã§ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º")
        print("\nğŸ¯ ä»£æ›¿NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:")
        print("  - partial: éƒ¨åˆ†çš„NPUå‡¦ç†ï¼ˆæœ€çµ‚å±¤ã®ã¿NPUï¼‰")
        print("  - directml: DirectMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä½¿ç”¨")
        print("  - simple: ç°¡æ˜“NPUå‡¦ç†ï¼ˆè² è·ç”Ÿæˆ + CPUæ¨è«–ï¼‰")
        print("\nğŸ’¡ NPUè² è·ç¢ºèª:")
        print("  - ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§NPUä½¿ç”¨ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        print("  - ä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚‚NPUè² è·ç‡ãŒä¸Šæ˜‡ã—ã¾ã™")
    
    def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.npu_engine:
            self.npu_engine.cleanup()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="ä»£æ›¿NPUãƒ‡ãƒ¢")
    parser.add_argument("--model", type=str, default="rinna/youri-7b-chat",
                       help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--interactive", action="store_true",
                       help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    parser.add_argument("--prompt", type=str,
                       help="å˜ç™ºå®Ÿè¡Œç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-tokens", type=int, default=50,
                       help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="ç”Ÿæˆæ¸©åº¦")
    
    args = parser.parse_args()
    
    print("ğŸš€ ä»£æ›¿NPUãƒ‡ãƒ¢é–‹å§‹")
    print("ğŸ¯ ONNXå¤‰æ›å›°é›£ãƒ¢ãƒ‡ãƒ«å¯¾å¿œç‰ˆ")
    print("=" * 60)
    
    # ãƒ‡ãƒ¢åˆæœŸåŒ–
    demo = AlternativeNPUDemo(args.model)
    
    try:
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        if not demo.setup():
            print("âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
        if args.interactive:
            demo.run_interactive()
        elif args.prompt:
            print(f"\nğŸ”„ å˜ç™ºå®Ÿè¡Œ: {args.prompt}")
            result = demo.generate_text(args.prompt, args.max_tokens, args.temperature)
            
            if "error" in result:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
            else:
                print(f"âœ… ç”Ÿæˆçµæœ: {result['generated_text']}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
                print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
                print(f"ğŸ”§ æ¨è«–æ–¹æ³•: {result['inference_method']}")
        else:
            print("ğŸ’¡ --interactive ã¾ãŸã¯ --prompt ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("ä¾‹: python run_alternative_demo.py --interactive")
            print("ä¾‹: python run_alternative_demo.py --prompt \"äººå‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚\"")
    
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        demo.cleanup()

if __name__ == "__main__":
    main()

