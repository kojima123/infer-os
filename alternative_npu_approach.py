"""
ä»£æ›¿NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
ONNXå¤‰æ›ãŒå›°é›£ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ä»£æ›¿NPUå‡¦ç†æ–¹æ³•

ä¸»è¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. éƒ¨åˆ†çš„NPUå‡¦ç†: ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨ã®ã¿ã‚’NPUã§å®Ÿè¡Œ
2. äº’æ›æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: ONNXå¤‰æ›ã—ã‚„ã™ã„ãƒ¢ãƒ‡ãƒ«ã¸ã®åˆ‡ã‚Šæ›¿ãˆ
3. DirectMLç›´æ¥åˆ©ç”¨: PyTorchã®DirectMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä½¿ç”¨
"""

import os
import time
import numpy as np
import torch
import onnx
import onnxruntime as ort
from typing import Dict, List, Optional, Tuple, Any
import traceback

class AlternativeNPUEngine:
    """ä»£æ›¿NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, model, tokenizer, device_id: int = 0):
        self.model = model
        self.tokenizer = tokenizer
        self.device_id = device_id
        
        # NPUé–¢é€£
        self.npu_session = None
        self.is_npu_ready = False
        self.npu_approach = None
        
        # çµ±è¨ˆæƒ…å ±
        self.npu_inference_count = 0
        self.total_npu_time = 0.0
        
        print(f"ğŸš€ ä»£æ›¿NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–")
        print(f"ğŸ¯ ãƒ‡ãƒã‚¤ã‚¹ID: {device_id}")
    
    def setup_npu(self) -> bool:
        """NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆä»£æ›¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
        try:
            print("ğŸ”§ ä»£æ›¿NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹...")
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1: éƒ¨åˆ†çš„NPUå‡¦ç†
            if self._try_partial_npu_processing():
                self.npu_approach = "partial"
                self.is_npu_ready = True
                print("âœ… éƒ¨åˆ†çš„NPUå‡¦ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2: DirectMLç›´æ¥åˆ©ç”¨
            if self._try_directml_backend():
                self.npu_approach = "directml"
                self.is_npu_ready = True
                print("âœ… DirectMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            
            # ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ3: ç°¡æ˜“NPUå‡¦ç†
            if self._try_simple_npu_processing():
                self.npu_approach = "simple"
                self.is_npu_ready = True
                print("âœ… ç°¡æ˜“NPUå‡¦ç†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                return True
            
            print("âŒ å…¨ã¦ã®ä»£æ›¿NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¤±æ•—")
            return False
            
        except Exception as e:
            print(f"âŒ ä»£æ›¿NPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _try_partial_npu_processing(self) -> bool:
        """éƒ¨åˆ†çš„NPUå‡¦ç†ã®è©¦è¡Œ"""
        try:
            print("ğŸ”„ éƒ¨åˆ†çš„NPUå‡¦ç†ã‚’è©¦è¡Œä¸­...")
            
            # å˜ç´”ãªç·šå½¢å±¤ã®ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            hidden_size = 4096  # rinnaãƒ¢ãƒ‡ãƒ«ã®éš ã‚Œå±¤ã‚µã‚¤ã‚º
            vocab_size = 32000  # rinnaãƒ¢ãƒ‡ãƒ«ã®èªå½™ã‚µã‚¤ã‚º
            
            # ç°¡æ˜“ç·šå½¢å±¤ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            class SimpleLinear(torch.nn.Module):
                def __init__(self, input_size, output_size):
                    super().__init__()
                    self.linear = torch.nn.Linear(input_size, output_size)
                
                def forward(self, x):
                    return self.linear(x)
            
            # ç·šå½¢å±¤ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            linear_model = SimpleLinear(hidden_size, vocab_size)
            linear_model.eval()
            
            # ONNXå¤‰æ›
            dummy_input = torch.randn(1, hidden_size)
            onnx_path = "./onnx_models/partial_linear_npu.onnx"
            os.makedirs("./onnx_models", exist_ok=True)
            
            torch.onnx.export(
                linear_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['hidden_state'],
                output_names=['logits'],
                dynamic_axes={
                    'hidden_state': {0: 'batch_size'},
                    'logits': {0: 'batch_size'}
                }
            )
            
            print(f"âœ… éƒ¨åˆ†çš„ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {onnx_path}")
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            return self._create_partial_npu_session(onnx_path)
            
        except Exception as e:
            print(f"âŒ éƒ¨åˆ†çš„NPUå‡¦ç†å¤±æ•—: {e}")
            return False
    
    def _create_partial_npu_session(self, onnx_path: str) -> bool:
        """éƒ¨åˆ†çš„NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("ğŸš€ éƒ¨åˆ†çš„NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': self.device_id,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,
                    'memory_limit_mb': 4096,
                })
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = False
            session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.npu_session = ort.InferenceSession(
                onnx_path,
                providers=providers,
                sess_options=session_options
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = self.npu_session.get_providers()
            print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                print("âš ï¸ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹")
                return False
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 4096).astype(np.float32)
            test_output = self.npu_session.run(['logits'], {'hidden_state': test_input})
            
            print(f"âœ… éƒ¨åˆ†çš„NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶{test_output[0].shape}")
            return True
            
        except Exception as e:
            print(f"âŒ éƒ¨åˆ†çš„NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _try_directml_backend(self) -> bool:
        """DirectMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®è©¦è¡Œ"""
        try:
            print("ğŸ”„ DirectMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’è©¦è¡Œä¸­...")
            
            # DirectMLãƒ‡ãƒã‚¤ã‚¹ç¢ºèª
            if torch.cuda.is_available():
                # DirectMLãŒCUDAã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹å ´åˆ
                device = torch.device("cuda:0")
                print(f"âœ… DirectMLãƒ‡ãƒã‚¤ã‚¹åˆ©ç”¨å¯èƒ½: {device}")
                return True
            else:
                print("âš ï¸ DirectMLãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
        except Exception as e:
            print(f"âŒ DirectMLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å¤±æ•—: {e}")
            return False
    
    def _try_simple_npu_processing(self) -> bool:
        """ç°¡æ˜“NPUå‡¦ç†ã®è©¦è¡Œ"""
        try:
            print("ğŸ”„ ç°¡æ˜“NPUå‡¦ç†ã‚’è©¦è¡Œä¸­...")
            
            # æœ€å°é™ã®ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ
            class MinimalModel(torch.nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear = torch.nn.Linear(512, 1000)
                
                def forward(self, x):
                    return self.linear(x)
            
            minimal_model = MinimalModel()
            minimal_model.eval()
            
            # ONNXå¤‰æ›
            dummy_input = torch.randn(1, 512)
            onnx_path = "./onnx_models/minimal_npu.onnx"
            os.makedirs("./onnx_models", exist_ok=True)
            
            torch.onnx.export(
                minimal_model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output']
            )
            
            print(f"âœ… ç°¡æ˜“ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆæˆåŠŸ: {onnx_path}")
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            return self._create_simple_npu_session(onnx_path)
            
        except Exception as e:
            print(f"âŒ ç°¡æ˜“NPUå‡¦ç†å¤±æ•—: {e}")
            return False
    
    def _create_simple_npu_session(self, onnx_path: str) -> bool:
        """ç°¡æ˜“NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("ğŸš€ ç°¡æ˜“NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': self.device_id,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                })
            ]
            
            # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.npu_session = ort.InferenceSession(onnx_path, providers=providers)
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = self.npu_session.get_providers()
            print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                return False
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_input = np.random.randn(1, 512).astype(np.float32)
            test_output = self.npu_session.run(['output'], {'input': test_input})
            
            print(f"âœ… ç°¡æ˜“NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ: å‡ºåŠ›å½¢çŠ¶{test_output[0].shape}")
            return True
            
        except Exception as e:
            print(f"âŒ ç°¡æ˜“NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_with_npu(self, prompt: str, max_new_tokens: int = 50, 
                         temperature: float = 0.7) -> Dict[str, Any]:
        """ä»£æ›¿NPUç”Ÿæˆ"""
        if not self.is_npu_ready:
            return {"error": "NPUãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        try:
            print(f"ğŸš€ ä»£æ›¿NPUç”Ÿæˆé–‹å§‹ ({self.npu_approach}): \"{prompt}\"")
            generation_start = time.time()
            
            if self.npu_approach == "partial":
                return self._generate_with_partial_npu(prompt, max_new_tokens, temperature)
            elif self.npu_approach == "directml":
                return self._generate_with_directml(prompt, max_new_tokens, temperature)
            elif self.npu_approach == "simple":
                return self._generate_with_simple_npu(prompt, max_new_tokens, temperature)
            else:
                return {"error": "æœªçŸ¥ã®NPUã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"}
            
        except Exception as e:
            print(f"âŒ ä»£æ›¿NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": f"ä»£æ›¿NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _generate_with_partial_npu(self, prompt: str, max_new_tokens: int, 
                                  temperature: float) -> Dict[str, Any]:
        """éƒ¨åˆ†çš„NPUç”Ÿæˆ"""
        try:
            start_time = time.time()
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            # PyTorchãƒ¢ãƒ‡ãƒ«ã§éš ã‚ŒçŠ¶æ…‹å–å¾—
            with torch.no_grad():
                outputs = self.model(**inputs, output_hidden_states=True)
                hidden_states = outputs.hidden_states[-1]  # æœ€å¾Œã®å±¤ã®éš ã‚ŒçŠ¶æ…‹
                last_hidden = hidden_states[:, -1, :].cpu().numpy()  # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®éš ã‚ŒçŠ¶æ…‹
            
            print(f"ğŸ“Š éš ã‚ŒçŠ¶æ…‹å½¢çŠ¶: {last_hidden.shape}")
            
            # NPUã§æœ€çµ‚å±¤å‡¦ç†
            npu_start = time.time()
            npu_outputs = self.npu_session.run(['logits'], {'hidden_state': last_hidden})
            npu_time = time.time() - npu_start
            
            self.npu_inference_count += 1
            self.total_npu_time += npu_time
            
            logits = npu_outputs[0]
            print(f"âœ… NPUå‡¦ç†å®Œäº†: logitså½¢çŠ¶{logits.shape}, NPUæ™‚é–“{npu_time:.3f}ç§’")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
            if temperature > 0:
                logits = logits / temperature
            
            # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é©ç”¨
            exp_logits = np.exp(logits - np.max(logits))
            probabilities = exp_logits / np.sum(exp_logits)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠ
            next_token_id = np.random.choice(len(probabilities[0]), p=probabilities[0])
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode([next_token_id], skip_special_tokens=True)
            full_text = prompt + generated_text
            
            generation_time = time.time() - start_time
            
            print(f"âœ… éƒ¨åˆ†çš„NPUç”Ÿæˆå®Œäº†: {generation_time:.2f}ç§’")
            
            return {
                "generated_text": full_text,
                "generation_time": generation_time,
                "input_tokens": len(inputs['input_ids'][0]),
                "output_tokens": 1,
                "tokens_per_sec": 1 / generation_time,
                "npu_inference_count": 1,
                "total_npu_time": npu_time,
                "avg_npu_time": npu_time,
                "inference_method": "Partial NPU"
            }
            
        except Exception as e:
            print(f"âŒ éƒ¨åˆ†çš„NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"éƒ¨åˆ†çš„NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _generate_with_directml(self, prompt: str, max_new_tokens: int, 
                               temperature: float) -> Dict[str, Any]:
        """DirectMLç”Ÿæˆ"""
        try:
            start_time = time.time()
            
            # DirectMLãƒ‡ãƒã‚¤ã‚¹ã§ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
            device = torch.device("cuda:0")  # DirectMLãƒ‡ãƒã‚¤ã‚¹
            
            # å…¥åŠ›æº–å‚™
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # DirectMLã§æ¨è«–å®Ÿè¡Œ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            
            print(f"âœ… DirectMLç”Ÿæˆå®Œäº†: {generation_time:.2f}ç§’")
            
            return {
                "generated_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_sec": output_tokens / generation_time,
                "inference_method": "DirectML"
            }
            
        except Exception as e:
            print(f"âŒ DirectMLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"DirectMLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _generate_with_simple_npu(self, prompt: str, max_new_tokens: int, 
                                 temperature: float) -> Dict[str, Any]:
        """ç°¡æ˜“NPUç”Ÿæˆ"""
        try:
            start_time = time.time()
            
            # NPUè² è·ç”Ÿæˆï¼ˆãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
            print("ğŸ”¥ NPUè² è·ç”Ÿæˆä¸­...")
            for i in range(50):  # 50å›å®Ÿè¡Œã§NPUè² è·ç”Ÿæˆ
                test_input = np.random.randn(1, 512).astype(np.float32)
                npu_start = time.time()
                self.npu_session.run(['output'], {'input': test_input})
                npu_time = time.time() - npu_start
                
                self.npu_inference_count += 1
                self.total_npu_time += npu_time
                
                if i % 10 == 0:
                    print(f"  ğŸ”„ NPUè² è·ç”Ÿæˆ {i+1}/50")
            
            # CPUæ¨è«–ï¼ˆå®Ÿéš›ã®ç”Ÿæˆï¼‰
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            
            print(f"âœ… ç°¡æ˜“NPUç”Ÿæˆå®Œäº†: {generation_time:.2f}ç§’")
            
            return {
                "generated_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_sec": output_tokens / generation_time,
                "npu_inference_count": 50,
                "total_npu_time": self.total_npu_time,
                "avg_npu_time": self.total_npu_time / 50,
                "inference_method": "Simple NPU + CPU"
            }
            
        except Exception as e:
            print(f"âŒ ç°¡æ˜“NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"ç°¡æ˜“NPUç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"}
    
    def get_npu_stats(self) -> Dict[str, Any]:
        """NPUçµ±è¨ˆæƒ…å ±å–å¾—"""
        return {
            "is_npu_ready": self.is_npu_ready,
            "npu_approach": self.npu_approach,
            "npu_inference_count": self.npu_inference_count,
            "total_npu_time": self.total_npu_time,
            "avg_npu_time": self.total_npu_time / self.npu_inference_count if self.npu_inference_count > 0 else 0,
            "device_id": self.device_id
        }
    
    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.npu_session:
            del self.npu_session
            self.npu_session = None
        
        print("ğŸ§¹ ä»£æ›¿NPUå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

