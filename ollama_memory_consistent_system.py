#!/usr/bin/env python3
"""
Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ä¹–é›¢ä¿®æ­£ç‰ˆï¼‰
ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡æ¸¬å®šã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã—ã€æ­£ç¢ºãªinfer-OSæœ€é©åŒ–åŠ¹æœã‚’æ¸¬å®š
"""

import os
import sys
import time
import json
import argparse
import threading
import requests
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import psutil
    import onnxruntime as ort
    import numpy as np
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("pip install psutil onnxruntime requests")
    sys.exit(1)

class OllamaMemoryConsistentController:
    """Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ä¹–é›¢ä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self, ollama_host: str = "http://localhost:11434"):
        self.ollama_host = ollama_host
        self.ollama_api = f"{ollama_host}/api"
        
        # infer-OSæœ€é©åŒ–åˆ¶å¾¡
        self.infer_os_enabled = True
        self.infer_os_config = {
            "npu_optimization": True,
            "memory_optimization": True,
            "cpu_optimization": True,
            "gpu_acceleration": True,
            "quantization": True,
            "parallel_processing": True,
        }
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
        self.available_models = []
        self.current_model = None
        self.npu_monitoring = False
        self.npu_stats = {"usage_changes": 0, "max_usage": 0.0, "avg_usage": 0.0}
        self.onnx_session = None
        self.generating = False  # ç”Ÿæˆä¸­ãƒ•ãƒ©ã‚°
        
        # ãƒ¡ãƒ¢ãƒªæ¸¬å®šå±¥æ­´
        self.memory_history = []
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates = {
            "conversation": "{prompt}",
            "instruction": "æŒ‡ç¤º: {prompt}\n\nå›ç­”:",
            "reasoning": "å•é¡Œ: {prompt}\n\nè§£ç­”:",
            "creative": "ãƒ†ãƒ¼ãƒ: {prompt}\n\nå†…å®¹:",
            "simple": "{prompt}"
        }
        
        self.current_template = "simple"
        
        print("ğŸš€ Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ä¹–é›¢ä¿®æ­£ç‰ˆï¼‰åˆæœŸåŒ–")
        print(f"ğŸ”— Ollamaæ¥ç¶šå…ˆ: {ollama_host}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        print(f"ğŸ¯ è¨­è¨ˆæ–¹é‡: ãƒ¡ãƒ¢ãƒªæ¸¬å®šä¸€è²«æ€§ + æ­£ç¢ºãªæœ€é©åŒ–åŠ¹æœæ¸¬å®š")
        print(f"ğŸ”§ ä¿®æ­£å†…å®¹: çµ±ä¸€æ¸¬å®šã‚¿ã‚¤ãƒŸãƒ³ã‚° + å®‰å®šåŒ–å¾…æ©Ÿ + è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    
    def measure_memory_consistently(self, context: str = "", wait_seconds: int = 5) -> float:
        """ä¸€è²«ã—ãŸãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡æ¸¬å®šï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            if wait_seconds > 0:
                print(f"â³ ãƒ¡ãƒ¢ãƒªå®‰å®šåŒ–å¾…æ©Ÿä¸­ï¼ˆ{wait_seconds}ç§’ï¼‰...")
                time.sleep(wait_seconds)
            
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent()
            
            measurement = {
                "timestamp": time.time(),
                "context": context,
                "memory_percent": memory.percent,
                "memory_used_gb": memory.used / (1024**3),
                "memory_total_gb": memory.total / (1024**3),
                "cpu_percent": cpu_percent
            }
            
            self.memory_history.append(measurement)
            
            print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ ({context}): {memory.percent:.1f}% ({memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB)")
            print(f"ğŸ’» CPUä½¿ç”¨ç‡ ({context}): {cpu_percent:.1f}%")
            
            return memory.percent
            
        except Exception as e:
            print(f"âŒ ãƒ¡ãƒ¢ãƒªæ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def comprehensive_memory_analysis(self, prompt: str, max_tokens: int = 100, template: str = None) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¡ãƒ¢ãƒªåˆ†æï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("ğŸ” åŒ…æ‹¬çš„ãƒ¡ãƒ¢ãƒªåˆ†æé–‹å§‹...")
        
        measurements = {
            "pre_generation": None,
            "during_generation_max": 0.0,
            "post_generation": None,
            "stabilized": None
        }
        
        # ç”Ÿæˆå‰æ¸¬å®š
        print("ğŸ“Š ç”Ÿæˆå‰ãƒ¡ãƒ¢ãƒªæ¸¬å®š...")
        measurements["pre_generation"] = self.measure_memory_consistently("ç”Ÿæˆå‰", wait_seconds=2)
        
        # ç”Ÿæˆä¸­ç›£è¦–ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰
        def monitor_during_generation():
            max_usage = 0.0
            sample_count = 0
            while self.generating:
                try:
                    current = psutil.virtual_memory().percent
                    max_usage = max(max_usage, current)
                    sample_count += 1
                    if sample_count % 10 == 0:  # 5ç§’ã”ã¨ã«è¡¨ç¤º
                        print(f"ğŸ”¥ ç”Ÿæˆä¸­ãƒ¡ãƒ¢ãƒªç›£è¦–: {current:.1f}% (æœ€å¤§: {max_usage:.1f}%)")
                    time.sleep(0.5)
                except:
                    break
            measurements["during_generation_max"] = max_usage
        
        # ç”Ÿæˆå®Ÿè¡Œ
        print("ğŸš€ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œä¸­...")
        monitor_thread = threading.Thread(target=monitor_during_generation, daemon=True)
        self.generating = True
        monitor_thread.start()
        
        start_time = time.time()
        result = self.generate_text_with_ollama_fixed(prompt, max_tokens, template)
        end_time = time.time()
        
        self.generating = False
        monitor_thread.join(timeout=1)
        
        # ç”Ÿæˆç›´å¾Œæ¸¬å®š
        print("ğŸ“Š ç”Ÿæˆç›´å¾Œãƒ¡ãƒ¢ãƒªæ¸¬å®š...")
        measurements["post_generation"] = self.measure_memory_consistently("ç”Ÿæˆç›´å¾Œ", wait_seconds=0)
        
        # å®‰å®šåŒ–å¾Œæ¸¬å®š
        print("ğŸ“Š å®‰å®šåŒ–å¾Œãƒ¡ãƒ¢ãƒªæ¸¬å®š...")
        measurements["stabilized"] = self.measure_memory_consistently("å®‰å®šåŒ–å¾Œ", wait_seconds=5)
        
        analysis_result = {
            "measurements": measurements,
            "result": result,
            "generation_time": end_time - start_time,
            "memory_reduction": measurements["post_generation"] - measurements["stabilized"] if measurements["post_generation"] and measurements["stabilized"] else 0
        }
        
        print("âœ… åŒ…æ‹¬çš„ãƒ¡ãƒ¢ãƒªåˆ†æå®Œäº†")
        return analysis_result
    
    def clear_memory_cache(self):
        """ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            print("ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢é–‹å§‹...")
            
            # Ollamaãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ
            if self.current_model:
                try:
                    unload_response = requests.post(
                        f"{self.ollama_api}/unload",
                        json={"model": self.current_model["name"]},
                        timeout=10
                    )
                    if unload_response.status_code == 200:
                        print("âœ… Ollamaãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                    else:
                        print("âš ï¸ Ollamaãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ï¼ˆç¶™ç¶šï¼‰")
                except:
                    print("âš ï¸ Ollamaãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œå¤±æ•—ï¼ˆç¶™ç¶šï¼‰")
            
            # Python ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            collected = gc.collect()
            print(f"ğŸ—‘ï¸ ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collected}ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå›å")
            
            # çŸ­æ™‚é–“å¾…æ©Ÿ
            time.sleep(3)
            
            print("âœ… ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    def accurate_optimization_comparison(self, prompt: str, tokens: int = 100) -> Dict[str, Any]:
        """æ­£ç¢ºãªæœ€é©åŒ–åŠ¹æœæ¯”è¼ƒï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("ğŸ¯ æ­£ç¢ºãªinfer-OSæœ€é©åŒ–åŠ¹æœæ¯”è¼ƒé–‹å§‹...")
        
        comparison_result = {
            "optimization_on": None,
            "optimization_off": None,
            "effectiveness": {}
        }
        
        # æœ€é©åŒ–æœ‰åŠ¹ã§æ¸¬å®š
        print("\nâš¡ infer-OSæœ€é©åŒ–æœ‰åŠ¹ã§ã®æ¸¬å®š...")
        self.infer_os_enabled = True
        self.apply_infer_os_optimizations()
        comparison_result["optimization_on"] = self.comprehensive_memory_analysis(prompt, tokens)
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        print("\nğŸ§¹ æ¸¬å®šé–“ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢...")
        self.clear_memory_cache()
        time.sleep(10)  # ååˆ†ãªå¾…æ©Ÿæ™‚é–“
        
        # æœ€é©åŒ–ç„¡åŠ¹ã§æ¸¬å®š
        print("\nâŒ infer-OSæœ€é©åŒ–ç„¡åŠ¹ã§ã®æ¸¬å®š...")
        self.infer_os_enabled = False
        comparison_result["optimization_off"] = self.comprehensive_memory_analysis(prompt, tokens)
        
        # åŠ¹æœè¨ˆç®—
        on_data = comparison_result["optimization_on"]["measurements"]
        off_data = comparison_result["optimization_off"]["measurements"]
        
        if on_data["stabilized"] and off_data["stabilized"]:
            comparison_result["effectiveness"] = {
                "memory_reduction": off_data["stabilized"] - on_data["stabilized"],
                "memory_reduction_percent": ((off_data["stabilized"] - on_data["stabilized"]) / off_data["stabilized"]) * 100,
                "peak_reduction": off_data["during_generation_max"] - on_data["during_generation_max"],
                "generation_time_diff": comparison_result["optimization_off"]["generation_time"] - comparison_result["optimization_on"]["generation_time"]
            }
        
        print("âœ… æ­£ç¢ºãªinfer-OSæœ€é©åŒ–åŠ¹æœæ¯”è¼ƒå®Œäº†")
        return comparison_result
    
    def show_detailed_memory_stats(self):
        """è©³ç´°ãƒ¡ãƒ¢ãƒªçµ±è¨ˆè¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if not self.memory_history:
            print("ğŸ“Š ãƒ¡ãƒ¢ãƒªæ¸¬å®šå±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print("\nğŸ“Š è©³ç´°ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡çµ±è¨ˆ:")
        print("=" * 60)
        
        for i, measurement in enumerate(self.memory_history[-10:], 1):  # ç›´è¿‘10ä»¶
            timestamp = time.strftime("%H:%M:%S", time.localtime(measurement["timestamp"]))
            print(f"  {i:2d}. [{timestamp}] {measurement['context']:15s}: "
                  f"{measurement['memory_percent']:5.1f}% "
                  f"({measurement['memory_used_gb']:4.1f}GB) "
                  f"CPU: {measurement['cpu_percent']:4.1f}%")
        
        if len(self.memory_history) >= 2:
            latest = self.memory_history[-1]
            previous = self.memory_history[-2]
            change = latest["memory_percent"] - previous["memory_percent"]
            change_symbol = "ğŸ“ˆ" if change > 0 else "ğŸ“‰" if change < 0 else "â¡ï¸"
            print(f"\n{change_symbol} å‰å›æ¯”è¼ƒ: {change:+.1f}%")
        
        # çµ±è¨ˆæƒ…å ±
        memory_values = [m["memory_percent"] for m in self.memory_history]
        print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
        print(f"  æœ€å¤§ä½¿ç”¨ç‡: {max(memory_values):.1f}%")
        print(f"  æœ€å°ä½¿ç”¨ç‡: {min(memory_values):.1f}%")
        print(f"  å¹³å‡ä½¿ç”¨ç‡: {sum(memory_values) / len(memory_values):.1f}%")
        print(f"  æ¸¬å®šå›æ•°: {len(memory_values)}å›")
    
    def check_ollama_connection(self) -> bool:
        """Ollamaæ¥ç¶šç¢ºèª"""
        try:
            print("ğŸ” Ollamaæ¥ç¶šç¢ºèªä¸­...")
            response = requests.get(f"{self.ollama_api}/tags", timeout=10)
            
            if response.status_code == 200:
                print("âœ… Ollamaæ¥ç¶šæˆåŠŸ")
                return True
            else:
                print(f"âŒ Ollamaæ¥ç¶šå¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print("âŒ Ollamaæ¥ç¶šå¤±æ•—: æ¥ç¶šã‚¨ãƒ©ãƒ¼")
            print("ğŸ’¡ OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            print("   Windows: ollama serve")
            return False
        except Exception as e:
            print(f"âŒ Ollamaæ¥ç¶šç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_available_models(self) -> List[Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—"""
        try:
            print("ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ä¸­...")
            response = requests.get(f"{self.ollama_api}/tags", timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                
                self.available_models = []
                for model in models:
                    model_info = {
                        "name": model.get("name", "unknown"),
                        "size": model.get("size", 0),
                        "modified_at": model.get("modified_at", ""),
                        "digest": model.get("digest", ""),
                        "details": model.get("details", {})
                    }
                    self.available_models.append(model_info)
                
                print(f"âœ… {len(self.available_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½")
                for i, model in enumerate(self.available_models, 1):
                    size_gb = model["size"] / (1024**3) if model["size"] > 0 else 0
                    print(f"  {i}. {model['name']} ({size_gb:.1f}GB)")
                
                return self.available_models
            else:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def select_model(self, model_name: str = None) -> bool:
        """ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        try:
            if not self.available_models:
                self.get_available_models()
            
            if not self.available_models:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            if model_name is None:
                selected_model = self.available_models[0]
            else:
                selected_model = None
                for model in self.available_models:
                    if model_name in model["name"]:
                        selected_model = model
                        break
                
                if selected_model is None:
                    print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{model_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
            
            self.current_model = selected_model
            size_gb = selected_model["size"] / (1024**3) if selected_model["size"] > 0 else 0
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«é¸æŠå®Œäº†: {selected_model['name']}")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: {size_gb:.1f}GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def apply_infer_os_optimizations(self):
        """infer-OSæœ€é©åŒ–é©ç”¨"""
        if not self.infer_os_enabled:
            print("âš ï¸ infer-OSæœ€é©åŒ–ã¯ç„¡åŠ¹ã§ã™")
            return
        
        print("âš¡ infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨ä¸­...")
        
        if self.infer_os_config["memory_optimization"]:
            print("ğŸ”§ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: æœ‰åŠ¹")
            os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'
            os.environ['OLLAMA_NUM_PARALLEL'] = '1'
            os.environ['OLLAMA_LOAD_TIMEOUT'] = '60'
        
        if self.infer_os_config["cpu_optimization"]:
            print("ğŸ”§ CPUæœ€é©åŒ–: æœ‰åŠ¹")
            cpu_count = os.cpu_count()
            os.environ['OLLAMA_NUM_THREADS'] = str(min(2, cpu_count))
        
        if self.infer_os_config["gpu_acceleration"]:
            print("ğŸ”§ GPUåŠ é€Ÿ: æœ‰åŠ¹")
            os.environ['OLLAMA_GPU_LAYERS'] = '20'
        
        if self.infer_os_config["npu_optimization"]:
            print("ğŸ”§ NPUæœ€é©åŒ–: æœ‰åŠ¹")
            os.environ['ONNXRUNTIME_PROVIDERS'] = 'DmlExecutionProvider,CPUExecutionProvider'
        
        print("âœ… infer-OSæœ€é©åŒ–è¨­å®šé©ç”¨å®Œäº†")
    
    def create_safe_onnx_session(self) -> bool:
        """å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            print("ğŸ”§ å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            os.makedirs("models", exist_ok=True)
            
            import torch
            import torch.nn as nn
            
            class SimpleModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear = nn.Linear(256, 512)
                    
                def forward(self, x):
                    return self.linear(x)
            
            model = SimpleModel()
            model.eval()
            
            dummy_input = torch.randn(1, 256)
            onnx_path = "models/ollama_memory_consistent_model.onnx"
            
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output']
            )
            
            print(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            
            providers = []
            available_providers = ort.get_available_providers()
            
            if 'DmlExecutionProvider' in available_providers:
                providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
                print("ğŸ¯ DmlExecutionProviderä½¿ç”¨")
            elif 'VitisAIExecutionProvider' in available_providers:
                providers = ['VitisAIExecutionProvider', 'CPUExecutionProvider']
                print("ğŸ¯ VitisAIExecutionProviderä½¿ç”¨")
            else:
                providers = ['CPUExecutionProvider']
                print("ğŸ¯ CPUExecutionProviderä½¿ç”¨")
            
            session_options = ort.SessionOptions()
            session_options.log_severity_level = 3
            session_options.enable_cpu_mem_arena = True
            
            self.onnx_session = ort.InferenceSession(
                onnx_path,
                sess_options=session_options,
                providers=providers
            )
            
            active_provider = self.onnx_session.get_providers()[0]
            print(f"âœ… å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_provider}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å®‰å…¨ãªONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_text_with_ollama_fixed(self, prompt: str, max_tokens: int = 100, template: str = None) -> str:
        """Ollamaã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if self.current_model is None:
            return "âŒ ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“"
        
        try:
            if template and template in self.templates:
                formatted_prompt = self.templates[template].format(prompt=prompt)
            else:
                formatted_prompt = prompt
            
            print(f"ğŸ’¬ Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{formatted_prompt[:30]}...'")
            print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.current_model['name']}")
            print(f"ğŸ¯ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
            print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
            
            payload = {
                "model": self.current_model["name"],
                "prompt": formatted_prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.8,
                    "top_p": 0.95,
                    "top_k": 40,
                    "repeat_penalty": 1.05,
                    "stop": ["\n\n", "äººé–“:", "Human:", "Assistant:"],
                }
            }
            
            print("ğŸ”§ Ollama APIå‘¼ã³å‡ºã—ä¸­...")
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_api}/generate",
                json=payload,
                timeout=30
            )
            
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("response", "").strip()
                
                print(f"âœ… Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                print(f"ğŸ“ ç”Ÿæˆæ–‡å­—æ•°: {len(generated_text)}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
                
                if len(generated_text) < 5:
                    print("âš ï¸ ç”ŸæˆçµæœãŒçŸ­ã™ãã¾ã™")
                    return self.generate_fallback_text(prompt)
                
                return generated_text
            else:
                print(f"âŒ Ollama APIå‘¼ã³å‡ºã—å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}")
                return self.generate_fallback_text(prompt)
                
        except requests.exceptions.Timeout:
            print("âŒ Ollama APIå‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ30ç§’ï¼‰")
            return self.generate_fallback_text(prompt)
        except Exception as e:
            print(f"âŒ Ollamaãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self.generate_fallback_text(prompt)
    
    def generate_fallback_text(self, prompt: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        fallback_responses = {
            "äººå·¥çŸ¥èƒ½": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ãªã©ã®æŠ€è¡“ã‚’ç”¨ã„ã¦ã€äººé–“ã®ã‚ˆã†ãªçŸ¥çš„ãªå‡¦ç†ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚ç¾åœ¨ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ãŒé€²ã‚“ã§ãŠã‚Šã€ä»Šå¾Œã•ã‚‰ãªã‚‹ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "é‡å­": "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸé©æ–°çš„ãªè¨ˆç®—æŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§ã¯å›°é›£ãªå•é¡Œã‚’é«˜é€Ÿã§è§£æ±ºã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€æš—å·è§£èª­ã‚„è–¬ç‰©é–‹ç™ºãªã©ã®åˆ†é‡ã§ã®å¿œç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "äººå‚": "äººå‚ï¼ˆã«ã‚“ã˜ã‚“ï¼‰ã¯ã€ã‚»ãƒªç§‘ã®é‡èœã§ã€Î²-ã‚«ãƒ­ãƒ†ãƒ³ã‚’è±Šå¯Œã«å«ã‚€æ „é¤Šä¾¡ã®é«˜ã„é£Ÿæã§ã™ã€‚ç”Ÿé£Ÿã€ç…®ç‰©ã€ç‚’ã‚ç‰©ãªã©æ§˜ã€…ãªèª¿ç†æ³•ã§æ¥½ã—ã‚ã€ç”˜ã¿ãŒã‚ã£ã¦å­ä¾›ã«ã‚‚äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãƒ†ã‚¹ãƒˆ": "ãƒ†ã‚¹ãƒˆã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã‚„çŸ¥è­˜ã®å‹•ä½œç¢ºèªã‚„è©•ä¾¡ã‚’è¡Œã†é‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚é©åˆ‡ãªãƒ†ã‚¹ãƒˆã«ã‚ˆã‚Šã€å“è³ªã®å‘ä¸Šã‚„å•é¡Œã®æ—©æœŸç™ºè¦‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",
            "default": f"ã€Œ{prompt}ã€ã«ã¤ã„ã¦ã€åŸºæœ¬çš„ãªæƒ…å ±ã‚’ãŠä¼ãˆã—ã¾ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ãŒå¿…è¦ã§ã—ãŸã‚‰ã€å…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚"
        }
        
        for keyword, response in fallback_responses.items():
            if keyword in prompt and keyword != "default":
                return response
        
        return fallback_responses["default"]
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            print("ğŸš€ Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹ï¼ˆãƒ¡ãƒ¢ãƒªä¸€è²«æ€§ç‰ˆï¼‰")
            
            # åˆæœŸåŒ–å‰ãƒ¡ãƒ¢ãƒªæ¸¬å®š
            self.measure_memory_consistently("åˆæœŸåŒ–å‰", wait_seconds=0)
            
            if not self.check_ollama_connection():
                return False
            
            models = self.get_available_models()
            if not models:
                print("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                return False
            
            if not self.select_model():
                return False
            
            # åˆæœŸåŒ–å¾Œãƒ¡ãƒ¢ãƒªæ¸¬å®š
            self.measure_memory_consistently("åˆæœŸåŒ–å¾Œ", wait_seconds=2)
            
            self.apply_infer_os_optimizations()
            
            # æœ€é©åŒ–é©ç”¨å¾Œãƒ¡ãƒ¢ãƒªæ¸¬å®š
            self.measure_memory_consistently("æœ€é©åŒ–é©ç”¨å¾Œ", wait_seconds=2)
            
            onnx_created = self.create_safe_onnx_session()
            if not onnx_created:
                print("âš ï¸ ONNXæ¨è«–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
            
            # å®Œäº†å¾Œãƒ¡ãƒ¢ãƒªæ¸¬å®š
            self.measure_memory_consistently("åˆæœŸåŒ–å®Œäº†", wait_seconds=2)
            
            print("âœ… Ollama + infer-OSåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼ˆãƒ¡ãƒ¢ãƒªä¸€è²«æ€§ç‰ˆï¼‰")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªä¸€è²«æ€§ç‰ˆï¼‰"""
        print("\nğŸ¯ Ollama + infer-OSåˆ¶å¾¡ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªä¸€è²«æ€§ç‰ˆï¼‰")
        print(f"ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.current_model['name'] if self.current_model else 'ãªã—'}")
        print(f"âš¡ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        print("ğŸ’¡ ã‚³ãƒãƒ³ãƒ‰:")
        print("  'quit' - çµ‚äº†")
        print("  'memory' - è©³ç´°ãƒ¡ãƒ¢ãƒªçµ±è¨ˆè¡¨ç¤º")
        print("  'analysis' - åŒ…æ‹¬çš„ãƒ¡ãƒ¢ãƒªåˆ†æ")
        print("  'compare' - æœ€é©åŒ–åŠ¹æœæ¯”è¼ƒ")
        print("  'clear' - ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢")
        print("  'toggle' - infer-OSæœ€é©åŒ–ON/OFFåˆ‡ã‚Šæ›¿ãˆ")
        print("=" * 70)
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹æ™‚ãƒ¡ãƒ¢ãƒªæ¸¬å®š
        self.measure_memory_consistently("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–é–‹å§‹", wait_seconds=2)
        
        try:
            while True:
                try:
                    infer_os_status = "ON" if self.infer_os_enabled else "OFF"
                    prompt = input(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [infer-OS:{infer_os_status}]: ").strip()
                    
                    if not prompt:
                        continue
                    
                    if prompt.lower() == 'quit':
                        print("ğŸ‘‹ ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                        break
                    
                    if prompt.lower() == 'memory':
                        self.show_detailed_memory_stats()
                        continue
                    
                    if prompt.lower() == 'analysis':
                        analysis = self.comprehensive_memory_analysis("ãƒ†ã‚¹ãƒˆåˆ†æ", 50)
                        print(f"\nğŸ¯ åˆ†æçµæœ: {analysis['result'][:100]}...")
                        continue
                    
                    if prompt.lower() == 'compare':
                        comparison = self.accurate_optimization_comparison("æ¯”è¼ƒãƒ†ã‚¹ãƒˆ", 50)
                        print(f"\nğŸ“Š æœ€é©åŒ–åŠ¹æœ: {comparison['effectiveness']}")
                        continue
                    
                    if prompt.lower() == 'clear':
                        self.clear_memory_cache()
                        self.measure_memory_consistently("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å¾Œ", wait_seconds=3)
                        continue
                    
                    if prompt.lower() == 'toggle':
                        self.infer_os_enabled = not self.infer_os_enabled
                        status = "æœ‰åŠ¹" if self.infer_os_enabled else "ç„¡åŠ¹"
                        print(f"ğŸ”„ infer-OSæœ€é©åŒ–ã‚’{status}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                        if self.infer_os_enabled:
                            self.apply_infer_os_optimizations()
                        continue
                    
                    # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆä¸€è²«ã—ãŸãƒ¡ãƒ¢ãƒªæ¸¬å®šä»˜ãï¼‰
                    print("ğŸš€ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹...")
                    analysis = self.comprehensive_memory_analysis(prompt, 100)
                    
                    print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
                    print(analysis['result'])
                    print(f"\nâ±ï¸ ç”Ÿæˆæ™‚é–“: {analysis['generation_time']:.2f}ç§’")
                    print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {analysis['memory_reduction']:.1f}%")
                    
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        finally:
            # çµ‚äº†æ™‚ãƒ¡ãƒ¢ãƒªæ¸¬å®š
            self.measure_memory_consistently("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–çµ‚äº†", wait_seconds=2)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆãƒ¡ãƒ¢ãƒªä¸€è²«æ€§ç‰ˆï¼‰"""
    parser = argparse.ArgumentParser(description="Ollama + infer-OSæœ€é©åŒ–åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ä¹–é›¢ä¿®æ­£ç‰ˆï¼‰")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--prompt", type=str, help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=100, help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--template", type=str, default="simple", help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    parser.add_argument("--model", type=str, help="ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--ollama-host", type=str, default="http://localhost:11434", help="Ollamaæ¥ç¶šå…ˆ")
    parser.add_argument("--infer-os", action="store_true", default=True, help="infer-OSæœ€é©åŒ–æœ‰åŠ¹")
    parser.add_argument("--no-infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ç„¡åŠ¹")
    parser.add_argument("--compare", action="store_true", help="æœ€é©åŒ–åŠ¹æœæ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰")
    
    args = parser.parse_args()
    
    infer_os_enabled = args.infer_os and not args.no_infer_os
    
    system = OllamaMemoryConsistentController(ollama_host=args.ollama_host)
    system.infer_os_enabled = infer_os_enabled
    
    if not system.initialize_system():
        print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    if args.model:
        if not system.select_model(args.model):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ« '{args.model}' ã®é¸æŠã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
    
    try:
        if args.interactive:
            system.run_interactive_mode()
        elif args.compare:
            # æœ€é©åŒ–åŠ¹æœæ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰
            prompt = args.prompt or "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"
            comparison = system.accurate_optimization_comparison(prompt, args.tokens)
            
            print("\nğŸ“Š infer-OSæœ€é©åŒ–åŠ¹æœæ¯”è¼ƒçµæœ:")
            print("=" * 60)
            
            on_data = comparison["optimization_on"]["measurements"]
            off_data = comparison["optimization_off"]["measurements"]
            effectiveness = comparison["effectiveness"]
            
            print(f"âš¡ æœ€é©åŒ–æœ‰åŠ¹:")
            print(f"  ğŸ“Š å®‰å®šåŒ–å¾Œ: {on_data['stabilized']:.1f}%")
            print(f"  ğŸ”¥ ç”Ÿæˆä¸­æœ€å¤§: {on_data['during_generation_max']:.1f}%")
            
            print(f"\nâŒ æœ€é©åŒ–ç„¡åŠ¹:")
            print(f"  ğŸ“Š å®‰å®šåŒ–å¾Œ: {off_data['stabilized']:.1f}%")
            print(f"  ğŸ”¥ ç”Ÿæˆä¸­æœ€å¤§: {off_data['during_generation_max']:.1f}%")
            
            print(f"\nğŸ’¡ æœ€é©åŒ–åŠ¹æœ:")
            print(f"  ğŸ“‰ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {effectiveness['memory_reduction']:.1f}% ({effectiveness['memory_reduction_percent']:.1f}%å‰Šæ¸›)")
            print(f"  ğŸ”¥ ãƒ”ãƒ¼ã‚¯å‰Šæ¸›: {effectiveness['peak_reduction']:.1f}%")
            print(f"  â±ï¸ æ™‚é–“çŸ­ç¸®: {effectiveness['generation_time_diff']:.2f}ç§’")
            
            system.show_detailed_memory_stats()
            
        elif args.prompt:
            # å˜ç™ºç”Ÿæˆï¼ˆä¸€è²«ã—ãŸãƒ¡ãƒ¢ãƒªæ¸¬å®šä»˜ãï¼‰
            analysis = system.comprehensive_memory_analysis(args.prompt, args.tokens, args.template)
            
            print(f"\nğŸ¯ ç”Ÿæˆçµæœ:")
            print(analysis['result'])
            print(f"\nğŸ“Š è©³ç´°ãƒ¡ãƒ¢ãƒªåˆ†æ:")
            print(f"  ğŸ“Š ç”Ÿæˆå‰: {analysis['measurements']['pre_generation']:.1f}%")
            print(f"  ğŸ”¥ ç”Ÿæˆä¸­æœ€å¤§: {analysis['measurements']['during_generation_max']:.1f}%")
            print(f"  ğŸ“Š ç”Ÿæˆç›´å¾Œ: {analysis['measurements']['post_generation']:.1f}%")
            print(f"  âœ… å®‰å®šåŒ–å¾Œ: {analysis['measurements']['stabilized']:.1f}%")
            print(f"  ğŸ“‰ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {analysis['memory_reduction']:.1f}%")
            print(f"  â±ï¸ ç”Ÿæˆæ™‚é–“: {analysis['generation_time']:.2f}ç§’")
            
            system.show_detailed_memory_stats()
        else:
            print("ä½¿ç”¨æ–¹æ³•: --interactive, --prompt, ã¾ãŸã¯ --compare ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("ä¾‹: python ollama_memory_consistent_system.py --interactive")
            print("ä¾‹: python ollama_memory_consistent_system.py --prompt 'äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦' --tokens 200")
            print("ä¾‹: python ollama_memory_consistent_system.py --compare --prompt 'äººå·¥çŸ¥èƒ½ã®æœªæ¥' --tokens 150")
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

