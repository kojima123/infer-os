#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œæœ€å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢

æ—¥æœ¬èªã«å¯¾å¿œã—ãŸæœ€ã‚‚é‡ã„LLMãƒ¢ãƒ‡ãƒ«ï¼ˆmatsuo-lab/weblab-10bï¼‰ã§ã®
Infer-OSæœ€é©åŒ–åŠ¹æœã‚’å®Ÿéš›ã®æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ã§ä½“é¨“

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
- matsuo-lab/weblab-10b (10Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - æœ€é‡é‡ç´šæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
- rinna/youri-7b-chat (7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - é‡é‡ç´šãƒãƒ£ãƒƒãƒˆç‰¹åŒ–
- cyberagent/open-calm-7b (7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - é‡é‡ç´šãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«
- stabilityai/japanese-stablelm-instruct-alpha-7b (7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) - é‡é‡ç´šæŒ‡ç¤ºè¿½å¾“

ç‰¹å¾´:
- æ—¥æœ¬èªãƒã‚¤ãƒ†ã‚£ãƒ–å¯¾å¿œ
- æ–‡åŒ–çš„ç†è§£ãƒ»æ•¬èªå¯¾å¿œ
- å°‚é–€ç”¨èªãƒ»æŠ€è¡“æ–‡æ›¸å¯¾å¿œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–

ä½¿ç”¨æ–¹æ³•:
    python japanese_heavy_llm_demo.py --model matsuo-lab/weblab-10b --use-8bit --interactive
"""

import sys
import os
import gc
import time
import traceback
import argparse
from typing import Dict, List, Optional, Any
from infer_os_comparison_benchmark import ComparisonBenchmark, InferOSMode
import psutil
import re
import datetime

# ONNXå¤‰æ›æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from onnx_converter import ONNXModelConverter, ONNXTextGenerator, ONNX_AVAILABLE
except ImportError:
    ONNX_AVAILABLE = False
    ONNXModelConverter = None
    ONNXTextGenerator = None

# é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from advanced_quantization_optimizer import (
        AdvancedQuantizationOptimizer, QuantizationProfile, QuantizationConfig,
        WeightQuantizer, KVCacheQuantizer, IOBindingOptimizer, QLinearMatMulOptimizer
    )
    ADVANCED_QUANT_AVAILABLE = True
except ImportError:
    ADVANCED_QUANT_AVAILABLE = False
    AdvancedQuantizationOptimizer = None
    QuantizationProfile = None

try:
    import torch
    import torch.nn as nn
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
    import numpy as np
    
    # æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    try:
        from accelerate import init_empty_weights, load_checkpoint_and_dispatch
        from accelerate.utils import get_balanced_memory
        ACCELERATE_AVAILABLE = True
    except ImportError:
        ACCELERATE_AVAILABLE = False
    
    try:
        from bitsandbytes import BitsAndBytesConfig
        BITSANDBYTES_AVAILABLE = True
    except ImportError:
        BITSANDBYTES_AVAILABLE = False
        
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install torch transformers accelerate bitsandbytes numpy psutil")
    sys.exit(1)

# æ—¥æœ¬èªå¯¾å¿œæœ€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å®šç¾©
JAPANESE_HEAVY_MODELS = {
    "openai/gpt-oss-20b": {
        "parameters": 20_000_000_000,
        "size_gb": {"fp32": 80, "fp16": 40, "int8": 20, "int4": 10},
        "min_memory_gb": 64,
        "recommended_memory_gb": 80,
        "description": "è¶…é‡é‡ç´š 20Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ GPT-OSSï¼ˆOpenAIï¼‰",
        "rank": 1,
        "japanese_quality": "é«˜",
        "speciality": "æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ»å¤šè¨€èªå¯¾å¿œ"
    },
    "matsuo-lab/weblab-10b": {
        "parameters": 10_737_418_240,
        "size_gb": {"fp32": 43, "fp16": 21.5, "int8": 10.8, "int4": 5.4},
        "min_memory_gb": 48,
        "recommended_memory_gb": 64,
        "description": "æœ€é‡é‡ç´š 10Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªç‰¹åŒ–ï¼ˆæ±å¤§æ¾å°¾ç ”ï¼‰",
        "rank": 2,
        "japanese_quality": "æœ€é«˜",
        "speciality": "å­¦è¡“ãƒ»æŠ€è¡“æ–‡æ›¸"
    },
    "rinna/youri-7b-chat": {
        "parameters": 7_241_732_096,
        "size_gb": {"fp32": 28, "fp16": 14, "int8": 7, "int4": 3.5},
        "min_memory_gb": 32,
        "recommended_memory_gb": 48,
        "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆç‰¹åŒ–",
        "rank": 3,
        "japanese_quality": "é«˜",
        "speciality": "å¯¾è©±ãƒ»ãƒãƒ£ãƒƒãƒˆ"
    },
    "cyberagent/open-calm-7b": {
        "parameters": 6_853_681_152,
        "size_gb": {"fp32": 27, "fp16": 13.5, "int8": 6.8, "int4": 3.4},
        "min_memory_gb": 32,
        "recommended_memory_gb": 48,
        "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥è‹±ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«",
        "rank": 4,
        "japanese_quality": "é«˜",
        "speciality": "ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ãƒ»ãƒ“ã‚¸ãƒã‚¹"
    },
    "stabilityai/japanese-stablelm-instruct-alpha-7b": {
        "parameters": 6_738_415_616,
        "size_gb": {"fp32": 27, "fp16": 13.5, "int8": 6.8, "int4": 3.4},
        "min_memory_gb": 32,
        "recommended_memory_gb": 48,
        "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æŒ‡ç¤ºè¿½å¾“ç‰¹åŒ–",
        "rank": 5,
        "japanese_quality": "é«˜",
        "speciality": "æŒ‡ç¤ºè¿½å¾“ãƒ»ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"
    },
    "rinna/japanese-gpt-neox-3.6b": {
        "parameters": 3_600_000_000,
        "size_gb": {"fp32": 14, "fp16": 7, "int8": 3.5, "int4": 1.8},
        "min_memory_gb": 16,
        "recommended_memory_gb": 24,
        "description": "ä¸­é‡ç´š 3.6Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªGPT",
        "rank": 6,
        "japanese_quality": "ä¸­",
        "speciality": "æ±ç”¨æ—¥æœ¬èªç”Ÿæˆ"
    }
}

# æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«
JAPANESE_PROMPTS = {
    "æ–‡ç« ç”Ÿæˆ": [
        "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦ã€æŠ€è¡“çš„ãªè¦³ç‚¹ã‹ã‚‰è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "æ¡œãŒå’²ãæ˜¥ã®æ—¥ã«ã€ä¸»äººå…¬ãŒæ–°ã—ã„å‡ºä¼šã„ã‚’çµŒé¨“ã™ã‚‹çŸ­ç·¨å°èª¬ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
        "æ—¥æœ¬ã®å››å­£ã®ç¾ã—ã•ã«ã¤ã„ã¦ã€è©©çš„ãªè¡¨ç¾ã§æå†™ã—ã¦ãã ã•ã„ã€‚"
    ],
    "æŠ€è¡“è§£èª¬": [
        "æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹æ·±å±¤å­¦ç¿’ã®ä»•çµ„ã¿ã‚’ã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®åŸºæœ¬åŸç†ã¨å°†æ¥ã®å¿œç”¨å¯èƒ½æ€§ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³æŠ€è¡“ã®ä»•çµ„ã¿ã¨ãƒ“ã‚¸ãƒã‚¹ã¸ã®å¿œç”¨ä¾‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    ],
    "ãƒ“ã‚¸ãƒã‚¹": [
        "æ–°è£½å“ã®å¸‚å ´æŠ•å…¥ã«é–¢ã™ã‚‹ææ¡ˆæ›¸ã®æ¦‚è¦ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
        "ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯å°å…¥ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚",
        "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®é‡è¦æ€§ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    ],
    "å‰µä½œ": [
        "æœªæ¥éƒ½å¸‚ã‚’èˆå°ã«ã—ãŸSFå°èª¬ã®å†’é ­éƒ¨åˆ†ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
        "æ–™ç†ã‚’ãƒ†ãƒ¼ãƒã«ã—ãŸå¿ƒæ¸©ã¾ã‚‹ã‚¨ãƒƒã‚»ã‚¤ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚",
        "å®‡å®™æ¢æŸ»ã‚’ãƒ†ãƒ¼ãƒã«ã—ãŸå†’é™ºå°èª¬ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’è€ƒãˆã¦ãã ã•ã„ã€‚"
    ],
    "æ•™è‚²": [
        "å°å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«ã€åœ°çƒæ¸©æš–åŒ–ã®åŸå› ã¨å¯¾ç­–ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "æ—¥æœ¬ã®æ­´å²ã«ãŠã‘ã‚‹æ˜æ²»ç¶­æ–°ã®æ„ç¾©ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
        "æ•°å­¦ã®å¾®åˆ†ç©åˆ†ã®åŸºæœ¬æ¦‚å¿µã‚’å…·ä½“ä¾‹ã¨ã¨ã‚‚ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
    ]
}

class JapaneseHeavyLLMDemo:
    """æ—¥æœ¬èªå¯¾å¿œæœ€å¤§è¦æ¨¡LLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str, use_4bit: bool = False, use_8bit: bool = False, 
                 use_onnx: bool = False, onnx_optimization_level: int = 2,
                 quantization_profile: str = "balanced", use_advanced_quant: bool = False,
                 infer_os_enabled: bool = True):
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±ã®å–å¾—
        import platform
        self.platform_info = {
            "system": platform.system(),
            "version": platform.version(),
            "machine": platform.machine(),
            "python_version": platform.python_version()
        }
        
        # Windowsç’°å¢ƒã®ç‰¹åˆ¥å‡¦ç†
        self.is_windows = self.platform_info["system"] == "Windows"
        if self.is_windows:
            print(f"ğŸªŸ Windowsç’°å¢ƒã‚’æ¤œå‡º: {self.platform_info['system']} {self.platform_info['version']}")
            print("ğŸ’¡ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        self.model_name = model_name
        self.use_4bit = use_4bit
        self.use_8bit = use_8bit
        self.use_onnx = use_onnx
        self.onnx_optimization_level = onnx_optimization_level
        self.use_advanced_quant = use_advanced_quant
        self.quantization_profile = quantization_profile
        self.infer_os_enabled = infer_os_enabled  # Infer-OSæ©Ÿèƒ½ã®æœ‰åŠ¹/ç„¡åŠ¹
        self.model = None
        self.tokenizer = None
        self.onnx_converter = None
        self.onnx_generator = None
        self.advanced_quantizer = None
        self.optimization_applied = False
        
        # Infer-OSæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        self.comparison_benchmark = None
        
        # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–å™¨ã®åˆæœŸåŒ–
        if self.use_advanced_quant and ADVANCED_QUANT_AVAILABLE:
            profile_map = {
                "safe": QuantizationProfile.SAFE,
                "balanced": QuantizationProfile.BALANCED,
                "aggressive": QuantizationProfile.AGGRESSIVE
            }
            profile = profile_map.get(quantization_profile, QuantizationProfile.BALANCED)
            self.advanced_quantizer = AdvancedQuantizationOptimizer(model_name, profile)
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
        self.system_info = self._get_system_info()
        
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œæœ€å¤§è¦æ¨¡LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
        print(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {model_name}")
        if self.use_onnx:
            print(f"ğŸš€ ONNX Runtimeæœ€é©åŒ–: æœ‰åŠ¹")
        if self.use_advanced_quant:
            print(f"âš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–: æœ‰åŠ¹ ({quantization_profile}ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«)")
        print(f"ğŸ”§ Infer-OSæ©Ÿèƒ½: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        self._print_system_info()
        self._validate_system_requirements()
    
    def _get_system_info(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        memory = psutil.virtual_memory()
        
        info = {
            "python_version": sys.version,
            "torch_version": torch.__version__,
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_used_gb": memory.used / (1024**3),
            "memory_percent": memory.percent,
        }
        
        # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå¯¾å¿œçŠ¶æ³
        info.update({
            "accelerate_available": ACCELERATE_AVAILABLE,
            "bitsandbytes_available": BITSANDBYTES_AVAILABLE,
        })
        
        return info
    
    def _print_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {sys.version.split()[0]}")
        print(f"  PyTorch: {torch.__version__}")
        print(f"  CPU: {self.system_info['cpu_count']}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {self.system_info['memory_total_gb']:.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {self.system_info['memory_used_gb']:.1f}GB ({self.system_info['memory_percent']:.1f}%)")
        print(f"  åˆ©ç”¨å¯èƒ½: {self.system_info['memory_available_gb']:.1f}GB")
        
        print(f"\nğŸ”§ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
        print(f"  Accelerate: {'âœ…' if ACCELERATE_AVAILABLE else 'âŒ'}")
        print(f"  BitsAndBytes: {'âœ…' if BITSANDBYTES_AVAILABLE else 'âŒ'}")
        print(f"  ONNX Runtime: {'âœ…' if ONNX_AVAILABLE else 'âŒ'}")
        print(f"  é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–: {'âœ…' if ADVANCED_QUANT_AVAILABLE else 'âŒ'}")
    
    def _validate_system_requirements(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æ¤œè¨¼ã—ã€ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã¯è‡ªå‹•æœ€é©åŒ–ã‚’é©ç”¨"""
        if self.model_name in JAPANESE_HEAVY_MODELS:
            model_info = JAPANESE_HEAVY_MODELS[self.model_name]
            min_memory = model_info["min_memory_gb"]
            recommended_memory = model_info["recommended_memory_gb"]
            
            print(f"\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«è¦ä»¶:")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {model_info['description']}")
            print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_info['parameters']:,}")
            print(f"  æ—¥æœ¬èªå“è³ª: {model_info['japanese_quality']}")
            print(f"  å°‚é–€åˆ†é‡: {model_info['speciality']}")
            print(f"  æœ€å°ãƒ¡ãƒ¢ãƒª: {min_memory}GB")
            print(f"  æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {recommended_memory}GB")
            
            # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªå–å¾—
            available_memory = self.system_info['memory_available_gb']
            
            # ãƒ¡ãƒ¢ãƒªä¸è¶³ãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•æœ€é©åŒ–
            original_use_4bit = self.use_4bit
            original_use_8bit = self.use_8bit
            
            # é‡å­åŒ–é©ç”¨æ™‚ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶è¨ˆç®—
            if self.use_4bit:
                required_memory = model_info["size_gb"]["int4"]
                print(f"  INT4é‡å­åŒ–æ™‚: {required_memory}GB")
            elif self.use_8bit:
                required_memory = model_info["size_gb"]["int8"]
                print(f"  INT8é‡å­åŒ–æ™‚: {required_memory}GB")
            else:
                required_memory = model_info["size_gb"]["fp16"]
                print(f"  FP16æ™‚: {required_memory}GB")
            
            # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®è‡ªå‹•æœ€é©åŒ–ãƒ­ã‚¸ãƒƒã‚¯
            if available_memory < required_memory:
                print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                print(f"  å¿…è¦: {required_memory}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
                
                # æ®µéšçš„æœ€é©åŒ–é©ç”¨
                if not self.use_4bit and not self.use_8bit:
                    # é‡å­åŒ–æœªé©ç”¨ã®å ´åˆã€8bité‡å­åŒ–ã‚’è‡ªå‹•é©ç”¨
                    self.use_8bit = True
                    required_memory = model_info["size_gb"]["int8"]
                    print(f"ğŸ”§ è‡ªå‹•æœ€é©åŒ–: 8bité‡å­åŒ–ã‚’é©ç”¨ã—ã¾ã™")
                    print(f"  æœ€é©åŒ–å¾Œå¿…è¦ãƒ¡ãƒ¢ãƒª: {required_memory}GB")
                    
                    if available_memory < required_memory:
                        # 8bité‡å­åŒ–ã§ã‚‚ä¸è¶³ã®å ´åˆã€4bité‡å­åŒ–ã‚’é©ç”¨
                        self.use_8bit = False
                        self.use_4bit = True
                        required_memory = model_info["size_gb"]["int4"]
                        print(f"ğŸ”§ è¿½åŠ æœ€é©åŒ–: 4bité‡å­åŒ–ã‚’é©ç”¨ã—ã¾ã™")
                        print(f"  æœ€é©åŒ–å¾Œå¿…è¦ãƒ¡ãƒ¢ãƒª: {required_memory}GB")
                        
                        if available_memory < required_memory:
                            print(f"âŒ 4bité‡å­åŒ–ã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã™")
                            print(f"ğŸ’¡ ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™")
                            return False
                        else:
                            print(f"âœ… 4bité‡å­åŒ–ã«ã‚ˆã‚Šå®Ÿè¡Œå¯èƒ½ã§ã™")
                    else:
                        print(f"âœ… 8bité‡å­åŒ–ã«ã‚ˆã‚Šå®Ÿè¡Œå¯èƒ½ã§ã™")
                
                elif self.use_8bit and not self.use_4bit:
                    # 8bité‡å­åŒ–ã§ã‚‚ä¸è¶³ã®å ´åˆã€4bité‡å­åŒ–ã«å¤‰æ›´
                    self.use_8bit = False
                    self.use_4bit = True
                    required_memory = model_info["size_gb"]["int4"]
                    print(f"ğŸ”§ è‡ªå‹•æœ€é©åŒ–: 4bité‡å­åŒ–ã«å¤‰æ›´ã—ã¾ã™")
                    print(f"  æœ€é©åŒ–å¾Œå¿…è¦ãƒ¡ãƒ¢ãƒª: {required_memory}GB")
                    
                    if available_memory < required_memory:
                        print(f"âŒ 4bité‡å­åŒ–ã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã™")
                        print(f"ğŸ’¡ ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™")
                        return False
                    else:
                        print(f"âœ… 4bité‡å­åŒ–ã«ã‚ˆã‚Šå®Ÿè¡Œå¯èƒ½ã§ã™")
                
                else:
                    # æ—¢ã«4bité‡å­åŒ–é©ç”¨æ¸ˆã¿ã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³
                    print(f"âŒ æœ€å¤§æœ€é©åŒ–ã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã§ã™")
                    print(f"ğŸ’¡ ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™")
                    return False
                
                # æœ€é©åŒ–è¨­å®šå¤‰æ›´ã®é€šçŸ¥
                if original_use_4bit != self.use_4bit or original_use_8bit != self.use_8bit:
                    print(f"\nğŸ¯ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚æœ€é©åŒ–è¨­å®šã‚’è‡ªå‹•å¤‰æ›´ã—ã¾ã—ãŸ:")
                    print(f"  å¤‰æ›´å‰: 4bit={original_use_4bit}, 8bit={original_use_8bit}")
                    print(f"  å¤‰æ›´å¾Œ: 4bit={self.use_4bit}, 8bit={self.use_8bit}")
                    print(f"  ã“ã‚Œã«ã‚ˆã‚Šæœ€é©åŒ–å¾Œã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™")
                
            elif available_memory < recommended_memory:
                print(f"âš ï¸  æ¨å¥¨ãƒ¡ãƒ¢ãƒªæœªæº€ã§ã™")
                print(f"  æ¨å¥¨: {recommended_memory}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
                print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šæ€§å‘ä¸Š")
            else:
                print(f"âœ… ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™")
        
        return True
    
    def list_available_models(self):
        """åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º"""
        print(f"\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œæœ€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        
        sorted_models = sorted(JAPANESE_HEAVY_MODELS.items(), key=lambda x: x[1]["rank"])
        
        for model_name, info in sorted_models:
            rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ…", "ğŸ“‹"][info["rank"] - 1] if info["rank"] <= 5 else "ğŸ“‹"
            print(f"  {rank_emoji} {model_name}")
            print(f"    {info['description']}")
            print(f"    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {info['parameters']:,}")
            print(f"    æ—¥æœ¬èªå“è³ª: {info['japanese_quality']}")
            print(f"    å°‚é–€åˆ†é‡: {info['speciality']}")
            print(f"    æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {info['recommended_memory_gb']}GB")
            print()
    
    def show_sample_prompts(self):
        """æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“ æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        
        for category, prompts in JAPANESE_PROMPTS.items():
            print(f"\nã€{category}ã€‘")
            for i, prompt in enumerate(prompts, 1):
                print(f"  {i}. {prompt}")
    
    def create_quantization_config(self) -> Optional[Any]:
        """é‡å­åŒ–è¨­å®šã‚’ä½œæˆï¼ˆCPUå¯¾å¿œç‰ˆï¼‰"""
        if not BITSANDBYTES_AVAILABLE:
            print("âš ï¸ BitsAndBytesæœªå¯¾å¿œã®ãŸã‚ã€é‡å­åŒ–ç„¡ã—ã§å®Ÿè¡Œã—ã¾ã™")
            return None
        
        try:
            if self.use_4bit:
                print("ğŸ”§ 4bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸï¼ˆæ—¥æœ¬èªæœ€é©åŒ–ï¼‰")
                config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float32,  # CPUç”¨
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    llm_int8_enable_fp32_cpu_offload=True
                )
                return config
            elif self.use_8bit:
                print("ğŸ”§ 8bité‡å­åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸï¼ˆæ—¥æœ¬èªæœ€é©åŒ–ï¼‰")
                config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_enable_fp32_cpu_offload=True
                )
                return config
        except Exception as e:
            print(f"âš ï¸ é‡å­åŒ–è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            return None
        
        return None
    
    def pre_download_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆMXFP4ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰"""
        try:
            from huggingface_hub import snapshot_download
            import os
            
            print("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print("âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            print(f"ğŸ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {cache_dir}")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            model_path = snapshot_download(
                repo_id=self.model_name,
                cache_dir=cache_dir,
                resume_download=True,
                local_files_only=False
            )
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {model_path}")
            return model_path
            
        except Exception as download_error:
            print(f"âš ï¸ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {download_error}")
            print("ğŸ’¡ æ¨™æº–ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ç¶šè¡Œã—ã¾ã™")
            return None
    
    def download_model_safely(self):
        """MXFP4ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            import os
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å…ˆã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            from transformers import AutoConfig
            print("ğŸ”§ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            config = AutoConfig.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                cache_dir=os.path.expanduser("~/.cache/huggingface/hub")
            )
            
            # MXFP4é‡å­åŒ–è¨­å®šã‚’å®Œå…¨ç„¡åŠ¹åŒ–ï¼ˆNoneType getã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            if hasattr(config, 'quantization_config') and config.quantization_config is not None:
                print(f"âš ï¸ MXFP4é‡å­åŒ–è¨­å®šã‚’æ¤œå‡º: {type(config.quantization_config)}")
                print("ğŸ”§ CPUç’°å¢ƒã®ãŸã‚MXFP4é‡å­åŒ–è¨­å®šã‚’å®Œå…¨å‰Šé™¤ã—ã¾ã™")
                # quantization_configå±æ€§è‡ªä½“ã‚’å‰Šé™¤
                delattr(config, 'quantization_config')
                print("âœ… quantization_configå±æ€§ã‚’å®Œå…¨å‰Šé™¤ã—ã¾ã—ãŸ")
            
            # è¨­å®šè¾æ›¸ã‹ã‚‰ã‚‚é‡å­åŒ–è¨­å®šã‚’å‰Šé™¤ï¼ˆNoneType getã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            if hasattr(config, '_name_or_path'):
                config_dict = config.to_dict()
                if 'quantization_config' in config_dict:
                    del config_dict['quantization_config']
                    print("âœ… è¨­å®šè¾æ›¸ã‹ã‚‰ã‚‚quantization_configã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            from transformers import AutoTokenizer
            print("ğŸ”§ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                cache_dir=os.path.expanduser("~/.cache/huggingface/hub")
            )
            
            print("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return config, tokenizer
            
        except Exception as e:
            print(f"âš ï¸ å®‰å…¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None

    def load_model_with_pre_download(self):
        """äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        
        # Step 1: äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        model_path = self.pre_download_model()
        
        # Step 2: è¨­å®šã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®‰å…¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        config, tokenizer = self.download_model_safely()
        
        # Step 3: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        if model_path and config:
            try:
                print("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
                
                model_kwargs = {
                    "config": config,  # MXFP4ç„¡åŠ¹åŒ–æ¸ˆã¿è¨­å®š
                    "trust_remote_code": True,
                    "torch_dtype": torch.float32,
                    "low_cpu_mem_usage": True,
                    "device_map": "cpu",
                    "local_files_only": True,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ä½¿ç”¨
                }
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    **model_kwargs
                )
                
                self.tokenizer = tokenizer
                print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆäº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹å¼ï¼‰")
                return True
                
            except Exception as load_error:
                print(f"âš ï¸ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ–¹å¼ã‚¨ãƒ©ãƒ¼: {load_error}")
                print("ğŸ’¡ æ¨™æº–æ–¹å¼ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
        
        # Step 4: æ¨™æº–æ–¹å¼ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return self.load_model_with_optimization()

    def load_model_with_optimization(self) -> bool:
        """æœ€é©åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("ğŸ“¥ æ—¥æœ¬èªå¯¾å¿œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print("âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶æ¤œè¨¼ã¨è‡ªå‹•æœ€é©åŒ–
            if not self._validate_system_requirements():
                print("âŒ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“")
                return False
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–é–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory:.1f}GB")
            
            # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
            if self.use_advanced_quant and self.advanced_quantizer:
                return self._load_with_advanced_quantization()
            
            # å¾“æ¥ã®é‡å­åŒ–è¨­å®š
            quantization_config = self.create_quantization_config()
            
            # MXFP4é‡å­åŒ–ã‚¨ãƒ©ãƒ¼å›é¿: ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’äº‹å‰ã«èª­ã¿è¾¼ã¿ã€é‡å­åŒ–è¨­å®šã‚’ç„¡åŠ¹åŒ–
            try:
                from transformers import AutoConfig
                print("ğŸ”§ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’äº‹å‰èª­ã¿è¾¼ã¿ä¸­...")
                model_config = AutoConfig.from_pretrained(
                    self.model_name,
                    trust_remote_code=True
                )
                
                # MXFP4é‡å­åŒ–è¨­å®šã‚’å¼·åˆ¶çš„ã«å®Œå…¨å‰Šé™¤ï¼ˆNoneType getã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                if hasattr(model_config, 'quantization_config') and model_config.quantization_config is not None:
                    print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ã«MXFP4é‡å­åŒ–è¨­å®šãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {type(model_config.quantization_config)}")
                    print("ğŸ”§ CPUç’°å¢ƒã®ãŸã‚MXFP4é‡å­åŒ–è¨­å®šã‚’å®Œå…¨å‰Šé™¤ã—ã¾ã™")
                    # quantization_configå±æ€§è‡ªä½“ã‚’å‰Šé™¤
                    delattr(model_config, 'quantization_config')
                    print("âœ… quantization_configå±æ€§ã‚’å®Œå…¨å‰Šé™¤ã—ã¾ã—ãŸ")
                
                # è¨­å®šè¾æ›¸ã‹ã‚‰ã‚‚é‡å­åŒ–è¨­å®šã‚’å‰Šé™¤ï¼ˆNoneType getã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                if hasattr(model_config, '_name_or_path'):
                    config_dict = model_config.to_dict()
                    if 'quantization_config' in config_dict:
                        del config_dict['quantization_config']
                        print("âœ… è¨­å®šè¾æ›¸ã‹ã‚‰ã‚‚quantization_configã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    
            except Exception as config_error:
                print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {config_error}")
                print("ğŸ’¡ æ¨™æº–è¨­å®šã§ç¶šè¡Œã—ã¾ã™")
                model_config = None
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰è¨­å®šï¼ˆCPUæœ€é©åŒ–ï¼‰
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float32,  # CPUç”¨
                "low_cpu_mem_usage": True,
                "device_map": "cpu",
            }
            
            # äº‹å‰èª­ã¿è¾¼ã¿ã—ãŸmodel_configã‚’ä½¿ç”¨ï¼ˆMXFP4é‡å­åŒ–ç„¡åŠ¹åŒ–æ¸ˆã¿ï¼‰
            if model_config is not None:
                model_kwargs["config"] = model_config
                print("ğŸ”§ MXFP4é‡å­åŒ–ç„¡åŠ¹åŒ–æ¸ˆã¿è¨­å®šã‚’é©ç”¨ã—ã¾ã™")
            
            # é‡å­åŒ–è¨­å®šã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
            # MXFP4é‡å­åŒ–ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã€CPUç’°å¢ƒã§ã¯é‡å­åŒ–è¨­å®šã‚’å®Œå…¨ã«é™¤å¤–
            if quantization_config is not None and torch.cuda.is_available():
                try:
                    model_kwargs["quantization_config"] = quantization_config
                    print("ğŸ”§ GPUç’°å¢ƒã®ãŸã‚é‡å­åŒ–è¨­å®šã‚’é©ç”¨ã—ã¾ã™")
                except Exception as e:
                    print(f"âš ï¸ é‡å­åŒ–è¨­å®šé©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
                    print("ğŸ’¡ é‡å­åŒ–ç„¡ã—ã§ç¶šè¡Œã—ã¾ã™")
            else:
                print("ğŸ’¡ CPUç’°å¢ƒã®ãŸã‚é‡å­åŒ–è¨­å®šã‚’å®Œå…¨ã«é™¤å¤–ã—ã¾ã™")
                # quantization_configã‚­ãƒ¼è‡ªä½“ã‚’è¨­å®šã—ãªã„ï¼ˆNoneType to_dictã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            
            print(f"ğŸ“¥ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ« '{self.model_name}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆæ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    **model_kwargs
                )
            except Exception as e:
                print(f"âš ï¸ æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                print("ğŸ’¡ åŸºæœ¬è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1: é‡å­åŒ–ç„¡ã— + CPUå¼·åˆ¶è¨­å®š
                basic_kwargs = {
                    "trust_remote_code": True,
                    "torch_dtype": torch.float32,
                    "low_cpu_mem_usage": True,
                    "device_map": "cpu",
                    # quantization_configã‚­ãƒ¼ã‚’å®Œå…¨ã«é™¤å¤–ï¼ˆNoneType to_dictã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
                }
                
                try:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        **basic_kwargs
                    )
                except Exception as e2:
                    print(f"âš ï¸ åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e2}")
                    print("ğŸ’¡ æœ€å°è¨­å®šã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2: æœ€å°è¨­å®šï¼ˆquantization_configã‚­ãƒ¼å®Œå…¨é™¤å¤–ï¼‰
                    minimal_kwargs = {
                        "trust_remote_code": True,
                        "torch_dtype": torch.float32,
                        "device_map": "cpu",
                        # quantization_configã‚­ãƒ¼ã‚’å®Œå…¨ã«é™¤å¤–
                    }
                    
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            **minimal_kwargs
                        )
                    except Exception as e3:
                        print(f"âš ï¸ æœ€å°è¨­å®šãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e3}")
                        print("ğŸ’¡ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨è¨­å®šãƒªã‚»ãƒƒãƒˆ")
                        
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯3: ç·Šæ€¥è¨­å®šï¼ˆquantization_configå®Œå…¨é™¤å¤–ï¼‰
                        emergency_kwargs = {
                            "trust_remote_code": True,
                            # quantization_configã‚­ãƒ¼ã‚’å®Œå…¨ã«é™¤å¤–
                        }
                        
                        self.model = AutoModelForCausalLM.from_pretrained(
                            self.model_name,
                            **emergency_kwargs
                        )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“ æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            # æ—¥æœ¬èªæœ€é©åŒ–é©ç”¨
            self._apply_japanese_optimizations()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–çµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            memory_used = final_memory - initial_memory
            
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.1f}GB")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}GB")
            print("âœ… æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            return False
    
    def _apply_japanese_optimizations(self):
        """æ—¥æœ¬èªå°‚ç”¨æœ€é©åŒ–ã‚’é©ç”¨"""
        try:
            print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå°‚ç”¨æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
            
            # CPUæœ€é©åŒ–è¨­å®š
            torch.set_num_threads(psutil.cpu_count())
            print(f"  âœ… CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š: {psutil.cpu_count()}")
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                print("  âœ… ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæœ‰åŠ¹åŒ–")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = True
                print("  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–")
            
            # æ—¥æœ¬èªç‰¹åŒ–è¨­å®š
            try:
                # æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†æœ€é©åŒ–
                if hasattr(self.model.config, 'vocab_size'):
                    print(f"  âœ… èªå½™ã‚µã‚¤ã‚º: {self.model.config.vocab_size:,}")
                
                # æ—¥æœ¬èªæ–‡è„ˆé•·æœ€é©åŒ–
                if hasattr(self.model.config, 'max_position_embeddings'):
                    print(f"  âœ… æœ€å¤§æ–‡è„ˆé•·: {self.model.config.max_position_embeddings}")
                
            except:
                pass
            
            self.optimization_applied = True
            print("ğŸš€ æ—¥æœ¬èªå°‚ç”¨æœ€é©åŒ–é©ç”¨å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ æ—¥æœ¬èªæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_japanese_text(self, prompt: str, max_length: int = 300, max_new_tokens: int = None) -> Dict:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰"}
        
        try:
            print(f"\nğŸ¯ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            print(f"æœ€å¤§é•·: {max_length}")
            
            # ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            initial_cpu = psutil.cpu_percent(interval=None)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # max_new_tokensã®é©åˆ‡ãªå‡¦ç†
            if max_new_tokens is not None:
                actual_max_new_tokens = max_new_tokens
            else:
                actual_max_new_tokens = max_length
            
            # ç”Ÿæˆè¨­å®šï¼ˆæ—¥æœ¬èªæœ€é©åŒ–ï¼‰
            generation_config = {
                "max_new_tokens": min(actual_max_new_tokens, 200),  # æœ€å¤§200ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ¶é™
                "min_new_tokens": 5,  # æœ€å°ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›
                "num_return_sequences": 1,
                "temperature": 0.7,  # æ¸©åº¦ã‚’ä¸‹ã’ã¦å®‰å®šæ€§å‘ä¸Š
                "do_sample": True,
                "top_p": 0.8,  # top_pã‚’ä¸‹ã’ã¦è¨ˆç®—é‡å‰Šæ¸›
                "top_k": 30,   # top_kã‚’ä¸‹ã’ã¦è¨ˆç®—é‡å‰Šæ¸›
                "repetition_penalty": 1.1,  # ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶ã‚’è»½æ¸›
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
                "early_stopping": True,  # æ—©æœŸåœæ­¢ã‚’æœ‰åŠ¹åŒ–
                "num_beams": 1,  # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã‚’ç„¡åŠ¹åŒ–ï¼ˆé«˜é€ŸåŒ–ï¼‰
                "length_penalty": 1.0,  # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ç„¡åŠ¹åŒ–
            }
            
            # ç”Ÿæˆå®Ÿè¡Œï¼ˆæ™‚é–“ãƒ»ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®šï¼‰
            start_time = time.time()
            
            # token_type_idsã‚¨ãƒ©ãƒ¼å›é¿: ä¸è¦ãªã‚­ãƒ¼ã‚’é™¤å»
            model_inputs = {k: v for k, v in inputs.items() if k != 'token_type_ids'}
            
            # ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ä»˜ãæ¨è«–å®Ÿè¡Œ
            import threading
            import platform
            import queue
            
            def run_inference_with_timeout(model_inputs, generation_config, timeout_seconds):
                """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°"""
                result_queue = queue.Queue()
                exception_queue = queue.Queue()
                
                def inference_worker():
                    try:
                        print(f"â±ï¸ æ¨è«–å®Ÿè¡Œä¸­ï¼ˆæœ€å¤§{timeout_seconds//60}åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰...")
                        with torch.no_grad():
                            outputs = self.model.generate(
                                **model_inputs,
                                **generation_config
                            )
                        result_queue.put(outputs)
                        print("âœ… æ¨è«–å®Œäº†")
                    except Exception as e:
                        exception_queue.put(e)
                
                # æ¨è«–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                inference_thread = threading.Thread(target=inference_worker)
                inference_thread.daemon = True
                inference_thread.start()
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾…æ©Ÿ
                inference_thread.join(timeout=timeout_seconds)
                
                if inference_thread.is_alive():
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ
                    print(f"â° æ¨è«–å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout_seconds//60}åˆ†åˆ¶é™ï¼‰")
                    return None
                
                # ä¾‹å¤–ãƒã‚§ãƒƒã‚¯
                if not exception_queue.empty():
                    raise exception_queue.get()
                
                # çµæœå–å¾—
                if not result_queue.empty():
                    return result_queue.get()
                
                return None
            
            # æ®µéšçš„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå®Ÿè¡Œ
            outputs = None
            
            try:
                # ç¬¬1æ®µéš: é€šå¸¸è¨­å®šã§10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                outputs = run_inference_with_timeout(model_inputs, generation_config, 600)
                
                if outputs is None:
                    print("ğŸ’¡ ã‚ˆã‚Šè»½é‡ãªè¨­å®šã§å†è©¦è¡Œã—ã¾ã™")
                    
                    # è»½é‡è¨­å®šã§å†è©¦è¡Œ
                    lightweight_config = {
                        "max_new_tokens": min(50, actual_max_new_tokens),  # æœ€å¤§50ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ¶é™
                        "num_return_sequences": 1,
                        "temperature": 0.7,
                        "do_sample": True,
                        "top_p": 0.8,
                        "top_k": 30,
                        "repetition_penalty": 1.1,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "eos_token_id": self.tokenizer.eos_token_id,
                        "use_cache": True,
                        "early_stopping": True,  # æ—©æœŸåœæ­¢ã‚’æœ‰åŠ¹åŒ–
                    }
                    
                    # ç¬¬2æ®µéš: è»½é‡è¨­å®šã§3åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    outputs = run_inference_with_timeout(model_inputs, lightweight_config, 180)
                    
                    if outputs is None:
                        print("ğŸ’¡ æœ€å°è¨­å®šã§æœ€çµ‚è©¦è¡Œã—ã¾ã™")
                        
                        # æœ€å°è¨­å®šã§æœ€çµ‚è©¦è¡Œ
                        minimal_config = {
                            "max_new_tokens": 20,  # æœ€å¤§20ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ¶é™
                            "num_return_sequences": 1,
                            "temperature": 0.5,
                            "do_sample": False,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–
                            "pad_token_id": self.tokenizer.eos_token_id,
                            "eos_token_id": self.tokenizer.eos_token_id,
                            "use_cache": True,
                            "early_stopping": True,
                        }
                        
                        # ç¬¬3æ®µéš: æœ€å°è¨­å®šã§1åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        outputs = run_inference_with_timeout(model_inputs, minimal_config, 60)
                        
                        if outputs is None:
                            raise Exception("æ¨è«–å‡¦ç†ãŒå…¨ã¦ã®è¨­å®šã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ç’°å¢ƒã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
            except Exception as inference_error:
                print(f"âš ï¸ æ¨è«–å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {inference_error}")
                raise inference_error
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True
            )
            
            # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
            generated_only = generated_text[len(prompt):].strip()
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            final_cpu = psutil.cpu_percent(interval=None)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            total_tokens = len(outputs[0])
            
            # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            # æ—¥æœ¬èªå“è³ªè©•ä¾¡
            japanese_quality = self._evaluate_japanese_quality(generated_only)
            
            result = {
                "prompt": prompt,
                "generated_text": generated_only,
                "full_text": generated_text,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "tokens_per_second": tokens_per_second,
                "japanese_quality": japanese_quality,
                "resource_usage": {
                    "memory_used_gb": final_memory - initial_memory,
                    "memory_total_gb": final_memory,
                    "cpu_usage_percent": final_cpu
                },
                "optimization_applied": self.optimization_applied,
                "quantization_info": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit
                }
            }
            
            self._print_japanese_generation_results(result)
            return result
            
        except Exception as e:
            error_msg = f"æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ“Š ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                print("ğŸš¨ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€å°è¨­å®šã§å†è©¦è¡Œ")
                
                # æœ€å°é™ã®è¨­å®šã§å†è©¦è¡Œ
                emergency_inputs = self.tokenizer(
                    prompt[:100],  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’100æ–‡å­—ã«åˆ¶é™
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=128  # å…¥åŠ›é•·ã‚’å¤§å¹…ã«åˆ¶é™
                )
                
                emergency_config = {
                    "max_new_tokens": 20,  # æœ€å¤§20ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ¶é™
                    "num_return_sequences": 1,
                    "temperature": 0.5,
                    "do_sample": False,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–
                    "pad_token_id": self.tokenizer.eos_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                    "use_cache": True,
                    "early_stopping": True,
                }
                
                # 1åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ç·Šæ€¥å®Ÿè¡Œï¼ˆã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œï¼‰
                def emergency_inference():
                    result_queue = queue.Queue()
                    exception_queue = queue.Queue()
                    
                    def emergency_worker():
                        try:
                            print("â±ï¸ ç·Šæ€¥è¨­å®šã§å®Ÿè¡Œä¸­ï¼ˆæœ€å¤§1åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰...")
                            emergency_inputs = {k: v for k, v in emergency_inputs.items() if k != 'token_type_ids'}
                            
                            with torch.no_grad():
                                emergency_outputs = self.model.generate(
                                    **emergency_inputs,
                                    **emergency_config
                                )
                            result_queue.put(emergency_outputs)
                            print("âœ… ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
                        except Exception as e:
                            exception_queue.put(e)
                    
                    # ç·Šæ€¥æ¨è«–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
                    emergency_thread = threading.Thread(target=emergency_worker)
                    emergency_thread.daemon = True
                    emergency_thread.start()
                    
                    # 1åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾…æ©Ÿ
                    emergency_thread.join(timeout=60)
                    
                    if emergency_thread.is_alive():
                        print("âŒ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                        return None
                    
                    # ä¾‹å¤–ãƒã‚§ãƒƒã‚¯
                    if not exception_queue.empty():
                        raise exception_queue.get()
                    
                    # çµæœå–å¾—
                    if not result_queue.empty():
                        return result_queue.get()
                    
                    return None
                
                try:
                    emergency_outputs = emergency_inference()
                    
                    if emergency_outputs is not None:
                        emergency_text = self.tokenizer.decode(
                            emergency_outputs[0],
                            skip_special_tokens=True
                        )
                        
                        return {
                            "error": error_msg,
                            "emergency_result": emergency_text,
                            "note": "ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ã‚ˆã‚Šéƒ¨åˆ†çš„ãªçµæœã‚’ç”Ÿæˆ",
                            "traceback": traceback.format_exc()
                        }
                    else:
                        print("âŒ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—ã—ã¾ã—ãŸ")
                        
                except Exception as emergency_error:
                    print(f"âŒ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {emergency_error}")
                    
            except Exception as fallback_error:
                print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {fallback_error}")
            
            return {"error": error_msg, "traceback": traceback.format_exc()}
    
    def _evaluate_japanese_quality(self, text: str) -> Dict:
        """æ—¥æœ¬èªå“è³ªã‚’è©•ä¾¡"""
        try:
            # åŸºæœ¬çš„ãªæ—¥æœ¬èªå“è³ªæŒ‡æ¨™
            hiragana_count = sum(1 for c in text if '\u3040' <= c <= '\u309F')
            katakana_count = sum(1 for c in text if '\u30A0' <= c <= '\u30FF')
            kanji_count = sum(1 for c in text if '\u4E00' <= c <= '\u9FAF')
            ascii_count = sum(1 for c in text if ord(c) < 128)
            
            total_chars = len(text)
            
            if total_chars == 0:
                return {"error": "ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™"}
            
            japanese_ratio = (hiragana_count + katakana_count + kanji_count) / total_chars
            
            # å“è³ªè©•ä¾¡
            if japanese_ratio > 0.8:
                quality_level = "å„ªç§€"
            elif japanese_ratio > 0.6:
                quality_level = "è‰¯å¥½"
            elif japanese_ratio > 0.4:
                quality_level = "æ™®é€š"
            else:
                quality_level = "è¦æ”¹å–„"
            
            return {
                "japanese_ratio": japanese_ratio,
                "quality_level": quality_level,
                "character_breakdown": {
                    "hiragana": hiragana_count,
                    "katakana": katakana_count,
                    "kanji": kanji_count,
                    "ascii": ascii_count,
                    "total": total_chars
                }
            }
            
        except Exception as e:
            return {"error": f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}"}
    
    def _print_japanese_generation_results(self, result: Dict):
        """æ—¥æœ¬èªç”Ÿæˆçµæœã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š æ—¥æœ¬èªç”Ÿæˆçµæœ:")
        print(f"  ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
        print(f"  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['input_tokens']}")
        print(f"  å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {result['output_tokens']}")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result['tokens_per_second']:.1f} tokens/sec")
        
        print(f"\nğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡:")
        print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {result['resource_usage']['memory_used_gb']:.1f}GB")
        print(f"  ç·ãƒ¡ãƒ¢ãƒª: {result['resource_usage']['memory_total_gb']:.1f}GB")
        print(f"  CPUä½¿ç”¨ç‡: {result['resource_usage']['cpu_usage_percent']:.1f}%")
        
        print(f"\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå“è³ª:")
        if "error" not in result['japanese_quality']:
            quality = result['japanese_quality']
            print(f"  å“è³ªãƒ¬ãƒ™ãƒ«: {quality['quality_level']}")
            print(f"  æ—¥æœ¬èªæ¯”ç‡: {quality['japanese_ratio']:.1%}")
            breakdown = quality['character_breakdown']
            print(f"  æ–‡å­—æ§‹æˆ: ã²ã‚‰ãŒãª{breakdown['hiragana']}, ã‚«ã‚¿ã‚«ãƒŠ{breakdown['katakana']}, æ¼¢å­—{breakdown['kanji']}")
        else:
            print(f"  è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {result['japanese_quality']['error']}")
        
        print(f"\nğŸ”§ æœ€é©åŒ–çŠ¶æ…‹:")
        print(f"  æ—¥æœ¬èªæœ€é©åŒ–: {'âœ…' if result['optimization_applied'] else 'âŒ'}")
        print(f"  4bité‡å­åŒ–: {'âœ…' if result['quantization_info']['use_4bit'] else 'âŒ'}")
        print(f"  8bité‡å­åŒ–: {'âœ…' if result['quantization_info']['use_8bit'] else 'âŒ'}")
        
        print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ:")
        print(f"  \"{result['generated_text'][:300]}{'...' if len(result['generated_text']) > 300 else ''}\"")
    
    def interactive_japanese_mode(self):
        """æ—¥æœ¬èªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print(f"\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print(f"æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ã€'samples'ã§ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰:")
        
        results = []
        
        while True:
            try:
                prompt = input("\nğŸ‡¯ğŸ‡µ > ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'q', 'çµ‚äº†']:
                    break
                
                if prompt.lower() in ['samples', 'sample', 'ã‚µãƒ³ãƒ—ãƒ«']:
                    self.show_sample_prompts()
                    continue
                
                if not prompt:
                    continue
                
                result = self.generate_japanese_text(prompt)
                if "error" not in result:
                    results.append(result)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ æ—¥æœ¬èªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœä¿å­˜
        if results:
            self._save_japanese_session_results(results)
    
    def _save_japanese_session_results(self, results: List[Dict]):
        """æ—¥æœ¬èªã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_safe_name = self.model_name.replace("/", "_").replace("-", "_")
            filename = f'japanese_heavy_llm_session_{model_safe_name}_{timestamp}.json'
            
            session_data = {
                "model_name": self.model_name,
                "timestamp": datetime.now().isoformat(),
                "system_info": self.system_info,
                "optimization_config": {
                    "use_4bit": self.use_4bit,
                    "use_8bit": self.use_8bit,
                    "optimization_applied": self.optimization_applied
                },
                "results": results,
                "summary": self._calculate_japanese_session_summary(results)
            }
            
            os.makedirs('demo_results', exist_ok=True)
            filepath = os.path.join('demo_results', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ æ—¥æœ¬èªã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_japanese_session_summary(self, results: List[Dict]) -> Dict:
        """æ—¥æœ¬èªã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—"""
        if not results:
            return {}
        
        generation_times = [r['generation_time'] for r in results]
        tokens_per_second = [r['tokens_per_second'] for r in results]
        output_tokens = [r['output_tokens'] for r in results]
        memory_used = [r['resource_usage']['memory_used_gb'] for r in results]
        
        # æ—¥æœ¬èªå“è³ªã‚µãƒãƒªãƒ¼
        quality_levels = []
        japanese_ratios = []
        
        for r in results:
            if "error" not in r['japanese_quality']:
                quality_levels.append(r['japanese_quality']['quality_level'])
                japanese_ratios.append(r['japanese_quality']['japanese_ratio'])
        
        avg_japanese_ratio = sum(japanese_ratios) / len(japanese_ratios) if japanese_ratios else 0
        
        return {
            "total_generations": len(results),
            "avg_generation_time": sum(generation_times) / len(generation_times),
            "avg_tokens_per_second": sum(tokens_per_second) / len(tokens_per_second),
            "total_output_tokens": sum(output_tokens),
            "avg_memory_used_gb": sum(memory_used) / len(memory_used),
            "min_generation_time": min(generation_times),
            "max_generation_time": max(generation_times),
            "avg_japanese_ratio": avg_japanese_ratio,
            "quality_distribution": {level: quality_levels.count(level) for level in set(quality_levels)}
        }

    def _load_with_advanced_quantization(self) -> bool:
        """é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("âš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            # Step 1: æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            from transformers import AutoModelForCausalLM, AutoTokenizer
            
            print("ğŸ“¥ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True,
                device_map="cpu",
                trust_remote_code=True
            )
            
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print("âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # Step 2: é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–é©ç”¨
            print(f"ğŸ”§ {self.quantization_profile}ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§é‡å­åŒ–æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
            optimized_model = self.advanced_quantizer.optimize_model(model, tokenizer)
            
            # Step 3: ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜
            self.model = optimized_model
            self.tokenizer = tokenizer
            self.optimization_applied = True
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            final_memory = psutil.virtual_memory().used / (1024**3)
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {final_memory:.1f}GB")
            
            # æœ€é©åŒ–åŠ¹æœã®æ¨å®šè¡¨ç¤º
            self._display_optimization_effects()
            
            print("âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            print("ğŸ’¡ å¾“æ¥ã®æœ€é©åŒ–æ–¹å¼ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
            self.use_advanced_quant = False
            return self.load_model_with_optimization()
    
    def _display_optimization_effects(self):
        """æœ€é©åŒ–åŠ¹æœã®æ¨å®šè¡¨ç¤º"""
        if not self.advanced_quantizer:
            return
        
        config = self.advanced_quantizer.config
        profile = self.advanced_quantizer.profile
        
        print(f"\nâš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–åŠ¹æœï¼ˆæ¨å®šï¼‰:")
        print(f"  ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {profile.value}")
        print(f"  é‡ã¿é‡å­åŒ–: {config.weight_bits}bit")
        print(f"  æ´»æ€§åŒ–é‡å­åŒ–: {config.activation_bits}bit")
        print(f"  KVã‚­ãƒ£ãƒƒã‚·ãƒ¥: K={config.key_bits}bit, V={config.value_bits}bit")
        
        # åŠ¹æœæ¨å®š
        if profile == QuantizationProfile.SAFE:
            print(f"  æ¨å®šåŠ¹æœ:")
            print(f"    ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: ç´„50%")
            print(f"    é€Ÿåº¦å‘ä¸Š: 1.3-1.8å€")
            print(f"    å“è³ªç¶­æŒ: 98%ä»¥ä¸Š")
        elif profile == QuantizationProfile.BALANCED:
            print(f"  æ¨å®šåŠ¹æœ:")
            print(f"    ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: ç´„75%")
            print(f"    é€Ÿåº¦å‘ä¸Š: 1.5-2.5å€")
            print(f"    å“è³ªç¶­æŒ: 95%ä»¥ä¸Š")
        elif profile == QuantizationProfile.AGGRESSIVE:
            print(f"  æ¨å®šåŠ¹æœ:")
            print(f"    ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: ç´„85%")
            print(f"    é€Ÿåº¦å‘ä¸Š: 2.0-4.0å€")
            print(f"    å“è³ªç¶­æŒ: 90%ä»¥ä¸Š")
    
    def benchmark_advanced_quantization(self, test_prompts: List[str] = None) -> Dict[str, float]:
        """é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        if not self.advanced_quantizer or not self.model:
            print("âŒ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ãŒæœ‰åŠ¹ã§ãªã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return {}
        
        if test_prompts is None:
            test_prompts = [
                "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
                "æ·±å±¤å­¦ç¿’ã®å¿œç”¨ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚"
            ]
        
        print("ğŸ“Š é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        
        # å…ƒãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒã¯å›°é›£ãªãŸã‚ã€æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¸¬å®š
        start_time = time.time()
        total_tokens = 0
        
        for prompt in test_prompts:
            result = self.generate_japanese_text(prompt, max_new_tokens=50)
            if "output_tokens" in result:
                total_tokens += result["output_tokens"]
        
        total_time = time.time() - start_time
        tokens_per_second = total_tokens / total_time if total_time > 0 else 0
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory_used = psutil.virtual_memory().used / (1024**3)
        
        benchmark_results = {
            "tokens_per_second": tokens_per_second,
            "memory_used_gb": memory_used,
            "total_time": total_time,
            "total_tokens": total_tokens,
            "quantization_profile": self.quantization_profile
        }
        
        print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  æ¨è«–é€Ÿåº¦: {tokens_per_second:.1f} tokens/sec")
        print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {memory_used:.1f}GB")
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
        print(f"  ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}")
        print(f"  é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {self.quantization_profile}")
        
        return benchmark_results

    def run_infer_os_comparison_benchmark(self, num_iterations: int = 5) -> Dict:
        """Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"\nğŸ”¥ Infer-OSçµ±åˆåŠ¹æœæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        
        # æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆæœŸåŒ–
        self.comparison_benchmark = ComparisonBenchmark(
            self.model_name, 
            self.quantization_profile
        )
        
        # æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        results = self.comparison_benchmark.run_comparison_benchmark(
            JapaneseHeavyLLMDemo, 
            num_iterations=num_iterations
        )
        
        # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if results:
            report = self.comparison_benchmark.generate_comparison_report()
            print(report)
            
            # çµæœä¿å­˜
            filename = self.comparison_benchmark.save_results()
            print(f"\nğŸ“ è©³ç´°çµæœãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            
            return {
                'comparison_results': results,
                'report': report,
                'results_file': filename
            }
        else:
            print("âŒ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
            return {}
    
    def display_infer_os_integration_summary(self):
        """Infer-OSçµ±åˆåŠ¹æœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print(f"\nğŸ¯ **Infer-OSçµ±åˆåŠ¹æœã‚µãƒãƒªãƒ¼**")
        print(f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {self.quantization_profile}")
        print(f"Infer-OSæ©Ÿèƒ½: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        if self.infer_os_enabled:
            print(f"\nâš¡ **Infer-OSçµ±åˆã«ã‚ˆã‚‹æœŸå¾…åŠ¹æœ**:")
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—
            model_info = JAPANESE_HEAVY_MODELS.get(self.model_name, {})
            parameters = model_info.get('parameters', 0)
            
            if parameters >= 10_000_000_000:  # 10Bä»¥ä¸Š
                print(f"  æ¨è«–é€Ÿåº¦å‘ä¸Š: 2.5-4.0å€")
                print(f"  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 75-85%")
                print(f"  å¿œç­”æ™‚é–“çŸ­ç¸®: 60-75%")
                print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: 3-5å€")
            elif parameters >= 5_000_000_000:  # 5Bä»¥ä¸Š
                print(f"  æ¨è«–é€Ÿåº¦å‘ä¸Š: 2.0-3.0å€")
                print(f"  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 65-75%")
                print(f"  å¿œç­”æ™‚é–“çŸ­ç¸®: 50-65%")
                print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: 2.5-4å€")
            else:  # 5Bæœªæº€
                print(f"  æ¨è«–é€Ÿåº¦å‘ä¸Š: 1.5-2.5å€")
                print(f"  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 50-65%")
                print(f"  å¿œç­”æ™‚é–“çŸ­ç¸®: 40-55%")
                print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: 2-3å€")
            
            print(f"\nğŸ”§ **çµ±åˆæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**:")
            print(f"  âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ– (W4/W8 + KVé‡å­åŒ–)")
            print(f"  âœ… ONNX Runtimeæœ€é©åŒ– (3ãƒ¬ãƒ™ãƒ«æœ€é©åŒ–)")
            print(f"  âœ… IOBindingæœ€é©åŒ– (ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼è»¢é€)")
            print(f"  âœ… QLinearMatMulæœ€é©åŒ– (CPUä¸¦åˆ—å‡¦ç†)")
            print(f"  âœ… æ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ã‚¨ãƒ©ãƒ¼å›å¾©)")
            print(f"  âœ… è‡ªå‹•ãƒ¡ãƒ¢ãƒªç®¡ç† (å‹•çš„æœ€é©åŒ–)")
            
        else:
            print(f"\nâš ï¸ **Infer-OSç„¡åŠ¹æ™‚ã®åˆ¶é™**:")
            print(f"  æ¨™æº–çš„ãªé‡å­åŒ–ã®ã¿")
            print(f"  åŸºæœ¬çš„ãªPyTorchæ¨è«–")
            print(f"  é™å®šçš„ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–")
            print(f"  æ‰‹å‹•ã‚¨ãƒ©ãƒ¼å‡¦ç†")
            
        print(f"\nğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:")
        if self.infer_os_enabled:
            print(f"  ğŸš€ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŠ¹æœã‚’å®šé‡æ¸¬å®š")
            print(f"  ğŸ“Š --benchmark ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ€§èƒ½ç¢ºèª")
            print(f"  ğŸ¯ æœ¬æ ¼é‹ç”¨ã§ã®åŠ¹æœä½“é¨“")
        else:
            print(f"  ğŸ”§ Infer-OSæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–ã—ã¦åŠ¹æœã‚’ä½“é¨“")
            print(f"  ğŸ“ˆ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å·®ç•°ã‚’ç¢ºèª")
            print(f"  âš¡ çµ±åˆåŠ¹æœã®å®šé‡çš„æ¸¬å®šã‚’å®Ÿæ–½")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="æ—¥æœ¬èªé‡é‡ç´šLLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
    
    # åŸºæœ¬è¨­å®š
    parser.add_argument("--model", type=str, default="matsuo-lab/weblab-10b",
                        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--use-4bit", action="store_true",
                        help="4bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-8bit", action="store_true", 
                        help="8bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-advanced-quant", action="store_true",
                        help="é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--quantization-profile", type=str, default="balanced",
                        choices=["safe", "balanced", "aggressive"],
                        help="é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ONNXè¨­å®š
    parser.add_argument("--convert-to-onnx", action="store_true",
                        help="ONNXã«å¤‰æ›")
    parser.add_argument("--use-onnx-runtime", action="store_true",
                        help="ONNX Runtimeã‚’ä½¿ç”¨")
    parser.add_argument("--onnx-optimization-level", type=int, default=2,
                        choices=[0, 1, 2], help="ONNXæœ€é©åŒ–ãƒ¬ãƒ™ãƒ«")
    
    # Infer-OSæ¯”è¼ƒè¨­å®š
    parser.add_argument("--compare-infer-os", action="store_true", 
                        help="Infer-OSæœ‰ã‚Šç„¡ã—ã®æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ")
    parser.add_argument("--infer-os-enabled", action="store_true", default=True,
                        help="Infer-OSæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰")
    parser.add_argument("--disable-infer-os", action="store_true",
                        help="Infer-OSæ©Ÿèƒ½ã‚’ç„¡åŠ¹ã«ã™ã‚‹")
    parser.add_argument("--comparison-iterations", type=int, default=5,
                        help="æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰")
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    parser.add_argument("--interactive", action="store_true",
                        help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    parser.add_argument("--benchmark", action="store_true", 
                        help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    parser.add_argument("--prompt", type=str,
                        help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
    parser.add_argument("--max-length", type=int, default=300,
                        help="æœ€å¤§ç”Ÿæˆé•·")
    
    # æƒ…å ±è¡¨ç¤º
    parser.add_argument("--list-models", action="store_true",
                        help="å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º")
    parser.add_argument("--samples", action="store_true",
                        help="æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º")
    parser.add_argument("--pre-download", action="store_true",
                        help="äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’ä½¿ç”¨")
    
    args = parser.parse_args()
    
    # Infer-OSæ©Ÿèƒ½ã®è¨­å®š
    infer_os_enabled = args.infer_os_enabled and not args.disable_infer_os
    
    # æƒ…å ±è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
    if args.list_models:
        print("\nğŸ‡¯ğŸ‡µ å¯¾å¿œæ—¥æœ¬èªé‡é‡ç´šãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        for model_name, info in JAPANESE_HEAVY_MODELS.items():
            print(f"\nğŸ“‹ {model_name}")
            print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {info['parameters']:,}")
            print(f"  èª¬æ˜: {info['description']}")
            print(f"  æ—¥æœ¬èªå“è³ª: {info['japanese_quality']}")
            print(f"  å°‚é–€åˆ†é‡: {info['speciality']}")
            print(f"  æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {info['recommended_memory_gb']}GB")
        return
    
    if args.samples:
        print("\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        for category, prompts in JAPANESE_PROMPTS.items():
            print(f"\nğŸ“ {category}:")
            for i, prompt in enumerate(prompts, 1):
                print(f"  {i}. {prompt}")
        return
    
    try:
        # ãƒ‡ãƒ¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        demo = JapaneseHeavyLLMDemo(
            model_name=args.model,
            use_4bit=args.use_4bit,
            use_8bit=args.use_8bit,
            use_onnx=args.use_onnx_runtime,
            onnx_optimization_level=args.onnx_optimization_level,
            quantization_profile=args.quantization_profile,
            use_advanced_quant=args.use_advanced_quant,
            infer_os_enabled=infer_os_enabled
        )
        
        # Infer-OSçµ±åˆåŠ¹æœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        demo.display_infer_os_integration_summary()
        
        # Infer-OSæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        if args.compare_infer_os:
            print(f"\nğŸ”¥ Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
            comparison_results = demo.run_infer_os_comparison_benchmark(
                num_iterations=args.comparison_iterations
            )
            
            if comparison_results:
                print(f"\nâœ… æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
                print(f"ğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
            return
        
        # äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if args.pre_download:
            print(f"\nğŸ“¥ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œä¸­...")
            if demo.pre_download_model():
                print(f"âœ… äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            else:
                print(f"âŒ äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                return
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print(f"\nğŸ“¥ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        if not demo.load_model_with_optimization():
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # ONNXå¤‰æ›
        if args.convert_to_onnx:
            print(f"\nğŸš€ ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            if demo.convert_to_onnx():
                print(f"âœ… ONNXå¤‰æ›å®Œäº†")
            else:
                print(f"âŒ ONNXå¤‰æ›å¤±æ•—")
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ†å²
        if args.benchmark:
            print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            results = demo.run_benchmark()
            print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
            
        elif args.prompt:
            print(f"\nğŸ¯ å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œä¸­...")
            result = demo.generate_japanese_text(args.prompt, max_new_tokens=args.max_length)
            print(f"\nç”Ÿæˆçµæœ:")
            print(f"{result.get('generated_text', '')}")
            
        elif args.interactive:
            print(f"\nğŸ‡¯ğŸ‡µ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
            demo.interactive_mode()
            
        else:
            print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print(f"  --interactive: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
            print(f"  --benchmark: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
            print(f"  --compare-infer-os: Infer-OSæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
            print(f"  --prompt 'ãƒ†ã‚­ã‚¹ãƒˆ': å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
            print(f"  --list-models: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º")
            print(f"  --samples: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º")
            
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()

