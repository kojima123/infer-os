"""
VitisAI NPUç”Ÿæˆãƒ†ã‚¹ãƒˆå°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1ãƒˆãƒ¼ã‚¯ãƒ³å•é¡Œã®è§£æ±ºç¢ºèªç”¨

ä½¿ç”¨æ–¹æ³•:
    python test_vitisai_generation.py
"""

import os
import sys

# ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
os.environ['RYZEN_AI_INSTALLATION_PATH'] = r"C:\Program Files\RyzenAI\1.5"

def test_vitisai_generation():
    """VitisAI NPUç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª VitisAI NPUç”Ÿæˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("ğŸ¯ 1ãƒˆãƒ¼ã‚¯ãƒ³å•é¡Œè§£æ±ºç¢ºèª")
    print("=" * 50)
    
    try:
        # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        import torch
        import transformers
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from vitisai_npu_engine import VitisAINPUEngine
        
        print("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
        
        # è»½é‡ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        model_name = "rinna/youri-7b-chat"
        test_prompt = "äººå‚ã«ã¤ã„ã¦"
        max_tokens = 20  # ãƒ†ã‚¹ãƒˆç”¨ã«çŸ­ã
        
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ğŸ’¬ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test_prompt}")
        print(f"ğŸ”¢ æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
        print("\nğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            use_fast=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        print(f"ğŸ“Š èªå½™ã‚µã‚¤ã‚º: {len(tokenizer)}")
        print(f"ğŸ”š çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ID: {tokenizer.eos_token_id}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆè»½é‡è¨­å®šï¼‰
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        
        # VitisAI NPUã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ
        print("\nğŸš€ VitisAI NPUã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        vitisai_engine = VitisAINPUEngine(model, tokenizer)
        
        if vitisai_engine.setup_vitisai_npu():
            print("âœ… VitisAI NPUã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æˆåŠŸ")
            
            # ç”Ÿæˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            print(f"\nâš¡ VitisAI NPUç”Ÿæˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
            print(f"ğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{test_prompt}\"")
            
            result = vitisai_engine.generate_with_vitisai_npu(
                test_prompt, 
                max_new_tokens=max_tokens,
                temperature=0.8
            )
            
            if "error" not in result:
                print(f"\nâœ… ç”Ÿæˆãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
                print(f"ğŸ“ ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ: {result['generated_text']}")
                print(f"ğŸ“Š å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['output_tokens']}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {result['generation_time']:.2f}ç§’")
                print(f"ğŸš€ ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
                print(f"ğŸ”§ æ¨è«–æ–¹æ³•: {result['inference_method']}")
                
                # æˆåŠŸåˆ¤å®š
                if result['output_tokens'] > 1:
                    print(f"\nğŸ‰ 1ãƒˆãƒ¼ã‚¯ãƒ³å•é¡Œè§£æ±ºæˆåŠŸï¼")
                    print(f"âœ… {result['output_tokens']}ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ")
                else:
                    print(f"\nâš ï¸ 1ãƒˆãƒ¼ã‚¯ãƒ³å•é¡Œç¶™ç¶š")
                    print(f"âŒ {result['output_tokens']}ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿")
            else:
                print(f"âŒ ç”Ÿæˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {result['error']}")
        else:
            print("âŒ VitisAI NPUã‚¨ãƒ³ã‚¸ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            
            # CPUæ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆæ¯”è¼ƒç”¨ï¼‰
            print("\nğŸ–¥ï¸ CPUæ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆæ¯”è¼ƒç”¨ï¼‰...")
            inputs = tokenizer(test_prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"ğŸ“ CPUç”Ÿæˆçµæœ: {generated_text}")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

def show_token_analysis():
    """ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æè¡¨ç¤º"""
    print("\nğŸ” ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ:")
    
    try:
        from transformers import AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained(
            "rinna/youri-7b-chat",
            trust_remote_code=True
        )
        
        test_text = "äººå‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
        tokens = tokenizer.encode(test_text)
        
        print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ: {test_text}")
        print(f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³ID: {tokens}")
        print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(tokens)}")
        print(f"ğŸ”š çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ID: {tokenizer.eos_token_id}")
        
        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã®è©³ç´°
        for i, token_id in enumerate(tokens):
            token_text = tokenizer.decode([token_id])
            print(f"  {i}: {token_id} â†’ '{token_text}'")
        
    except Exception as e:
        print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    print("ğŸ§ª VitisAI NPUç”Ÿæˆãƒ†ã‚¹ãƒˆ")
    print("ğŸ¯ 1ãƒˆãƒ¼ã‚¯ãƒ³å•é¡Œè§£æ±ºç¢ºèª")
    print("=" * 60)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ
    show_token_analysis()
    
    # ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    test_vitisai_generation()
    
    print("\nğŸ ãƒ†ã‚¹ãƒˆå®Œäº†")

