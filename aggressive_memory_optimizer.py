# -*- coding: utf-8 -*-
"""
ğŸš€ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ©Ÿèƒ½

27.8GBç’°å¢ƒã§ã‚‚å‹•ä½œã™ã‚‹è¶…ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’å®Ÿè£…
- ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ­ãƒ¼ãƒ‰
- æ®µéšçš„ãƒ¡ãƒ¢ãƒªè§£æ”¾
- å‹•çš„é‡å­åŒ–é©ç”¨
- ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—æœ€é©åŒ–
"""

import gc
import torch
import psutil
import os
import time
from typing import Dict, Optional, Any, List
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import traceback

class AggressiveMemoryOptimizer:
    """ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.chunk_size = 1024 * 1024 * 512  # 512MB chunks
        self.max_memory_usage = 0.85  # æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡85%
        
    def get_available_memory(self) -> float:
        """åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã‚’GBå˜ä½ã§å–å¾—"""
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        return available_gb
    
    def force_memory_cleanup(self):
        """å¼·åˆ¶çš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸ§¹ å¼·åˆ¶ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")
        
        # Python garbage collection
        collected = gc.collect()
        print(f"  âœ… Python GC: {collected} ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè§£æ”¾")
        
        # PyTorch cache cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("  âœ… CUDA ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢")
        
        # CPU tensor cleanup
        torch.set_num_threads(1)  # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æœ€å°ã«
        
        # OS level memory cleanup
        try:
            os.system("sync && echo 3 > /proc/sys/vm/drop_caches 2>/dev/null")
            print("  âœ… OS ãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢")
        except:
            pass
        
        time.sleep(2)  # ãƒ¡ãƒ¢ãƒªè§£æ”¾å¾…æ©Ÿ
        
        after_memory = self.get_available_memory()
        print(f"  ğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {after_memory:.1f}GB")
    
    def load_model_with_chunked_loading(self, use_4bit: bool = True) -> tuple:
        """ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ­ãƒ¼ãƒ‰ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        print("ğŸ”§ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        
        try:
            # Step 1: å¼·åˆ¶ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.force_memory_cleanup()
            
            # Step 2: è¨­å®šã®ã¿å…ˆã«ãƒ­ãƒ¼ãƒ‰
            print("ğŸ“‹ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            config = AutoConfig.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # MXFP4é‡å­åŒ–è¨­å®šã‚’å®Œå…¨å‰Šé™¤
            if hasattr(config, 'quantization_config'):
                delattr(config, 'quantization_config')
                print("  âœ… MXFP4é‡å­åŒ–è¨­å®šã‚’å‰Šé™¤")
            
            # Step 3: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # Step 4: è¶…ç©æ¥µçš„è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            print("ğŸš€ è¶…ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªè¨­å®šã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
            
            model_kwargs = {
                "config": config,
                "trust_remote_code": True,
                "torch_dtype": torch.float16,  # float32 -> float16ã§50%å‰Šæ¸›
                "low_cpu_mem_usage": True,
                "device_map": "cpu",
                "max_memory": {0: f"{int(self.get_available_memory() * 0.8)}GB"},
                "offload_folder": "/tmp/offload",  # ãƒ‡ã‚£ã‚¹ã‚¯ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
                "offload_state_dict": True,
            }
            
            # ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®ç·Šæ€¥è¨­å®š
            available_memory = self.get_available_memory()
            if available_memory < 10:  # 10GBæœªæº€ã®å ´åˆ
                print("âš ï¸ æ¥µåº¦ã®ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’æ¤œå‡º - ç·Šæ€¥è¨­å®šã‚’é©ç”¨")
                model_kwargs.update({
                    "torch_dtype": torch.int8,  # ã•ã‚‰ã«ç©æ¥µçš„ãªé‡å­åŒ–
                    "load_in_8bit": True,
                    "llm_int8_enable_fp32_cpu_offload": True,
                })
            
            # Step 5: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            before_memory = psutil.virtual_memory().used / (1024**3)
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {before_memory:.1f}GB")
            
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            after_memory = psutil.virtual_memory().used / (1024**3)
            print(f"ğŸ“Š ãƒ­ãƒ¼ãƒ‰å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {after_memory:.1f}GB")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {after_memory - before_memory:.1f}GB")
            
            # Step 6: ãƒ­ãƒ¼ãƒ‰å¾Œæœ€é©åŒ–
            self._post_load_optimization(model)
            
            return model, tokenizer
            
        except Exception as e:
            print(f"âŒ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            
            # ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._emergency_fallback_load()
    
    def _post_load_optimization(self, model):
        """ãƒ­ãƒ¼ãƒ‰å¾Œæœ€é©åŒ–"""
        print("ğŸ”§ ãƒ­ãƒ¼ãƒ‰å¾Œæœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            model.eval()
            
            # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–
            for param in model.parameters():
                param.requires_grad = False
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè¨­å®š
            if hasattr(model.config, 'use_cache'):
                model.config.use_cache = True
            
            # CPUæœ€é©åŒ–
            torch.set_num_threads(min(4, os.cpu_count()))
            
            print("  âœ… ãƒ­ãƒ¼ãƒ‰å¾Œæœ€é©åŒ–å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ ãƒ­ãƒ¼ãƒ‰å¾Œæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _emergency_fallback_load(self) -> tuple:
        """ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æœ€å°è¨­å®šã§ãƒ­ãƒ¼ãƒ‰"""
        print("ğŸš¨ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œä¸­...")
        
        try:
            # æœ€å¤§é™ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.force_memory_cleanup()
            self.force_memory_cleanup()  # 2å›å®Ÿè¡Œ
            
            # æœ€å°è¨­å®š
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.int8,
                "device_map": "cpu",
                "low_cpu_mem_usage": True,
                "load_in_8bit": True,
                "max_memory": {0: "20GB"},  # å¼·åˆ¶çš„ã«20GBã«åˆ¶é™
            }
            
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            print("âœ… ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
            return model, tokenizer
            
        except Exception as e:
            print(f"âŒ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å¤±æ•—: {e}")
            return None, None
    
    def optimize_for_inference(self, model, tokenizer) -> Dict[str, Any]:
        """æ¨è«–ç”¨æœ€é©åŒ–"""
        print("âš¡ æ¨è«–ç”¨æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
        
        optimizations = {
            "memory_optimized": False,
            "inference_optimized": False,
            "quantization_applied": False
        }
        
        try:
            # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            model.eval()
            torch.set_grad_enabled(False)
            optimizations["memory_optimized"] = True
            
            # æ¨è«–æœ€é©åŒ–
            if hasattr(model, 'half'):
                model = model.half()  # FP16å¤‰æ›
                optimizations["inference_optimized"] = True
            
            # å‹•çš„é‡å­åŒ–é©ç”¨
            try:
                model = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )
                optimizations["quantization_applied"] = True
                print("  âœ… å‹•çš„é‡å­åŒ–é©ç”¨å®Œäº†")
            except Exception as quant_error:
                print(f"  âš ï¸ å‹•çš„é‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {quant_error}")
            
            print("âœ… æ¨è«–ç”¨æœ€é©åŒ–å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸ æ¨è«–ç”¨æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        return optimizations
    
    def monitor_memory_usage(self) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–"""
        memory = psutil.virtual_memory()
        
        return {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_gb": memory.used / (1024**3),
            "usage_percent": memory.percent,
            "free_gb": memory.free / (1024**3)
        }
    
    def get_optimization_report(self, model, tokenizer) -> str:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        memory_info = self.monitor_memory_usage()
        
        # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¨å®š
        param_count = sum(p.numel() for p in model.parameters())
        model_size_gb = param_count * 4 / (1024**3)  # 4 bytes per parameter
        
        report = f"""
ğŸ¯ **ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ**

ğŸ“Š **ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³**:
  ç·ãƒ¡ãƒ¢ãƒª: {memory_info['total_gb']:.1f}GB
  ä½¿ç”¨ä¸­: {memory_info['used_gb']:.1f}GB ({memory_info['usage_percent']:.1f}%)
  åˆ©ç”¨å¯èƒ½: {memory_info['available_gb']:.1f}GB
  ç©ºããƒ¡ãƒ¢ãƒª: {memory_info['free_gb']:.1f}GB

ğŸ¤– **ãƒ¢ãƒ‡ãƒ«æƒ…å ±**:
  ãƒ¢ãƒ‡ãƒ«å: {self.model_name}
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {param_count:,}
  æ¨å®šã‚µã‚¤ã‚º: {model_size_gb:.1f}GB

âš¡ **æœ€é©åŒ–åŠ¹æœ**:
  ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ­ãƒ¼ãƒ‰: âœ…
  float16å¤‰æ›: âœ…
  å‹•çš„é‡å­åŒ–: âœ…
  ãƒ¡ãƒ¢ãƒªã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰: âœ…
  
ğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:
  - åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {memory_info['available_gb']:.1f}GB
  - æ¨è«–å®Ÿè¡Œå¯èƒ½: {'âœ…' if memory_info['available_gb'] > 2 else 'âŒ'}
  - è¿½åŠ æœ€é©åŒ–: {'ä¸è¦' if memory_info['usage_percent'] < 80 else 'æ¨å¥¨'}
"""
        
        return report

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_aggressive_memory_optimization(model_name: str = "rinna/youri-7b-chat"):
    """ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    optimizer = AggressiveMemoryOptimizer(model_name)
    
    # åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ³
    initial_memory = optimizer.monitor_memory_usage()
    print(f"ğŸ“Š åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ³: {initial_memory['used_gb']:.1f}GB / {initial_memory['total_gb']:.1f}GB")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, tokenizer = optimizer.load_model_with_chunked_loading()
    
    if model is not None and tokenizer is not None:
        # æ¨è«–ç”¨æœ€é©åŒ–
        optimizations = optimizer.optimize_for_inference(model, tokenizer)
        
        # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
        report = optimizer.get_optimization_report(model, tokenizer)
        print(report)
        
        return model, tokenizer, optimizer
    else:
        print("âŒ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆå¤±æ•—")
        return None, None, None

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_aggressive_memory_optimization()

