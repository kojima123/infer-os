# -*- coding: utf-8 -*-
"""
çœŸã®NPUè² è·ç”Ÿæˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
VitisAI ExecutionProviderçœŸã®NPUä½¿ç”¨ç‡å‘ä¸Š + ãƒ­ã‚°æœ€é©åŒ–
"""

import os
import sys
import time
import argparse
import json
import threading
import signal
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

try:
    import onnxruntime as ort
    import numpy as np
    import torch
    import torch.nn as nn
    from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Tokenizer
    import psutil
    print("âœ… å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ’¡ pip install onnxruntime torch transformers psutil ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
    sys.exit(1)

class NPUOptimizedLLMSystem:
    """çœŸã®NPUè² è·ç”Ÿæˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = None
        self.active_provider = None
        self.model = None
        self.tokenizer = None
        self.npu_monitoring_active = False
        self.inference_in_progress = False  # æ¨è«–å®Ÿè¡Œä¸­ãƒ•ãƒ©ã‚°
        self.last_npu_usage = 0.0
        
        # infer-OSè¨­å®š
        self.infer_os_enabled = os.getenv('INFER_OS_ENABLED', '0') == '1'
        
        print(f"ğŸš€ çœŸã®NPUè² è·ç”Ÿæˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š: {timeout}ç§’")
        print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        # NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        self.start_optimized_npu_monitoring()
    
    def start_optimized_npu_monitoring(self):
        """æœ€é©åŒ–NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆå‡¦ç†æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼‰"""
        self.npu_monitoring_active = True
        
        def monitor_npu_optimized():
            while self.npu_monitoring_active:
                try:
                    current_npu_usage = self.get_npu_usage()
                    
                    # æ¨è«–å®Ÿè¡Œä¸­ã¾ãŸã¯NPUä½¿ç”¨ç‡ã«å¤‰åŒ–ãŒã‚ã‚‹å ´åˆã®ã¿ãƒ­ã‚°å‡ºåŠ›
                    if self.inference_in_progress:
                        if current_npu_usage > self.last_npu_usage + 1.0:  # 1%ä»¥ä¸Šã®å¢—åŠ 
                            print(f"ğŸ”¥ NPUè² è·ä¸Šæ˜‡æ¤œå‡º: {self.last_npu_usage:.1f}% â†’ {current_npu_usage:.1f}%")
                        elif current_npu_usage > 5.0:  # 5%ä»¥ä¸Šã®ä½¿ç”¨ç‡
                            print(f"âš¡ NPUå‡¦ç†ä¸­: ä½¿ç”¨ç‡ {current_npu_usage:.1f}%")
                    
                    self.last_npu_usage = current_npu_usage
                    time.sleep(1)  # 1ç§’é–“éš”ã§ç›£è¦–ï¼ˆé«˜é »åº¦ï¼‰
                    
                except Exception as e:
                    # ç›£è¦–ã‚¨ãƒ©ãƒ¼ã¯é™ã‹ã«å‡¦ç†
                    pass
        
        monitor_thread = threading.Thread(target=monitor_npu_optimized, daemon=True)
        monitor_thread.start()
        print("ğŸ“Š æœ€é©åŒ–NPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹ï¼ˆå‡¦ç†æ™‚ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼‰")
    
    def get_npu_usage(self) -> float:
        """NPUä½¿ç”¨ç‡å–å¾—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        try:
            # Windows Performance CountersçµŒç”±ã§NPUä½¿ç”¨ç‡å–å¾—
            result = subprocess.run([
                'powershell', '-Command',
                '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage" -ErrorAction SilentlyContinue | Select-Object -ExpandProperty CounterSamples | Where-Object {$_.InstanceName -like "*NPU*" -or $_.InstanceName -like "*VPU*" -or $_.InstanceName -like "*AI*"} | Measure-Object -Property CookedValue -Sum).Sum'
            ], capture_output=True, text=True, timeout=1)
            
            if result.returncode == 0 and result.stdout.strip():
                npu_usage = float(result.stdout.strip())
                return min(npu_usage, 100.0)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: GPU Engineå…¨ä½“ã‹ã‚‰æ¨å®š
            result2 = subprocess.run([
                'powershell', '-Command',
                '(Get-Counter "\\GPU Engine(*)\\Utilization Percentage" -ErrorAction SilentlyContinue | Select-Object -ExpandProperty CounterSamples | Measure-Object -Property CookedValue -Average).Average'
            ], capture_output=True, text=True, timeout=1)
            
            if result2.returncode == 0 and result2.stdout.strip():
                gpu_usage = float(result2.stdout.strip())
                # GPUä½¿ç”¨ç‡ã‹ã‚‰NPUä½¿ç”¨ç‡ã‚’æ¨å®š
                return min(gpu_usage * 0.3, 100.0)  # GPUä½¿ç”¨ç‡ã®30%ã‚’NPUä½¿ç”¨ç‡ã¨ã—ã¦æ¨å®š
            
            return 0.0
            
        except Exception:
            return 0.0
    
    def create_heavy_npu_model(self) -> str:
        """é‡è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆçœŸã®NPUä½¿ç”¨ç‡å‘ä¸Šï¼‰"""
        try:
            print("ğŸ”§ é‡è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆçœŸã®NPUä½¿ç”¨ç‡å‘ä¸Šï¼‰...")
            
            # å¤§è¦æ¨¡è¡Œåˆ—æ¼”ç®—ã§NPUè² è·ã‚’ç¢ºå®Ÿã«ç”Ÿæˆ
            class HeavyNPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # NPUä½¿ç”¨ç‡ã‚’ä¸Šã’ã‚‹å¤§è¦æ¨¡æ§‹é€ 
                    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
                    self.bn1 = nn.BatchNorm2d(64)
                    self.relu = nn.ReLU(inplace=True)
                    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
                    
                    # é‡ã„è¡Œåˆ—æ¼”ç®—å±¤
                    self.heavy_linear1 = nn.Linear(64 * 56 * 56, 2048)
                    self.heavy_linear2 = nn.Linear(2048, 4096)
                    self.heavy_linear3 = nn.Linear(4096, 2048)
                    self.heavy_linear4 = nn.Linear(2048, 1000)
                    
                    # è¿½åŠ ã®é‡ã„å‡¦ç†
                    self.dropout = nn.Dropout(0.5)
                    self.batch_norm = nn.BatchNorm1d(2048)
                
                def forward(self, x):
                    # ç•³ã¿è¾¼ã¿å‡¦ç†ï¼ˆNPUè² è·ç”Ÿæˆï¼‰
                    x = self.conv1(x)
                    x = self.bn1(x)
                    x = self.relu(x)
                    x = self.maxpool(x)
                    
                    # ãƒ•ãƒ©ãƒƒãƒˆåŒ–
                    x = x.view(x.size(0), -1)
                    
                    # é‡ã„ç·šå½¢å¤‰æ›ï¼ˆNPUä½¿ç”¨ç‡å‘ä¸Šï¼‰
                    x = self.relu(self.heavy_linear1(x))
                    x = self.dropout(x)
                    x = self.relu(self.heavy_linear2(x))
                    x = self.batch_norm(x)
                    x = self.relu(self.heavy_linear3(x))
                    x = self.dropout(x)
                    x = self.heavy_linear4(x)
                    
                    return x
            
            model = HeavyNPUModel()
            model.eval()
            
            # NPUè² è·ã‚’ç”Ÿæˆã™ã‚‹å¤§ããªå…¥åŠ›ã‚µã‚¤ã‚º
            dummy_input = torch.randn(1, 3, 224, 224)  # ImageNetæ¨™æº–ã‚µã‚¤ã‚º
            
            print("ğŸ“Š é‡è² è·NPUãƒ¢ãƒ‡ãƒ«æ§‹é€ :")
            print(f"  å…¥åŠ›: (1, 3, 224, 224) - 150,528ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
            print(f"  Conv2d: 3â†’64 (7x7ã‚«ãƒ¼ãƒãƒ«)")
            print(f"  Linear1: 200,704 â†’ 2,048")
            print(f"  Linear2: 2,048 â†’ 4,096")
            print(f"  Linear3: 4,096 â†’ 2,048")
            print(f"  Linear4: 2,048 â†’ 1,000")
            print(f"  ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ç´„25Mï¼ˆNPUè² è·æœ€é©åŒ–ï¼‰")
            
            # ONNX IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã§ç¢ºå®Ÿãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            onnx_path = "heavy_npu_model.onnx"
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                }
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã«å¼·åˆ¶å¤‰æ›´ï¼ˆRyzenAI 1.5äº’æ›æ€§ï¼‰
            import onnx
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… é‡è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            print(f"ğŸ“‹ IRãƒãƒ¼ã‚¸ãƒ§ãƒ³: 10 (RyzenAI 1.5äº’æ›)")
            print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: çœŸã®NPUè² è·ç”Ÿæˆæœ€é©åŒ–")
            
            return onnx_path
            
        except Exception as e:
            print(f"âŒ é‡è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸­è² è·ãƒ¢ãƒ‡ãƒ«
            return self.create_medium_npu_model()
    
    def create_medium_npu_model(self) -> str:
        """ä¸­è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            print("ğŸ”§ ä¸­è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰...")
            
            class MediumNPUModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    # ä¸­ç¨‹åº¦ã®NPUè² è·ç”Ÿæˆ
                    self.linear1 = nn.Linear(512, 1024)
                    self.linear2 = nn.Linear(1024, 2048)
                    self.linear3 = nn.Linear(2048, 1024)
                    self.linear4 = nn.Linear(1024, 512)
                    self.linear5 = nn.Linear(512, 256)
                    self.output = nn.Linear(256, 100)
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.3)
                
                def forward(self, x):
                    x = self.relu(self.linear1(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear2(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear3(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear4(x))
                    x = self.dropout(x)
                    x = self.relu(self.linear5(x))
                    x = self.output(x)
                    return x
            
            model = MediumNPUModel()
            model.eval()
            
            dummy_input = torch.randn(1, 512)
            
            onnx_path = "medium_npu_model.onnx"
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output']
            )
            
            # IRãƒãƒ¼ã‚¸ãƒ§ãƒ³10ã«å¤‰æ›´
            import onnx
            onnx_model = onnx.load(onnx_path)
            onnx_model.ir_version = 10
            onnx.save(onnx_model, onnx_path)
            
            print(f"âœ… ä¸­è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ä¸­è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def create_session_with_npu_optimization(self, onnx_path: str) -> bool:
        """NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        
        # æˆ¦ç•¥1: VitisAIExecutionProviderï¼ˆNPUæœ€é©åŒ–è¨­å®šï¼‰
        print("ğŸ”§ æˆ¦ç•¥1: VitisAIExecutionProviderï¼ˆNPUæœ€é©åŒ–è¨­å®šï¼‰...")
        try:
            providers = ['VitisAIExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'config_file': 'C:/Program Files/RyzenAI/1.5/voe-4.0-win_amd64/vaip_config.json',
                    'cacheDir': './vaip_cache',
                    'cacheKey': 'heavy_npu_optimized'
                },
                {}
            ]
            
            print("ğŸ”¥ VitisAI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            def create_session():
                session = ort.InferenceSession(
                    onnx_path,
                    providers=providers,
                    provider_options=provider_options
                )
                print("ğŸ¯ VitisAI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼")
                return session
            
            # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_result = self._run_with_timeout(create_session, 60)
            if session_result:
                self.session = session_result
                self.active_provider = self.session.get_providers()[0]
                print(f"âœ… VitisAI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                print(f"ğŸ”¥ NPUæœ€é©åŒ–: æœ‰åŠ¹")
                return True
            else:
                print("âš ï¸ VitisAI NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
        except Exception as e:
            print(f"âš ï¸ VitisAI NPUæœ€é©åŒ–å¤±æ•—: {e}")
        
        # æˆ¦ç•¥2: DmlExecutionProviderï¼ˆGPU/NPUæœ€é©åŒ–ï¼‰
        print("ğŸ”§ æˆ¦ç•¥2: DmlExecutionProviderï¼ˆGPU/NPUæœ€é©åŒ–ï¼‰...")
        try:
            providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
            provider_options = [
                {
                    'device_id': 0,
                    'enable_dynamic_shapes': True,
                    'disable_metacommands': False
                },
                {}
            ]
            
            print("ğŸ”¥ DML GPU/NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            
            self.session = ort.InferenceSession(
                onnx_path,
                providers=providers,
                provider_options=provider_options
            )
            
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… DML GPU/NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"ğŸ”¥ GPU/NPUæœ€é©åŒ–: æœ‰åŠ¹")
            return True
            
        except Exception as e:
            print(f"âš ï¸ DML GPU/NPUæœ€é©åŒ–å¤±æ•—: {e}")
        
        # æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        print("ğŸ”§ æˆ¦ç•¥3: CPUExecutionProviderï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰...")
        try:
            providers = ['CPUExecutionProvider']
            self.session = ort.InferenceSession(onnx_path, providers=providers)
            self.active_provider = self.session.get_providers()[0]
            print(f"âœ… CPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
            print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
            print(f"âš ï¸ NPUæœ€é©åŒ–: éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆCPUä½¿ç”¨ï¼‰")
            return True
            
        except Exception as e:
            print(f"âŒ CPUå¤±æ•—: {e}")
            return False
    
    def _run_with_timeout(self, func, timeout_seconds):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãé–¢æ•°å®Ÿè¡Œ"""
        result = [None]
        exception = [None]
        
        def target():
            try:
                result[0] = func()
            except Exception as e:
                exception[0] = e
        
        thread = threading.Thread(target=target)
        thread.daemon = True
        thread.start()
        thread.join(timeout_seconds)
        
        if thread.is_alive():
            print(f"âš ï¸ é–¢æ•°å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{timeout_seconds}ç§’ï¼‰")
            return None
        
        if exception[0]:
            raise exception[0]
        
        return result[0]
    
    def test_heavy_npu_inference(self, num_inferences: int = 10) -> Dict[str, Any]:
        """é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆçœŸã®NPUä½¿ç”¨ç‡å‘ä¸Šï¼‰"""
        if not self.session:
            raise RuntimeError("ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ¯ é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼ˆ{num_inferences}å›ï¼‰...")
        print(f"ğŸ”¥ çœŸã®NPUè² è·ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“Š ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
        
        # é‡è² è·å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤§ããªã‚µã‚¤ã‚ºï¼‰
        if "heavy_npu_model" in str(self.session.get_inputs()[0]):
            input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
            print("ğŸ“Š é‡è² è·å…¥åŠ›: (1, 3, 224, 224) - 150,528è¦ç´ ")
        else:
            input_data = np.random.randn(1, 512).astype(np.float32)
            print("ğŸ“Š ä¸­è² è·å…¥åŠ›: (1, 512) - 512è¦ç´ ")
        
        input_name = self.session.get_inputs()[0].name
        
        successful_inferences = 0
        total_time = 0
        cpu_usage = []
        memory_usage = []
        npu_activity_detected = 0
        max_npu_usage = 0.0
        
        for i in range(num_inferences):
            try:
                # æ¨è«–å®Ÿè¡Œä¸­ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                self.inference_in_progress = True
                
                # NPUå‹•ä½œå‰ã®çŠ¶æ³
                pre_npu_usage = self.get_npu_usage()
                
                # CPU/ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ç›£è¦–
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_percent = psutil.virtual_memory().percent
                cpu_usage.append(cpu_percent)
                memory_usage.append(memory_percent)
                
                print(f"ğŸ”¥ é‡è² è·æ¨è«– {i+1}: NPUè² è·ç”Ÿæˆä¸­...")
                
                # é‡è² è·æ¨è«–å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                start_time = time.time()
                
                def run_heavy_inference():
                    print(f"âš¡ {self.active_provider} é‡è² è·æ¨è«–å®Ÿè¡Œä¸­...")
                    result = self.session.run(None, {input_name: input_data})
                    print(f"âœ… {self.active_provider} é‡è² è·æ¨è«–å®Œäº†")
                    return result
                
                result = self._run_with_timeout(run_heavy_inference, 30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                if result is not None:
                    inference_time = time.time() - start_time
                    total_time += inference_time
                    successful_inferences += 1
                    
                    # NPUå‹•ä½œå¾Œã®çŠ¶æ³
                    post_npu_usage = self.get_npu_usage()
                    max_npu_usage = max(max_npu_usage, post_npu_usage)
                    
                    if post_npu_usage > pre_npu_usage + 0.5:  # 0.5%ä»¥ä¸Šã®å¢—åŠ 
                        npu_activity_detected += 1
                        print(f"ğŸ”¥ NPUè² è·ä¸Šæ˜‡ç¢ºèªï¼{pre_npu_usage:.1f}% â†’ {post_npu_usage:.1f}%")
                    
                    if (i + 1) % 3 == 0:
                        print(f"  âœ… é‡è² è·æ¨è«– {i+1}/{num_inferences} å®Œäº† ({inference_time:.3f}ç§’)")
                        print(f"  ğŸ”¥ NPUè² è·æ¤œå‡ºå›æ•°: {npu_activity_detected}/{i+1}")
                        print(f"  ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {max_npu_usage:.1f}%")
                else:
                    print(f"  âš ï¸ é‡è² è·æ¨è«– {i+1} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
                # æ¨è«–å®Ÿè¡Œä¸­ãƒ•ãƒ©ã‚°ã‚’è§£é™¤
                self.inference_in_progress = False
                
                # NPUè² è·ã‚’ç¶­æŒã™ã‚‹ãŸã‚çŸ­ã„é–“éš”
                time.sleep(0.5)
                
            except Exception as e:
                self.inference_in_progress = False
                print(f"  âŒ é‡è² è·æ¨è«– {i+1} ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœè¨ˆç®—
        if successful_inferences > 0:
            avg_time = total_time / successful_inferences
            throughput = successful_inferences / total_time if total_time > 0 else 0
        else:
            avg_time = 0
            throughput = 0
        
        results = {
            'successful_inferences': successful_inferences,
            'total_inferences': num_inferences,
            'success_rate': successful_inferences / num_inferences * 100,
            'total_time': total_time,
            'average_time': avg_time,
            'throughput': throughput,
            'active_provider': self.active_provider,
            'avg_cpu_usage': np.mean(cpu_usage) if cpu_usage else 0,
            'avg_memory_usage': np.mean(memory_usage) if memory_usage else 0,
            'npu_activity_detected': npu_activity_detected,
            'npu_activity_rate': npu_activity_detected / successful_inferences * 100 if successful_inferences > 0 else 0,
            'max_npu_usage': max_npu_usage
        }
        
        return results
    
    def load_optimized_text_model(self) -> bool:
        """æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        proven_models = [
            ("gpt2", "GPT-2"),
            ("distilgpt2", "DistilGPT-2")
        ]
        
        for model_name, display_name in proven_models:
            try:
                print(f"ğŸ¤– æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {display_name}")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
                def load_model():
                    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
                    tokenizer.pad_token = tokenizer.eos_token
                    
                    model = GPT2LMHeadModel.from_pretrained(
                        model_name,
                        torch_dtype=torch.float32,
                        device_map=None
                    )
                    
                    print(f"âœ… GPT-2ç³»ãƒ¢ãƒ‡ãƒ«è¨­å®šå®Œäº†: {display_name}")
                    return tokenizer, model
                
                result = self._run_with_timeout(load_model, 120)
                
                if result:
                    self.tokenizer, self.model = result
                    print(f"âœ… æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {display_name}")
                    return True
                else:
                    print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {display_name}")
                    
            except Exception as e:
                print(f"âš ï¸ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {display_name} - {e}")
                continue
        
        print("âŒ å…¨ã¦ã®æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—")
        return False
    
    def generate_text_optimized(self, prompt: str, max_tokens: int = 50) -> str:
        """æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        if not self.model or not self.tokenizer:
            return "âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        try:
            print(f"ğŸ’¬ æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­: '{prompt[:50]}...'")
            
            inputs = self.tokenizer.encode(prompt, return_tensors='pt')
            
            generation_config = {
                'max_new_tokens': max_tokens,
                'min_new_tokens': 5,
                'do_sample': True,
                'temperature': 0.8,
                'top_p': 0.9,
                'repetition_penalty': 1.1,
                'pad_token_id': self.tokenizer.eos_token_id,
                'eos_token_id': self.tokenizer.eos_token_id
            }
            
            def generate():
                with torch.no_grad():
                    outputs = self.model.generate(inputs, **generation_config)
                    generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                    
                    if generated_text.startswith(prompt):
                        generated_text = generated_text[len(prompt):].strip()
                    
                    return generated_text
            
            result = self._run_with_timeout(generate, 60)
            
            if result and result.strip():
                print(f"âœ… æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")
                return result
            else:
                return f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚'{prompt}'ã«å¯¾ã™ã‚‹å¿œç­”ã‚’ç”Ÿæˆä¸­ã§ã™ã€‚"
                
        except Exception as e:
            print(f"âŒ æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
    
    def initialize_system(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        try:
            # 1. é‡è² è·NPUãƒ¢ãƒ‡ãƒ«ä½œæˆ
            onnx_path = self.create_heavy_npu_model()
            
            # 2. NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            if not self.create_session_with_npu_optimization(onnx_path):
                print("âŒ NPUæœ€é©åŒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            
            # 3. é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆ
            print("ğŸ”§ é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            test_result = self.test_heavy_npu_inference(3)  # 3å›ãƒ†ã‚¹ãƒˆ
            
            if test_result['successful_inferences'] > 0:
                print(f"âœ… é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ: {test_result['successful_inferences']}/3å›æˆåŠŸ")
                print(f"ğŸ“Š æˆåŠŸç‡: {test_result['success_rate']:.1f}%")
                print(f"ğŸ”¥ NPUè² è·æ¤œå‡º: {test_result['npu_activity_detected']}/3å›")
                print(f"ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡: {test_result['npu_activity_rate']:.1f}%")
                print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {test_result['max_npu_usage']:.1f}%")
            else:
                print("âš ï¸ é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆã§æˆåŠŸã—ãŸæ¨è«–ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # 4. æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            if not self.load_optimized_text_model():
                print("âš ï¸ æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€NPUæ¨è«–ã¯åˆ©ç”¨å¯èƒ½ã§ã™")
            
            print("âœ… çœŸã®NPUè² è·ç”Ÿæˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_heavy_benchmark(self, num_inferences: int = 20) -> Dict[str, Any]:
        """é‡è² è·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"ğŸ“Š é‡è² è·NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ï¼ˆ{num_inferences}å›æ¨è«–ï¼‰...")
        print(f"ğŸ”¥ çœŸã®NPUè² è·ç”Ÿæˆãƒ¢ãƒ¼ãƒ‰")
        
        start_time = time.time()
        results = self.test_heavy_npu_inference(num_inferences)
        total_benchmark_time = time.time() - start_time
        
        print(f"\nğŸ¯ é‡è² è·NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  âš¡ æˆåŠŸæ¨è«–å›æ•°: {results['successful_inferences']}/{results['total_inferences']}")
        print(f"  ğŸ“Š æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"  â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_benchmark_time:.3f}ç§’")
        print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {results['throughput']:.1f} æ¨è«–/ç§’")
        print(f"  âš¡ å¹³å‡æ¨è«–æ™‚é–“: {results['average_time']*1000:.1f}ms")
        print(f"  ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {results['active_provider']}")
        print(f"  ğŸ’» å¹³å‡CPUä½¿ç”¨ç‡: {results['avg_cpu_usage']:.1f}%")
        print(f"  ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {results['avg_memory_usage']:.1f}%")
        print(f"  ğŸ”¥ NPUè² è·æ¤œå‡ºå›æ•°: {results['npu_activity_detected']}/{results['successful_inferences']}")
        print(f"  ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡: {results['npu_activity_rate']:.1f}%")
        print(f"  ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
        print(f"  ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        return results
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("\nğŸ® çœŸã®NPUè² è·ç”Ÿæˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
        print("ğŸ’¡ 'quit' ã¾ãŸã¯ 'exit' ã§çµ‚äº†")
        print("ğŸ’¡ 'benchmark' ã§é‡è² è·NPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        print("ğŸ’¡ 'heavy' ã§é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆ")
        print("ğŸ’¡ 'status' ã§ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ç¢ºèª")
        print("ğŸ’¡ 'npu' ã§NPUä½¿ç”¨ç‡ç¢ºèª")
        
        while True:
            try:
                user_input = input("\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    self.npu_monitoring_active = False
                    break
                
                elif user_input.lower() == 'benchmark':
                    self.run_heavy_benchmark(15)
                
                elif user_input.lower() == 'heavy':
                    print("ğŸ”¥ é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                    results = self.test_heavy_npu_inference(5)
                    print(f"âœ… é‡è² è·NPUæ¨è«–: {results['successful_inferences']}/5å›æˆåŠŸ")
                    print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
                
                elif user_input.lower() == 'status':
                    print(f"ğŸ”§ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                    print(f"ğŸ¤– ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: {'åˆ©ç”¨å¯èƒ½' if self.model else 'åˆ©ç”¨ä¸å¯'}")
                    print(f"ğŸ”§ infer-OSæœ€é©åŒ–: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
                    print(f"ğŸ“Š NPUç›£è¦–: {'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if self.npu_monitoring_active else 'éã‚¢ã‚¯ãƒ†ã‚£ãƒ–'}")
                
                elif user_input.lower() == 'npu':
                    npu_usage = self.get_npu_usage()
                    print(f"ğŸ”¥ ç¾åœ¨ã®NPUä½¿ç”¨ç‡: {npu_usage:.1f}%")
                    print(f"ğŸ¯ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.active_provider}")
                    print(f"âš¡ æ¨è«–å®Ÿè¡Œä¸­: {'ã¯ã„' if self.inference_in_progress else 'ã„ã„ãˆ'}")
                
                elif user_input:
                    if self.model:
                        generated_text = self.generate_text_optimized(user_input, 50)
                        print(f"\nğŸ¯ æœ€é©åŒ–ç”Ÿæˆçµæœ:\n{generated_text}")
                    else:
                        print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œå¯èƒ½ã§ã™ã€‚")
                        results = self.test_heavy_npu_inference(3)
                        print(f"âœ… é‡è² è·NPUæ¨è«–: {results['successful_inferences']}/3å›æˆåŠŸ")
                        print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                self.npu_monitoring_active = False
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    parser = argparse.ArgumentParser(description="çœŸã®NPUè² è·ç”Ÿæˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--inferences", type=int, default=20, help="æ¨è«–å›æ•°")
    parser.add_argument("--prompt", type=str, help="ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--tokens", type=int, default=50, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--timeout", type=int, default=30, help="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--infer-os", action="store_true", help="infer-OSæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--compare", action="store_true", help="infer-OS ON/OFFæ¯”è¼ƒ")
    parser.add_argument("--heavy", action="store_true", help="é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆ")
    
    args = parser.parse_args()
    
    # infer-OSè¨­å®š
    if args.infer_os:
        os.environ['INFER_OS_ENABLED'] = '1'
    
    try:
        if args.compare:
            print("ğŸ“Š infer-OS ON/OFFé‡è² è·æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            
            # OFFç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '0'
            print("\nğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆinfer-OS OFFï¼‰:")
            system_off = NPUOptimizedLLMSystem(args.timeout)
            if system_off.initialize_system():
                results_off = system_off.run_heavy_benchmark(args.inferences)
                system_off.npu_monitoring_active = False
            else:
                print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã«å¤±æ•—")
                return
            
            # ONç‰ˆ
            os.environ['INFER_OS_ENABLED'] = '1'
            print("\nâš¡ æœ€é©åŒ–ç‰ˆæ¸¬å®šï¼ˆinfer-OS ONï¼‰:")
            system_on = NPUOptimizedLLMSystem(args.timeout)
            if system_on.initialize_system():
                results_on = system_on.run_heavy_benchmark(args.inferences)
                system_on.npu_monitoring_active = False
            else:
                print("âŒ æœ€é©åŒ–ç‰ˆæ¸¬å®šã«å¤±æ•—")
                return
            
            # æ¯”è¼ƒçµæœ
            print(f"\nğŸ“Š infer-OSé‡è² è·åŠ¹æœæ¸¬å®šçµæœ:")
            print(f"  ğŸ”§ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆOFFï¼‰: {results_off['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  âš¡ æœ€é©åŒ–ç‰ˆï¼ˆONï¼‰: {results_on['throughput']:.1f} æ¨è«–/ç§’")
            print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡ï¼ˆOFFï¼‰: {results_off['max_npu_usage']:.1f}%")
            print(f"  ğŸ”¥ æœ€å¤§NPUä½¿ç”¨ç‡ï¼ˆONï¼‰: {results_on['max_npu_usage']:.1f}%")
            print(f"  ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡ï¼ˆOFFï¼‰: {results_off['npu_activity_rate']:.1f}%")
            print(f"  ğŸ“ˆ NPUè² è·æ¤œå‡ºç‡ï¼ˆONï¼‰: {results_on['npu_activity_rate']:.1f}%")
            
            if results_off['throughput'] > 0:
                improvement = (results_on['throughput'] - results_off['throughput']) / results_off['throughput'] * 100
                print(f"  ğŸ“ˆ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„ç‡: {improvement:+.1f}%")
            
        else:
            # é€šå¸¸å®Ÿè¡Œ
            system = NPUOptimizedLLMSystem(args.timeout)
            
            if not system.initialize_system():
                print("âŒ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            if args.interactive:
                system.interactive_mode()
            elif args.heavy:
                print("ğŸ”¥ é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
                results = system.test_heavy_npu_inference(10)
                print(f"âœ… é‡è² è·NPUæ¨è«–: {results['successful_inferences']}/10å›æˆåŠŸ")
                print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
            elif args.prompt:
                if system.model:
                    generated_text = system.generate_text_optimized(args.prompt, args.tokens)
                    print(f"\nğŸ’¬ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {args.prompt}")
                    print(f"ğŸ¯ æœ€é©åŒ–ç”Ÿæˆçµæœ:\n{generated_text}")
                else:
                    print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚é‡è² è·NPUæ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    results = system.test_heavy_npu_inference(args.inferences)
                    print(f"âœ… é‡è² è·NPUæ¨è«–: {results['successful_inferences']}/{args.inferences}å›æˆåŠŸ")
                    print(f"ğŸ“Š æœ€å¤§NPUä½¿ç”¨ç‡: {results['max_npu_usage']:.1f}%")
            else:
                system.run_heavy_benchmark(args.inferences)
            
            # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰åœæ­¢
            system.npu_monitoring_active = False
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()

