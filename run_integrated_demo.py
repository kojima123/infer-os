#!/usr/bin/env python3
"""
çµ±åˆNPU + Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åŒ…æ‹¬çš„ãªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ‡ãƒ¢
"""

import os
import sys
import time
import argparse
import subprocess
from pathlib import Path


class IntegratedOptimizationDemo:
    """çµ±åˆæœ€é©åŒ–ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.available_models = {
            "llama3-8b-amd-npu": {
                "size": "8B",
                "type": "NPUæœ€é©åŒ–æ¸ˆã¿",
                "infer_os_compatible": True,
                "npu_ready": True,
                "recommended": True,
                "description": "NPU + Infer-OSçµ±åˆæœ€é©åŒ–å¯¾å¿œ"
            },
            "ALMA-Ja-V3-amd-npu": {
                "size": "7B",
                "type": "ç¿»è¨³ç‰¹åŒ–NPU",
                "infer_os_compatible": True,
                "npu_ready": True,
                "recommended": True,
                "description": "ç¿»è¨³ç‰¹åŒ– + Infer-OSçµ±åˆæœ€é©åŒ–"
            },
            "cyberagent/Llama-3.1-70B-Japanese-Instruct-2407": {
                "size": "70B",
                "type": "å¤§è¦æ¨¡æ—¥æœ¬èª",
                "infer_os_compatible": True,
                "npu_ready": False,
                "recommended": False,
                "description": "æœ€é‡é‡ç´š + Infer-OSçµ±åˆæœ€é©åŒ–"
            },
            "rinna/youri-7b-chat": {
                "size": "7B",
                "type": "æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆ",
                "infer_os_compatible": True,
                "npu_ready": False,
                "recommended": True,
                "description": "Infer-OSæœ€é©åŒ–å¯¾å¿œï¼ˆNPUå¤‰æ›å¯èƒ½ï¼‰"
            }
        }
        
        self.optimization_modes = {
            "full": {
                "name": "å®Œå…¨çµ±åˆæœ€é©åŒ–",
                "npu": True,
                "infer_os": True,
                "aggressive_memory": True,
                "advanced_quant": True,
                "windows_npu": True,
                "description": "å…¨ã¦ã®æœ€é©åŒ–æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–"
            },
            "npu_only": {
                "name": "NPUæœ€é©åŒ–ã®ã¿",
                "npu": True,
                "infer_os": False,
                "aggressive_memory": False,
                "advanced_quant": False,
                "windows_npu": False,
                "description": "NPUæœ€é©åŒ–ã®ã¿æœ‰åŠ¹"
            },
            "infer_os_only": {
                "name": "Infer-OSæœ€é©åŒ–ã®ã¿",
                "npu": False,
                "infer_os": True,
                "aggressive_memory": True,
                "advanced_quant": True,
                "windows_npu": True,
                "description": "Infer-OSæœ€é©åŒ–ã®ã¿æœ‰åŠ¹"
            },
            "balanced": {
                "name": "ãƒãƒ©ãƒ³ã‚¹æœ€é©åŒ–",
                "npu": True,
                "infer_os": True,
                "aggressive_memory": True,
                "advanced_quant": False,
                "windows_npu": False,
                "description": "å®‰å®šæ€§é‡è¦–ã®æœ€é©åŒ–"
            }
        }
        
        self.test_scenarios = {
            "basic": {
                "name": "åŸºæœ¬æ€§èƒ½ãƒ†ã‚¹ãƒˆ",
                "prompts": [
                    "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                    "æ—¥æœ¬ã®æ–‡åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
                    "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ"
                ],
                "max_tokens": 100
            },
            "advanced": {
                "name": "é«˜åº¦ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ",
                "prompts": [
                    "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦è©³ã—ãè«–ã˜ã¦ãã ã•ã„ã€‚æŠ€è¡“çš„ãªé€²æ­©ã€ç¤¾ä¼šã¸ã®å½±éŸ¿ã€å€«ç†çš„ãªèª²é¡Œã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
                    "æ—¥æœ¬ã®å››å­£ã®ç¾ã—ã•ã«ã¤ã„ã¦ã€æ–‡å­¦çš„ãªè¡¨ç¾ã‚’ç”¨ã„ã¦è©©çš„ã«æå†™ã—ã¦ãã ã•ã„ã€‚",
                    "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã®ä»•çµ„ã¿ã‚’ã€å°‚é–€çŸ¥è­˜ã®ãªã„äººã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€å…·ä½“ä¾‹ã‚’äº¤ãˆã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
                ],
                "max_tokens": 300
            },
            "translation": {
                "name": "ç¿»è¨³æ€§èƒ½ãƒ†ã‚¹ãƒˆ",
                "prompts": [
                    "æ¬¡ã®è‹±èªã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„: 'The future of artificial intelligence is bright and full of possibilities.'",
                    "æ¬¡ã®æ—¥æœ¬èªã‚’è‡ªç„¶ãªè‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„: 'æ¡œã®èŠ±ãŒå’²ãæ˜¥ã¯æ—¥æœ¬ã§æœ€ã‚‚ç¾ã—ã„å­£ç¯€ã§ã™ã€‚'",
                    "æ¬¡ã®æŠ€è¡“æ–‡æ›¸ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„: 'Machine learning algorithms are revolutionizing various industries by enabling automated decision-making processes.'"
                ],
                "max_tokens": 150
            }
        }
    
    def show_welcome(self):
        """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º"""
        print("ğŸš€ çµ±åˆNPU + Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
        print("ğŸ¯ çœŸã®åŒ…æ‹¬çš„æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ä½“é¨“")
        print("=" * 80)
        print("ğŸ’¡ ã“ã®ãƒ‡ãƒ¢ã§ã¯ä»¥ä¸‹ã®çµ±åˆæœ€é©åŒ–ã‚’ä½“é¨“ã§ãã¾ã™:")
        print("  âš¡ NPUæœ€é©åŒ– (VitisAI ExecutionProvider)")
        print("  ğŸ§  Infer-OSæœ€é©åŒ– (ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã€é«˜åº¦é‡å­åŒ–)")
        print("  ğŸªŸ Windows NPUæœ€é©åŒ– (AMD/Intel/Qualcomm)")
        print("  ğŸ“Š åŒ…æ‹¬çš„æ€§èƒ½ç›£è¦–")
        print("  ğŸ® ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯¾è©±")
        print("=" * 80)
    
    def show_model_selection(self):
        """ãƒ¢ãƒ‡ãƒ«é¸æŠç”»é¢è¡¨ç¤º"""
        print("\nğŸ“± çµ±åˆæœ€é©åŒ–å¯¾å¿œãƒ¢ãƒ‡ãƒ«:")
        print("-" * 60)
        
        for i, (model_key, info) in enumerate(self.available_models.items(), 1):
            status = "âœ… æ¨å¥¨" if info["recommended"] else "ğŸ”„ å®Ÿé¨“çš„"
            npu_status = "âš¡ NPUå¯¾å¿œ" if info["npu_ready"] else "ğŸ”§ NPUå¤‰æ›å¯èƒ½"
            infer_os_status = "ğŸ§  Infer-OSå¯¾å¿œ" if info["infer_os_compatible"] else "âŒ éå¯¾å¿œ"
            
            print(f"{i}. {model_key}")
            print(f"   ğŸ“Š ã‚µã‚¤ã‚º: {info['size']}")
            print(f"   ğŸ”§ ã‚¿ã‚¤ãƒ—: {info['type']}")
            print(f"   {npu_status} | {infer_os_status} | {status}")
            print(f"   ğŸ“ èª¬æ˜: {info['description']}")
            print()
    
    def show_optimization_modes(self):
        """æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰é¸æŠç”»é¢è¡¨ç¤º"""
        print("\nğŸ”§ æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰:")
        print("-" * 50)
        
        for i, (mode_key, info) in enumerate(self.optimization_modes.items(), 1):
            print(f"{i}. {info['name']}")
            print(f"   ğŸ“ èª¬æ˜: {info['description']}")
            print(f"   âš¡ NPU: {'âœ…' if info['npu'] else 'âŒ'}")
            print(f"   ğŸ§  Infer-OS: {'âœ…' if info['infer_os'] else 'âŒ'}")
            print(f"   ğŸ’¾ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒª: {'âœ…' if info['aggressive_memory'] else 'âŒ'}")
            print(f"   ğŸ“Š é«˜åº¦é‡å­åŒ–: {'âœ…' if info['advanced_quant'] else 'âŒ'}")
            print()
    
    def show_test_scenarios(self):
        """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªé¸æŠç”»é¢è¡¨ç¤º"""
        print("\nğŸ¯ ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª:")
        print("-" * 40)
        
        for i, (scenario_key, info) in enumerate(self.test_scenarios.items(), 1):
            print(f"{i}. {info['name']}")
            print(f"   ğŸ“ èª¬æ˜: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ{len(info['prompts'])}å€‹ã€æœ€å¤§{info['max_tokens']}ãƒˆãƒ¼ã‚¯ãƒ³")
            print()
    
    def check_dependencies(self) -> bool:
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        print("ğŸ” çµ±åˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        required_files = [
            "integrated_npu_infer_os.py",
            "npu_optimized_japanese_models.py",
            "download_npu_models.py"
        ]
        
        missing_files = []
        for file_name in required_files:
            if not os.path.exists(file_name):
                missing_files.append(file_name)
        
        if missing_files:
            print("âŒ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
            for file_name in missing_files:
                print(f"  - {file_name}")
            return False
        
        print("âœ… çµ±åˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªå®Œäº†")
        return True
    
    def run_integrated_demo(self, model_name: str, optimization_mode: str, test_scenario: str):
        """çµ±åˆãƒ‡ãƒ¢å®Ÿè¡Œ"""
        print(f"\nğŸš€ çµ±åˆæœ€é©åŒ–ãƒ‡ãƒ¢å®Ÿè¡Œé–‹å§‹")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ğŸ”§ æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰: {optimization_mode}")
        print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª: {test_scenario}")
        print("-" * 60)
        
        # æœ€é©åŒ–è¨­å®šå–å¾—
        opt_config = self.optimization_modes[optimization_mode]
        scenario_config = self.test_scenarios[test_scenario]
        
        # ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
        cmd = ["python", "integrated_npu_infer_os.py", "--model", model_name]
        
        if not opt_config["npu"]:
            cmd.append("--disable-npu")
        if not opt_config["infer_os"]:
            cmd.append("--disable-infer-os")
        
        # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        for i, prompt in enumerate(scenario_config["prompts"], 1):
            print(f"\nğŸ¤– ãƒ†ã‚¹ãƒˆ {i}/{len(scenario_config['prompts'])}")
            print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
            print("ğŸ”„ çµ±åˆæœ€é©åŒ–ç”Ÿæˆä¸­...")
            
            test_cmd = cmd + [
                "--prompt", prompt,
                "--max-tokens", str(scenario_config["max_tokens"])
            ]
            
            start_time = time.time()
            
            try:
                result = subprocess.run(test_cmd, capture_output=True, text=True, timeout=300)
                execution_time = time.time() - start_time
                
                if result.returncode == 0:
                    print(f"âœ… ç”Ÿæˆå®Œäº† ({execution_time:.1f}ç§’)")
                    
                    # å¿œç­”æŠ½å‡º
                    output_lines = result.stdout.split('\n')
                    for line in output_lines:
                        if "ğŸ“ å¿œç­”:" in line:
                            response = line.replace("ğŸ“ å¿œç­”:", "").strip()
                            print(f"ğŸ’¬ å¿œç­”: {response}")
                            break
                else:
                    print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})")
                    print("ğŸ“„ ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:")
                    print(result.stderr[-300:])
                    
            except subprocess.TimeoutExpired:
                print("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ5åˆ†ï¼‰")
            except Exception as e:
                print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            
            print("-" * 40)
    
    def run_performance_comparison(self):
        """æ€§èƒ½æ¯”è¼ƒå®Ÿè¡Œ"""
        print("\nğŸ çµ±åˆæœ€é©åŒ–æ€§èƒ½æ¯”è¼ƒ")
        print("=" * 70)
        
        # æ¯”è¼ƒå¯¾è±¡ãƒ¢ãƒ‡ãƒ«
        comparison_models = ["llama3-8b-amd-npu", "rinna/youri-7b-chat"]
        comparison_modes = ["full", "npu_only", "infer_os_only"]
        
        test_prompt = "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        
        results = {}
        
        for model_name in comparison_models:
            if model_name not in self.available_models:
                continue
            
            results[model_name] = {}
            
            for mode in comparison_modes:
                print(f"\nğŸ“Š {model_name} - {self.optimization_modes[mode]['name']}")
                
                opt_config = self.optimization_modes[mode]
                cmd = ["python", "integrated_npu_infer_os.py", "--model", model_name, "--prompt", test_prompt, "--max-tokens", "100"]
                
                if not opt_config["npu"]:
                    cmd.append("--disable-npu")
                if not opt_config["infer_os"]:
                    cmd.append("--disable-infer-os")
                
                start_time = time.time()
                
                try:
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
                    execution_time = time.time() - start_time
                    
                    if result.returncode == 0:
                        results[model_name][mode] = {
                            "time": execution_time,
                            "success": True
                        }
                        print(f"âœ… å®Œäº†: {execution_time:.1f}ç§’")
                    else:
                        results[model_name][mode] = {
                            "time": execution_time,
                            "success": False
                        }
                        print(f"âŒ å¤±æ•—: {execution_time:.1f}ç§’")
                        
                except subprocess.TimeoutExpired:
                    results[model_name][mode] = {
                        "time": 180,
                        "success": False
                    }
                    print("â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ")
        print("=" * 70)
        
        for model_name, model_results in results.items():
            print(f"\nğŸ“± {model_name}")
            for mode, result in model_results.items():
                status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±æ•—"
                mode_name = self.optimization_modes[mode]["name"]
                print(f"   ğŸ”§ {mode_name}: {result['time']:.1f}ç§’ {status}")
    
    def run_interactive_mode(self, model_name: str, optimization_mode: str):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        print(f"\nğŸ® çµ±åˆæœ€é©åŒ–ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ğŸ”§ æœ€é©åŒ–: {self.optimization_modes[optimization_mode]['name']}")
        print("ğŸ’¡ 'exit'ã§çµ‚äº†")
        print("-" * 60)
        
        opt_config = self.optimization_modes[optimization_mode]
        cmd = ["python", "integrated_npu_infer_os.py", "--model", model_name, "--interactive"]
        
        if not opt_config["npu"]:
            cmd.append("--disable-npu")
        if not opt_config["infer_os"]:
            cmd.append("--disable-infer-os")
        
        try:
            subprocess.run(cmd, check=True)
        except subprocess.CalledProcessError as e:
            print(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="çµ±åˆNPU + Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
    parser.add_argument("--model", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--optimization-mode", default="full", 
                       choices=["full", "npu_only", "infer_os_only", "balanced"],
                       help="æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--test-scenario", default="basic",
                       choices=["basic", "advanced", "translation"],
                       help="ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--compare", action="store_true", help="æ€§èƒ½æ¯”è¼ƒå®Ÿè¡Œ")
    parser.add_argument("--check-deps", action="store_true", help="ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯")
    
    args = parser.parse_args()
    
    demo = IntegratedOptimizationDemo()
    demo.show_welcome()
    
    if args.check_deps:
        if demo.check_dependencies():
            print("âœ… å…¨ã¦ã®ä¾å­˜é–¢ä¿‚ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã™")
        else:
            print("âŒ ä¾å­˜é–¢ä¿‚ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        return
    
    if args.compare:
        demo.run_performance_comparison()
        return
    
    if args.model:
        # æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã§ãƒ‡ãƒ¢å®Ÿè¡Œ
        if args.model not in demo.available_models:
            print(f"âŒ æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«: {args.model}")
            demo.show_model_selection()
            return
        
        if args.interactive:
            demo.run_interactive_mode(args.model, args.optimization_mode)
        else:
            demo.run_integrated_demo(args.model, args.optimization_mode, args.test_scenario)
    else:
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªé¸æŠ
        if not demo.check_dependencies():
            print("âŒ ä¾å­˜é–¢ä¿‚ã‚’å…ˆã«è§£æ±ºã—ã¦ãã ã•ã„")
            return
        
        demo.show_model_selection()
        
        try:
            # ãƒ¢ãƒ‡ãƒ«é¸æŠ
            choice = input("\nğŸ“± ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ (1-4): ").strip()
            model_list = list(demo.available_models.keys())
            
            if not (choice.isdigit() and 1 <= int(choice) <= len(model_list)):
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
                return
            
            selected_model = model_list[int(choice) - 1]
            
            # æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰é¸æŠ
            demo.show_optimization_modes()
            mode_choice = input("\nğŸ”§ æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ (1-4): ").strip()
            mode_list = list(demo.optimization_modes.keys())
            
            if not (mode_choice.isdigit() and 1 <= int(mode_choice) <= len(mode_list)):
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
                return
            
            selected_mode = mode_list[int(mode_choice) - 1]
            
            # å®Ÿè¡Œã‚¿ã‚¤ãƒ—é¸æŠ
            print("\nğŸ¯ å®Ÿè¡Œã‚¿ã‚¤ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„:")
            print("1. åŸºæœ¬ãƒ†ã‚¹ãƒˆ")
            print("2. é«˜åº¦ãƒ†ã‚¹ãƒˆ")
            print("3. ç¿»è¨³ãƒ†ã‚¹ãƒˆ")
            print("4. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
            print("5. æ€§èƒ½æ¯”è¼ƒ")
            
            exec_choice = input("é¸æŠ (1-5): ").strip()
            
            if exec_choice == "1":
                demo.run_integrated_demo(selected_model, selected_mode, "basic")
            elif exec_choice == "2":
                demo.run_integrated_demo(selected_model, selected_mode, "advanced")
            elif exec_choice == "3":
                demo.run_integrated_demo(selected_model, selected_mode, "translation")
            elif exec_choice == "4":
                demo.run_interactive_mode(selected_model, selected_mode)
            elif exec_choice == "5":
                demo.run_performance_comparison()
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ãƒ‡ãƒ¢ã‚’çµ‚äº†ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()

