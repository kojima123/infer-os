"""
NPUç”¨ONNXå¤‰æ›å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤§è¦æ¨¡LLMãƒ¢ãƒ‡ãƒ«ã‚’NPUï¼ˆDirectMLï¼‰å‘ã‘ONNXå½¢å¼ã«å¤‰æ›

ä½¿ç”¨æ–¹æ³•:
    python convert_to_onnx_npu.py --model rinna/youri-7b-chat --output ./onnx_models/
"""

import os
import sys
import argparse
import time
import traceback
from typing import Optional

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import onnx
import onnxruntime as ort

class NPUONNXConverter:
    """NPUç”¨ONNXå¤‰æ›å™¨"""
    
    def __init__(self, model_name: str, output_dir: str = "./onnx_models/"):
        self.model_name = model_name
        self.output_dir = output_dir
        self.model = None
        self.tokenizer = None
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸš€ NPUç”¨ONNXå¤‰æ›å™¨åˆæœŸåŒ–")
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    
    def load_model(self) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰"""
        try:
            print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            load_start = time.time()
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="cpu",  # CPUä¸Šã§ãƒ­ãƒ¼ãƒ‰ï¼ˆONNXå¤‰æ›ç”¨ï¼‰
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            load_time = time.time() - load_start
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† ({load_time:.1f}ç§’)")
            
            # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            self.model.eval()
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def convert_to_onnx(self, max_sequence_length: int = 512) -> Optional[str]:
        """ONNXå¤‰æ›å®Ÿè¡Œ"""
        try:
            print("ğŸ”„ ONNXå¤‰æ›é–‹å§‹...")
            
            if self.model is None or self.tokenizer is None:
                print("âŒ ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰")
                return None
            
            # ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ä½œæˆ
            sample_text = "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"
            sample_inputs = self.tokenizer(
                sample_text,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=max_sequence_length
            )
            
            print(f"ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›å½¢çŠ¶: {sample_inputs['input_ids'].shape}")
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model_safe_name = self.model_name.replace('/', '_').replace('-', '_')
            onnx_path = os.path.join(self.output_dir, f"{model_safe_name}_npu.onnx")
            
            print(f"ğŸ“ å‡ºåŠ›ãƒ‘ã‚¹: {onnx_path}")
            
            # å‹•çš„è»¸è¨­å®šï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å‹•çš„ã«ï¼‰
            dynamic_axes = {
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'attention_mask': {0: 'batch_size', 1: 'sequence'},
                'logits': {0: 'batch_size', 1: 'sequence'}
            }
            
            print("ğŸ”§ ONNXå¤‰æ›å®Ÿè¡Œä¸­...")
            convert_start = time.time()
            
            # ONNXå¤‰æ›å®Ÿè¡Œ
            with torch.no_grad():
                torch.onnx.export(
                    self.model,
                    (sample_inputs['input_ids'], sample_inputs['attention_mask']),
                    onnx_path,
                    export_params=True,
                    opset_version=11,  # DirectMLäº’æ›ãƒãƒ¼ã‚¸ãƒ§ãƒ³
                    do_constant_folding=True,
                    input_names=['input_ids', 'attention_mask'],
                    output_names=['logits'],
                    dynamic_axes=dynamic_axes,
                    verbose=False
                )
            
            convert_time = time.time() - convert_start
            print(f"âœ… ONNXå¤‰æ›å®Œäº† ({convert_time:.1f}ç§’)")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size = os.path.getsize(onnx_path) / (1024**3)  # GB
            print(f"ğŸ“Š ONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:.2f}GB")
            
            # ONNXæ¤œè¨¼
            print("ğŸ” ONNXæ¤œè¨¼ä¸­...")
            try:
                onnx_model = onnx.load(onnx_path)
                onnx.checker.check_model(onnx_model)
                print("âœ… ONNXæ¤œè¨¼æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ ONNXæ¤œè¨¼è­¦å‘Š: {e}")
            
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return None
    
    def test_npu_session(self, onnx_path: str) -> bool:
        """NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        try:
            print("ğŸ§ª NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹...")
            
            # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,
                    'enable_dynamic_graph_fusion': True,
                    'enable_graph_optimization': True,
                    'disable_memory_arena': False,
                    'memory_limit_mb': 4096,
                })
            ]
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = False
            session_options.enable_cpu_mem_arena = False
            session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            print("ğŸ”§ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆä¸­...")
            session = ort.InferenceSession(
                onnx_path,
                providers=providers,
                sess_options=session_options
            )
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            active_providers = session.get_providers()
            print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {active_providers}")
            
            if 'DmlExecutionProvider' not in active_providers:
                print("âš ï¸ DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒç„¡åŠ¹")
                return False
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            print("ğŸš€ NPUãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            
            # ãƒ†ã‚¹ãƒˆå…¥åŠ›ä½œæˆ
            test_text = "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
            test_inputs = self.tokenizer(
                test_text,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=512
            )
            
            input_ids = test_inputs['input_ids'].numpy().astype(np.int64)
            attention_mask = test_inputs['attention_mask'].numpy().astype(np.int64)
            
            # NPUæ¨è«–å®Ÿè¡Œ
            test_start = time.time()
            outputs = session.run(
                ['logits'],
                {
                    'input_ids': input_ids,
                    'attention_mask': attention_mask
                }
            )
            test_time = time.time() - test_start
            
            logits = outputs[0]
            print(f"âœ… NPUãƒ†ã‚¹ãƒˆæˆåŠŸ: logitså½¢çŠ¶{logits.shape}, å®Ÿè¡Œæ™‚é–“{test_time:.3f}ç§’")
            
            # è¤‡æ•°å›å®Ÿè¡Œã§NPUè² è·ãƒ†ã‚¹ãƒˆ
            print("ğŸ”¥ NPUè² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            for i in range(10):
                session.run(
                    ['logits'],
                    {
                        'input_ids': input_ids,
                        'attention_mask': attention_mask
                    }
                )
                if i % 3 == 0:
                    print(f"  ğŸ”„ NPUè² è·ãƒ†ã‚¹ãƒˆ {i+1}/10")
            
            print("âœ… NPUè² è·ãƒ†ã‚¹ãƒˆå®Œäº†")
            print("ğŸ¯ ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§NPUä½¿ç”¨ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            
            return True
            
        except Exception as e:
            print(f"âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="NPUç”¨ONNXå¤‰æ›å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--model", type=str, default="rinna/youri-7b-chat",
                       help="å¤‰æ›ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--output", type=str, default="./onnx_models/",
                       help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--max-length", type=int, default=512,
                       help="æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·")
    parser.add_argument("--test-npu", action="store_true",
                       help="å¤‰æ›å¾Œã«NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    print("ğŸš€ NPUç”¨ONNXå¤‰æ›é–‹å§‹")
    print("=" * 60)
    
    # å¤‰æ›å™¨åˆæœŸåŒ–
    converter = NPUONNXConverter(args.model, args.output)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    if not converter.load_model():
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    # ONNXå¤‰æ›å®Ÿè¡Œ
    onnx_path = converter.convert_to_onnx(args.max_length)
    if not onnx_path:
        print("âŒ ONNXå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
    
    print(f"âœ… ONNXå¤‰æ›æˆåŠŸ: {onnx_path}")
    
    # NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
    if args.test_npu:
        print("\nğŸ§ª NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        if converter.test_npu_session(onnx_path):
            print("âœ… NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        else:
            print("âŒ NPUã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆå¤±æ•—")
            sys.exit(1)
    
    print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"ğŸ“ ONNXãƒ•ã‚¡ã‚¤ãƒ«: {onnx_path}")
    print("ğŸ’¡ ä¿®æ­£ç‰ˆãƒ‡ãƒ¢ã§ä½¿ç”¨ã—ã¦ãã ã•ã„:")
    print(f"   python infer_os_japanese_llm_demo_fixed.py --model {args.model} --enable-npu --interactive")

if __name__ == "__main__":
    main()

