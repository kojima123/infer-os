#!/usr/bin/env python3
"""
Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã®Infer-OSçµ±åˆåŠ¹æœã‚’å®šé‡çš„ã«æ¸¬å®š
"""

import time
import psutil
import threading
import statistics
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import json
import os
from datetime import datetime

@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    model_name: str
    quantization_profile: str
    infer_os_enabled: bool
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
    avg_tokens_per_sec: float
    peak_memory_gb: float
    avg_memory_gb: float
    avg_cpu_percent: float
    avg_response_time_sec: float
    
    # å“è³ªæŒ‡æ¨™
    avg_japanese_ratio: float
    avg_text_length: int
    coherence_score: float
    
    # å®‰å®šæ€§æŒ‡æ¨™
    performance_variance: float
    error_count: int
    total_requests: int
    
    # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    first_token_latency: float
    throughput_requests_per_min: float
    memory_efficiency_score: float

class InferOSMode(Enum):
    """Infer-OSå‹•ä½œãƒ¢ãƒ¼ãƒ‰"""
    DISABLED = "disabled"
    ENABLED = "enabled"
    COMPARISON = "comparison"

class ComparisonBenchmark:
    """Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    def __init__(self, model_name: str, quantization_profile: str = "balanced"):
        self.model_name = model_name
        self.quantization_profile = quantization_profile
        self.results: Dict[str, BenchmarkResult] = {}
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
        self.test_prompts = [
            "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã¨å¿œç”¨ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
            "æ·±å±¤å­¦ç¿’ã¨ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é–¢ä¿‚ã‚’è§£èª¬ã—ã¦ãã ã•ã„ã€‚",
            "è‡ªç„¶è¨€èªå‡¦ç†ã®æœ€æ–°æŠ€è¡“ã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®å¯èƒ½æ€§ã‚’è«–ã˜ã¦ãã ã•ã„ã€‚",
            "ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®é‡è¦æ€§ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªPythonã®ç‰¹å¾´ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚",
            "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®åˆ©ç‚¹ã‚’æŒ™ã’ã¦ãã ã•ã„ã€‚"
        ]
        
        self.monitoring_active = False
        self.performance_data = []
    
    def start_system_monitoring(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹"""
        self.monitoring_active = True
        self.performance_data = []
        
        def monitor():
            while self.monitoring_active:
                data = {
                    'timestamp': time.time(),
                    'memory_percent': psutil.virtual_memory().percent,
                    'memory_used_gb': psutil.virtual_memory().used / (1024**3),
                    'cpu_percent': psutil.cpu_percent(interval=0.1),
                    'cpu_per_core': psutil.cpu_percent(interval=0.1, percpu=True)
                }
                self.performance_data.append(data)
                time.sleep(0.5)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_system_monitoring(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1.0)
        
        if not self.performance_data:
            return {}
        
        memory_usage = [d['memory_used_gb'] for d in self.performance_data]
        cpu_usage = [d['cpu_percent'] for d in self.performance_data]
        
        return {
            'peak_memory_gb': max(memory_usage),
            'avg_memory_gb': statistics.mean(memory_usage),
            'min_memory_gb': min(memory_usage),
            'avg_cpu_percent': statistics.mean(cpu_usage),
            'max_cpu_percent': max(cpu_usage),
            'memory_variance': statistics.variance(memory_usage) if len(memory_usage) > 1 else 0,
            'cpu_variance': statistics.variance(cpu_usage) if len(cpu_usage) > 1 else 0
        }
    
    def measure_text_quality(self, text: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆå“è³ªæ¸¬å®š"""
        if not text:
            return {
                'japanese_ratio': 0.0,
                'text_length': 0,
                'coherence_score': 0.0
            }
        
        # æ—¥æœ¬èªæ–‡å­—æ¯”ç‡è¨ˆç®—
        japanese_chars = 0
        total_chars = len(text)
        
        for char in text:
            if '\u3040' <= char <= '\u309F':  # ã²ã‚‰ãŒãª
                japanese_chars += 1
            elif '\u30A0' <= char <= '\u30FF':  # ã‚«ã‚¿ã‚«ãƒŠ
                japanese_chars += 1
            elif '\u4E00' <= char <= '\u9FAF':  # æ¼¢å­—
                japanese_chars += 1
        
        japanese_ratio = japanese_chars / total_chars if total_chars > 0 else 0
        
        # ç°¡æ˜“çš„ãªä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ–‡ã®é•·ã•ã®åˆ†æ•£ã‹ã‚‰æ¨å®šï¼‰
        sentences = text.split('ã€‚')
        sentence_lengths = [len(s.strip()) for s in sentences if s.strip()]
        
        if len(sentence_lengths) > 1:
            coherence_score = 1.0 / (1.0 + statistics.variance(sentence_lengths) / 100)
        else:
            coherence_score = 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        return {
            'japanese_ratio': japanese_ratio,
            'text_length': total_chars,
            'coherence_score': min(1.0, coherence_score)
        }
    
    def run_single_benchmark(self, demo_instance, infer_os_enabled: bool, num_iterations: int = 5) -> BenchmarkResult:
        """å˜ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­: Infer-OS {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        
        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
        self.start_system_monitoring()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ‡ãƒ¼ã‚¿
        response_times = []
        tokens_per_sec_list = []
        first_token_latencies = []
        quality_metrics = []
        error_count = 0
        
        start_benchmark_time = time.time()
        
        for i, prompt in enumerate(self.test_prompts[:num_iterations]):
            try:
                print(f"  ãƒ†ã‚¹ãƒˆ {i+1}/{num_iterations}: {prompt[:30]}...")
                
                # æ¨è«–å®Ÿè¡Œ
                start_time = time.time()
                result = demo_instance.generate_japanese_text(prompt, max_new_tokens=100)
                end_time = time.time()
                
                response_time = end_time - start_time
                response_times.append(response_time)
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
                if 'generation_speed' in result:
                    tokens_per_sec_list.append(result['generation_speed'])
                
                if 'first_token_time' in result:
                    first_token_latencies.append(result['first_token_time'])
                else:
                    first_token_latencies.append(response_time * 0.1)  # æ¨å®šå€¤
                
                # å“è³ªæŒ‡æ¨™
                generated_text = result.get('generated_text', '')
                quality = self.measure_text_quality(generated_text)
                quality_metrics.append(quality)
                
                # çŸ­ã„ä¼‘æ†©ï¼ˆãƒ¡ãƒ¢ãƒªå®‰å®šåŒ–ï¼‰
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                error_count += 1
                continue
        
        end_benchmark_time = time.time()
        total_benchmark_time = end_benchmark_time - start_benchmark_time
        
        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–åœæ­¢
        system_metrics = self.stop_system_monitoring()
        
        # çµæœé›†è¨ˆ
        if response_times:
            avg_response_time = statistics.mean(response_times)
            performance_variance = statistics.variance(response_times) if len(response_times) > 1 else 0
        else:
            avg_response_time = 0
            performance_variance = 0
        
        if tokens_per_sec_list:
            avg_tokens_per_sec = statistics.mean(tokens_per_sec_list)
        else:
            avg_tokens_per_sec = 0
        
        if first_token_latencies:
            avg_first_token_latency = statistics.mean(first_token_latencies)
        else:
            avg_first_token_latency = 0
        
        if quality_metrics:
            avg_japanese_ratio = statistics.mean([q['japanese_ratio'] for q in quality_metrics])
            avg_text_length = statistics.mean([q['text_length'] for q in quality_metrics])
            avg_coherence_score = statistics.mean([q['coherence_score'] for q in quality_metrics])
        else:
            avg_japanese_ratio = 0
            avg_text_length = 0
            avg_coherence_score = 0
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        successful_requests = len(response_times)
        throughput = (successful_requests / total_benchmark_time) * 60 if total_benchmark_time > 0 else 0
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        memory_efficiency = 0
        if system_metrics.get('avg_memory_gb', 0) > 0:
            memory_efficiency = avg_tokens_per_sec / system_metrics['avg_memory_gb']
        
        return BenchmarkResult(
            model_name=self.model_name,
            quantization_profile=self.quantization_profile,
            infer_os_enabled=infer_os_enabled,
            avg_tokens_per_sec=avg_tokens_per_sec,
            peak_memory_gb=system_metrics.get('peak_memory_gb', 0),
            avg_memory_gb=system_metrics.get('avg_memory_gb', 0),
            avg_cpu_percent=system_metrics.get('avg_cpu_percent', 0),
            avg_response_time_sec=avg_response_time,
            avg_japanese_ratio=avg_japanese_ratio,
            avg_text_length=int(avg_text_length),
            coherence_score=avg_coherence_score,
            performance_variance=performance_variance,
            error_count=error_count,
            total_requests=num_iterations,
            first_token_latency=avg_first_token_latency,
            throughput_requests_per_min=throughput,
            memory_efficiency_score=memory_efficiency
        )
    
    def run_comparison_benchmark(self, demo_class, num_iterations: int = 5) -> Dict[str, BenchmarkResult]:
        """Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print(f"\nğŸ”¥ Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {self.quantization_profile}")
        print(f"ãƒ†ã‚¹ãƒˆå›æ•°: {num_iterations}")
        
        results = {}
        
        # Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        print(f"\nğŸ“Š Phase 1: Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        try:
            demo_without_infer_os = demo_class(
                model_name=self.model_name,
                use_4bit=True,
                use_8bit=False,
                use_advanced_quant=True,
                quantization_profile=self.quantization_profile,
                infer_os_enabled=False  # Infer-OSç„¡åŠ¹
            )
            
            if demo_without_infer_os.load_model_with_optimization():
                results['without_infer_os'] = self.run_single_benchmark(
                    demo_without_infer_os, False, num_iterations
                )
                print("âœ… Infer-OSç„¡åŠ¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
            else:
                print("âŒ Infer-OSç„¡åŠ¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ Infer-OSç„¡åŠ¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        if 'demo_without_infer_os' in locals():
            del demo_without_infer_os
        
        time.sleep(2)  # ãƒ¡ãƒ¢ãƒªå®‰å®šåŒ–
        
        # Infer-OSæœ‰åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        print(f"\nğŸ“Š Phase 2: Infer-OSæœ‰åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        try:
            demo_with_infer_os = demo_class(
                model_name=self.model_name,
                use_4bit=True,
                use_8bit=False,
                use_advanced_quant=True,
                quantization_profile=self.quantization_profile,
                infer_os_enabled=True  # Infer-OSæœ‰åŠ¹
            )
            
            if demo_with_infer_os.load_model_with_optimization():
                results['with_infer_os'] = self.run_single_benchmark(
                    demo_with_infer_os, True, num_iterations
                )
                print("âœ… Infer-OSæœ‰åŠ¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
            else:
                print("âŒ Infer-OSæœ‰åŠ¹ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ Infer-OSæœ‰åŠ¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        
        self.results = results
        return results
    
    def generate_comparison_report(self) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if len(self.results) != 2:
            return "âŒ æ¯”è¼ƒã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
        
        without_infer_os = self.results['without_infer_os']
        with_infer_os = self.results['with_infer_os']
        
        # æ”¹å–„ç‡è¨ˆç®—
        def calc_improvement(old_val, new_val, higher_is_better=True):
            if old_val == 0:
                return 0
            if higher_is_better:
                return ((new_val - old_val) / old_val) * 100
            else:
                return ((old_val - new_val) / old_val) * 100
        
        speed_improvement = calc_improvement(
            without_infer_os.avg_tokens_per_sec,
            with_infer_os.avg_tokens_per_sec
        )
        
        memory_improvement = calc_improvement(
            without_infer_os.avg_memory_gb,
            with_infer_os.avg_memory_gb,
            higher_is_better=False
        )
        
        response_time_improvement = calc_improvement(
            without_infer_os.avg_response_time_sec,
            with_infer_os.avg_response_time_sec,
            higher_is_better=False
        )
        
        throughput_improvement = calc_improvement(
            without_infer_os.throughput_requests_per_min,
            with_infer_os.throughput_requests_per_min
        )
        
        report = f"""
ğŸ”¥ **Infer-OSçµ±åˆåŠ¹æœ è©³ç´°æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ**

## ğŸ“‹ **ãƒ†ã‚¹ãƒˆç’°å¢ƒ**
- **ãƒ¢ãƒ‡ãƒ«**: {self.model_name}
- **é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: {self.quantization_profile}
- **ãƒ†ã‚¹ãƒˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ**

### **æ¨è«–é€Ÿåº¦**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.avg_tokens_per_sec:.1f} tokens/sec
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.avg_tokens_per_sec:.1f} tokens/sec
- **æ”¹å–„ç‡**: {speed_improvement:+.1f}% {'ğŸš€' if speed_improvement > 0 else 'âš ï¸'}

### **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.avg_memory_gb:.1f}GB
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.avg_memory_gb:.1f}GB
- **å‰Šæ¸›ç‡**: {memory_improvement:+.1f}% {'ğŸ’¾' if memory_improvement > 0 else 'âš ï¸'}

### **å¿œç­”æ™‚é–“**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.avg_response_time_sec:.2f}ç§’
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.avg_response_time_sec:.2f}ç§’
- **çŸ­ç¸®ç‡**: {response_time_improvement:+.1f}% {'âš¡' if response_time_improvement > 0 else 'âš ï¸'}

### **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.throughput_requests_per_min:.1f} requests/min
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.throughput_requests_per_min:.1f} requests/min
- **å‘ä¸Šç‡**: {throughput_improvement:+.1f}% {'ğŸ“ˆ' if throughput_improvement > 0 else 'âš ï¸'}

## ğŸ¯ **å“è³ªæ¯”è¼ƒ**

### **æ—¥æœ¬èªå“è³ª**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.avg_japanese_ratio:.1%}
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.avg_japanese_ratio:.1%}
- **å·®ç•°**: {(with_infer_os.avg_japanese_ratio - without_infer_os.avg_japanese_ratio)*100:+.1f}%

### **æ–‡ç« é•·**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.avg_text_length}æ–‡å­—
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.avg_text_length}æ–‡å­—
- **å·®ç•°**: {with_infer_os.avg_text_length - without_infer_os.avg_text_length:+d}æ–‡å­—

### **ä¸€è²«æ€§ã‚¹ã‚³ã‚¢**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.coherence_score:.3f}
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.coherence_score:.3f}
- **å·®ç•°**: {(with_infer_os.coherence_score - without_infer_os.coherence_score):+.3f}

## ğŸ”§ **ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æ¯”è¼ƒ**

### **CPUä½¿ç”¨ç‡**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.avg_cpu_percent:.1f}%
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.avg_cpu_percent:.1f}%
- **å·®ç•°**: {(with_infer_os.avg_cpu_percent - without_infer_os.avg_cpu_percent):+.1f}%

### **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**
- **Infer-OSç„¡åŠ¹**: {without_infer_os.memory_efficiency_score:.2f} tokens/sec/GB
- **Infer-OSæœ‰åŠ¹**: {with_infer_os.memory_efficiency_score:.2f} tokens/sec/GB
- **å‘ä¸Šç‡**: {calc_improvement(without_infer_os.memory_efficiency_score, with_infer_os.memory_efficiency_score):+.1f}%

### **å®‰å®šæ€§**
- **Infer-OSç„¡åŠ¹**: åˆ†æ•£ {without_infer_os.performance_variance:.3f}
- **Infer-OSæœ‰åŠ¹**: åˆ†æ•£ {with_infer_os.performance_variance:.3f}
- **å®‰å®šæ€§**: {'å‘ä¸Š' if with_infer_os.performance_variance < without_infer_os.performance_variance else 'ä½ä¸‹'}

## ğŸ‰ **ç·åˆè©•ä¾¡**

### **Infer-OSçµ±åˆã®åŠ¹æœ**
"""
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        total_score = 0
        score_count = 0
        
        if speed_improvement > 0:
            total_score += min(speed_improvement, 100)
            score_count += 1
        
        if memory_improvement > 0:
            total_score += min(memory_improvement, 100)
            score_count += 1
        
        if response_time_improvement > 0:
            total_score += min(response_time_improvement, 100)
            score_count += 1
        
        if throughput_improvement > 0:
            total_score += min(throughput_improvement, 100)
            score_count += 1
        
        overall_score = total_score / score_count if score_count > 0 else 0
        
        if overall_score >= 50:
            evaluation = "ğŸ† **å„ªç§€** - Infer-OSçµ±åˆã«ã‚ˆã‚Šå¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾"
        elif overall_score >= 25:
            evaluation = "âœ… **è‰¯å¥½** - Infer-OSçµ±åˆã«ã‚ˆã‚Šæ˜ç¢ºãªæ€§èƒ½å‘ä¸Šã‚’ç¢ºèª"
        elif overall_score >= 10:
            evaluation = "ğŸ“ˆ **æ”¹å–„** - Infer-OSçµ±åˆã«ã‚ˆã‚Šä¸€å®šã®æ€§èƒ½å‘ä¸Šã‚’ç¢ºèª"
        else:
            evaluation = "âš ï¸ **è¦æ¤œè¨¼** - Infer-OSçµ±åˆã®åŠ¹æœãŒé™å®šçš„"
        
        report += f"""
- **ç·åˆæ”¹å–„ã‚¹ã‚³ã‚¢**: {overall_score:.1f}%
- **è©•ä¾¡**: {evaluation}

### **æ¨å¥¨äº‹é …**
"""
        
        if speed_improvement > 20:
            report += "- âœ… æ¨è«–é€Ÿåº¦ã®å¤§å¹…å‘ä¸Šã«ã‚ˆã‚Šã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç”¨ã«é©ç”¨å¯èƒ½\n"
        
        if memory_improvement > 30:
            report += "- âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®å¤§å¹…æ”¹å–„ã«ã‚ˆã‚Šã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡ŒãŒå¯èƒ½\n"
        
        if response_time_improvement > 25:
            report += "- âœ… å¿œç­”æ™‚é–“ã®å¤§å¹…çŸ­ç¸®ã«ã‚ˆã‚Šã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªç”¨é€”ã«æœ€é©\n"
        
        if overall_score < 10:
            report += "- âš ï¸ ç’°å¢ƒè¨­å®šã‚„ãƒ¢ãƒ‡ãƒ«é¸æŠã®è¦‹ç›´ã—ã‚’æ¨å¥¨\n"
            report += "- ğŸ’¡ ã‚ˆã‚Šè»½é‡ãªãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ç•°ãªã‚‹é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©¦è¡Œ\n"
        
        return report
    
    def save_results(self, filename: str = None):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"infer_os_comparison_{self.model_name.replace('/', '_')}_{timestamp}.json"
        
        # BenchmarkResultã‚’dictå½¢å¼ã«å¤‰æ›
        results_dict = {}
        for key, result in self.results.items():
            results_dict[key] = {
                'model_name': result.model_name,
                'quantization_profile': result.quantization_profile,
                'infer_os_enabled': result.infer_os_enabled,
                'avg_tokens_per_sec': result.avg_tokens_per_sec,
                'peak_memory_gb': result.peak_memory_gb,
                'avg_memory_gb': result.avg_memory_gb,
                'avg_cpu_percent': result.avg_cpu_percent,
                'avg_response_time_sec': result.avg_response_time_sec,
                'avg_japanese_ratio': result.avg_japanese_ratio,
                'avg_text_length': result.avg_text_length,
                'coherence_score': result.coherence_score,
                'performance_variance': result.performance_variance,
                'error_count': result.error_count,
                'total_requests': result.total_requests,
                'first_token_latency': result.first_token_latency,
                'throughput_requests_per_min': result.throughput_requests_per_min,
                'memory_efficiency_score': result.memory_efficiency_score
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        return filename

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¥ Infer-OSçµ±åˆåŠ¹æœæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ç°¡æ˜“å®Ÿè¡Œ
    benchmark = ComparisonBenchmark("rinna/youri-7b-chat", "balanced")
    
    # å®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯ã€JapaneseHeavyLLMDemoã‚¯ãƒ©ã‚¹ã‚’æ¸¡ã™
    # results = benchmark.run_comparison_benchmark(JapaneseHeavyLLMDemo, num_iterations=3)
    # report = benchmark.generate_comparison_report()
    # print(report)
    # benchmark.save_results()
    
    print("âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½å®Ÿè£…å®Œäº†")

if __name__ == "__main__":
    main()

