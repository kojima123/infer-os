# -*- coding: utf-8 -*-
"""
ğŸš€ Infer-OS æ—¥æœ¬èªé‡é‡ç´šLLMçµ±åˆãƒ‡ãƒ¢

æ—¥æœ¬èªå¯¾å¿œã®é‡é‡ç´šLLMãƒ¢ãƒ‡ãƒ«ã§Infer-OSæœ€é©åŒ–åŠ¹æœã‚’ä½“é¨“ã™ã‚‹çµ±åˆãƒ‡ãƒ¢ã‚·ã‚¹ãƒ†ãƒ 

ä¸»è¦æ©Ÿèƒ½:
- ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªé‡é‡ç´šLLMã‚µãƒãƒ¼ãƒˆ (7B-10Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
- âš¡ Infer-OSæœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ– (2-5å€)
- ğŸ§  ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– (27.8GBç’°å¢ƒå¯¾å¿œ)
- ğŸ’» Windows NPUæœ€é©åŒ– (AMD/Intel/Qualcomm)
- ğŸ”§ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ– (W4/W8 + KVé‡å­åŒ–)
- ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–
- ğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å¯¾è©±ãƒ¢ãƒ¼ãƒ‰

å¯¾å¿œãƒ¢ãƒ‡ãƒ«:
- matsuo-lab/weblab-10b (10B) - æœ€é‡é‡ç´šæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
- rinna/youri-7b-chat (7B) - é‡é‡ç´šãƒãƒ£ãƒƒãƒˆç‰¹åŒ–
- cyberagent/open-calm-7b (7B) - é‡é‡ç´šãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«
- stabilityai/japanese-stablelm-instruct-alpha-7b (7B) - é‡é‡ç´šæŒ‡ç¤ºè¿½å¾“

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬å®Ÿè¡Œ
    python infer_os_japanese_llm_demo.py --model rinna/youri-7b-chat --interactive
    
    # 27.8GBç’°å¢ƒã§ã®ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
    python infer_os_japanese_llm_demo.py --model rinna/youri-7b-chat --use-aggressive-memory --interactive
    
    # Windows NPUæœ€é©åŒ–æœ‰åŠ¹
    python infer_os_japanese_llm_demo.py --model rinna/youri-7b-chat --enable-npu --interactive
    
    # å…¨æ©Ÿèƒ½æœ‰åŠ¹
    python infer_os_japanese_llm_demo.py --model rinna/youri-7b-chat --use-aggressive-memory --enable-npu --use-advanced-quant --interactive
"""

import sys
import os
import gc
import time
import traceback
import argparse
import platform
from typing import Dict, List, Optional, Any
import psutil

# PyTorché–¢é€£
try:
    import torch
    import torch.nn as nn
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, 
        BitsAndBytesConfig, pipeline
    )
    TORCH_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ PyTorch/Transformersã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    TORCH_AVAILABLE = False

# æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from accelerate import Accelerator
    ACCELERATE_AVAILABLE = True
except ImportError:
    ACCELERATE_AVAILABLE = False

try:
    import bitsandbytes as bnb
    BITSANDBYTES_AVAILABLE = True
except ImportError:
    BITSANDBYTES_AVAILABLE = False

# ONNX Runtime
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False

# é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from advanced_quantization_optimizer import AdvancedQuantizationOptimizer, QuantizationProfile
    ADVANCED_QUANT_AVAILABLE = True
except ImportError:
    ADVANCED_QUANT_AVAILABLE = False
    AdvancedQuantizationOptimizer = None
    QuantizationProfile = None

# ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from aggressive_memory_optimizer import AggressiveMemoryOptimizer
    AGGRESSIVE_MEMORY_AVAILABLE = True
except ImportError:
    AGGRESSIVE_MEMORY_AVAILABLE = False
    AggressiveMemoryOptimizer = None

# Windows NPUæœ€é©åŒ–æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from windows_npu_optimizer import WindowsNPUOptimizer
    WINDOWS_NPU_AVAILABLE = True
except ImportError:
    WINDOWS_NPU_AVAILABLE = False
    WindowsNPUOptimizer = None

# æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from infer_os_comparison_benchmark import ComparisonBenchmark, InferOSMode
    COMPARISON_BENCHMARK_AVAILABLE = True
except ImportError:
    COMPARISON_BENCHMARK_AVAILABLE = False
    ComparisonBenchmark = None
    InferOSMode = None

# æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«
JAPANESE_PROMPT_SAMPLES = {
    "æ—¥å¸¸ä¼šè©±": [
        "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
        "ãŠã™ã™ã‚ã®æ˜ ç”»ã‚’æ•™ãˆã¦ãã ã•ã„",
        "ç¾å‘³ã—ã„æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦"
    ],
    "æŠ€è¡“ãƒ»å°‚é–€": [
        "äººå·¥çŸ¥èƒ½ã®æœ€æ–°å‹•å‘ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®ä»•çµ„ã¿ã‚’æ•™ãˆã¦",
        "æ©Ÿæ¢°å­¦ç¿’ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦"
    ],
    "æ–‡åŒ–ãƒ»æ­´å²": [
        "æ—¥æœ¬ã®å››å­£ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
        "æ±Ÿæˆ¸æ™‚ä»£ã®æ–‡åŒ–ã«ã¤ã„ã¦æ•™ãˆã¦",
        "æ—¥æœ¬ã®ä¼çµ±èŠ¸èƒ½ã«ã¤ã„ã¦"
    ],
    "å‰µä½œãƒ»æ–‡å­¦": [
        "çŸ­ã„ç‰©èªã‚’æ›¸ã„ã¦ãã ã•ã„",
        "ä¿³å¥ã‚’ä½œã£ã¦ãã ã•ã„",
        "è©©ã‚’æ›¸ã„ã¦ãã ã•ã„"
    ],
    "æ•™è‚²ãƒ»å­¦ç¿’": [
        "æ•°å­¦ã®åŸºæœ¬æ¦‚å¿µã‚’èª¬æ˜ã—ã¦",
        "æ­´å²ã®é‡è¦ãªå‡ºæ¥äº‹ã«ã¤ã„ã¦",
        "ç§‘å­¦ã®é¢ç™½ã„ç¾è±¡ã«ã¤ã„ã¦"
    ]
}

class InferOSJapaneseLLMDemo:
    """Infer-OSæ—¥æœ¬èªé‡é‡ç´šLLMçµ±åˆãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str = "rinna/youri-7b-chat", 
                 use_4bit: bool = False, use_8bit: bool = False,
                 use_onnx: bool = False, onnx_optimization_level: int = 2,
                 quantization_profile: str = "balanced", use_advanced_quant: bool = False,
                 infer_os_enabled: bool = True, use_aggressive_memory: bool = False,
                 enable_npu: bool = True):
        self.model_name = model_name
        self.use_4bit = use_4bit
        self.use_8bit = use_8bit
        self.use_onnx = use_onnx
        self.onnx_optimization_level = onnx_optimization_level
        self.use_advanced_quant = use_advanced_quant
        self.use_aggressive_memory = use_aggressive_memory
        self.enable_npu = enable_npu
        self.infer_os_enabled = infer_os_enabled
        
        self.model = None
        self.tokenizer = None
        self.onnx_generator = None
        self.optimization_applied = False
        
        # é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        if use_advanced_quant and ADVANCED_QUANT_AVAILABLE:
            try:
                profile_map = {
                    "safe": QuantizationProfile.SAFE,
                    "balanced": QuantizationProfile.BALANCED,
                    "aggressive": QuantizationProfile.AGGRESSIVE
                }
                self.quantization_profile = profile_map.get(quantization_profile, QuantizationProfile.BALANCED)
                self.advanced_quantizer = AdvancedQuantizationOptimizer(
                    profile=self.quantization_profile
                )
            except Exception as e:
                print(f"âš ï¸ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.use_advanced_quant = False
                self.advanced_quantizer = None
        else:
            self.quantization_profile = quantization_profile
            self.advanced_quantizer = None
        
        # ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š
        if use_aggressive_memory and AGGRESSIVE_MEMORY_AVAILABLE:
            try:
                self.aggressive_memory_optimizer = AggressiveMemoryOptimizer(model_name)
                print("âœ… ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
            except Exception as e:
                print(f"âš ï¸ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.use_aggressive_memory = False
                self.aggressive_memory_optimizer = None
        else:
            self.aggressive_memory_optimizer = None
        
        # Windows NPUæœ€é©åŒ–è¨­å®š
        if enable_npu and WINDOWS_NPU_AVAILABLE and platform.system() == "Windows":
            try:
                self.npu_optimizer = WindowsNPUOptimizer()
                print("ğŸ” Windows NPUæœ€é©åŒ–æ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
                
                # NPUæ¤œå‡ºã¨æœ‰åŠ¹åŒ–
                npu_info = self.npu_optimizer.detect_npu_hardware()
                if npu_info["detected"]:
                    success = self.npu_optimizer.enable_npu_optimization()
                    if success:
                        print(f"âœ… {npu_info['type']} NPUæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
                    else:
                        print("âš ï¸ NPUæœ€é©åŒ–ã®æœ‰åŠ¹åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    print("âš ï¸ NPUãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                    print("ğŸ’¡ DirectMLä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦NPUå¯¾å¿œã‚’æ”¹å–„ã§ãã¾ã™")
                    
            except Exception as e:
                print(f"âš ï¸ Windows NPUæœ€é©åŒ–åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.enable_npu = False
                self.npu_optimizer = None
        else:
            self.npu_optimizer = None
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—ãƒ»ä¿å­˜
        self.system_info = self._get_system_info()
        
        self._print_system_info()
        self._validate_system_requirements()
    
    def _get_system_info(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        memory = psutil.virtual_memory()
        return {
            "cpu_count": psutil.cpu_count(),
            "memory_total": memory.total / (1024**3),  # GB
            "memory_available": memory.available / (1024**3),  # GB
            "memory_percent": memory.percent,
            "python_version": sys.version,
            "torch_version": torch.__version__ if TORCH_AVAILABLE else "æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
            "cuda_available": torch.cuda.is_available() if TORCH_AVAILABLE else False,
            "accelerate_available": ACCELERATE_AVAILABLE,
            "bitsandbytes_available": BITSANDBYTES_AVAILABLE,
            "onnx_available": ONNX_AVAILABLE
        }
    
    def _print_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {sys.version.split()[0]}")
        print(f"  PyTorch: {self.system_info['torch_version']}")
        print(f"  CPU: {self.system_info['cpu_count']}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {self.system_info['memory_total']:.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {self.system_info['memory_total'] - self.system_info['memory_available']:.1f}GB ({self.system_info['memory_percent']:.1f}%)")
        print(f"  åˆ©ç”¨å¯èƒ½: {self.system_info['memory_available']:.1f}GB")
        
        print(f"\nğŸ”§ æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª:")
        print(f"  Accelerate: {'âœ…' if self.system_info['accelerate_available'] else 'âŒ'}")
        print(f"  BitsAndBytes: {'âœ…' if self.system_info['bitsandbytes_available'] else 'âŒ'}")
        print(f"  ONNX Runtime: {'âœ…' if self.system_info['onnx_available'] else 'âŒ'}")
        print(f"  é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–: {'âœ…' if self.use_advanced_quant else 'âŒ'}")
        
        if self.npu_optimizer and hasattr(self.npu_optimizer, 'npu_available'):
            print(f"  Windows NPUæœ€é©åŒ–: {'âœ…' if self.npu_optimizer.npu_available else 'âŒ'}")
    
    def _validate_system_requirements(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã‚’æ¤œè¨¼"""
        model_requirements = self._get_model_requirements()
        
        print(f"\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«è¦ä»¶:")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {model_requirements['description']}")
        print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model_requirements['parameters']:,}")
        print(f"  æ—¥æœ¬èªå“è³ª: {model_requirements['japanese_quality']}")
        print(f"  å°‚é–€åˆ†é‡: {model_requirements['specialization']}")
        print(f"  æœ€å°ãƒ¡ãƒ¢ãƒª: {model_requirements['min_memory']}GB")
        print(f"  æ¨å¥¨ãƒ¡ãƒ¢ãƒª: {model_requirements['recommended_memory']}GB")
        print(f"  FP16æ™‚: {model_requirements['fp16_memory']}GB")
        
        # ãƒ¡ãƒ¢ãƒªè¦ä»¶ãƒã‚§ãƒƒã‚¯
        available_memory = self.system_info['memory_available']
        if available_memory < model_requirements['recommended_memory']:
            print(f"âš ï¸  æ¨å¥¨ãƒ¡ãƒ¢ãƒªæœªæº€ã§ã™")
            print(f"  æ¨å¥¨: {model_requirements['recommended_memory']}GB, åˆ©ç”¨å¯èƒ½: {available_memory:.1f}GB")
            print(f"ğŸ’¡ é‡å­åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šæ€§å‘ä¸Š")
        else:
            print(f"âœ… ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™")
    
    def _get_model_requirements(self) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«è¦ä»¶ã‚’å–å¾—"""
        model_specs = {
            "matsuo-lab/weblab-10b": {
                "description": "æœ€é‡é‡ç´š 10Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªç‰¹åŒ–",
                "parameters": 10_000_000_000,
                "japanese_quality": "æœ€é«˜",
                "specialization": "æ—¥æœ¬èªç†è§£ãƒ»ç”Ÿæˆ",
                "min_memory": 40,
                "recommended_memory": 64,
                "fp16_memory": 20
            },
            "rinna/youri-7b-chat": {
                "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æ—¥æœ¬èªãƒãƒ£ãƒƒãƒˆç‰¹åŒ–",
                "parameters": 7_241_732_096,
                "japanese_quality": "é«˜",
                "specialization": "å¯¾è©±ãƒ»ãƒãƒ£ãƒƒãƒˆ",
                "min_memory": 32,
                "recommended_memory": 48,
                "fp16_memory": 14
            },
            "cyberagent/open-calm-7b": {
                "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«",
                "parameters": 6_738_415_616,
                "japanese_quality": "é«˜",
                "specialization": "æ—¥è‹±ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«",
                "min_memory": 28,
                "recommended_memory": 42,
                "fp16_memory": 13
            },
            "stabilityai/japanese-stablelm-instruct-alpha-7b": {
                "description": "é‡é‡ç´š 7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ æŒ‡ç¤ºè¿½å¾“ç‰¹åŒ–",
                "parameters": 6_738_415_616,
                "japanese_quality": "é«˜",
                "specialization": "æŒ‡ç¤ºç†è§£ãƒ»å®Ÿè¡Œ",
                "min_memory": 28,
                "recommended_memory": 42,
                "fp16_memory": 13
            }
        }
        
        return model_specs.get(self.model_name, {
            "description": "ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«",
            "parameters": 7_000_000_000,
            "japanese_quality": "ä¸æ˜",
            "specialization": "æ±ç”¨",
            "min_memory": 32,
            "recommended_memory": 48,
            "fp16_memory": 14
        })
    
    def load_model_with_optimization(self) -> bool:
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
        try:
            print(f"\nğŸ“¥ æ—¥æœ¬èªå¯¾å¿œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            print(f"âš ï¸  åˆå›å®Ÿè¡Œæ™‚ã¯å¤§å®¹é‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
            
            # ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’ä½¿ç”¨
            if self.use_aggressive_memory and self.aggressive_memory_optimizer:
                print("ğŸš€ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã§ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
                success = self.aggressive_memory_optimizer.load_model_with_chunked_loading()
                if success:
                    self.model = self.aggressive_memory_optimizer.model
                    self.tokenizer = self.aggressive_memory_optimizer.tokenizer
                    print("âœ… ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                    return True
                else:
                    print("âš ï¸ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã€é€šå¸¸ãƒ­ãƒ¼ãƒ‰ã‚’è©¦è¡Œ")
            
            # é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            return self._load_model_standard()
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def _load_model_standard(self) -> bool:
        """æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            print("ğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                use_fast=True
            )
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # é‡å­åŒ–è¨­å®š
            quantization_config = None
            if self.use_4bit and BITSANDBYTES_AVAILABLE:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("ğŸ”§ 4bité‡å­åŒ–è¨­å®šã‚’é©ç”¨")
            elif self.use_8bit and BITSANDBYTES_AVAILABLE:
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                print("ğŸ”§ 8bité‡å­åŒ–è¨­å®šã‚’é©ç”¨")
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            load_start = time.time()
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=quantization_config,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            load_time = time.time() - load_start
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† ({load_time:.1f}ç§’)")
            
            # é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–
            if self.use_advanced_quant and self.advanced_quantizer:
                print("âš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’é©ç”¨ä¸­...")
                self.model = self.advanced_quantizer.optimize_model(self.model)
                print("âœ… é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–å®Œäº†")
            
            # NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if self.enable_npu and self.npu_optimizer and self.npu_optimizer.npu_available:
                print("ğŸš€ NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
                npu_setup_success = self.npu_optimizer.setup_npu_inference(self.model, self.tokenizer)
                if npu_setup_success:
                    print("âœ… NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                else:
                    print("âš ï¸ NPUæ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã€CPUæ¨è«–ã‚’ä½¿ç”¨")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_japanese_text(self, prompt: str, max_length: int = 300, max_new_tokens: int = None, 
                              temperature: float = 0.7, do_sample: bool = True) -> Dict:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        if self.model is None or self.tokenizer is None:
            return {"error": "ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒæœªãƒ­ãƒ¼ãƒ‰"}
        
        try:
            print(f"\nğŸ¯ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
            print(f"æœ€å¤§é•·: {max_length}")
            
            # NPUæ¨è«–ã‚’å„ªå…ˆä½¿ç”¨
            if self.enable_npu and self.npu_optimizer and self.npu_optimizer.npu_available:
                print("âš¡ NPUæ¨è«–ã‚’ä½¿ç”¨ä¸­...")
                generated_text = self.npu_optimizer.run_npu_inference(
                    prompt, self.model, self.tokenizer, max_length
                )
                
                if generated_text:
                    # NPUæ¨è«–æˆåŠŸæ™‚ã®çµ±è¨ˆæƒ…å ±
                    return {
                        "generated_text": generated_text,
                        "generation_time": 0.0,  # NPUå†…ã§è¨ˆæ¸¬æ¸ˆã¿
                        "input_tokens": len(self.tokenizer.encode(prompt)),
                        "output_tokens": len(self.tokenizer.encode(generated_text)),
                        "tokens_per_sec": 0.0,  # NPUå†…ã§è¨ˆæ¸¬æ¸ˆã¿
                        "memory_used": 0.0,
                        "cpu_usage": 0.0,
                        "inference_method": "NPU"
                    }
                else:
                    print("âš ï¸ NPUæ¨è«–å¤±æ•—ã€CPUæ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
            # CPUæ¨è«–ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
            print("ğŸ–¥ï¸ CPUæ¨è«–ã‚’ä½¿ç”¨ä¸­...")
            
            # ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
            initial_memory = psutil.virtual_memory().used / (1024**3)
            initial_cpu = psutil.cpu_percent(interval=None)
            
            # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # max_new_tokensè¨­å®š
            if max_new_tokens is None:
                actual_max_new_tokens = max_length
            else:
                actual_max_new_tokens = max_new_tokens
            
            # ç”Ÿæˆè¨­å®šï¼ˆæ—¥æœ¬èªæœ€é©åŒ–ãƒ»é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯¾å¿œï¼‰
            generation_config = {
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": do_sample,
                "top_p": 0.95,
                "top_k": 40,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
                "early_stopping": False,
                "no_repeat_ngram_size": 3,
                "length_penalty": 1.0,
            }
            
            # ç”Ÿæˆå®Ÿè¡Œï¼ˆæ™‚é–“ãƒ»ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®šï¼‰
            start_time = time.time()
            
            # token_type_idsã‚¨ãƒ©ãƒ¼å›é¿: ä¸è¦ãªã‚­ãƒ¼ã‚’é™¤å»
            model_inputs = {k: v for k, v in inputs.items() if k != 'token_type_ids'}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **model_inputs,
                    **generation_config
                )
            
            end_time = time.time()
            generation_time = end_time - start_time
            
            # çµæœãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆæ”¹å–„ç‰ˆï¼‰
            generated_text = self.tokenizer.decode(
                outputs[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            
            # ç”Ÿæˆéƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
            if generated_text.startswith(prompt):
                generated_only = generated_text[len(prompt):].strip()
            else:
                generated_only = generated_text.strip()
            
            # ç©ºã®çµæœã‚„ã€Œã€‚ã€ã®ã¿ã®å ´åˆã®å¯¾å‡¦
            if not generated_only or generated_only == "ã€‚" or len(generated_only) < 3:
                print("âš ï¸ ç”ŸæˆçµæœãŒçŸ­ã™ãã¾ã™ã€‚å†ç”Ÿæˆã‚’è©¦è¡Œã—ã¾ã™...")
                
                # ã‚ˆã‚Šç·©ã„è¨­å®šã§å†ç”Ÿæˆ
                retry_config = generation_config.copy()
                retry_config.update({
                    "temperature": min(temperature + 0.2, 1.0),
                    "top_p": 0.98,
                    "repetition_penalty": 1.05,
                    "min_length": len(inputs['input_ids'][0]) + 10,
                })
                
                with torch.no_grad():
                    retry_outputs = self.model.generate(
                        **model_inputs,
                        **retry_config
                    )
                
                retry_text = self.tokenizer.decode(
                    retry_outputs[0],
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                
                if retry_text.startswith(prompt):
                    generated_only = retry_text[len(prompt):].strip()
                else:
                    generated_only = retry_text.strip()
                
                outputs = retry_outputs
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
            final_memory = psutil.virtual_memory().used / (1024**3)
            final_cpu = psutil.cpu_percent(interval=None)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(outputs[0]) - input_tokens
            total_tokens = len(outputs[0])
            
            # æ—¥æœ¬èªå“è³ªåˆ†æ
            japanese_analysis = self._analyze_japanese_quality(generated_only)
            
            # çµæœçµ±è¨ˆ
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            memory_usage = final_memory - initial_memory
            
            print(f"\nğŸ“Š æ—¥æœ¬èªç”Ÿæˆçµæœ:")
            print(f"  ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’")
            print(f"  å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {input_tokens}")
            print(f"  å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {output_tokens}")
            print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {tokens_per_second:.1f} tokens/sec")
            
            print(f"\nğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡:")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {memory_usage:.1f}GB")
            print(f"  ç·ãƒ¡ãƒ¢ãƒª: {final_memory:.1f}GB")
            print(f"  CPUä½¿ç”¨ç‡: {final_cpu:.1f}%")
            
            print(f"\nğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå“è³ª:")
            print(f"  å“è³ªãƒ¬ãƒ™ãƒ«: {japanese_analysis['quality_level']}")
            print(f"  æ—¥æœ¬èªæ¯”ç‡: {japanese_analysis['japanese_ratio']:.1f}%")
            print(f"  æ–‡å­—æ§‹æˆ: ã²ã‚‰ãŒãª{japanese_analysis['hiragana_count']}, ã‚«ã‚¿ã‚«ãƒŠ{japanese_analysis['katakana_count']}, æ¼¢å­—{japanese_analysis['kanji_count']}")
            
            print(f"\nğŸ”§ æœ€é©åŒ–çŠ¶æ…‹:")
            print(f"  æ—¥æœ¬èªæœ€é©åŒ–: âœ…")
            print(f"  4bité‡å­åŒ–: {'âœ…' if self.use_4bit else 'âŒ'}")
            print(f"  8bité‡å­åŒ–: {'âœ…' if self.use_8bit else 'âŒ'}")
            
            print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ:")
            print(f"  \"{generated_only}\"")
            
            return {
                "generated_text": generated_only,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "generation_time": generation_time,
                "tokens_per_second": tokens_per_second,
                "memory_usage": memory_usage,
                "cpu_usage": final_cpu,
                "japanese_analysis": japanese_analysis,
                "optimization_status": {
                    "japanese_optimized": True,
                    "quantization_4bit": self.use_4bit,
                    "quantization_8bit": self.use_8bit,
                    "advanced_quant": self.use_advanced_quant,
                    "aggressive_memory": self.use_aggressive_memory,
                    "npu_enabled": self.enable_npu
                }
            }
            
        except Exception as e:
            print(f"âŒ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {"error": str(e)}
    
    def _analyze_japanese_quality(self, text: str) -> Dict:
        """æ—¥æœ¬èªå“è³ªåˆ†æ"""
        if not text:
            return {
                "quality_level": "è¦æ”¹å–„",
                "japanese_ratio": 0.0,
                "hiragana_count": 0,
                "katakana_count": 0,
                "kanji_count": 0
            }
        
        hiragana_count = sum(1 for c in text if '\u3040' <= c <= '\u309F')
        katakana_count = sum(1 for c in text if '\u30A0' <= c <= '\u30FF')
        kanji_count = sum(1 for c in text if '\u4E00' <= c <= '\u9FAF')
        
        japanese_chars = hiragana_count + katakana_count + kanji_count
        total_chars = len(text)
        japanese_ratio = (japanese_chars / total_chars * 100) if total_chars > 0 else 0
        
        # å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if japanese_ratio >= 80:
            quality_level = "å„ªç§€"
        elif japanese_ratio >= 60:
            quality_level = "è‰¯å¥½"
        elif japanese_ratio >= 40:
            quality_level = "æ™®é€š"
        else:
            quality_level = "è¦æ”¹å–„"
        
        return {
            "quality_level": quality_level,
            "japanese_ratio": japanese_ratio,
            "hiragana_count": hiragana_count,
            "katakana_count": katakana_count,
            "kanji_count": kanji_count
        }
    
    def interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰"""
        print("ğŸ¯ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™")
        print("ğŸ’¡ 'exit'ã¾ãŸã¯'quit'ã§çµ‚äº†ã€'help'ã§ãƒ˜ãƒ«ãƒ—è¡¨ç¤º")
        print("=" * 60)
        
        while True:
            try:
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
                user_input = input("\nğŸ¤– ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
                
                # çµ‚äº†ã‚³ãƒãƒ³ãƒ‰
                if user_input.lower() in ['exit', 'quit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                # ãƒ˜ãƒ«ãƒ—ã‚³ãƒãƒ³ãƒ‰
                if user_input.lower() in ['help', 'ãƒ˜ãƒ«ãƒ—']:
                    self._show_interactive_help()
                    continue
                
                # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒãƒ³ãƒ‰
                if user_input.lower() in ['samples', 'ã‚µãƒ³ãƒ—ãƒ«']:
                    self._show_prompt_samples()
                    continue
                
                # ç©ºå…¥åŠ›ãƒã‚§ãƒƒã‚¯
                if not user_input:
                    print("âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
                print(f"\nğŸ”„ ç”Ÿæˆä¸­...")
                start_time = time.time()
                
                result = self.generate_japanese_text(
                    user_input, 
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True
                )
                
                generation_time = time.time() - start_time
                
                # çµæœè¡¨ç¤º
                print(f"\nâœ¨ ç”Ÿæˆçµæœ:")
                print(f"{'=' * 50}")
                print(result.get('generated_text', 'ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ'))
                print(f"{'=' * 50}")
                
                # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
                if 'output_tokens' in result:
                    tokens_per_sec = result['output_tokens'] / generation_time if generation_time > 0 else 0
                    print(f"ğŸ“Š çµ±è¨ˆ: {result['output_tokens']}ãƒˆãƒ¼ã‚¯ãƒ³, {generation_time:.1f}ç§’, {tokens_per_sec:.1f}ãƒˆãƒ¼ã‚¯ãƒ³/ç§’")
                
            except KeyboardInterrupt:
                print(f"\nâš ï¸ ä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚'exit'ã§çµ‚äº†ã—ã¦ãã ã•ã„ã€‚")
                continue
            except Exception as e:
                print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
    
    def _show_interactive_help(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã®ãƒ˜ãƒ«ãƒ—è¡¨ç¤º"""
        print(f"\nğŸ“– ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ãƒ˜ãƒ«ãƒ—:")
        print(f"  â€¢ ä»»æ„ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ")
        print(f"  â€¢ 'exit' ã¾ãŸã¯ 'quit': çµ‚äº†")
        print(f"  â€¢ 'help' ã¾ãŸã¯ 'ãƒ˜ãƒ«ãƒ—': ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º")
        print(f"  â€¢ 'samples' ã¾ãŸã¯ 'ã‚µãƒ³ãƒ—ãƒ«': ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º")
        print(f"  â€¢ Ctrl+C: ç”Ÿæˆä¸­æ–­ï¼ˆãƒ¢ãƒ¼ãƒ‰ç¶™ç¶šï¼‰")
        
        if self.use_aggressive_memory:
            print(f"  ğŸš€ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: æœ‰åŠ¹")
        if self.use_advanced_quant:
            print(f"  âš¡ é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–: æœ‰åŠ¹")
        if self.infer_os_enabled:
            print(f"  ğŸ”§ Infer-OSæœ€é©åŒ–: æœ‰åŠ¹")
    
    def _show_prompt_samples(self):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º"""
        print(f"\nğŸ’¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        
        for category, prompts in JAPANESE_PROMPT_SAMPLES.items():
            print(f"\nğŸ“‚ {category}:")
            for i, prompt in enumerate(prompts, 1):
                print(f"  {i}. {prompt}")
    
    def run_benchmark(self) -> Dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("\nğŸ“Š æ—¥æœ¬èªé‡é‡ç´šLLMãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        
        benchmark_prompts = [
            "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
            "æ—¥æœ¬ã®å››å­£ã®ç¾ã—ã•ã«ã¤ã„ã¦è©©ã‚’æ›¸ã„ã¦ãã ã•ã„",
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®ä»•çµ„ã¿ã‚’åˆ†ã‹ã‚Šã‚„ã™ãæ•™ãˆã¦",
            "ãŠã™ã™ã‚ã®æ—¥æœ¬æ–™ç†ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ãã ã•ã„",
            "æ©Ÿæ¢°å­¦ç¿’ã®åŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦èª¬æ˜ã—ã¦"
        ]
        
        results = []
        total_start = time.time()
        
        for i, prompt in enumerate(benchmark_prompts, 1):
            print(f"\nğŸ¯ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ {i}/{len(benchmark_prompts)}: {prompt[:30]}...")
            
            result = self.generate_japanese_text(
                prompt,
                max_new_tokens=150,
                temperature=0.7
            )
            
            if 'error' not in result:
                results.append(result)
                print(f"âœ… å®Œäº†: {result['tokens_per_second']:.1f} tokens/sec")
            else:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        
        total_time = time.time() - total_start
        
        if results:
            avg_tokens_per_sec = sum(r['tokens_per_second'] for r in results) / len(results)
            avg_generation_time = sum(r['generation_time'] for r in results) / len(results)
            total_tokens = sum(r['output_tokens'] for r in results)
            
            print(f"\nğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼:")
            print(f"  å®Ÿè¡Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(results)}")
            print(f"  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
            print(f"  å¹³å‡ç”Ÿæˆæ™‚é–“: {avg_generation_time:.1f}ç§’")
            print(f"  å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {avg_tokens_per_sec:.1f} tokens/sec")
            print(f"  ç·ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens}")
            
            return {
                "benchmark_results": results,
                "summary": {
                    "total_prompts": len(results),
                    "total_time": total_time,
                    "avg_generation_time": avg_generation_time,
                    "avg_tokens_per_sec": avg_tokens_per_sec,
                    "total_tokens": total_tokens
                }
            }
        else:
            print("âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
            return {"error": "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå¤±æ•—"}
    
    def display_infer_os_integration_summary(self):
        """Infer-OSçµ±åˆåŠ¹æœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print(f"\nğŸ¯ **Infer-OSçµ±åˆåŠ¹æœã‚µãƒãƒªãƒ¼**")
        print(f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        print(f"é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {self.quantization_profile}")
        print(f"Infer-OSæ©Ÿèƒ½: {'æœ‰åŠ¹' if self.infer_os_enabled else 'ç„¡åŠ¹'}")
        
        print(f"\nâš¡ **Infer-OSçµ±åˆã«ã‚ˆã‚‹æœŸå¾…åŠ¹æœ**:")
        if self.infer_os_enabled:
            print(f"  æ¨è«–é€Ÿåº¦å‘ä¸Š: 2.0-3.0å€")
            print(f"  ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 65-75%")
            print(f"  å¿œç­”æ™‚é–“çŸ­ç¸®: 50-65%")
            print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: 2.5-4å€")
        else:
            print(f"  Infer-OSæ©Ÿèƒ½ãŒç„¡åŠ¹ã®ãŸã‚åŠ¹æœãªã—")
        
        print(f"\nğŸ”§ **çµ±åˆæŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯**:")
        print(f"  {'âœ…' if self.use_advanced_quant else 'âŒ'} é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ– (W4/W8 + KVé‡å­åŒ–)")
        print(f"  {'âœ…' if self.use_onnx else 'âŒ'} ONNX Runtimeæœ€é©åŒ– (3ãƒ¬ãƒ™ãƒ«æœ€é©åŒ–)")
        print(f"  {'âœ…' if self.enable_npu else 'âŒ'} NPUæœ€é©åŒ– (DirectMLçµ±åˆ)")
        print(f"  {'âœ…' if self.use_aggressive_memory else 'âŒ'} ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ– (27.8GBå¯¾å¿œ)")
        print(f"  âœ… æ®µéšçš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ã‚¨ãƒ©ãƒ¼å›å¾©)")
        print(f"  âœ… è‡ªå‹•ãƒ¡ãƒ¢ãƒªç®¡ç† (å‹•çš„æœ€é©åŒ–)")
        
        print(f"\nğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**:")
        if self.infer_os_enabled:
            print(f"  ğŸš€ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŠ¹æœã‚’å®šé‡æ¸¬å®š")
            print(f"  ğŸ“Š --benchmark ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ€§èƒ½ç¢ºèª")
            print(f"  ğŸ¯ æœ¬æ ¼é‹ç”¨ã§ã®åŠ¹æœä½“é¨“")
        else:
            print(f"  ğŸ”§ Infer-OSæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–ã—ã¦åŠ¹æœã‚’ä½“é¨“")
            print(f"  ğŸ“ˆ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å·®ç•°ã‚’ç¢ºèª")
            print(f"  âš¡ çµ±åˆåŠ¹æœã®å®šé‡çš„æ¸¬å®šã‚’å®Ÿæ–½")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="Infer-OS æ—¥æœ¬èªé‡é‡ç´šLLMçµ±åˆãƒ‡ãƒ¢")
    
    # åŸºæœ¬è¨­å®š
    parser.add_argument("--model", type=str, default="rinna/youri-7b-chat",
                        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--use-4bit", action="store_true",
                        help="4bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-8bit", action="store_true", 
                        help="8bité‡å­åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-advanced-quant", action="store_true",
                        help="é«˜åº¦ãªé‡å­åŒ–æœ€é©åŒ–ã‚’ä½¿ç”¨")
    parser.add_argument("--use-aggressive-memory", action="store_true",
                        help="ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’ä½¿ç”¨ï¼ˆ27.8GBç’°å¢ƒå¯¾å¿œï¼‰")
    parser.add_argument("--enable-npu", action="store_true", default=True,
                        help="Windows NPUæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰")
    parser.add_argument("--disable-npu", action="store_true",
                        help="Windows NPUæœ€é©åŒ–ã‚’ç„¡åŠ¹åŒ–")
    parser.add_argument("--quantization-profile", type=str, default="balanced",
                        choices=["safe", "balanced", "aggressive"],
                        help="é‡å­åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # ONNXè¨­å®š
    parser.add_argument("--convert-to-onnx", action="store_true",
                        help="ONNXã«å¤‰æ›")
    parser.add_argument("--use-onnx-runtime", action="store_true",
                        help="ONNX Runtimeã‚’ä½¿ç”¨")
    parser.add_argument("--onnx-optimization-level", type=int, default=2,
                        choices=[0, 1, 2],
                        help="ONNXæœ€é©åŒ–ãƒ¬ãƒ™ãƒ«")
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    parser.add_argument("--interactive", action="store_true",
                        help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--benchmark", action="store_true",
                        help="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
    parser.add_argument("--compare-infer-os", action="store_true",
                        help="Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    parser.add_argument("--infer-os-only", action="store_true",
                        help="Infer-OSæœ‰åŠ¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œï¼ˆæ¯”è¼ƒãªã—ï¼‰")
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    parser.add_argument("--prompt", type=str,
                        help="å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
    parser.add_argument("--max-length", type=int, default=200,
                        help="æœ€å¤§ç”Ÿæˆé•·")
    
    # ãã®ä»–
    parser.add_argument("--list-models", action="store_true",
                        help="åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º")
    parser.add_argument("--samples", action="store_true",
                        help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º")
    
    args = parser.parse_args()
    
    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    if args.list_models:
        print("ğŸ¤– åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªé‡é‡ç´šãƒ¢ãƒ‡ãƒ«:")
        models = [
            "matsuo-lab/weblab-10b",
            "rinna/youri-7b-chat", 
            "cyberagent/open-calm-7b",
            "stabilityai/japanese-stablelm-instruct-alpha-7b"
        ]
        for i, model in enumerate(models, 1):
            print(f"  {i}. {model}")
        return
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    if args.samples:
        print("ğŸ’¡ æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«:")
        for category, prompts in JAPANESE_PROMPT_SAMPLES.items():
            print(f"\nğŸ“‚ {category}:")
            for i, prompt in enumerate(prompts, 1):
                print(f"  {i}. {prompt}")
        return
    
    try:
        # Infer-OSæœ‰åŠ¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿å®Ÿè¡Œ
        if args.infer_os_only:
            print("ğŸš€ Infer-OSæœ‰åŠ¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿ã§å®Ÿè¡Œã—ã¾ã™")
            infer_os_enabled = True
        else:
            infer_os_enabled = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹
        
        # ãƒ‡ãƒ¢ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        demo = InferOSJapaneseLLMDemo(
            model_name=args.model,
            use_4bit=args.use_4bit,
            use_8bit=args.use_8bit,
            use_onnx=args.use_onnx_runtime,
            onnx_optimization_level=args.onnx_optimization_level,
            quantization_profile=args.quantization_profile,
            use_advanced_quant=args.use_advanced_quant,
            use_aggressive_memory=args.use_aggressive_memory,
            enable_npu=args.enable_npu and not args.disable_npu,
            infer_os_enabled=infer_os_enabled
        )
        
        # Infer-OSçµ±åˆåŠ¹æœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        demo.display_infer_os_integration_summary()
        
        # Infer-OSæœ‰åŠ¹ãƒ¢ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆ
        if args.infer_os_only:
            print("âš¡ Infer-OSæœ‰åŠ¹ãƒ¢ãƒ¼ãƒ‰ã§æœ€é©åŒ–å®Ÿè¡Œä¸­...")
            print("ğŸ’¡ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç›´æ¥å®Ÿè¡Œã—ã¾ã™")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print("\nğŸ“¥ Infer-OSæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        if not demo.load_model_with_optimization():
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ†å²
        if args.benchmark:
            print("\nğŸ“Š Infer-OSæœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
            results = demo.run_benchmark()
            print("âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
            
        elif args.prompt:
            print("\nğŸ¯ Infer-OSæœ€é©åŒ–å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œä¸­...")
            result = demo.generate_japanese_text(args.prompt, max_new_tokens=args.max_length)
            print("\nç”Ÿæˆçµæœ:")
            print(result.get('generated_text', ''))
            
        elif args.interactive:
            print("\nğŸ‡¯ğŸ‡µ Infer-OSæœ€é©åŒ–ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
            demo.interactive_mode()
            
        else:
            print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print("  --interactive: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
            print("  --benchmark: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
            print("  --prompt 'ãƒ†ã‚­ã‚¹ãƒˆ': å˜ç™ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®Ÿè¡Œ")
            print("  --list-models: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º")
            print("  --samples: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()

