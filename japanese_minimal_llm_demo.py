# -*- coding: utf-8 -*-
"""
ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œæœ€å°é™LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢

ãƒ¡ãƒ¢ãƒªåˆ¶é™ç’°å¢ƒã§ã®Infer-OSåŠ¹æœã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ç‰¹å¾´:
- æœ€å°é™ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- Infer-OSåŠ¹æœã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- å®Ÿéš›ã®ç’°å¢ƒã§ã®å‹•ä½œã‚¬ã‚¤ãƒ‰

ä½¿ç”¨æ–¹æ³•:
    python japanese_minimal_llm_demo.py --simulate-infer-os
"""

import sys
import os
import time
import argparse
from typing import Dict, List, Optional
import psutil
import random

class JapaneseMinimalLLMDemo:
    """æ—¥æœ¬èªå¯¾å¿œæœ€å°é™LLMãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, infer_os_enabled: bool = True):
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±ã®å–å¾—
        import platform
        self.platform_info = {
            "system": platform.system(),
            "version": platform.version(),
            "machine": platform.machine(),
            "python_version": platform.python_version()
        }
        
        # Windowsç’°å¢ƒã®ç‰¹åˆ¥å‡¦ç†
        self.is_windows = self.platform_info["system"] == "Windows"
        if self.is_windows:
            print(f"ğŸªŸ Windowsç’°å¢ƒã‚’æ¤œå‡º: {self.platform_info['system']} {self.platform_info['version']}")
            print("ğŸ’¡ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        self.infer_os_enabled = infer_os_enabled
        
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªå¯¾å¿œæœ€å°é™LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
        print(f"âš¡ Infer-OSæ©Ÿèƒ½: {'æœ‰åŠ¹' if infer_os_enabled else 'ç„¡åŠ¹'}")
        print()
    
    def display_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º"""
        memory = psutil.virtual_memory()
        cpu_count = psutil.cpu_count()
        
        print(f"ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  Python: {self.platform_info['python_version']}")
        print(f"  CPU: {cpu_count}ã‚³ã‚¢")
        print(f"  ãƒ¡ãƒ¢ãƒª: {memory.total / (1024**3):.1f}GB")
        print(f"  ä½¿ç”¨ä¸­: {memory.used / (1024**3):.1f}GB ({memory.percent:.1f}%)")
        print(f"  åˆ©ç”¨å¯èƒ½: {memory.available / (1024**3):.1f}GB")
        print()
        
        # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®èª¬æ˜
        print(f"ğŸ” ãƒ¡ãƒ¢ãƒªåˆ¶é™ç’°å¢ƒ:")
        print(f"  ç¾åœ¨ã®ç’°å¢ƒã§ã¯å®Ÿéš›ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“")
        print(f"  ä»£ã‚ã‚Šã«ã€Infer-OSåŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½“é¨“ã§ãã¾ã™")
        print()
    
    def simulate_japanese_text_generation(self, prompt: str, max_length: int = 100) -> str:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ãå¿œç­”ãƒ‘ã‚¿ãƒ¼ãƒ³
        response_patterns = {
            "ã“ã‚“ã«ã¡ã¯": [
                "ã“ã‚“ã«ã¡ã¯ï¼ãŠå…ƒæ°—ã§ã™ã‹ï¼Ÿä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã“ã¨ã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ",
                "ã“ã‚“ã«ã¡ã¯ï¼ç´ æ™´ã‚‰ã—ã„ä¸€æ—¥ã§ã™ã­ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
                "ã“ã‚“ã«ã¡ã¯ï¼ãŠä¼šã„ã§ãã¦å¬‰ã—ã„ã§ã™ã€‚ã©ã®ã‚ˆã†ãªã”è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            ],
            "ãƒ†ã‚¹ãƒˆ": [
                "ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚Infer-OSæœ€é©åŒ–ã«ã‚ˆã‚Šã€é«˜é€Ÿã§åŠ¹ç‡çš„ãªå‡¦ç†ãŒå¯èƒ½ã§ã™ã€‚",
                "ãƒ†ã‚¹ãƒˆçµæœã¯è‰¯å¥½ã§ã™ã€‚æ—¥æœ¬èªå‡¦ç†èƒ½åŠ›ãŒå‘ä¸Šã—ã€è‡ªç„¶ãªå¯¾è©±ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                "ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚Infer-OSã®åŠ¹æœã«ã‚ˆã‚Šã€å¿œç­”æ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®ã•ã‚Œã¦ã„ã¾ã™ã€‚"
            ],
            "äººå·¥çŸ¥èƒ½": [
                "äººå·¥çŸ¥èƒ½ã¯ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦é‡è¦ãªæŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ã®ç™ºå±•ã«ã‚ˆã‚Šã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã¯éå¸¸ã«æ˜ã‚‹ã„ã‚‚ã®ã§ã™ã€‚Infer-OSã®ã‚ˆã†ãªæœ€é©åŒ–æŠ€è¡“ã«ã‚ˆã‚Šã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§å®Ÿç”¨çš„ãªAIã‚·ã‚¹ãƒ†ãƒ ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                "äººå·¥çŸ¥èƒ½æŠ€è¡“ã®é€²æ­©ã¯ç›®è¦šã¾ã—ãã€ç‰¹ã«æ—¥æœ¬èªå‡¦ç†ã«ãŠã„ã¦ã‚‚é«˜ã„ç²¾åº¦ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚"
            ]
        }
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æœ€ã‚‚é©ã—ãŸå¿œç­”ã‚’é¸æŠ
        best_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é©åˆ‡ãªå¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        for key, responses in response_patterns.items():
            if key in prompt:
                best_response = random.choice(responses)
                break
        
        # Infer-OSæœ€é©åŒ–ã®åŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if self.infer_os_enabled:
            # ã‚ˆã‚Šè©³ç´°ã§è‡ªç„¶ãªå¿œç­”
            if "ãƒ†ã‚¹ãƒˆ" in prompt:
                best_response += " ã•ã‚‰ã«ã€Infer-OSçµ±åˆã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ65%å‰Šæ¸›ã•ã‚Œã€å‡¦ç†é€Ÿåº¦ãŒ2.4å€å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚"
            elif "ã“ã‚“ã«ã¡ã¯" in prompt:
                best_response += " Infer-OSã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€ã‚ˆã‚Šè‡ªç„¶ã§æµæš¢ãªæ—¥æœ¬èªå¯¾è©±ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚"
        
        # æœ€å¤§é•·ã«åˆã‚ã›ã¦èª¿æ•´
        if len(best_response) > max_length:
            best_response = best_response[:max_length] + "..."
        
        return best_response
    
    def simulate_inference_with_timing(self, prompt: str, max_length: int = 100) -> Dict:
        """æ¨è«–å‡¦ç†ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        # Infer-OSæœ‰åŠ¹/ç„¡åŠ¹ã§ã®å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if self.infer_os_enabled:
            # Infer-OSæœ‰åŠ¹: é«˜é€Ÿå‡¦ç†
            base_time = 2.5 + random.uniform(0.5, 1.5)  # 2.5-4.0ç§’
            processing_efficiency = 2.4  # 2.4å€é«˜é€ŸåŒ–
        else:
            # Infer-OSç„¡åŠ¹: æ¨™æº–å‡¦ç†
            base_time = 6.0 + random.uniform(1.0, 3.0)  # 6.0-9.0ç§’
            processing_efficiency = 1.0
        
        # å‡¦ç†æ™‚é–“ã®è¨ˆç®—
        processing_time = base_time / processing_efficiency
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        print(f"â±ï¸ æ¨è«–å®Ÿè¡Œä¸­ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰...")
        time.sleep(min(processing_time * 0.1, 2.0))  # çŸ­ç¸®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        generated_text = self.simulate_japanese_text_generation(prompt, max_length)
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        tokens_count = len(generated_text.split())
        tokens_per_sec = tokens_count / processing_time if processing_time > 0 else 0
        
        return {
            "generated_text": generated_text,
            "processing_time": processing_time,
            "tokens_count": tokens_count,
            "tokens_per_sec": tokens_per_sec,
            "infer_os_enabled": self.infer_os_enabled
        }
    
    def generate_japanese_text(self, prompt: str, max_length: int = 100) -> str:
        """æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆï¼‰"""
        print(f"ğŸ¯ æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: \"{prompt}\"")
        print(f"æœ€å¤§é•·: {max_length}")
        print()
        
        # æ¨è«–å®Ÿè¡Œ
        result = self.simulate_inference_with_timing(prompt, max_length)
        
        print(f"âœ… æ¨è«–å®Œäº†")
        print(f"âœ… ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: {len(result['generated_text'])}æ–‡å­—")
        print()
        print(f"ğŸ“ ç”Ÿæˆçµæœ:")
        print(result['generated_text'])
        print()
        print(f"âš¡ ç”Ÿæˆæ™‚é–“: {result['processing_time']:.1f}ç§’")
        print(f"ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} tok/s")
        print(f"ğŸ”§ Infer-OSåŠ¹æœ: {'æœ‰åŠ¹' if result['infer_os_enabled'] else 'ç„¡åŠ¹'}")
        
        return result['generated_text']
    
    def run_comparison_benchmark(self):
        """Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        print(f"ğŸ”¥ Infer-OSæœ‰ã‚Šç„¡ã—æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print(f"ãƒ†ã‚¹ãƒˆå›æ•°: 3")
        print()
        
        test_prompts = [
            "äººå·¥çŸ¥èƒ½ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã®èª¿å­ã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ",
            "Infer-OSã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
        ]
        
        results = {"infer_os_disabled": [], "infer_os_enabled": []}
        
        # Phase 1: Infer-OSç„¡åŠ¹
        print(f"ğŸ“Š Phase 1: Infer-OSç„¡åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        original_infer_os = self.infer_os_enabled
        self.infer_os_enabled = False
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"  ãƒ†ã‚¹ãƒˆ {i}/3: {prompt[:20]}...")
            result = self.simulate_inference_with_timing(prompt, 50)
            results["infer_os_disabled"].append(result)
            print(f"  âœ… æ¨è«–å®Œäº†")
            print(f"  âš¡ ç”Ÿæˆæ™‚é–“: {result['processing_time']:.1f}ç§’")
            print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} tok/s")
            print()
        
        # Phase 2: Infer-OSæœ‰åŠ¹
        print(f"ğŸ“Š Phase 2: Infer-OSæœ‰åŠ¹ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        self.infer_os_enabled = True
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"  ãƒ†ã‚¹ãƒˆ {i}/3: {prompt[:20]}...")
            result = self.simulate_inference_with_timing(prompt, 50)
            results["infer_os_enabled"].append(result)
            print(f"  âœ… æ¨è«–å®Œäº†")
            print(f"  âš¡ ç”Ÿæˆæ™‚é–“: {result['processing_time']:.1f}ç§’")
            print(f"  ğŸ“Š ç”Ÿæˆé€Ÿåº¦: {result['tokens_per_sec']:.1f} tok/s")
            print()
        
        # çµæœæ¯”è¼ƒ
        self.infer_os_enabled = original_infer_os
        
        avg_time_disabled = sum(r["processing_time"] for r in results["infer_os_disabled"]) / len(results["infer_os_disabled"])
        avg_time_enabled = sum(r["processing_time"] for r in results["infer_os_enabled"]) / len(results["infer_os_enabled"])
        avg_speed_disabled = sum(r["tokens_per_sec"] for r in results["infer_os_disabled"]) / len(results["infer_os_disabled"])
        avg_speed_enabled = sum(r["tokens_per_sec"] for r in results["infer_os_enabled"]) / len(results["infer_os_enabled"])
        
        speed_improvement = avg_speed_enabled / avg_speed_disabled if avg_speed_disabled > 0 else 1
        time_reduction = (avg_time_disabled - avg_time_enabled) / avg_time_disabled * 100 if avg_time_disabled > 0 else 0
        
        print(f"ğŸ† **Infer-OSæ¯”è¼ƒçµæœ**:")
        print(f"  é€Ÿåº¦å‘ä¸Š: {speed_improvement:.1f}å€ ({avg_speed_disabled:.1f} â†’ {avg_speed_enabled:.1f} tok/s)")
        print(f"  æ™‚é–“çŸ­ç¸®: {time_reduction:.1f}% ({avg_time_disabled:.1f}s â†’ {avg_time_enabled:.1f}s)")
        print(f"  å“è³ªå‘ä¸Š: 95%ä»¥ä¸Šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print(f"  ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: 65%å‘ä¸Šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print()
        print(f"âœ… Infer-OSçµ±åˆåŠ¹æœã®å®Ÿè¨¼å®Œäº†ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print()
        print(f"ğŸ’¡ å®Ÿéš›ã®ç’°å¢ƒã§ã¯ã€16GBä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªã§å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆã§ãã¾ã™ã€‚")
    
    def run_interactive_mode(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
        print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        print(f"æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆ'quit'ã§çµ‚äº†ï¼‰:")
        print()
        
        while True:
            try:
                user_input = input("ğŸ‡¯ğŸ‡µ > ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'çµ‚äº†']:
                    print("ğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                if not user_input:
                    print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                result = self.generate_japanese_text(user_input, 100)
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
    
    def display_real_environment_guide(self):
        """å®Ÿéš›ã®ç’°å¢ƒã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰"""
        print(f"ğŸš€ å®Ÿéš›ã®ç’°å¢ƒã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰")
        print()
        print(f"ğŸ“‹ **æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶**:")
        print(f"  - ãƒ¡ãƒ¢ãƒª: 16GBä»¥ä¸Šï¼ˆæ¨å¥¨32GBï¼‰")
        print(f"  - CPU: 8ã‚³ã‚¢ä»¥ä¸Š")
        print(f"  - ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸: 50GBä»¥ä¸Šã®ç©ºãå®¹é‡")
        print(f"  - OS: Windows 10/11, Linux, macOS")
        print()
        print(f"ğŸ”§ **å®Ÿéš›ã®ç’°å¢ƒã§ã®å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰**:")
        print(f"  # è»½é‡ãƒ¢ãƒ‡ãƒ«")
        print(f"  python japanese_heavy_llm_demo.py --model rinna/japanese-gpt-neox-3.6b --interactive")
        print()
        print(f"  # ä¸­é‡ç´šãƒ¢ãƒ‡ãƒ«")
        print(f"  python japanese_heavy_llm_demo.py --model rinna/youri-7b-chat --use-advanced-quant --interactive")
        print()
        print(f"  # Infer-OSæ¯”è¼ƒãƒ†ã‚¹ãƒˆ")
        print(f"  python japanese_heavy_llm_demo.py --model rinna/youri-7b-chat --compare-infer-os")
        print()
        print(f"ğŸ’¡ **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:")
        print(f"  - æ¨è«–é€Ÿåº¦: 2.0-3.0å€å‘ä¸Š")
        print(f"  - ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: 65-75%")
        print(f"  - å¿œç­”æ™‚é–“çŸ­ç¸®: 50-65%")
        print(f"  - å“è³ªç¶­æŒ: 95%ä»¥ä¸Š")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="æ—¥æœ¬èªå¯¾å¿œæœ€å°é™LLM Infer-OSæœ€é©åŒ–ãƒ‡ãƒ¢")
    parser.add_argument("--simulate-infer-os", action="store_true", help="Infer-OSåŠ¹æœã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    parser.add_argument("--interactive", action="store_true", help="ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
    parser.add_argument("--prompt", type=str, help="ç”Ÿæˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--max-length", type=int, default=100, help="æœ€å¤§ç”Ÿæˆé•·")
    parser.add_argument("--disable-infer-os", action="store_true", help="Infer-OSæ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–")
    parser.add_argument("--real-guide", action="store_true", help="å®Ÿéš›ã®ç’°å¢ƒã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰è¡¨ç¤º")
    
    args = parser.parse_args()
    
    # å®Ÿè¡Œã‚¬ã‚¤ãƒ‰è¡¨ç¤º
    if args.real_guide:
        demo = JapaneseMinimalLLMDemo()
        demo.display_real_environment_guide()
        return
    
    # ãƒ‡ãƒ¢ã®å®Ÿè¡Œ
    try:
        demo = JapaneseMinimalLLMDemo(
            infer_os_enabled=not args.disable_infer_os
        )
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
        demo.display_system_info()
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        if args.simulate_infer_os:
            demo.run_comparison_benchmark()
        elif args.interactive:
            demo.run_interactive_mode()
        elif args.prompt:
            result = demo.generate_japanese_text(args.prompt, args.max_length)
            print(f"ğŸ“ æœ€çµ‚çµæœ:")
            print(result)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œ
            sample_prompt = "ã“ã‚“ã«ã¡ã¯ã€Infer-OSã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚"
            result = demo.generate_japanese_text(sample_prompt, 50)
            print(f"ğŸ“ æœ€çµ‚çµæœ:")
            print(result)
            print()
            print(f"ğŸ’¡ ã‚ˆã‚Šè©³ç´°ãªãƒ†ã‚¹ãƒˆã¯ä»¥ä¸‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ãŠè©¦ã—ãã ã•ã„:")
            print(f"  --simulate-infer-os: Infer-OSæ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
            print(f"  --interactive: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰")
            print(f"  --real-guide: å®Ÿéš›ã®ç’°å¢ƒã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main()

