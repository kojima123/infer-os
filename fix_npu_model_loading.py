#!/usr/bin/env python3
"""
NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å•é¡Œä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
PyTorch 2.6ã®weights_only=Trueå•é¡Œã‚’è§£æ±º
"""

import os
import sys
import torch
import traceback
from pathlib import Path
from typing import Optional, Dict, Any


class NPUModelLoadingFixer:
    """NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¿®æ­£ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.safe_globals = [
            'modeling_llama_amd.LlamaForCausalLM',
            'modeling_llama_amd.LlamaModel',
            'modeling_llama_amd.LlamaConfig',
            'torch.nn.Linear',
            'torch.nn.Embedding',
            'torch.nn.LayerNorm',
            'transformers.models.llama.modeling_llama.LlamaForCausalLM',
            'transformers.models.llama.modeling_llama.LlamaModel'
        ]
    
    def fix_model_loading(self, model_path: str) -> bool:
        """NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¿®æ­£"""
        print("ğŸ”§ NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¿®æ­£é–‹å§‹")
        print("=" * 60)
        
        try:
            # 1. ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ç¢ºèª
            model_file = os.path.join(model_path, "pytorch_llama3_8b_w_bit_4_awq_amd.pt")
            if not os.path.exists(model_file):
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")
                return False
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {model_file}")
            
            # 2. å®‰å…¨ãªã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
            print("ğŸ”§ å®‰å…¨ãªã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šä¸­...")
            
            # æ–¹æ³•1: add_safe_globalsä½¿ç”¨
            try:
                for global_name in self.safe_globals:
                    torch.serialization.add_safe_globals([global_name])
                print("âœ… add_safe_globalsè¨­å®šå®Œäº†")
            except Exception as e:
                print(f"âš ï¸ add_safe_globalsè¨­å®šå¤±æ•—: {e}")
            
            # 3. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆweights_only=Falseï¼‰
            print("ğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆweights_only=Falseï¼‰...")
            try:
                model_data = torch.load(model_file, weights_only=False, map_location='cpu')
                print("âœ… weights_only=False ã§ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                print(f"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼: {list(model_data.keys()) if isinstance(model_data, dict) else type(model_data)}")
                return True
                
            except Exception as e:
                print(f"âŒ weights_only=False ã§ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            
            # 4. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆsafe_globalsä½¿ç”¨ï¼‰
            print("ğŸ§ª ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆsafe_globalsä½¿ç”¨ï¼‰...")
            try:
                with torch.serialization.safe_globals(self.safe_globals):
                    model_data = torch.load(model_file, weights_only=True, map_location='cpu')
                print("âœ… safe_globals ã§ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                return True
                
            except Exception as e:
                print(f"âŒ safe_globals ã§ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            
            # 5. ä»£æ›¿ãƒ­ãƒ¼ãƒ‰æ–¹æ³•
            print("ğŸ”„ ä»£æ›¿ãƒ­ãƒ¼ãƒ‰æ–¹æ³•è©¦è¡Œ...")
            try:
                # pickle_moduleæŒ‡å®š
                import pickle
                model_data = torch.load(model_file, pickle_module=pickle, map_location='cpu')
                print("âœ… pickle_moduleæŒ‡å®šã§ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                return True
                
            except Exception as e:
                print(f"âŒ ä»£æ›¿ãƒ­ãƒ¼ãƒ‰æ–¹æ³•å¤±æ•—: {e}")
            
            return False
            
        except Exception as e:
            print(f"âŒ ä¿®æ­£å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return False
    
    def create_fixed_loader(self, output_path: str = "fixed_npu_model_loader.py"):
        """ä¿®æ­£ç‰ˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ"""
        print(f"ğŸ“ ä¿®æ­£ç‰ˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ: {output_path}")
        
        loader_code = '''#!/usr/bin/env python3
"""
ä¿®æ­£ç‰ˆNPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼
PyTorch 2.6 weights_onlyå•é¡Œå¯¾å¿œç‰ˆ
"""

import torch
import os
from typing import Optional, Any


class FixedNPUModelLoader:
    """ä¿®æ­£ç‰ˆNPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self):
        # å®‰å…¨ãªã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¹å®šç¾©
        self.safe_globals = [
            'modeling_llama_amd.LlamaForCausalLM',
            'modeling_llama_amd.LlamaModel', 
            'modeling_llama_amd.LlamaConfig',
            'torch.nn.Linear',
            'torch.nn.Embedding',
            'torch.nn.LayerNorm',
            'transformers.models.llama.modeling_llama.LlamaForCausalLM'
        ]
        
        # å®‰å…¨ãªã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
        self._setup_safe_globals()
    
    def _setup_safe_globals(self):
        """å®‰å…¨ãªã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š"""
        try:
            for global_name in self.safe_globals:
                torch.serialization.add_safe_globals([global_name])
        except Exception:
            pass  # æ—¢ã«è¨­å®šæ¸ˆã¿ã®å ´åˆã¯ç„¡è¦–
    
    def load_npu_model(self, model_path: str, weights_only: bool = False) -> Optional[Any]:
        """NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«å®‰å…¨ãƒ­ãƒ¼ãƒ‰"""
        model_file = os.path.join(model_path, "pytorch_llama3_8b_w_bit_4_awq_amd.pt")
        
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")
        
        # æ–¹æ³•1: weights_only=Falseï¼ˆæ¨å¥¨ï¼‰
        if not weights_only:
            try:
                return torch.load(model_file, weights_only=False, map_location='cpu')
            except Exception as e:
                print(f"weights_only=Falseå¤±æ•—: {e}")
        
        # æ–¹æ³•2: safe_globalsä½¿ç”¨
        try:
            with torch.serialization.safe_globals(self.safe_globals):
                return torch.load(model_file, weights_only=True, map_location='cpu')
        except Exception as e:
            print(f"safe_globalså¤±æ•—: {e}")
        
        # æ–¹æ³•3: å¼·åˆ¶çš„ã«weights_only=False
        try:
            return torch.load(model_file, weights_only=False, map_location='cpu')
        except Exception as e:
            raise RuntimeError(f"å…¨ã¦ã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ãŒå¤±æ•—: {e}")


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    loader = FixedNPUModelLoader()
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
    model_path = "llama3-8b-amd-npu"
    try:
        model_data = loader.load_npu_model(model_path)
        print("âœ… NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‹: {type(model_data)}")
        if isinstance(model_data, dict):
            print(f"ğŸ“Š ã‚­ãƒ¼: {list(model_data.keys())}")
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
'''
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(loader_code)
            print(f"âœ… ä¿®æ­£ç‰ˆãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†: {output_path}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå¤±æ•—: {e}")
            return False
    
    def update_existing_scripts(self):
        """æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ›´æ–°"""
        print("ğŸ”„ æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°ä¸­...")
        
        scripts_to_update = [
            "npu_optimized_japanese_models.py",
            "integrated_npu_infer_os.py",
            "vitisai_npu_engine.py"
        ]
        
        for script in scripts_to_update:
            if os.path.exists(script):
                print(f"ğŸ”§ {script} æ›´æ–°ä¸­...")
                self._update_script_torch_load(script)
            else:
                print(f"âš ï¸ {script} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    def _update_script_torch_load(self, script_path: str):
        """ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã®torch.loadå‘¼ã³å‡ºã—ã‚’æ›´æ–°"""
        try:
            with open(script_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # torch.loadå‘¼ã³å‡ºã—ã‚’ä¿®æ­£
            updated_content = content.replace(
                'torch.load(',
                'torch.load('
            )
            
            # weights_only=Trueã‚’æ˜ç¤ºçš„ã«Falseã«å¤‰æ›´
            updated_content = updated_content.replace(
                'weights_only=True',
                'weights_only=False'
            )
            
            # å®‰å…¨ãªãƒ­ãƒ¼ãƒ‰é–¢æ•°ã®è¿½åŠ 
            if 'def safe_torch_load(' not in updated_content:
                safe_load_function = '''
def safe_torch_load(file_path, map_location='cpu'):
    """å®‰å…¨ãªtorch.loadãƒ©ãƒƒãƒ‘ãƒ¼"""
    try:
        # æ–¹æ³•1: weights_only=False
        return torch.load(file_path, weights_only=False, map_location=map_location)
    except Exception:
        # æ–¹æ³•2: å®‰å…¨ãªã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
        safe_globals = [
            'modeling_llama_amd.LlamaForCausalLM',
            'modeling_llama_amd.LlamaModel',
            'torch.nn.Linear'
        ]
        try:
            with torch.serialization.safe_globals(safe_globals):
                return torch.load(file_path, weights_only=True, map_location=map_location)
        except Exception:
            # æ–¹æ³•3: å¼·åˆ¶çš„ã«weights_only=False
            return torch.load(file_path, weights_only=False, map_location=map_location)

'''
                updated_content = safe_load_function + updated_content
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(updated_content)
            
            print(f"âœ… {script_path} æ›´æ–°å®Œäº†")
            
        except Exception as e:
            print(f"âŒ {script_path} æ›´æ–°å¤±æ•—: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å•é¡Œä¿®æ­£")
    parser.add_argument("--model-path", default="llama3-8b-amd-npu", help="ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹")
    parser.add_argument("--create-loader", action="store_true", help="ä¿®æ­£ç‰ˆãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ")
    parser.add_argument("--update-scripts", action="store_true", help="æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°")
    parser.add_argument("--test-loading", action="store_true", help="ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    
    args = parser.parse_args()
    
    fixer = NPUModelLoadingFixer()
    
    try:
        if args.test_loading:
            print("ğŸ§ª NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ")
            success = fixer.fix_model_loading(args.model_path)
            if success:
                print("ğŸ‰ ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆæˆåŠŸ")
            else:
                print("âŒ ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆå¤±æ•—")
        
        if args.create_loader:
            print("ğŸ“ ä¿®æ­£ç‰ˆãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ")
            fixer.create_fixed_loader()
        
        if args.update_scripts:
            print("ğŸ”„ æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°")
            fixer.update_existing_scripts()
        
        if not any([args.test_loading, args.create_loader, args.update_scripts]):
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨ã¦å®Ÿè¡Œ
            print("ğŸš€ NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å•é¡Œä¿®æ­£é–‹å§‹")
            print("=" * 60)
            
            # 1. ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
            print("\\n1. ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
            fixer.fix_model_loading(args.model_path)
            
            # 2. ä¿®æ­£ç‰ˆãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
            print("\\n2. ä¿®æ­£ç‰ˆãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ")
            fixer.create_fixed_loader()
            
            # 3. æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°
            print("\\n3. æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°")
            fixer.update_existing_scripts()
            
            print("\\nğŸ‰ NPUæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å•é¡Œä¿®æ­£å®Œäº†")
            print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ä¿®æ­£ç‰ˆã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:")
            print("   python fixed_npu_model_loader.py")
            print("   python integrated_npu_infer_os.py --model llama3-8b-amd-npu")
        
    except KeyboardInterrupt:
        print("\\nğŸ‘‹ ä¿®æ­£å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\\nâŒ ä¿®æ­£å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()

