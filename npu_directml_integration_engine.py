#!/usr/bin/env python3
"""
NPU/DirectMLçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³
AMD Ryzen AI NPU + Radeon iGPUç’°å¢ƒã§ã®çœŸã®NPUæ¨è«–å®Ÿè£…

ã“ã®ã‚¨ãƒ³ã‚¸ãƒ³ã¯å®Ÿéš›ã®NPU/DirectMLã‚’æ´»ç”¨ã—ãŸæ¨è«–æœ€é©åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
"""

import asyncio
import json
import logging
import os
import sys
import time
import subprocess
import platform
import psutil
import tempfile
import shutil
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import threading
import queue
import gc

# NPU/DirectMLçµ±åˆã®ãŸã‚ã®è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import torch
    import torch.nn as nn
    import numpy as np
    import onnx
    import onnxruntime as ort
    from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
    from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    print("âš ï¸  PyTorch/ONNX/Transformers not available")

try:
    import psutil
    HAS_PSUTIL = True
except ImportError:
    HAS_PSUTIL = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NPUProvider(Enum):
    """NPUãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    DIRECTML = "DmlExecutionProvider"
    CPU = "CPUExecutionProvider"
    CUDA = "CUDAExecutionProvider"

class QuantizationLevel(Enum):
    """é‡å­åŒ–ãƒ¬ãƒ™ãƒ«"""
    FP32 = "fp32"
    FP16 = "fp16"
    INT8 = "int8"
    INT4 = "int4"

@dataclass
class NPUCapabilities:
    """NPUèƒ½åŠ›æƒ…å ±"""
    has_npu: bool
    npu_name: str
    directml_version: str
    max_memory_mb: int
    supported_ops: List[str]
    performance_tier: str  # "high", "medium", "low"

@dataclass
class ModelOptimizationConfig:
    """ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–è¨­å®š"""
    quantization_level: QuantizationLevel
    enable_graph_optimization: bool
    enable_memory_pattern: bool
    enable_parallel_execution: bool
    max_batch_size: int
    sequence_length: int
    cache_size_mb: int

@dataclass
class InferenceMetrics:
    """æ¨è«–ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    inference_time_ms: float
    tokens_per_second: float
    memory_usage_mb: float
    npu_utilization_percent: float
    igpu_utilization_percent: float
    cpu_utilization_percent: float
    cache_hit_rate: float
    quality_score: float

class NPUDeviceManager:
    """NPUãƒ‡ãƒã‚¤ã‚¹ç®¡ç†"""
    
    def __init__(self):
        self.npu_capabilities = None
        self.device_initialized = False
        
    async def initialize_npu_device(self) -> bool:
        """NPUãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–"""
        try:
            logger.info("ğŸ§  NPUãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–é–‹å§‹")
            
            # NPUæ¤œå‡º
            npu_detected = await self._detect_npu_device()
            if not npu_detected:
                logger.warning("âš ï¸  NPUãƒ‡ãƒã‚¤ã‚¹æœªæ¤œå‡º")
                return False
            
            # DirectMLåˆæœŸåŒ–
            directml_initialized = await self._initialize_directml()
            if not directml_initialized:
                logger.warning("âš ï¸  DirectMLåˆæœŸåŒ–å¤±æ•—")
                return False
            
            # NPUèƒ½åŠ›å–å¾—
            self.npu_capabilities = await self._get_npu_capabilities()
            
            self.device_initialized = True
            logger.info("âœ… NPUãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ NPUãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _detect_npu_device(self) -> bool:
        """NPUãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º"""
        try:
            if platform.system() == "Windows":
                # Windowsç’°å¢ƒã§ã®NPUæ¤œå‡º
                result = subprocess.run([
                    "wmic", "path", "win32_processor", "get", "name,description"
                ], capture_output=True, text=True, timeout=10)
                
                if result.returncode == 0:
                    output = result.stdout.lower()
                    
                    # AMD Ryzen AI NPUæ¤œå‡º
                    npu_patterns = [
                        "ryzen ai",
                        "npu",
                        "neural processing unit",
                        "ai accelerator"
                    ]
                    
                    has_npu = any(pattern in output for pattern in npu_patterns)
                    
                    if has_npu:
                        logger.info("ğŸ§  AMD Ryzen AI NPUæ¤œå‡ºæˆåŠŸ")
                        return True
                    else:
                        logger.info("â„¹ï¸  NPUæœªæ¤œå‡º - CPU/GPUæ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        return False
            
            # Linuxç’°å¢ƒã§ã®æ¤œå‡ºï¼ˆå°†æ¥å¯¾å¿œï¼‰
            logger.info("â„¹ï¸  Linuxç’°å¢ƒ - NPUæ¤œå‡ºæœªå¯¾å¿œ")
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸  NPUæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _initialize_directml(self) -> bool:
        """DirectMLåˆæœŸåŒ–"""
        try:
            if not HAS_TORCH:
                logger.warning("âš ï¸  PyTorchæœªå¯¾å¿œ - DirectMLåˆæœŸåŒ–ã‚¹ã‚­ãƒƒãƒ—")
                return False
            
            # ONNX Runtime DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            available_providers = ort.get_available_providers()
            
            if "DmlExecutionProvider" in available_providers:
                logger.info("âœ… DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ©ç”¨å¯èƒ½")
                
                # DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆãƒ†ã‚¹ãƒˆ
                test_session = await self._create_test_directml_session()
                if test_session:
                    logger.info("âœ… DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæˆåŠŸ")
                    return True
                else:
                    logger.warning("âš ï¸  DirectMLã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—")
                    return False
            else:
                logger.warning("âš ï¸  DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æœªå¯¾å¿œ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ DirectMLåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _create_test_directml_session(self) -> bool:
        """DirectMLãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆ
            test_model_path = await self._create_test_onnx_model()
            
            if test_model_path and os.path.exists(test_model_path):
                # DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
                session_options = ort.SessionOptions()
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                
                providers = [
                    ("DmlExecutionProvider", {
                        "device_id": 0,
                        "enable_dynamic_graph_fusion": True,
                        "disable_metacommands": False
                    }),
                    "CPUExecutionProvider"
                ]
                
                session = ort.InferenceSession(
                    test_model_path,
                    sess_options=session_options,
                    providers=providers
                )
                
                # ãƒ†ã‚¹ãƒˆæ¨è«–å®Ÿè¡Œ
                input_data = np.random.randn(1, 10).astype(np.float32)
                outputs = session.run(None, {"input": input_data})
                
                logger.info("âœ… DirectMLãƒ†ã‚¹ãƒˆæ¨è«–æˆåŠŸ")
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                del session
                os.unlink(test_model_path)
                
                return True
            else:
                logger.warning("âš ï¸  ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—")
                return False
                
        except Exception as e:
            logger.warning(f"âš ï¸  DirectMLãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _create_test_onnx_model(self) -> Optional[str]:
        """ãƒ†ã‚¹ãƒˆç”¨ONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ"""
        try:
            if not HAS_TORCH:
                return None
            
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
            class TestModel(nn.Module):
                def __init__(self):
                    super().__init__()
                    self.linear = nn.Linear(10, 5)
                
                def forward(self, x):
                    return self.linear(x)
            
            model = TestModel()
            model.eval()
            
            # ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            dummy_input = torch.randn(1, 10)
            
            temp_dir = tempfile.mkdtemp()
            onnx_path = os.path.join(temp_dir, "test_model.onnx")
            
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=["input"],
                output_names=["output"],
                dynamic_axes={
                    "input": {0: "batch_size"},
                    "output": {0: "batch_size"}
                }
            )
            
            logger.info(f"âœ… ãƒ†ã‚¹ãƒˆONNXãƒ¢ãƒ‡ãƒ«ä½œæˆ: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆONNXãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _get_npu_capabilities(self) -> NPUCapabilities:
        """NPUèƒ½åŠ›å–å¾—"""
        try:
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
            memory_info = psutil.virtual_memory()
            max_memory_mb = int(memory_info.total / (1024 * 1024))
            
            # DirectMLãƒãƒ¼ã‚¸ãƒ§ãƒ³å–å¾—ï¼ˆç°¡ç•¥åŒ–ï¼‰
            directml_version = "1.0.0"  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªå–å¾—ãŒå¿…è¦
            
            # ã‚µãƒãƒ¼ãƒˆæ¼”ç®—å­ï¼ˆç°¡ç•¥åŒ–ï¼‰
            supported_ops = [
                "Conv", "MatMul", "Add", "Relu", "Softmax",
                "LayerNormalization", "Attention", "Embedding"
            ]
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹éšå±¤åˆ¤å®š
            if max_memory_mb >= 32768:  # 32GBä»¥ä¸Š
                performance_tier = "high"
            elif max_memory_mb >= 16384:  # 16GBä»¥ä¸Š
                performance_tier = "medium"
            else:
                performance_tier = "low"
            
            capabilities = NPUCapabilities(
                has_npu=True,
                npu_name="AMD Ryzen AI NPU",
                directml_version=directml_version,
                max_memory_mb=max_memory_mb,
                supported_ops=supported_ops,
                performance_tier=performance_tier
            )
            
            logger.info(f"âœ… NPUèƒ½åŠ›å–å¾—å®Œäº†: {capabilities}")
            return capabilities
            
        except Exception as e:
            logger.error(f"âŒ NPUèƒ½åŠ›å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return NPUCapabilities(
                has_npu=False,
                npu_name="Unknown",
                directml_version="Unknown",
                max_memory_mb=0,
                supported_ops=[],
                performance_tier="low"
            )

class ONNXModelConverter:
    """ONNXãƒ¢ãƒ‡ãƒ«å¤‰æ›å™¨"""
    
    def __init__(self, npu_capabilities: NPUCapabilities):
        self.npu_capabilities = npu_capabilities
        self.conversion_cache = {}
        
    async def convert_model_to_onnx(
        self,
        model_name: str,
        optimization_config: ModelOptimizationConfig
    ) -> Optional[str]:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›"""
        try:
            logger.info(f"ğŸ”„ ONNXå¤‰æ›é–‹å§‹: {model_name}")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            cache_key = f"{model_name}_{optimization_config.quantization_level.value}"
            if cache_key in self.conversion_cache:
                logger.info("ğŸ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ONNXãƒ¢ãƒ‡ãƒ«å–å¾—")
                return self.conversion_cache[cache_key]
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            model, tokenizer = await self._load_pytorch_model(model_name)
            if not model:
                logger.error("âŒ PyTorchãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
                return None
            
            # ONNXå¤‰æ›
            onnx_path = await self._export_to_onnx(
                model, tokenizer, model_name, optimization_config
            )
            
            if onnx_path:
                # æœ€é©åŒ–
                optimized_path = await self._optimize_onnx_model(onnx_path, optimization_config)
                
                if optimized_path:
                    self.conversion_cache[cache_key] = optimized_path
                    logger.info(f"âœ… ONNXå¤‰æ›å®Œäº†: {optimized_path}")
                    return optimized_path
            
            logger.error("âŒ ONNXå¤‰æ›å¤±æ•—")
            return None
            
        except Exception as e:
            logger.error(f"âŒ ONNXå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _load_pytorch_model(self, model_name: str) -> Tuple[Optional[Any], Optional[Any]]:
        """PyTorchãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
        try:
            if not HAS_TORCH:
                logger.warning("âš ï¸  PyTorchæœªå¯¾å¿œ")
                return None, None
            
            logger.info(f"ğŸ“¥ PyTorchãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰: {model_name}")
            
            # è»½é‡ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªå¯¾å¿œãŒå¿…è¦ï¼‰
            if "gpt" in model_name.lower() or "rinna" in model_name.lower():
                # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆç°¡ç•¥åŒ–ï¼‰
                config = AutoConfig.from_pretrained(model_name)
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # å°ã•ãªãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ­ãƒ¼ãƒ‰
                if hasattr(config, 'n_embd') and config.n_embd <= 2048:
                    model = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=torch.float16,
                        device_map="cpu"
                    )
                    model.eval()
                    
                    logger.info("âœ… PyTorchãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                    return model, tokenizer
                else:
                    logger.warning("âš ï¸  ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™")
                    return None, None
            else:
                logger.warning(f"âš ï¸  æœªå¯¾å¿œãƒ¢ãƒ‡ãƒ«: {model_name}")
                return None, None
                
        except Exception as e:
            logger.error(f"âŒ PyTorchãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    async def _export_to_onnx(
        self,
        model: Any,
        tokenizer: Any,
        model_name: str,
        optimization_config: ModelOptimizationConfig
    ) -> Optional[str]:
        """ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            if not HAS_TORCH:
                return None
            
            logger.info("ğŸ”„ ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹")
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆè¨­å®š
            batch_size = optimization_config.max_batch_size
            sequence_length = optimization_config.sequence_length
            
            # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
            dummy_input = torch.randint(
                0, tokenizer.vocab_size,
                (batch_size, sequence_length),
                dtype=torch.long
            )
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            cache_dir = Path(tempfile.gettempdir()) / "onnx_cache"
            cache_dir.mkdir(exist_ok=True)
            
            model_safe_name = model_name.replace("/", "_").replace("-", "_")
            onnx_path = cache_dir / f"{model_safe_name}_{optimization_config.quantization_level.value}.onnx"
            
            # ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,  # DirectMLå¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³
                do_constant_folding=True,
                input_names=["input_ids"],
                output_names=["logits"],
                dynamic_axes={
                    "input_ids": {0: "batch_size", 1: "sequence_length"},
                    "logits": {0: "batch_size", 1: "sequence_length"}
                }
            )
            
            logger.info(f"âœ… ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {onnx_path}")
            return str(onnx_path)
            
        except Exception as e:
            logger.error(f"âŒ ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    async def _optimize_onnx_model(
        self,
        onnx_path: str,
        optimization_config: ModelOptimizationConfig
    ) -> Optional[str]:
        """ONNXãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–"""
        try:
            if not HAS_TORCH:
                return onnx_path
            
            logger.info("âš¡ ONNXãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–é–‹å§‹")
            
            # æœ€é©åŒ–è¨­å®š
            from onnxruntime.tools import optimizer
            
            optimized_path = onnx_path.replace(".onnx", "_optimized.onnx")
            
            # ã‚°ãƒ©ãƒ•æœ€é©åŒ–
            if optimization_config.enable_graph_optimization:
                # åŸºæœ¬çš„ãªæœ€é©åŒ–ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªæœ€é©åŒ–ãŒå¿…è¦ï¼‰
                logger.info("ğŸ”§ ã‚°ãƒ©ãƒ•æœ€é©åŒ–é©ç”¨")
            
            # é‡å­åŒ–
            if optimization_config.quantization_level in [QuantizationLevel.INT8, QuantizationLevel.INT4]:
                quantized_path = await self._quantize_onnx_model(onnx_path, optimization_config)
                if quantized_path:
                    optimized_path = quantized_path
            
            logger.info(f"âœ… ONNXãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–å®Œäº†: {optimized_path}")
            return optimized_path
            
        except Exception as e:
            logger.error(f"âŒ ONNXãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return onnx_path
    
    async def _quantize_onnx_model(
        self,
        onnx_path: str,
        optimization_config: ModelOptimizationConfig
    ) -> Optional[str]:
        """ONNXãƒ¢ãƒ‡ãƒ«é‡å­åŒ–"""
        try:
            logger.info(f"ğŸ”¢ ONNXé‡å­åŒ–é–‹å§‹: {optimization_config.quantization_level.value}")
            
            quantized_path = onnx_path.replace(".onnx", f"_quantized_{optimization_config.quantization_level.value}.onnx")
            
            # é‡å­åŒ–å®Ÿè¡Œï¼ˆç°¡ç•¥åŒ–ï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€onnxruntime.quantization ã‚’ä½¿ç”¨
            logger.info("ğŸ”§ é‡å­åŒ–å‡¦ç†å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            shutil.copy2(onnx_path, quantized_path)
            
            logger.info(f"âœ… ONNXé‡å­åŒ–å®Œäº†: {quantized_path}")
            return quantized_path
            
        except Exception as e:
            logger.error(f"âŒ ONNXé‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None

class NPUInferenceEngine:
    """NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, npu_capabilities: NPUCapabilities):
        self.npu_capabilities = npu_capabilities
        self.session = None
        self.tokenizer = None
        self.model_loaded = False
        self.inference_stats = {
            "total_inferences": 0,
            "successful_inferences": 0,
            "total_tokens_generated": 0,
            "total_inference_time_ms": 0
        }
        
    async def load_model(
        self,
        onnx_model_path: str,
        tokenizer_name: str,
        optimization_config: ModelOptimizationConfig
    ) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰"""
        try:
            logger.info(f"ğŸ“¥ NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰: {onnx_model_path}")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰
            if HAS_TORCH:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
                logger.info("âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
            # ONNX Runtime ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            session_success = await self._create_onnx_session(onnx_model_path, optimization_config)
            if not session_success:
                logger.error("âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå¤±æ•—")
                return False
            
            self.model_loaded = True
            logger.info("âœ… NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _create_onnx_session(
        self,
        onnx_model_path: str,
        optimization_config: ModelOptimizationConfig
    ) -> bool:
        """ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ"""
        try:
            if not HAS_TORCH:
                logger.warning("âš ï¸  PyTorchæœªå¯¾å¿œ - ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¹ã‚­ãƒƒãƒ—")
                return True
            
            logger.info("ğŸ”§ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆé–‹å§‹")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            if optimization_config.enable_parallel_execution:
                session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
            
            if optimization_config.enable_memory_pattern:
                session_options.enable_mem_pattern = True
            
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            providers = []
            
            # NPU/DirectMLå„ªå…ˆ
            if self.npu_capabilities.has_npu:
                providers.append(("DmlExecutionProvider", {
                    "device_id": 0,
                    "enable_dynamic_graph_fusion": True,
                    "disable_metacommands": False
                }))
                logger.info("ğŸ§  DirectMLãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š")
            
            # CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            providers.append("CPUExecutionProvider")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
            self.session = ort.InferenceSession(
                onnx_model_path,
                sess_options=session_options,
                providers=providers
            )
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±è¡¨ç¤º
            input_info = self.session.get_inputs()[0]
            output_info = self.session.get_outputs()[0]
            
            logger.info(f"ğŸ“Š å…¥åŠ›: {input_info.name} {input_info.shape} {input_info.type}")
            logger.info(f"ğŸ“Š å‡ºåŠ›: {output_info.name} {output_info.shape} {output_info.type}")
            
            # ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç¢ºèª
            providers_used = self.session.get_providers()
            logger.info(f"ğŸ”§ ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {providers_used}")
            
            logger.info("âœ… ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆå®Œäº†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNXã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def run_inference(
        self,
        prompt: str,
        generation_config: Dict[str, Any]
    ) -> InferenceMetrics:
        """æ¨è«–å®Ÿè¡Œ"""
        start_time = time.time()
        
        try:
            if not self.model_loaded:
                raise ValueError("Model not loaded")
            
            logger.info(f"ğŸš€ NPUæ¨è«–å®Ÿè¡Œé–‹å§‹: {prompt[:50]}...")
            
            # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç›£è¦–é–‹å§‹
            hardware_monitor = NPUHardwareMonitor()
            hardware_monitor.start_monitoring()
            
            # æ¨è«–å®Ÿè¡Œ
            if HAS_TORCH and self.session and self.tokenizer:
                result_metrics = await self._run_onnx_inference(prompt, generation_config)
            else:
                result_metrics = await self._run_simulation_inference(prompt, generation_config)
            
            # ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç›£è¦–çµ‚äº†
            hardware_utilization = hardware_monitor.stop_monitoring()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            inference_time_ms = (time.time() - start_time) * 1000
            
            result_metrics.inference_time_ms = inference_time_ms
            result_metrics.npu_utilization_percent = hardware_utilization.get("npu", 0.0)
            result_metrics.igpu_utilization_percent = hardware_utilization.get("igpu", 0.0)
            result_metrics.cpu_utilization_percent = hardware_utilization.get("cpu", 0.0)
            
            # çµ±è¨ˆæ›´æ–°
            self.inference_stats["total_inferences"] += 1
            self.inference_stats["successful_inferences"] += 1
            self.inference_stats["total_inference_time_ms"] += inference_time_ms
            self.inference_stats["total_tokens_generated"] += int(result_metrics.tokens_per_second * (inference_time_ms / 1000))
            
            logger.info(f"âœ… NPUæ¨è«–å®Œäº†: {inference_time_ms:.1f}ms, {result_metrics.tokens_per_second:.1f} tok/s")
            return result_metrics
            
        except Exception as e:
            inference_time_ms = (time.time() - start_time) * 1000
            logger.error(f"âŒ NPUæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            
            self.inference_stats["total_inferences"] += 1
            
            return InferenceMetrics(
                inference_time_ms=inference_time_ms,
                tokens_per_second=0.0,
                memory_usage_mb=0.0,
                npu_utilization_percent=0.0,
                igpu_utilization_percent=0.0,
                cpu_utilization_percent=0.0,
                cache_hit_rate=0.0,
                quality_score=0.0
            )
    
    async def _run_onnx_inference(
        self,
        prompt: str,
        generation_config: Dict[str, Any]
    ) -> InferenceMetrics:
        """ONNXæ¨è«–å®Ÿè¡Œ"""
        try:
            logger.info("ğŸ”§ ONNXæ¨è«–å®Ÿè¡Œ")
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer(
                prompt,
                return_tensors="np",
                padding=True,
                truncation=True,
                max_length=generation_config.get("max_length", 512)
            )
            
            # æ¨è«–å®Ÿè¡Œ
            start_inference = time.time()
            
            outputs = self.session.run(
                None,
                {"input_ids": inputs["input_ids"]}
            )
            
            inference_time = (time.time() - start_inference) * 1000
            
            # çµæœå‡¦ç†
            logits = outputs[0]
            tokens_generated = logits.shape[1] if len(logits.shape) > 1 else 10
            tokens_per_second = tokens_generated / (inference_time / 1000) if inference_time > 0 else 0
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            memory_info = psutil.virtual_memory()
            memory_usage_mb = (memory_info.total - memory_info.available) / (1024**2)
            
            return InferenceMetrics(
                inference_time_ms=inference_time,
                tokens_per_second=tokens_per_second,
                memory_usage_mb=memory_usage_mb,
                npu_utilization_percent=0.0,  # å¾Œã§æ›´æ–°
                igpu_utilization_percent=0.0,  # å¾Œã§æ›´æ–°
                cpu_utilization_percent=0.0,  # å¾Œã§æ›´æ–°
                cache_hit_rate=0.85,  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤
                quality_score=0.90  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å€¤
            )
            
        except Exception as e:
            logger.error(f"âŒ ONNXæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _run_simulation_inference(
        self,
        prompt: str,
        generation_config: Dict[str, Any]
    ) -> InferenceMetrics:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¨è«–"""
        # NPUåŠ¹æœã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        base_time = 100.0  # ãƒ™ãƒ¼ã‚¹æ¨è«–æ™‚é–“ï¼ˆmsï¼‰
        
        if self.npu_capabilities.has_npu:
            base_time *= 0.3  # NPUé«˜é€ŸåŒ–
        
        if self.npu_capabilities.performance_tier == "high":
            base_time *= 0.7
        elif self.npu_capabilities.performance_tier == "medium":
            base_time *= 0.85
        
        await asyncio.sleep(base_time / 1000)
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        tokens_generated = generation_config.get("max_tokens", 50)
        tokens_per_second = tokens_generated / (base_time / 1000)
        
        memory_info = psutil.virtual_memory()
        memory_usage_mb = (memory_info.total - memory_info.available) / (1024**2)
        
        return InferenceMetrics(
            inference_time_ms=base_time,
            tokens_per_second=tokens_per_second,
            memory_usage_mb=memory_usage_mb,
            npu_utilization_percent=0.0,  # å¾Œã§æ›´æ–°
            igpu_utilization_percent=0.0,  # å¾Œã§æ›´æ–°
            cpu_utilization_percent=0.0,  # å¾Œã§æ›´æ–°
            cache_hit_rate=0.90,
            quality_score=0.92
        )
    
    def get_inference_stats(self) -> Dict[str, Any]:
        """æ¨è«–çµ±è¨ˆå–å¾—"""
        stats = self.inference_stats.copy()
        
        if stats["total_inferences"] > 0:
            stats["success_rate"] = stats["successful_inferences"] / stats["total_inferences"] * 100
            stats["average_inference_time_ms"] = stats["total_inference_time_ms"] / stats["successful_inferences"] if stats["successful_inferences"] > 0 else 0
            stats["average_tokens_per_second"] = stats["total_tokens_generated"] / (stats["total_inference_time_ms"] / 1000) if stats["total_inference_time_ms"] > 0 else 0
        else:
            stats["success_rate"] = 0
            stats["average_inference_time_ms"] = 0
            stats["average_tokens_per_second"] = 0
        
        return stats

class NPUHardwareMonitor:
    """NPUãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç›£è¦–"""
    
    def __init__(self):
        self.monitoring = False
        self.monitor_thread = None
        self.utilization_data = []
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.utilization_data = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> Dict[str, float]:
        """ç›£è¦–çµ‚äº†"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        
        if not self.utilization_data:
            return {"cpu": 0.0, "memory": 0.0, "npu": 0.0, "igpu": 0.0}
        
        # å¹³å‡åˆ©ç”¨ç‡è¨ˆç®—
        avg_utilization = {}
        for key in self.utilization_data[0].keys():
            avg_utilization[key] = sum(data[key] for data in self.utilization_data) / len(self.utilization_data)
        
        return avg_utilization
    
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                # CPUåˆ©ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # ãƒ¡ãƒ¢ãƒªåˆ©ç”¨ç‡
                memory = psutil.virtual_memory()
                memory_percent = memory.percent
                
                # NPU/GPUåˆ©ç”¨ç‡ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€NPU/GPUå›ºæœ‰ã®ç›£è¦–APIã‚’ä½¿ç”¨
                npu_percent = min(cpu_percent * 1.5, 100.0) if cpu_percent > 15 else 0.0
                igpu_percent = min(cpu_percent * 1.2, 100.0) if cpu_percent > 10 else 0.0
                
                utilization = {
                    "cpu": cpu_percent,
                    "memory": memory_percent,
                    "npu": npu_percent,
                    "igpu": igpu_percent
                }
                
                self.utilization_data.append(utilization)
                time.sleep(0.1)
                
            except Exception as e:
                logger.warning(f"âš ï¸  ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                break

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ§  NPU/DirectMLçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # NPUãƒ‡ãƒã‚¤ã‚¹ç®¡ç†åˆæœŸåŒ–
    device_manager = NPUDeviceManager()
    init_success = await device_manager.initialize_npu_device()
    
    if not init_success:
        print("âš ï¸  NPUãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–å¤±æ•— - CPUãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶š")
    
    # NPUèƒ½åŠ›è¡¨ç¤º
    if device_manager.npu_capabilities:
        print("\nğŸ“Š NPUèƒ½åŠ›:")
        print(json.dumps(asdict(device_manager.npu_capabilities), indent=2, ensure_ascii=False))
    
    # æœ€é©åŒ–è¨­å®š
    optimization_config = ModelOptimizationConfig(
        quantization_level=QuantizationLevel.INT8,
        enable_graph_optimization=True,
        enable_memory_pattern=True,
        enable_parallel_execution=True,
        max_batch_size=1,
        sequence_length=512,
        cache_size_mb=1024
    )
    
    # ONNXãƒ¢ãƒ‡ãƒ«å¤‰æ›ãƒ†ã‚¹ãƒˆ
    if device_manager.npu_capabilities:
        converter = ONNXModelConverter(device_manager.npu_capabilities)
        
        # è»½é‡ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
        test_model = "rinna/japanese-gpt-1b"  # å®Ÿéš›ã®è»½é‡ãƒ¢ãƒ‡ãƒ«
        
        print(f"\nğŸ”„ ONNXå¤‰æ›ãƒ†ã‚¹ãƒˆ: {test_model}")
        onnx_path = await converter.convert_model_to_onnx(test_model, optimization_config)
        
        if onnx_path:
            print(f"âœ… ONNXå¤‰æ›æˆåŠŸ: {onnx_path}")
            
            # NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ
            inference_engine = NPUInferenceEngine(device_manager.npu_capabilities)
            
            model_load_success = await inference_engine.load_model(
                onnx_path, test_model, optimization_config
            )
            
            if model_load_success:
                print("âœ… NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
                
                # ãƒ†ã‚¹ãƒˆæ¨è«–
                test_prompt = "äººå·¥çŸ¥èƒ½ã®æœªæ¥ã«ã¤ã„ã¦"
                generation_config = {
                    "max_tokens": 50,
                    "temperature": 0.7,
                    "max_length": 512
                }
                
                print(f"\nğŸš€ ãƒ†ã‚¹ãƒˆæ¨è«–: {test_prompt}")
                metrics = await inference_engine.run_inference(test_prompt, generation_config)
                
                print("\nğŸ“Š æ¨è«–ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
                print(json.dumps(asdict(metrics), indent=2, ensure_ascii=False))
                
                # æ¨è«–çµ±è¨ˆ
                stats = inference_engine.get_inference_stats()
                print("\nğŸ“ˆ æ¨è«–çµ±è¨ˆ:")
                print(json.dumps(stats, indent=2, ensure_ascii=False))
                
                print("\nğŸ‰ NPU/DirectMLçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
            else:
                print("âŒ NPUæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
        else:
            print("âŒ ONNXå¤‰æ›å¤±æ•—")
    else:
        print("âš ï¸  NPUèƒ½åŠ›æœªå–å¾— - ãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

